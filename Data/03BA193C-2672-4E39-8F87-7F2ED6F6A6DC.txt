æˆ‘å€‘æå‡ºä¸€å€‹å…¨åŸŸæœ€ä½³åŒ–çš„æ–¹å¼ä¾†æå‡é—œè¯çµæœã€‚åœ¨ç”¢ç”Ÿç‰©
é«”ç‰¹å¾µæ™‚ï¼Œç”±æ–¼åœ¨çœŸå¯¦ç’°å¢ƒåº•ä¸‹ï¼Œæ²’æœ‰ä¸€å€‹å®Œç¾çš„ç‰¹å¾µæ“·å–
æ–¹æ³•ï¼Œç„¡è«–ä½•ç¨®æ–¹å¼éƒ½æœƒæœ‰ä¸€å®šç¨‹åº¦çš„é›œè¨Šå¹²æ“¾ï¼Œè‹¥ç”¨å‚³çµ±
çš„æœ€è¿‘é„°å±…æˆ–æ˜¯è²ªå©ªæ³•ï¼Œé›–ç„¶å¯ä»¥ç¢ºå®šè©²é—œè¯çµæœè‡ªå·±æœ‰æœ€
å¤§å¯èƒ½æ€§ï¼Œä½†æ˜¯ä¸èƒ½ä¿è­‰å¾å…¨åŸŸçš„è§€é»ï¼Œæ‰€æœ‰çš„é—œè¯æ˜¯æœ€ä½³
çš„å¯èƒ½æ€§ã€‚æˆ‘å€‘æå‡ºçš„æœ€ä½³åŒ–æ–¹å¼ï¼Œæ˜¯åˆ©ç”¨æœ€å°æ³¥åœŸæ¬é‹ä»£
åƒ¹(earth moverâ€™s distance)çš„æ¦‚å¿µï¼Œé‡å°æ¯ä¸€å€‹å¯èƒ½çš„é…
å°ï¼Œå¾æ‰€æœ‰çš„çµ„åˆæ‰¾å‡ºæœ€ä½³è§£ã€‚è©²æ–¹æ³•é”åˆ°é—œè¯å…¨åŸŸæœ€ä½³
åŒ–ï¼Œä¸”å®ƒæ˜¯åœ¨ä¸€èˆ¬è·é›¢è¨ˆç®—çš„ä¸Šä¸€å€‹å±¤ç´šï¼Œæ‰€ä»¥å®ƒå¯ä»¥è¢«å„
ç¨®ä¸åŒçš„ç‰©é«”ç‰¹å¾µå’Œè·é›¢è¨ˆç®—æ–¹å¼æ‰€æ¡ç”¨ã€‚å¯¦é©—çµæœèˆ‡å‚³çµ±
çš„æœ€è¿‘é„°å±…æˆ–æ˜¯è²ªå©ªæ³•ç›¸æ¯”ï¼Œæ•´é«”æ­£ç¢ºç‡å¹³å‡æé«˜ 10.5%ã€‚
è€Œä¸”æœ¬æ–¹æ³•è¨ˆç®—é‡æ¥µä½ï¼Œåœ¨æ•´å€‹ç³»çµ±ä¸­æ˜¯å¯è¢«å¿½ç•¥ï¼Œä¾‹å¦‚ç•¶
ä¸åŒç•«é¢ä¸­å…±æœ‰çš„ç‰©é«”æ•¸é‡å¹³å‡ç‚º 26 å€‹æ™‚ï¼Œä»¥ 3GHz çš„ä¸­å¤®
è™•ç†å™¨ä¾†æ¨¡æ“¬ï¼Œè¨ˆç®—é€Ÿåº¦å¯é”æ¯ç§’è™•ç† 3381 å¼µç•«é¢ã€‚ 
æœ¬è«–æ–‡çš„ç¬¬äºŒéƒ¨ä»½åŒ…å«åˆ©ç”¨éåƒæ•¸ã€åƒæ•¸åŒ–æ–¹æ³•ä¾†å¯¦ç¾ç„¡æ§
åˆ¶å™¨çš„éŠæˆ²å¹³å°ï¼Œå’Œç”¨çŸ¥è­˜ç‚ºåŸºç¤çš„éºæ£„è¡Œæçš„åµæ¸¬ç³»çµ±ã€‚
æˆ‘å€‘éåƒæ•¸çš„æ–¹æ³•é¸ç”¨ä»¥ç£ç£šå’Œç§»å‹•å‘é‡ç‚ºåŸºç¤çš„ç‰¹å¾µï¼Œä¾†
è®“ä½¿ç”¨è€…ç”¨å…¨èº«çš„å‹•ä½œå»æ¨¡æ“¬æ’çƒå’Œè¶³çƒå®ˆé–€å“¡çš„å‹•ä½œï¼Œé€²
è€Œæ§åˆ¶éŠæˆ²ä¸­çš„è§’è‰²ã€‚é¸ç”¨ç£ç£šå’Œç§»å‹•å‘é‡çš„åŸå› ä¸»è¦æ˜¯é€™
å€‹ç‰¹æ€§è¢«å¤§é‡è¦–è¨Šå£“ç¸®æŠ€è¡“æ‰€æ¡ç”¨ï¼Œä¹Ÿå°±æ˜¯ç•¶ä¸€å€‹æ”å½±æ©Ÿç³»
çµ±æ‹å¾—ç•«é¢ä¹‹å¾Œå°±æœƒå–å¾—çš„è³‡è¨Šã€‚è€Œç£ç£šåŒ–çš„æ¦‚å¿µå°‡åœ°å€æ€§
çš„ç‰¹å¾µçµ¦è¡¨ç¾å‡ºä¾†ï¼Œå³æ¥è¿‘æ¨¡æ“¬äººé«”å››è‚¢åœ¨ä¸åŒçš„ä½ç½®æ‰€å‘ˆ
ç¾çš„ç‰¹å¾µã€‚æœ¬æ–¹æ³•èˆ‡å‰äººçš„æ™‚ç©ºæ¨£æ¿æ³•ç›¸æ¯”è¼ƒï¼Œå¹³å‡ä¾†èªªæ•ˆ
æœå¥½ 13%ã€‚æ™‚é–“åºåˆ—åƒæ•¸æ³•æˆ‘å€‘æ¡ç”¨å¾ç²’å­æ¿¾æ³¢å™¨æ‰€å–å¾—ä¹‹
äººé«”é—œç¯€çš„é‹å‹•è»Œè·¡ä¾†ç•¶åšå‹•ä½œæè¿°å­ã€‚æ¯æ¢è»Œè·¡å…ˆè¢«è½‰æ›
æˆç¬¦è™Ÿåºåˆ—ï¼Œç„¶å¾Œæ¡ç”¨å…©ç¨®è»Œè·¡è½‰æ›ç¬¦è™Ÿæ–¹æ³•å’Œå…©ç¨®è¡¨ç¤ºæ–¹
å¼ä¾†é€²è¡Œå¯¦é©—ã€‚æœ€å¾Œçµæœé¡¯ç¤ºä»¥å›ºå®šæ™‚é–“é•·åº¦çš„ç¬¦è™Ÿå’Œç¬¦è™Ÿ
çµ±è¨ˆç›´æ–¹åœ–çš„çµ„åˆï¼Œå°æ–¼é€™äº›é‹å‹•é¡ï¼ŒçŸ­å‹•ä½œçš„æ•ˆæœæœ€å¥½ï¼Œ
æ­£ç¢ºç‡å¹³å‡é” 96.7%ï¼Œæœ¬çµæœäº¦æ¯”å‰é¢çš„éåƒæ•¸æ³•å¥½ 7%ã€‚ 
éºæ£„è¡Œææœ¬èº«ä»£è¡¨ä¸€å€‹æ½›åœ¨çš„å…¬å…±å®‰å…¨å±æ©Ÿï¼Œå°¤å…¶æ˜¯ç‚¸å½ˆå¼
çš„æ”»æ“Šã€‚è¦è¾¨è­˜å‡ºå“ªäº›æ˜¯è¡Œæï¼Œå“ªäº›æ˜¯æ“æœ‰äººï¼Œç¢ºèªæ˜¯å¦æœ‰
è¡Œæè¢«éºæ£„ï¼Œæ˜¯éºæ£„è¡Œæçš„ä¸‰å€‹ä¸»è¦å•é¡Œã€‚ç„¶è€Œï¼Œå¤§å¤šæ•¸è§£
æ±ºé€™é¡æ–¹æ³•éƒ½å°‡å®ƒè¡¨ç¤ºæˆç‰©é«”è¿½ç¸±çš„å•é¡Œï¼Œè€Œå»è¿½ç•«é¢ç•¶ä¸­
æ‰€æœ‰çš„å‰æ™¯ç‰©é«”ï¼Œé€™é€ æˆç¾å¯¦å³æ™‚æ‡‰ç”¨ä¸Šæœ‰è‘—ç›¸ç•¶å¤§çš„å›°
é›£ã€‚æˆ‘å€‘æå‡ºå€åŸŸåŒ–é¸æ“‡æ€§è¿½ç¸±çš„æ¦‚å¿µï¼Œé¦–å…ˆåˆ©ç”¨äº¤é›†å‰æ™¯
å–æ¨£çš„çµæœä¾†åˆ¤æ–·å“ªè£¡å¯èƒ½æ˜¯éœç½®ç‰©é«”ï¼Œç„¶å¾Œé¸æ“‡æ€§è¿½ç¸±è©²
ç‰©é«”å‘¨åœæœ€è¿‘çš„äººé¡ã€‚ç”±æ–¼åœ¨ç›£æ§ç’°å¢ƒæ”å½±æ©Ÿæ˜¯ä¿¯è§’æ‹æ”ï¼Œ
äººé ­å’Œè‚©è¢«å…¶ä»–ç‰©é«”é®è”½çš„å¯èƒ½æ€§æœ€ä½ï¼Œæˆ‘å€‘é¸æ“‡é ­è‚©è¼ªå»“
çš„ç‰¹å¾µä¾†åˆ¤æ–·åŠè¿½ç¸±æ“æœ‰äººã€‚æœ€å¾Œçµæœåœ¨å…¬å…±çš„æ¸¬è©¦è³‡æ–™ä¸Š
å‡èƒ½åµæ¸¬å‡ºè¡Œæéºæ£„çš„äº‹ä»¶ï¼Œå’Œæ¨™æº–è­¦å ±æ™‚é–“çš„å·®ç•°ç´„åœ¨-
2.36~+6.8 ç§’ä¹‹é–“ï¼Œæ¡çµ•å°å€¼ä¹‹å¾Œï¼Œå¹³å‡å·®ç•°ç‚º 2.7 ç§’ã€‚æœ¬
è«–æ–‡æ•´åˆä¸Šè¿°æ‰€æœ‰ç”¨åœ¨è¡Œç‚ºè¾¨è­˜ç³»çµ±ä¸Šçš„æ ¸å¿ƒæ¨¡çµ„ï¼Œæ ¹æ“šä»–
--- trajectory, and the second part considers the 
model formation of these scenarios. 
è‹±æ–‡é—œéµè©ï¼š Human action recognition, Human activity recognition, 
Feature extraction, Architecture 
 
I 
 
 
è¨ˆç•«åç¨±:  
æ™ºæ…§å‹å¤šæ”å½±æ©Ÿç›£æ§ç³»çµ±åŠæ¶æ§‹ç ”ç©¶ 
 
 
ä»¥è¦–è¦ºç‚ºåŸºç¤ä¹‹äººï§å‹•ä½œè¾¨ï§¼çš„ 
æ¼”ç®—æ³•åŠæ¶æ§‹åˆ†æ 
 
Algorithm and Architecture Analysis of Video-based 
Human Action and Activity Recognition 
 
 
 
 
 
 
 
 
 
III 
 
é—œç¯€çš„é‹å‹•è»Œè·¡ï¤­ç•¶åšå‹•ä½œæè¿°å­ã€‚æ¯æ¢è»Œè·¡å…ˆè¢«è½‰æ›æˆç¬¦è™Ÿåºï¦œï¼Œç„¶å¾Œæ¡ç”¨ï¥¸ç¨®è»Œè·¡è½‰æ›ç¬¦è™Ÿæ–¹æ³•
å’Œï¥¸ç¨®è¡¨ç¤ºæ–¹å¼ï¤­é€²ï¨ˆå¯¦é©—ã€‚æœ€å¾Œçµæœé¡¯ç¤ºä»¥å›ºå®šæ™‚é–“é•·ï¨çš„ç¬¦è™Ÿå’Œç¬¦è™Ÿçµ±è¨ˆç›´æ–¹åœ–çš„çµ„åˆï¼Œå°æ–¼é€™
äº›é‹å‹•ï§ï¼ŒçŸ­å‹•ä½œçš„æ•ˆæœæœ€å¥½ï¼Œæ­£ç¢ºï¥¡å¹³å‡é” 96.7%ï¼Œæœ¬çµæœäº¦æ¯”å‰é¢çš„éï¥«ï¥©æ³•å¥½ 7%ã€‚ 
éºæ£„ï¨ˆï§¡æœ¬èº«ä»£è¡¨ä¸€å€‹æ½›åœ¨çš„å…¬å…±å®‰å…¨å±æ©Ÿï¼Œå°¤å…¶æ˜¯ç‚¸å½ˆå¼çš„æ”»æ“Šã€‚è¦è¾¨ï§¼å‡ºå“ªäº›æ˜¯ï¨ˆï§¡ï¼Œå“ªäº›
æ˜¯æ“æœ‰äººï¼Œç¢ºèªæ˜¯å¦æœ‰ï¨ˆï§¡è¢«éºæ£„ï¼Œæ˜¯éºæ£„ï¨ˆï§¡çš„ä¸‰å€‹ä¸»è¦å•é¡Œã€‚ç„¶è€Œï¼Œå¤§å¤šï¥©è§£æ±ºé€™ï§æ–¹æ³•ï¨¦å°‡å®ƒ
è¡¨ç¤ºæˆç‰©é«”è¿½ç¸±çš„å•é¡Œï¼Œè€Œå»è¿½ç•«é¢ç•¶ä¸­æ‰€æœ‰çš„å‰æ™¯ç‰©é«”ï¼Œé€™é€ æˆç¾å¯¦å³æ™‚æ‡‰ç”¨ä¸Šæœ‰è‘—ç›¸ç•¶å¤§çš„å›°é›£ã€‚
æˆ‘å€‘æå‡ºå€åŸŸåŒ–é¸æ“‡æ€§è¿½ç¸±çš„æ¦‚ï¦£ï¼Œé¦–å…ˆï§ç”¨äº¤é›†å‰æ™¯å–æ¨£çš„çµæœï¤­åˆ¤æ–·å“ªï§¨å¯èƒ½æ˜¯éœç½®ç‰©é«”ï¼Œç„¶å¾Œ
é¸æ“‡æ€§è¿½ç¸±è©²ç‰©é«”å‘¨åœæœ€è¿‘çš„äººï§ã€‚ç”±æ–¼åœ¨ç›£æ§ç’°å¢ƒæ”å½±æ©Ÿæ˜¯ä¿¯è§’æ‹æ”ï¼Œäººé ­å’Œè‚©è¢«å…¶ä»–ç‰©é«”é®è”½çš„
å¯èƒ½æ€§æœ€ä½ï¼Œæˆ‘å€‘é¸æ“‡é ­è‚©ï§—ï¨‹çš„ç‰¹å¾µï¤­åˆ¤æ–·åŠè¿½ç¸±æ“æœ‰äººã€‚æœ€å¾Œçµæœåœ¨å…¬å…±çš„æ¸¬è©¦è³‡ï¦¾ä¸Šå‡èƒ½åµæ¸¬
å‡ºï¨ˆï§¡éºæ£„çš„äº‹ä»¶ï¼Œå’Œæ¨™æº–è­¦å ±æ™‚é–“çš„å·®ï¥¢ç´„åœ¨-2.36~+6.8 ç§’ä¹‹é–“ï¼Œæ¡çµ•å°å€¼ä¹‹å¾Œï¼Œå¹³å‡å·®ï¥¢ç‚º 2.7 
ç§’ã€‚æœ¬ï¥æ–‡æ•´åˆä¸Šè¿°æ‰€æœ‰ç”¨åœ¨ï¨ˆç‚ºè¾¨ï§¼ç³»çµ±ä¸Šçš„æ ¸å¿ƒæ¨¡çµ„ï¼Œæ ¹æ“šä»–å€‘çš„é‹ç®—ç‰¹æ€§åŠæ€§è³ªï¼Œè¨­è¨ˆé«˜æ•ˆï¥¡
çš„ç¡¬é«”æ¶æ§‹å’Œé«˜æº–ç¢ºï¨çš„æ¼”ç®—æ³•ã€‚å¯ä¾›å¾ŒçºŒç ”ç©¶è€…ï¥«è€ƒä¸¦å»¶ä¼¸å…¶æ€§èƒ½å’Œæ‡‰ç”¨ã€‚ 
 
é—œéµå­—â€”äººé«”å‹•ä½œè¾¨ï§¼ï¼Œäººé«”æ´»å‹•è¾¨ï§¼ï¼Œç‰¹å¾µæ“·å–ï¼Œç¡¬é«”æ¶æ§‹ 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
V 
 
higher than 87%. 
The third module is an object corresponding method. This work proposes a global-optimization approach 
of spatial object correspondence in distributed surveillance systems. All object correspondence systems need 
to be calibrated. However, since the environment is variable and differences exist between cameras, camera 
calibration may be imprecise and the results of the examination of the similarity between individuals may be 
incorrect. The concept of earth mover's distance (EMD) is employed in an environment under imprecise 
feature measurement. The approach solves the problem of mutually exclusive object correspondence; finds the 
global optimum; allows partial matches, and is able to be used in combination with others' approach of feature 
measurement. Global optimization is achieved by exploring all mutually exclusive match candidates and 
choosing those that generate the global minimum cost value. Applying EMD with a geometry-based feature to 
public surveillance datasets, the precision of the EMD-based method exceeds that of the greedy-based method 
by 4.3% to 20.8%, with an average of 10.5%. 
In the second part, a nonparametric approach and a parametric approach are adopted for controller-free 
gaming applications, and a knowledge-based approach is proposed for abandoned luggage detection systems. 
The nonparametric approach is a tile-based, motion-vector-based approach to provide a function, which 
allows people for using their whole body parts to mimic the real Volleyball/GoalKeeper actions to control the 
role in the game. The idea of introducing motion vector pattern to action recognition is based on the fact that 
video compression is now a common function of camera systems, which is an abundant source of motion 
vectors in the video. Motion vectors represent the dynamics information of an environment. By utilizing 
motion vectors in the region of an object, these dynamics information can be used to analyze actions of the 
object. The performance is compared with that of temporal-template-based approach. Because feature vectors 
of the temporal-template-based approach are generated by the entire foreground of one people, no regional 
information of the foreground is gathered. Therefore, the proposed motion-vector-based approach outperforms 
the temporal-template-based approach. Another approach for controller-free gaming applications is a 
parametric time-series approach using joint trajectories extracted by particle filter as the action descriptors. 
Each trajectory is converted into a symbol sequence. The action recognition is accomplished using all 
combinations of two distance measuring methods and two dictionaries completed by fixed-size or 
adaptive-size segments. 
Abandoned luggage represents a potential threat to public safety. Identifying objects as luggage, 
identifying the owners of such objects, and identifying whether owners have left luggage behind, are the three 
main problems requiring solution. However, in crowded areas, solutions based on identifying what all objects 
are and tracking all objects, based on the possibility of their being abandoned luggage, are computationally 
extremely costly. Accordingly, such methods are difficult to utilize in real-time applications. The 
knowledge-based approach uses two techniques for effectively detecting abandoned luggage. 
"Foreground-mask sampling'' detects luggage with arbitrary appearance and â€œselective tracking'' locates and 
tracks owners based solely on looking only at the neighborhood of the luggage. A probability model using the 
maximum a posteriori is adopted to generate a confidence score and determine whether luggage has been 
abandoned deliberately. Experimental results demonstrate that once an owner abandons their luggage and 
leaves the scene, the alarm fires within few seconds. The processing speed of the proposed approach is 
approximately 15 to 20 frames per second, which is sufficient for real world applications.  
Keywords-- Human action recognition, Human activity recognition, Feature extraction, Architecture 
3.3.2 Particle Filter Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.4 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.4.1 Evaluation of Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.4.2 Evaluation of Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
4 Multiple Camera Tracking 46
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.1.1 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.2 Earth moverâ€™s distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.3 Spatial object correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
4.3.1 Homographic location projection . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.3.2 Remove outlier objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.3.3 EMD-based object correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.3.4 Greedy-based object correspondence . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.4 Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.5 Summery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
II Human Action Recognition 60
5 Nonparametric Action Recognition 61
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
5.2 Motion-Vector-Based Action Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
5.3 Temporal-Template-Based Action Recognition . . . . . . . . . . . . . . . . . . . . . . . . 63
5.4 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
5.5 Summery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
6 Parametric Action Recognition 68
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
6.2 Joint-Trajectory-Based Action Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . 68
6.2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
6.2.2 Segment Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6.2.3 Training Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.2.4 Matching Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.3 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
6.3.1 Testing Similar Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
6.3.2 Testing All Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
6.4 Summery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
7 Knowledge-based Action Recognition 76
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
7.1.1 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
7.1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
7.2 Foreground Mask Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
7.3 Selective Tracking Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
7.3.1 Cr Color Channel with Human Skin . . . . . . . . . . . . . . . . . . . . . . . . . . 82
2
List of Figures
1.1 E. J. Marey and E. Muybridge photographed moving subjects including humans and animals. 9
1.2 Point light displays attached to human body. . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.3 The semantic gap between low level features and high level features. . . . . . . . . . . . . 9
1.4 System architecture of video-based human action and activity recognition. . . . . . . . . 10
1.5 Example of 3D video volume. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.6 Example of the optical ï¬‚ow feature. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.7 Example of the optical ï¬‚ow feature. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.8 Example of foreground silhouette after background subtraction. . . . . . . . . . . . . . . 14
1.9 Example of spatio-temporal ï¬lter showing results of detecting spatio-temporal interest
points from the motion of the legs of a walking person. . . . . . . . . . . . . . . . . . . . 15
2.1 Two images have the same traditional histogram, but right one has much more gray com-
ponents in CSD description. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.2 CSD extraction ï¬‚ow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.3 Block diagram of CSD architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.4 Pixel scan order of three structuring windows. . . . . . . . . . . . . . . . . . . . . . . . . 22
2.5 Structuring window histogram updating architecture. . . . . . . . . . . . . . . . . . . . . 23
2.6 Folding skill on non-linear quantization. . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.7 CSD Chip layout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.1 Flow of dynamics modeling approach for human action recognition. The â€œtrackingâ€ module
tracks joints and generates trajectories. The â€œjoint trajectory modelingâ€ module extracts
speciï¬c features from trajectories. Finally, the â€œclassiï¬cation engineâ€ decides to which
category of actions each trajectory belongs by measuring the distance between extracted
features and features of all action models. . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.2 Sampling-importance-resampling particle ï¬lter. . . . . . . . . . . . . . . . . . . . . . . . 28
3.3 Processing ï¬‚ow of particle ï¬lter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.4 Comparison between runtimes of tracking phase and trajectory modeling phase. The
tracking phase is the computational bottleneck of the dynamic modeling approach. . . . . 34
3.5 Runtime proï¬ling result. â€œHistogram accumulationâ€ dominates the runtime of the particle
ï¬lter algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.6 Proposed four-parallel particle observation architecture. . . . . . . . . . . . . . . . . . . . 37
3.7 CAM architecture. â€œVirtual addressâ€ is deï¬ned as the original accumulation address.
â€œReal addressâ€ is deï¬ned as the actual address used to access the reduced-sized memory. 39
3.8 Hardware architecture scheduling using the ï¬rst class of parallel processing. The x-axis
represents time, and the y-axis represents hardware resources. â€œSW Gen.â€ stands for
spatial weight generation. â€œL1 Dist. Gen.â€ stands for L1 distance generation. . . . . . . . 40
4
5.4 Classiï¬cation results of the motion-vector-based approach. . . . . . . . . . . . . . . . . . 65
6.1 The trajectory modeling steps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6.2 Example of smoothing trajectory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6.3 Illustrations of ï¬xed-size and adaptive-size segments. . . . . . . . . . . . . . . . . . . . . 70
6.4 The feature extraction and distance measuring steps of the parametric method. . . . . . . 71
6.5 Confusion matrix of all games in game â€œVBâ€. . . . . . . . . . . . . . . . . . . . . . . . . 74
7.1 System work ï¬‚ow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
7.2 AVSS 2007 video dataset. Images captured via a typical surveillance camera are looking
down, causing the lower part of objects to appear larger and the upper part to appear
smaller. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
7.3 Foreground-mask sampling. The ï¬rst row shows input frames while the second row shows
corresponding foreground images. After obtaining the intersection result, two ï¬lters are
applied to remove certain static regions which are not the luggage items. One ï¬lter is
the human classiï¬er introduced in section 7.3, while the other removes unreasonably large
regions. Image on the right represents the intersection of the six foreground images sampled
over 30 seconds, and was obtained after removing non-luggage regions. . . . . . . . . . . 81
7.4 Left: input video frame with localized search region indicated by red circle. Right: the Cr
detection result within the search region. . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
7.5 Head-shoulder contour under HT. In template generation, the relative position of (xC , yC)
to (x, y) is recorded in (r, Î±); in contour matching, the pixel value on the assumed position
of (xC , yC) is incremented by 1 on the detection map. . . . . . . . . . . . . . . . . . . . . 83
7.6 The origin of the two red lines has a Ïˆ angle with degree m, corresponding to the red,
m-th, bin which contains two (r, Î±) pairs. Solid lines denote correct matches, while dashed
lines represent noise. Diï¬€erent points on the head-shoulder contour in the edge image
converge to a local maximum on the detection map on the right. At the bottom is the
180-bin reference table; a Gaussian weighting g is shown below the table, with the center
bin labeled in red and the neighboring ten bins in blue. . . . . . . . . . . . . . . . . . . . 84
7.7 Upper row: (left) input video frame; (middle) the input edge image obtained via 3 Ã— 3
sobel convolution; (right) input frame with HT detection map superimposed on it using the
improved implementation method. Lower row: (left) simple implementation of normalized
correlation between the input edge image and contour template, which yields the most false
positives, as expected. (middle) detection result with original implementation of HT, which
gives more false positives; (right) HT detection result with the improved implementation
method, which yields a single large response at the correct location of the most visible
head-shoulder contour in the input edge image. . . . . . . . . . . . . . . . . . . . . . . . 85
7.8 Sequence 1, 3 and 4: (from left) static luggage detected; owner tracking starts; owner
leaves the scene, alarm triggered. Sequence 2: static luggage detected; owner tracking
starts; owner lost due to occlusion, alarm triggered. Sequence 1 and 2 are from AVSS 2007
dataset. Sequence 3 and 4 are from PETS 2006 dataset. . . . . . . . . . . . . . . . . . . 89
6
Chapter 1
Introduction
In recent years, video-based human action recognition technology provides important applications of com-
puter vision, such as multimedia entertainment, surveillance systems, interactive environments, content-
based video analysis, and behavioral biometrics. These potential applications attract many researchers
from academia and industry to explore the essence of recognizing human actions. The earliest attempt
to explore the nature of human motion was carried out by E. J. Marey and E. Muybridge in the 19th
century. They photographed moving subjects and showed several aspects involved in human and animal
locomotion. Figure 1.1 shows the sample images of their work. In psychology, Johansson studied the
human perception of movement by experimenting with moving light displays (MLD) attached to human
body parts [1]. Figure 1.2 shows an MLD example in [2]. He discovered that people can recognize diï¬€erent
actions with only these moving dots without full structural information [3].
The major challenges of action recognition algorithms and systems lie in the semantic gaps between
human cognition systems and speciï¬c computation models, see ï¬g. 1.3. Eï¬€ective methods to model the
representation mathematically is a key to achieve intelligence of computers. Computer vision approaches
are ï¬rst adopted to cope with these challenges. The other approaches are machine learning and pattern
matching. Speciï¬cally, data mining and information retrieval techniques use the machine learning meth-
ods to identify interesting events given an abundant data or information. To combine the two approaches,
good modeling techniques are need to be discovered, which often involve feature selection and feature
matching.
Before discussing related works of video-based human action and activity recognition, the problem is
deï¬ned as follow: Given video clips with an action is performed by a person or an activity performed
by several persons, how to design a system that can recognize what has been performed in these clips.
Is there a methodology to solve the problem eï¬€ectively? Conceptually, actions and activities has slight
diï¬€erence by their literal meaning. That is, activities has higher semantic meaning then actions. An
activity may results from multiple actions. For example, the activity of â€˜graï¬ƒtiâ€™ may results from actions
â€˜walking toward the wallâ€™, â€˜preparing the paintsâ€™, and â€˜drawingâ€™. Or, the activity of â€˜left luggageâ€™ may
results from actions â€˜walkingâ€™, â€˜not movingâ€™, â€˜left bagâ€™, and â€˜abandoned bagâ€™. Several works have appeared
over the years. Aggarwal and Cai [4] addressed three problems in a complete action recognition system
â€” feature extraction of human body from images, tracking across frames, and action recognition. Cedras
and Shah [5] presented a survey on motion-based approaches and claimed that motion is an important
cue for action recognition than the structure of the human body. Gavrila [3] presented a survey focused
mainly on tracking of hands and humans via 2D or 3D models, and a discussion of action recognition
techniques. From these works, the general human action recognition systems can be integrated into
8

	

%

	

&	
 	
%
#'(	
 "
)	
*+
,	
	

-	
	

	

 	

.
%	/	
%	'+	/	0	.
Figure 1.4: System architecture of video-based human action and activity recognition.
the system architecture shown in Fig. 1.4. Basically, each functional block outputs speciï¬c information
and each output information can be adopted solely for one method of human action recognition. The
categories of human action recognition methods will be presented in the next section. Accordingly, the
thesis will include the related core processing modules for extracting useful features for human action
recognition and the several recognition methods adopting diï¬€erent features.
1.1 Categories of Human Action/Activity Recognition
Behavior analysis can be classiï¬ed into two classes â€” â€œactionâ€ and â€œactivityâ€ [6]. â€œActionsâ€ mean simple
motion patterns usually executed by a single person and typically lasting for short durations of time, on the
order of a few seconds. The applications of â€œaction recognitionâ€ includes human-computer interfaces, such
as game controllers and the interaction with appliances in intelligent buildings. â€œActivitiesâ€ means the
complex sequence of actions performed by several humans and objects that can interact with each other
for longer durations. Because of the deï¬nition of â€œactivitiesâ€, the applications of â€œactivity recognitionâ€
mainly focus on surveillance systems and team sport analysis.
Approaches to automatically analyzing â€œactionsâ€ can be classiï¬ed into three major classes â€” non-
parametric, volumetric, and parametric time-series approaches. Nonparametric methods extract features
from each frame of the video. The features are then matched to a stored template. Volumetric approaches
consider a video as an image-time volume of pixel intensities and extend standard image features such
as spatial ï¬lter responses to the 3D volume. Parametric time-series approaches build a model on the
temporal dynamics of the motion. The parameters of actions are then estimated from training data.
Examples of parametric approaches include hidden Markov models (HMMs) and linear dynamical sys-
tems (LDSs). On the other hand, â€œactivity recognitionâ€ often adopts approaches of describing relations
between conditions and events, or relies on formal logical rules of domain knowledge to detect activities.
In Part II, a nonparametric and a parametric approaches are proposed for action gaming systems, based
on the features of motion vectors and trajectories accordingly. A knowledge-based approach to detect
abandoned luggage for public security is proposed in Chapter 7. The following subsections collect several
10
Figure 1.5: Example of 3D video volume.
two paths in brain for visual processing [13]. Their system is divided into several stages. The input
video is ï¬rst matched by spatio-temporal ï¬lters. Later, in each stage, a ï¬lter (optical ï¬‚ow, gradient
or orientation) is applied and responses at local maxima points are captured, followed by a template
matching stage. The ï¬nal classiï¬cation uses multi-class SVM as the classiï¬er.
1.1.2 Parametric Methods
Time-Series-Model-Based Approaches
Temporal correspondence is a critical modeling step in human action analysis. Several models such
as Bayesian network, hidden Markov Models (HMM), or Markov networks, can be adopted to build
the temporal correspondence. HMM has been used to model a sequence of symbols and the temporal
relationship in the ï¬eld of speech and audio related researches. The concept can also be used to model
human action by properly ï¬tting features to symbols. Bregler tackled a human dynamics recognition
problem using a probabilistic compositional framework with HMM [14]. He proposed a multilayered
approach. The ï¬rst level is a sequence of input images. The next level consists of blob hypotheses where
each blob is a region of coherent motion. At the third level, blob tracks are grouped temporally. The ï¬nal
level consists of an HMM for representing the complex behavior. Recently, Markov-random-ï¬eld-based
and conditional-random-ï¬eld-based methods have also been proposed. Natarajan and Nevatia combined
the idea of event level and template level [15]. The template level consists of edge templates, and the
event level are modeled by a conditional random ï¬eld.
Dynamics-Modeling-Based Approaches
Another category to modeling human dynamics originates from psychological studies, which is discussed
at the beginning of the chapter. With the aid of human motion capture system, these joint movement data
can be obtained by special optical devices or sensors. Accordingly, joint-movement-modeling approaches
have been proposed. Ali et al. modeled the human movements of six joints by chaotic theories [16].
Assuming the trajectories of the joint can be obtained, the phase space embedding is ï¬rst applied and
chaotic invariants such as maximal Lyapunov exponent, correlation integral and correlation dimension
are adopted to model diï¬€erent types of joint movements generated by diï¬€erent actions. Raptis et al. also
solved the problem of action recognition in this way [17]. They quantized the continuous trajectories into
discrete labels by the aid of window dictionary, and classiï¬ed their results by bagging or string matching
algorithms.
12
(a) Pedestrian ï¬‚ow analysis. (b) 2D human pose tracking.
Figure 1.7: Example of the optical ï¬‚ow feature.
Figure 1.8: Example of foreground silhouette after background subtraction.
1.2.2 Trajectories
Trajectories of moving objects have been used as features to infer the activity of the object, such as the
ï¬‚ow analysis of traï¬ƒc monitoring. The trajectories of joints of living creatures can also be seen as the
extracted feature in Fig. 1.2 for action recognition. Because trajectories are sensitive to translations,
rotations, and scale changes, alternative representations such as trajectory velocities, speeds, spatio-
temporal curvature, relative motion, etc., have been proposed while they are invariant to some of these
variabilities. A good tracking algorithm should be robust under the inï¬‚uences, such as occlusions, noise,
and background clutter. Figure 1.7 shows two examples of trajectories of pedestrians and human joints.
Figure 1.7(b) from [20] shows a spatio-temporal 2D models framework for 2D-pose tracking.
1.2.3 Foreground Silhouettes
Foreground silhouettes mostly are obtained by background subtraction or frame diï¬€erentiation methods
with a static camera. Figure 1.8 is an example of background subtracted image showing a person is going
to block an incoming soccer. The shape of the human silhouette provides useful information in human
action recognition. Several methods using global, boundary, and skeletal descriptors have been proposed
to quantify silhouettes. Global methods use moments of a silhouette; boundary methods describe a
silhouette using its contour; skeletal methods use a set of 1D skeletal curves to represent a silhouette.
14
and chapter 6 presents a joint-trajectory-based action recognition for controller-free gaming applications.
Chapter 7 presents a knowledge-based action recognition focusing on abandoned luggage detection sys-
tems. Finally, chapter 8 concludes this dissertation.
16
Chapter 2
Object Descriptor
2.1 Introduction
With mature digital video technology, inexpensive camcorders gradually enter our life. More and more
multimedia are produced and shared among the world. Original intention of MPEG-7 is to provide a
powerful search engine which helps people easily ï¬nd what they are looking for. Some MPEG-7 toolkits
further integrate useful functionalities for categorizing and organizing their personal collection. However,
some related research [22] showed that most people only categorize their albums at semantic level, but
the recognition technique nowadays is still not able to meet this kind of demand. MPEG-7 descriptors
are good tools for indexing and retrieval but should not be limited to them. MPEG-7 descriptors can
be creatively extended and linked to applications such as rate control in real-time video coding and
movement detection in surveillance systems. In these applications, computational loads of the real-time
implementation for these descriptors will not be a trivial issue.
With statistics derived from MPEG-7 descriptors, good indication of image and video properties can
provide referable adjustment parameters for video pre-processing like auto white balance, RGB gains
tuning, saturation control, auto contrast, and edge enhancement. In video coding, it can assist fast
algorithm of motion estimation, rate control policy, probability distribution model of entropy coding,
and so on. When we use them in surveillance system, the system can notice police to keep an eye on
unusual behavior by analyzing object trajectory. Face descriptor can also provide auto identiï¬cation of
uncertiï¬ed people in certain degree.
MPEG-7 visual descriptors record statistics of images and video sequences in color, texture, shape of
objects, and motion. Because the variety of possible applications, we ï¬rst take implementation of color
descriptors as our start point. Color is one of important visual attributes for human vision and image
processing. It is also an expressive visual feature in image and video retrieval. Color descriptions usually
are irrelevant to viewing angle, translation and rotation. This advantage possesses good resistance to
undesired shacking of camera. In MPEG-7, six descriptors are selected to record color statistics of images
and video. Among them, CSD provides best image indexing and retrieval results [23]. The superiority
comes from that CSD considers space distribution of pixel colors by recording appearance of each color
in every structuring element window in its histogram [24]. In this work, we focus on the architecture and
analysis of CSD.
The challenge to realize CSD hardware accelerator for real-time video system is that each pixel in a
frame needs to be scanned 64 times. The vast data bandwidth and then excessive operating frequency
18
	

Figure 2.1: Two images have the same traditional histogram, but right one has much more gray compo-
nents in CSD description.

	


	
	
	
		
	
 	
		
 !"#

$
%&'!(')"#
#
#

*	+
Figure 2.2: CSD extraction ï¬‚ow.
color bins by only adding one, no matter how many same color pixels exist. Figure 2.1 shows that two
images have diï¬€erent CSD description with the same traditional histogram [25]. Right image looks more
scattered than left one. Such situation causes gray pixels exist in more SWs and reï¬‚ects on gray bin in
CSD description. This advantage let us easily distinguish those images with similar dispersion.
Figure 2.2 depicts CSD extraction procedure [26]. Our design chose highest number of bins for more
precise CSD description in real-time applications. The top path directs the ï¬‚ow of 256-bin CSD. It starts
with color transformation from RGB to HMMD. Next step is histogram accumulation which is followed
by a decision of number of bins needed. After a nonlinear quantization, CSD description is derived.
2.3 Computational Complexity and Architecture Design
As described in Section 2.1, we focus on real-time applications of MPEG-7 like video coding assistance
and surveillance systems. Besides, generated CSD descriptions still can be used for search of multimedia
contents. And for supporting comparison with descriptions generated by other tools, 256 levels of color
quantization is adopted for down-scale comparison.
Since a sub-sample factor is deï¬ned in the standard for large images, we choose 256 Ã— 256 as input
image size. The sub-sample factor, K, is deï¬ned as K = max{1, 2log2
âˆš
WÂ·Hâˆ’7.5}, where W and H are
the width and height of image. For example, K = 2 implies an image is sub-sampled by 2 horizontally
and vertically. Note that the SW size is always 8Ã— 8.
Our CSD block diagram is shown in Fig. 2.3. After color transformation, pixels are sent to correspond-
ing local histogram observing (LHO) blocks and index colors that exist in these windows. Summation
20
Table 2.2: Relationship between parallelism and operating frequency. Zero parallelism means no SW is
buï¬€ered. The minimum requirement to meet target frequency (27 MHz) is three parallelism.
Parallelism MB/s MHz
0 357 476
1 46 61
2 26 35
3 19 25
4 16 21

0123 012% 012
-
-
-
)
Figure 2.4: Pixel scan order of three structuring windows.
22
++
#
8	
%&
8	
999
999
999
999%:
!(
$

4	5 4"5
Figure 2.6: Folding skill on non-linear quantization.
else HUE = 240 + 60 x (R - G) / DIFF;
}
The most time consuming and area occupied part of this formula is evaluation of hue value. If the
divider were directly mapped into hardware, it would require high precision and set a bottleneck of the
chip. Since dividend and divisor are 8-bit integers and ï¬nal output of hue is quantized into 16 categories,
two drawbacks just mentioned would be eliminated by building a mapping table for hue evaluation. After
synthesizing direct divider architecture and mapping table, the latter saves 36% area.
2.3.5 Non-Linear Quantization
After CSD histogram accumulation is ï¬nished, non-linear histogram quantization is the ï¬nal step. Each
bin should be quantized into 8-bit via 255 comparisons. With binary comparison method and folding
skill, eight comparisons are needed to quantize one bin. This strategy is shown in Fig. 2.6. As shown
in (a), we compare the bin with center value of valid range each time. Since the latency of non-linear
quantization, which is compared with CSD histogram accumulation, is negligible, 255 comparators can
be folded into one. With (b) architecture, 2048 (256Ã—8) cycles and one comparator are needed to achieve
this work.
2.4 Experimental Result
Our indexing and retrieval database contains 526 images in 78 categories. Those images are collected
from Internet and manual categorized. Furthermore, for extending the concepts of these descriptors to
image and video coding, we replace default color spaces with YCbCr domain and the performance drops
slightly.
Here we use a quantitative measure method called query-by-example (QBE) suggested by MPEG-
7 [25]. QBE sorts the distances between description vector of query image and those of images contained
in a database. The smaller average normalized modiï¬ed retrieval rank (ANMRR) means the descriptor
provides better indexing and retrieval ability.
Table 2.3 shows the indexing and retrieval results of CSD and scalable color descriptor (SCD) with
designated and YCbCr color spaces. SCD listed here is for comparison. The results with YCbCr are
also acceptable and imply that we can apply the concepts to the ï¬eld of image and video coding which
chooses YCbCr as default color space.
24
Table 2.4: Chip speciï¬cation
Technology UMC 0.18 Î¼m CMOS 1P6M
Core size 1.36950Ã— 1.36584 mm2
Gate count 49865
On-chip single port SRAM 11136 Bits
Max frequency 31.25 MHz
Operating frequency 27 MHz
Processing speed 256Ã— 256, 30 fps@ 27 MHz
Power Consumption 39.53 mW@ 27 MHz, 1.8 V
26
Tracking
Joint Trajectory 
Modeling
Classification
Engine
Video
Frames
Figure 3.1: Flow of dynamics modeling approach for human action recognition. The â€œtrackingâ€ module
tracks joints and generates trajectories. The â€œjoint trajectory modelingâ€ module extracts speciï¬c features
from trajectories. Finally, the â€œclassiï¬cation engineâ€ decides to which category of actions each trajectory
belongs by measuring the distance between extracted features and features of all action models.
	1	
-
 $
"
Figure 3.2: Sampling-importance-resampling particle ï¬lter.
models.
Many tracking algorithms have been proposed to track targets eï¬€ectively from video frames. Among
these, tracking algorithms based on a Bayesian framework, such as the Kalman ï¬lter [32], the Extended
Kalman ï¬lter, the particle ï¬lter and many others, are popular because of their probabilistic nature.
Probabilistic approaches for tracking can easily escape from local minima, and are more capable of
dealing with environmental clutters [33].
Isard et al. originally developed the particle ï¬lter algorithm [34]. Figure 3.2 depicts a typical sampling-
importance-resampling (SIR) particle ï¬lter. The main concept of the particle ï¬lter is to approximate the
posterior distribution of states using a set of particles and Monte Carlo methods. Since estimating the
posterior distribution directly is commonly impossible, such an approximation not only makes the algo-
rithm tractable but also improves tracking performance. Subsequently, several approaches to targeting
particle-ï¬lter-based tracking have been proposed [33,35â€“38]. Such tracking systems use various features,
and successfully track objects in many scenarios. Among these features, the color histogram is one of the
most used because of its simplicity [33, 38â€“40].
Despite its success in vision-based object tracking, the particle ï¬lter algorithm requires high compu-
tation, mainly because of its probabilistic and highly order-dependent nature. Also, as the objects to be
tracked become more complex, the number of particles required to track accurately increases. Therefore,
in the provision of vision-based UI based on dynamics modeling approaches for gaming systems, an ef-
ï¬cient algorithm and an architecture design to run the algorithm in real time are required. This work
focuses on the realization of a particle ï¬lter tracking module.
28
3.2.1 Formal Deï¬nition
The goal of a Bayesian tracking framework is to estimate the state xt of a tracked object at each time t
from current and previous observations, z1:t, and the previous state xtâˆ’1. The two models that describe
how a system evolves with time are the dynamic model and the observation model. A dynamic model
xt+1 = f(xt,vt) describes how states evolve, where vt is the noise that is introduced to the model. An
observation model zt = h(xt,nt) describes the relationship between current state xt and observation zt,
where nt is the noise in the observation.
Given a dynamic model and an observation model, the Bayesian tracking framework works as follows.
Following proper initialization of the state variable xt, at each time t, the state variable is initially
predicted by
p(xt, z1:tâˆ’1) =
âˆ«
xtâˆ’1
p(xt|xtâˆ’1)p(xtâˆ’1|z1:tâˆ’1) (3.1)
where z1:tâˆ’1 is the observation from time 1 to time tâˆ’ 1. After the observation at time t, zt, is obtained,
and the ï¬t of the predicted state with the observation is given by
p(xt|z1:t) = p(zt|xtz1:tâˆ’1)p(xt|z1:tâˆ’1)
p(zt|z1:tâˆ’1) (3.2)
The particle ï¬lter algorithm approximates the posterior distribution with a set of particles:
p(xt|z1:t) â‰ˆ
âˆ‘
i
w
(i)
t Î´(xâˆ’ x(i)t ) (3.3)
where w
(i)
t and x
(i)
t denote the weight and the state of particle i at time t. The particle weight w
(i)
t
speciï¬es how the estimated state of this particle matches the real state, and can be computed from the
observation model.
3.2.2 Second Order Dynamic Model
The constant velocity assumption that is made by most particle ï¬lters is not always applicable in practice.
Rather than simply estimating the velocity from the position shift, the proposed model additionally
estimates the acceleration of the object. In the following, pt = {x, y}t represents the position; qt =
{vx, vy}t denotes the velocity, and rt = {ax, ay}t stands for the acceleration of the target object at time
t. The original forms of the equal-acceleration equations are{
qt = qo + rt
pt = qot+
1
2
rt2
(3.4)
Equation (3.4) can be transformed into recursive form,{
pËœt = ptâˆ’1 + qtâˆ’1 + 12r
qËœt = qtâˆ’1 + r
(3.5)
where pËœt and qËœt are predicted from ptâˆ’1, qtâˆ’1, and r.
When the estimated position pt has been obtained by observation, qt and r at the corresponding
frame must be estimated. Solving the dual equation equations (3.5) yields{
qt = [2(pt âˆ’ ptâˆ’1)âˆ’ qtâˆ’1]
r = [2(pt âˆ’ ptâˆ’1)âˆ’ 2qtâˆ’1]
(3.6)
30
weights are known, the state is computed as the weighted sum of all particles.
xt =
âˆ‘
i
w
(i)
t x
(i)
t (3.9)
A maximum a posteriori (MAP) method is implemented in hardware architecture to estimate states:
xt = argmaxx(i)t
p(xt|z1:t,x(i)t ) (3.10)
The performance of the MAP method in this case is not evaluated, but this alternative is provided for
system developers. After the state estimation stage, the resampling algorithm [48] is implemented to
reduce the eï¬€ects of particle degeneracy.
The size and the angles of orientation of the tracked object are updated following the above steps, by
estimating the size of the object using the MAP criterion. The selected nine candidate sizes are given
by [W âˆ’Î”W,W,W +Î”W ]Ã— [H âˆ’Î”H,H,H +Î”H], where Î”W and Î”H are parameters to specify the
increase of the object size. To estimate the angles of orientation of the object, the angle is quantized into
one of 32 bins that uniformly span [0, Ï€). For each candidate angle, the color histogram is calculated and
the ï¬nal angle that has the minimum distance to the histogram of the object in the previous frame is
selected. After all six steps have been completed, the algorithm proceeds to the next frame.
Notably, in the current implementation, the estimation of the size and the angles of orientation of the
object are separated from that of its position and velocity, because deviation in size and angle does not
inï¬‚uence the tracking result. Meanwhile, the optimal combination of size and angle for each particle is
found by making 9Ã—32 = 288 estimates for one position, which represents an excessive cost of architecture
design1.
3.3 Hardware Architecture
3.3.1 System Runtime Analysis
A system runtime analysis is conducted to identify the functions of the particle ï¬lter that require a large
computational load and thus need to be accelerated to enable real-time operation.
Our gaming application involves two full-body action games - â€œGoalKeeperâ€ and â€œVolleyballâ€. Gamersâ€™
actions are captured using a video camera at 30 fps with D1 frame size. Table 3.1 presents the runtime
of the tracking module on a PC with an Intel CoreDuo 2.66 GHz CPU and 2 GB memory for the test
sequences in â€œGoalKeeper.â€ This table gives the tracking time of a speciï¬c target in a single frame. The
objective is to track three targets (head, left hand, right hand) simultaneously to identify actions. The
runtime can also be measured in fps, and the last column of the table presents the overall fps joining the
three targets. This table reveals that the tracking module can only operate at 5 to 6 fps. Consequently,
the operating speed of the tracking module cannot satisfy real-time requirements even on a CPU with
such a high working frequency.
As well as tracking, joint trajectory modeling is another important component of dynamic modeling.
The purpose of joint trajectory modeling is to model the movement of various body parts to classify
human behaviors. In our system, the body joint trajectories are generated using a particle ï¬lter algorithm.
The method follows trajectory modeling steps presented elsewhere [17]. The concept of joint trajectory
modeling is to transform continuous high dimensional data into discrete symbols (words) using a symbol
1The cost of computing an estimate is similar to that of computing a â€œparticle observationâ€ in Fig. 3.3.
32

 
 






	

	 	!
"#$	%&	!

  




'() '(* '(+ ,)() ,)(-&
Figure 3.4: Comparison between runtimes of tracking phase and trajectory modeling phase. The tracking
phase is the computational bottleneck of the dynamic modeling approach.
system must be hardware-accelerated. In contrast, joint trajectory matching can be performed easily in
real-time using software solutions. Accordingly, a hardware architecture design for the tracking module
is proposed.
3.3.2 Particle Filter Architecture
Most particle ï¬lter algorithms are computationally costly. The software implementation herein can only
support up to 8 fps on a computer with an Intel Core Duo 2.66 GHz CPU and 2 GB RAM. Although
several works have focused on the parallelization of the resample, the proï¬ling results, displayed in Fig.
3.5, are consistent with those proposed elsewhere [33, 45], revealing that histogram accumulation and
comparison consume over 90% of the operation time. Therefore, in architecture design, the acceleration
of the histogram accumulation and calculation is emphasized.
Design Challenge
The main challenge of the architecture design is to deliver real-time performance. If M targets to be
tracked, each target requires Np particles, and the video is assumed to run at fs fps, then the processing
time of each particle is 1/(NpMfs) seconds. Also, the number of cycles required to complete the histogram
accumulation is proportional to the size of the object. If the number of cycles needed to accumulate one
histogram exceeds the processing time, calculated above, then real-time performance cannot be achieved.
For example, suppose the requirement is to perform real-time tracking with 30 fps video of six targets
using 256 particles per target. If the cycle length is 5 ns, then each particle has approximately only 4, 340
cycles to process SIR steps. Notably, 5 ns cycle length is a reasonable estimate in current digital signal
processing (DSP) or application-speciï¬c integrated circuit (ASIC) design. Furthermore, in this case, the
sizes of the targets are 128 Ã— 128 at most, requiring 16, 384 cycles to accumulate a single particle. The
needed number of cycles exceeds the cycle limit calculated above, and at least four-parallel operations
are necessary.
34
Table 3.3: Word length of each variable in hardware.
Variable #Bits
Gaussian Input 4.18
Spatial Weight Sum 12.12
Color Variance 3.12
Particle Weight Sum 8.12
Exponential Table Entry 1.12
Normalized Spatial Weight 1.17
Cosine Table Entry 2.14
Normalized Particle Weight 1.26
of major variables that are used in the ï¬nal implementation. The number after the dot is the number of
bits used to represent the fractional parts of the variables.
Particle-level Parallel Processing
To deliver real-time performance, the concept of particle-level parallel processing is proposed and applied
in the hardware architecture. Speciï¬cally, the mathematical operations in Eq. (3.7) and the algorithm
described in Section 3.2 are such that, during the histogram accumulation stage, the operations undergone
by the particles are mutually independent. The parallelization parameter P must ï¬rst be evaluated
from the constraints on number of cycles and processing time. Two parallel processing schemes and
architectures are proposed here.
The ï¬rst parallel processing scheme constructs the color histograms of particles in parallel. In the
sampling stage, the operations of the particles are mutually independent, allowing multiple particles to be
processed simultaneously, as in particle-level parallel processing. After the propagation stage, the color
histogram of each particle can be constructed independently without interference from any other. Figure
3.6(a) displays the hardware architecture of the basic implementation. The current position generator
generates the address of the frame data required for histogram accumulation, in raster scan order and
one pixel at a time. The position ï¬lter retains only those addresses that are actually required for the
accumulation, and fetches the frame data. The spatial weight PE generates spatial weights according
to Eq. (3.7), and the SRAM stores and accumulates color histograms. Figure 3.6(b) shows example
hardware architecture for a four-parallel scheme. Notably, if a P -parallel scheme is required, only P
copies of the â€œparticle observationâ€ have to be used â€” instead of the whole module.
The second class of parallel schemes is based on pixel-level and particle-level parallel processing. For
example, each current position generator generates two pixel positions. The spatial weights of the two
pixels are calculated at the same time. The dual-port SRAM accumulates two bins of the color histogram
simultaneously. The architecture is like that in Fig. 3.6(a), but dual port SRAMs are adopted instead of
single port SRAMs.
36
Table 3.4: Number of used bins.
Dataset Target Avg. #Bins Peak #Bins
GoalKeeper
Left Hand 19.57 67
Right Hand 18.12 57
Volleyball
Left Hand 13.95 44
Right Hand 12.46 39
Content Addressable Memory Mapping
The comparison of the color histogram of the target appearance model and that of the candidate appear-
ance model requires large memory. The size depends greatly on the number of histogram bins. Although
the proposed prioritized ï¬nite word length analysis can eï¬€ectively ï¬nd the word length of the memory,
a direct implementation still requires a large on-chip memory. For example, consider a color histogram
consists of a YUV color space that is quantized into 8 Ã— 8 Ã— 8 = 512 bins, and suppose each bin is
12 + 12 = 24-bit. If the parallelization parameter P is 4, then a 48 Kbits memory is required to accu-
mulate the histogram. The utilization of the histogram bins is considered, and the results presented in
Table 3.4, whose third column shows the average number of used bins for all particles, and whose fourth
column shows the peak number of used bins. The results indicate that the actual number of used bins
is well under 512 in the quantized YUV color histogram. The reason for this diï¬€erence is that tracked
targets typically have few colors. Therefore, most of the memory is wasted if the direct memory scheme
is adopted for histogram accumulation.
Based on proper understanding of the cause of this waste, CAM is adopted to increase memory
utilization and reduce memory size and chip area [46]. The idea is to use a reduced-size memory to store
the spatial weights, as CAM bridges the reduced-size memory and the original accumulation address. In
the following, the original accumulation address is called the â€œvirtual addressâ€, and the actual address,
used to access the reduced memory, is called the â€œreal addressâ€. Figure 3.7 depicts the architecture of
CAM and the associated terms.
CAM records the access address of the memory as the virtual address; allocates a slot to the real
address, and constructs a map between the new virtual address and the allocated real address. If another
access to the memory using the virtual address is required, then CAM looks up its mapping table to
determine whether the virtual address has been stored. If it has, the virtual address will be translated
into a real address. Otherwise, a new entry is produced, as before. The size of CAM can be chosen
with reference to the empirical data, such as the peak usage or the average usage in Table 3.4. CAM
can greatly reduce the memory requirement. For example, if 128 bins are chosen according to the peak
number of bins from the above examples, then the memory can be reduced to 25% (128/512). Since the
area overhead of the CAM circuit is typically less than 5% of the memory area, the adopted technique
can substantially reduce memory area and reduce chip cost. Notably, that the number of existing bins
may exceed the number of designed bins. This situation is not considered here. Were such a case to arise,
the new bin would be ignored.
38
 	
$*&
 
-	A3	2
2	=


$*&
 	
$*&
 
-	A> 2


$*&

3
=	
=
 	
A3	
3
=
 	
A> 
3
=
-
2
 	
A3	2
 	
A4	2
 	
A5 2
 	
A6 2
-B6


Figure 3.8: Hardware architecture scheduling using the ï¬rst class of parallel processing. The x-axis
represents time, and the y-axis represents hardware resources. â€œSW Gen.â€ stands for spatial weight
generation. â€œL1 Dist. Gen.â€ stands for L1 distance generation.
3.4.1 Evaluation of Algorithm
To ensure that particle ï¬lter tracks speciï¬c targets correctly, ï¬ve body parts - head, hands (two), and
knees (two) - are tracked for each test sequence. Since generating the ground truth of the target positions
of entire dataset or even for only a single test sequence is extremely laborious, and our applications herein
involve action recognition, and do not require highly precise location, the success rate rather than the
positional deviation is employed as an index of the tracking performance. A successful tracking is one
in which the tracker follows a speciï¬c target until the end of a test sequence, meaning that the tracker
still overlaps the target in the last frame. Table 3.5 shows the results. Some actions such as â€œGK Kickâ€,
â€œVB Settingâ€, and â€œVB Spikeâ€ are not tracked well because the target is occluded for a long period.
The targets in such cases are out of the frame or behind the body parts of the subject for more than
ten successive frames. However, we assert that the overall success rate is suï¬ƒciently high for action
recognition.
Following analysis of the prioritized ï¬nite word length in architecture design, to guarantee that the
ï¬xed-point version does not have diï¬€erent tracking results, the cases that were successful with ï¬‚oating-
point variables are tested again with ï¬xed-point variables. The tracking results remain the same, so the
success rate is as in Table 3.5.
3.4.2 Evaluation of Architecture
To demonstrate the eï¬€ectiveness of the proposed architecture, the four-parallel scheme is implemented
with single port SRAM. The chip is synthesized at 5 ns by UMC 90 nm process. Table 3.6 presents
the chip speciï¬cations and Fig. 3.10 depicts the layout. In this prototype, 64 bins are implemented in
the CAM mapping. For the four-parallel scheme, a target model, and a resized model, the SRAM uses
64Ã— 24Ã— (4 + 1 + 1) = 9216 bits. A comparison with the original quantized YUV color space with 512
bins indicates that the memory is reduced to 12.5% of its original size.
Figure 3.11 compares the frame rates of the software and hardware implementations of single-target
tracking, with a hardware cycle time of 5 ns. The nfps is the fps normalized to the ratio of object size
to 128 Ã— 128. Since the software frame rate is not normalized, the minimum acceleration ratio between
right-most column and left-most column for each object is approximately 132. The minimum normalized
frame rate also indicates satisfactory real-time performance even when the target is as large as 128Ã—128.
40
Table 3.5: Success rate of tracking using particle ï¬lter
Action Type #Cases #Successes Success Rate
GK Blocking 135 123 91.1%
GK Down 60 53 88.3%
GK Header 160 156 97.5%
GK Kick 145 110 75.9%
GK SingleHand 193 193 99.0%
VB Setting 150 124 82.7%
VB Spike 185 128 69.2%
VB Underhand 160 157 98.1%
Overall 1,190 1,044 87.7%
Table 3.6: Hardware speciï¬cations
Technology UMC 90 nm Logic & Mixed-Mode 1P9M Low-K
Chip Size 1.991Ã— 1.883 mm2
Logic Gate Count 152K
On-Chip Memory 9, 216 bits
Working / Max. Freq. 125 MHz / 200 MHz
Power / Max. Power 185.2 mW / 296.32 mW
42
3 .3
 
.7
 
8 .8
 
9.
2 82
.0
 
89
.5
 
92
.1
 
58
.9
 
60
.2
 
39
.5
 
1,
49
8.
9 
3,
75
8.
8 
4,
33
5.
2 
2,
60
8.
2 
3,
70
7.
3 
6,
22
4.
1 
Software fps/target Hardware nfps/target (5ns) Hardware fps/target (5ns)
11
.3
 
18
.3
 
18
.7
 
14
.8
 
21
.8
 
29
.2
 82
.0
 
89
.5
 
92
.1
 
58
.9
 
60
.2
 
39
.5
 
1,
49
8.
9 
3,
75
8.
8 
4,
33
5.
2 
2,
60
8.
2 
3,
70
7.
3 
6,
22
4.
1 
Head Left hand Right hand Head Left hand Right hand
GKBlockC01 GKHeaderR01
Figure 3.11: Comparison of runtimes of software implementation and hardware implementation.
Table 3.7: Frame rate of software and hardware implementations
Sequence Target SW fps HW fps Gain
GKBlockC01
Head 11.34 1,499 132
Left hand 18.34 3,759 205
Right hand 18.67 4,335 232
GKHeaderR01
Head 14.76 2,608 177
Left hand 21.77 3,707 170
Right hand 29.16 6,224 213
â€œSWâ€ means â€œsoftwareâ€ and â€œHWâ€ means â€œhardwareâ€. The hardware cycle time is 5 ns. This table
indicates the minimum gain between hardware and software implementations is 132.
44
Chapter 4
Multiple Camera Tracking
4.1 Introduction
As demand for intelligent living environments and personal safety market grow, surveillance systems are
becoming larger. Nowadays, a typical distributed surveillance system contains multiple cameras in a
network, several local storage servers that are connected to the cameras, and a control side which ad-
ministrators can access all of the recorded clips. Fig. 4.1presents such a system. Distributed surveillance
systems monitor an area from various views, integrate information from all cameras, and increase the
probability of capturing clear biometric features, such as faces and gaits. Although such a system has
many advantages, the excess of provided information causes diï¬ƒculties with tracking people or searching
for events without automatic association between data from diï¬€erent cameras. To associate diï¬€erent
views of the same event, object correspondence (also known as consistent labeling [49]) is essential. Fig.
4.2 presents the concept of object correspondence. Object correspondence refers to the fact that if some
foreground instances represent the same object, then they are associated by temporal object correspon-
dence (TOC, which is similar to multiple-object tracking in one camera) at diï¬€erent times, and are
associated by spatial object correspondence (SOC) among diï¬€erent views.
Object correspondence is a diï¬ƒcult topic because un-calibrated or imprecisely calibrated cameras
commonly capture inaccurate object features, making optimal object correspondence hard to achieve.
Imprecise calibration is caused by diverse intrinsic/extrinsic camera parameters, such as white balance,
lighting condition, color contrast, and viewing angle. The same object captured in diï¬€erent views always
exhibits very diï¬€erent characteristics. Imprecise calibration is also evident in the geometric projection
between a known world coordinate system and the imaging plane coordinate system in the camera.
The radial distortion of the camera lens, like barrel and pincushion distortion, and the low angle of
depression of the camera, are two sources of such imprecise calibration. Object occlusion is another cause
of degradation of the association quality since discriminating among all objects in a group is diï¬ƒcult.
4.1.1 Related Works
The deployment of the cameras in distributed surveillance systems can be classiï¬ed into two classes - one
with an overlapping ï¬eld of view (FOV) and one with a non-overlapping FOV. The non-overlapping FOV
deployment [50] extends the area under surveillance in a cost-eï¬ƒcient way. To detect suspicious strangers,
cameras are mostly installed at important places, such as entrances, corridors and restricted areas. The
46
main purpose of overlapping FOV is to acquire multi-angle views, especially of crowded locations such
as intersections, department stores, and stations. A system with either deployment must perform object
correspondence for object tracking applications.
To build object correspondence for non-overlapping-FOV settings, Kettnaker and Zabih [51] used
color and speed as features of objects and proposed a maximum a posteriori (MAP) solution for an oï¬ƒce
scenario. Kang et al. [52] employed color, edge, speed, and homographic information [53] for tracking in
an environment that is cover by stationary and dynamic pan-tilt-zoom (PTZ) cameras. These features
may diï¬€erent substantially between any pair of cameras because of imprecise calibration or variations in
time of the environment. To overcome the calibration issue, Javed et al. [54] proposed a training method
to ï¬nd all brightness transfer functions between cameras.
Overlapping-FOV settings are commonly adopted when cameras are placed in densely or sparsely
populated districts. For the case of a dense population, Nummiaro et al. [55] utilized color and epipolar
constraints to track a single human using pan-tilt cameras. To perform people correspondence in highly
crowded scenes, Khan and Shah [56] utilized homography and projected the foreground of each camera
onto a common view. The locations of the ground point of each individual are located at the intersections
of all projected foreground masks. However, these masks may not overlap if the homographic transfor-
mation matrix is imprecise. Mittal and Davis [57] used color models at diï¬€erent heights of the persons
to segment each person in each camera view and match each segmented region by applying epipolar
constraints. For sparse populations, Hu et al. [58] used the principal axis of a human body to locate
his/her ground point, which is the bottom center of an object and is employed in matching foreground
masks. Their approach is ineï¬€ective at locating ground points of non-human objects with large bottoms.
Calderara et al. [59] adopted the vertical axis of a person as the feature. They applied the concepts of
homography and the epipolar line to improve the robustness in building object correspondence by con-
sidering the degree of the forward/ backward ï¬tness between any pair of cameras. Khan and Shah [49]
also used the boundaries of FOV to determine whether one object appears in various views. Their work
focused on automatically ï¬nding homographic relations among views.
As mentioned above, two main categories of features that are used in object correspondence are
recognition-based and geometry-based. Recognition-based approaches adopt color, texture, or the shape
of masks to perform object correspondence. In object tracking [60â€“66] using a single camera, where
temporal object correspondence is considered, these features are important and useful. However, the
accuracy of ï¬nding object correspondence across multiple cameras depends on a wide range of intrin-
sic/extrinsic camera parameters, the diï¬€erent viewing angles of all of the cameras, and the variations
in the environment with time. An object captured from diï¬€erent views always exhibits highly diï¬€erent
characteristics, making object correspondence diï¬ƒcult without precise auto-dynamic calibration.
Geometry-based methods are adopted to elucidate the spatial relationship between the environment
and the imaging plane of the camera. One class of geometry-based approaches directly uses the relation-
ship between diï¬€erent views. These approaches include disparity, epipolar geometry, and homography
for example. Such methods construct the warping matrix between cameras. Several methods have been
proposed to derive this matrix [67â€“72]. Another geometric approach is to construct a 3D model of the
environment [73â€“76]], based on the relationship between a known world coordinate system and the imag-
ing plane of the camera. If object masks in diï¬€erent imaging planes in diï¬€erent views are projected onto
a world coordinate space, then their correspondence can be obtained. These approaches work well in a
restricted environment in which all camera calibration information and the world coordinates are known.
However, radial distortion of lenses, the setting of cameras at low angles of depression, or the use of
dynamic PTZ cameras may lead to imprecision in calibration.
48
Remove outlier objects
Spatial object correspondence
Evaluate precision/recall rate
Project object locations to global plane
Object list
Figure 4.3: Spatial object correspondence ï¬‚ow. List of objects contains all objects with locations in each
view. Each location address is converted into an address in the global plane. If the converted address of
an object is outside the FOV of another view, then this object is ignored in the SOC stage. SOC builds
the association using EMD-based and greedy-based method. The performances are then expressed in
terms of precision and recall rates.
EMD measures the distance between two distributions and permits partial matches. Therefore, the
essence of SOC is similar to that of EMD. Object correspondence permits that a foreground mask in
one camera can be matched to one foreground mask at most in the other camera, and the optimal
solution is that which minimizes global distance. The approach is similar to the measurement method in
EMD. When the global distance between two distributions is minimal, the system can tolerate a slightly
inaccurate calibration, especially when all measured features deviate from the true values in the same
direction. Not all objects can be captured simultaneously by all cameras, and so partial matching is
necessary.
4.3 Spatial object correspondence
SOC constructs the association between foreground instances of the same object from diï¬€erent views. In
the experiment, the homographic approach is adopted to complete SOC, and the EMD-based platform
is compared with the greedy-based platform. To prevent other factors from aï¬€ecting the experimental
results, the matching ground points used to build the homography matrix are selected manually and the
bounding box of each object is derived from the bounding box annotations of the testing database. Figure
4.3 presents the ï¬‚ow. After the bounding box of each object is obtained, the bottom center of each box
is identiï¬ed as the ground point (location) of the object. The address of the bottom center of each box is
projected onto the global plane. Before SOC, obviously outlying objects are removed by checking whether
their projected address is in another view. The features of the remaining objects are then mapped onto
the EMD platform or the greedy-based platform to perform SOC. Finally, the objective performance is
expressed in terms of the precision and recall rates. The following sections describe this method in detail.
50
V1 FOV
V2 FOV
Objects in V2
Objects in V1
H
V1-GP
H
GP-V2
V1 V2
Figure 4.5: Removing outlier objects. The address of a man in V1 is transformed into an address in
the global plane using HV 1âˆ’GP . The transformed address is inversely transformed into the address in V2
using HGPâˆ’V 2. The objects with white ground points in V2 are absent from V1 and are removed before
SOC.
4.3.3 EMD-based object correspondence
This section describes the mapping of the SOC problem onto the EMDmodel. P andQ are two signatures
that contain the feature and the weight of each object captured by cameras 1 and 2. First, the weights
wpi for all i and wqj for all j are set to the same positive real value. For simplicity, the point weights are
set to 1. This setting is intuitive because an object can not be fragmented into small pieces or paired to
multiple objects. Secondly, the captured features of objects i and j in cameras 1 and 2 form the point
representatives pi and qj. The identiï¬cation of the feature can be recognition-based, geometry-based,
or hybrid. For example, let the object location in the global plane be the point representative. Thirdly,
each distance element dij in D is computed using a user-deï¬ned distance metric. This metric can be as
simple as the L1 distance, or as complex as a probability model. In our case, the L2 distance is a natural
selection for specifying the distance between object locations. After EMD, the object i in camera 1 is
associated with object j in camera 2 if fij = 1. For those algorithms in which the cost function is in the
form of a continuous product, such as a product of probabilities, the function can also be converted into
the form of Eq. (4.1) by taking its logarithm.
4.3.4 Greedy-based object correspondence
To understand the extent to which SOC can be improved by adopting the EMD-based platform, a
greedy-based object correspondence method is adopted for comparison. The features and the user-deï¬ned
distance metric adopted in the greedy-based method are identical to those adopted in EMD-based object
correspondence. The concept of the greedy method is to select each pair (i, j) with distance dij in order
of increasing distance. Object correspondence depends on a mutually exclusive matching result. Hence,
after the lowest dij out of D is selected, the elements in row i and column j are eliminated. Then, the
52
T
ab
le
4.
1:
S
p
at
ia
l
O
b
je
ct
C
or
re
sp
on
d
en
ce
-
G
ro
u
n
d
T
ru
th
.
S
eq
u
en
ce
P
ai
r/
F
ra
m
e
P
ai
r
O
n
ly
P
a
ir
a
n
d
si
n
g
le
F
P
S
P
re
ci
si
o
n
(%
)
R
ec
a
ll
(%
)
P
re
ci
si
o
n
(%
)
R
ec
a
ll
(%
)
G
re
ed
y
E
M
D
G
re
ed
y
E
M
D
G
re
ed
y
E
M
D
G
re
ed
y
E
M
D
G
re
ed
y
E
M
D
C
A
V
IA
R
-O
n
eS
h
o
p
O
n
eW
ai
t1
1
.2
4
7
3
.0
8
0
.9
7
3
.9
8
3
.1
9
0
.5
9
2
.8
8
8
.9
9
1
.2
4
5
.5
4
5
.2
C
A
V
IA
R
-O
n
eS
to
p
M
ov
eE
n
te
r1
2
.2
5
C
A
V
IA
R
-T
w
oE
n
te
rS
h
o
p
2
1
.4
4
V
iS
O
R
-O
u
td
o
or
U
n
im
o
re
1
4
-1
2
0
.4
4
9
2
.5
9
8
.0
9
1
.8
9
7
.5
9
9
.1
9
9
.6
9
9
.2
9
9
.7
4
5
.6
4
4
.9
V
iS
O
R
-O
u
td
o
or
U
n
im
o
re
1
4
-1
3
0
.2
8
V
iS
O
R
-O
u
td
o
or
U
n
im
o
re
1
4
-2
3
0
.3
5
P
E
T
S
2
0
0
1
2
.2
2
6
8
.7
8
9
.5
6
6
.7
8
8
.8
7
6
.9
9
1
.2
7
8
.2
9
1
.8
4
5
.8
4
5
.3
P
E
T
S
2
0
0
7
S
0
2
-1
2
2
6
.1
1
6
7
.2
8
1
.4
6
8
.3
8
2
.5
8
0
.4
8
8
.6
7
9
.8
8
8
.0
4
0
.3
4
1
.3
P
E
T
S
2
0
0
7
S
0
2
-1
3
5
.1
1
P
E
T
S
2
0
0
7
S
0
2
-1
4
1
3
.1
1
P
E
T
S
2
0
0
7
S
0
2
-2
3
4
.8
9
P
E
T
S
2
0
0
7
S
0
2
-2
4
1
2
.7
8
P
E
T
S
2
0
0
7
S
0
2
-3
4
4
.5
6
P
E
T
S
2
0
0
7
S
0
7
-1
2
1
4
.0
0
8
7
.6
9
1
.9
9
3
.2
9
4
.8
9
2
.6
9
4
.3
9
0
.5
9
2
.3
4
1
.9
4
1
.2
P
E
T
S
2
0
0
7
S
0
7
-1
3
3
.3
3
P
E
T
S
2
0
0
7
S
0
7
-1
4
7
.0
0
P
E
T
S
2
0
0
7
S
0
7
-2
3
2
.4
2
P
E
T
S
2
0
0
7
S
0
7
-2
4
6
.0
8
P
E
T
S
2
0
0
7
S
0
7
-3
4
2
.5
8
A
v
er
a
g
e
5
.8
0
7
7
.8
8
8
.3
7
8
.8
8
9
.3
8
7
.9
9
3
.3
8
7
.3
9
2
.6
4
3
.9
4
3
.6
T
h
e
ta
b
le
sh
ow
s
p
re
ci
si
o
n
,
re
ca
ll
,
a
n
d
F
P
S
o
f
tw
o
ob
je
ct
co
rr
es
p
o
n
d
en
ce
m
et
h
o
d
s
g
iv
en
g
ro
u
n
d
tr
u
th
lo
ca
ti
o
n
.
â€œ
P
a
ir
o
n
ly
â€
m
ea
n
s
th
e
ra
te
on
ly
d
ep
en
d
s
o
n
th
e
ob
je
ct
s
ca
p
tu
re
d
in
b
o
th
ca
m
er
a
s
a
t
th
e
sa
m
e
ti
m
e.
â€œ
P
a
ir
a
n
d
si
n
g
le
â€
m
ea
n
s
th
e
ra
te
d
ep
en
d
s
o
n
a
ll
ob
je
ct
s
in
th
e
sc
en
e.
54
80
90
100
Pair Only - Precision(%)
GN
50
60
70
CAVIAR VISOR PETS01 PETS07S02PETS07S07
EN
GT
ET
(a)
80
90
100
Pair Only - Recall(%)
GN
50
60
70
CAVIAR VISOR PETS01 PETS07S02PETS07S07
EN
GT
ET
(b)
80
90
100
Pair & Single - Precision(%)
GN
50
60
70
CAVIAR VISOR PETS01 PETS07S02PETS07S07
EN
GT
ET
(c)
80
90
100
Pair & Single - Recall(%)
GN
50
60
70
CAVIAR VISOR PETS01 PETS07S02PETS07S07
EN
GT
ET
(d)
Figure 4.6: Summary of the experimental results.
To evaluate the processing speed of the SOC approach and to analyze the real-time consideration
of surveillance systems, two hypothetical frameworks, shown in Fig. 4.8, are considered. Fig. 4.8(a)
presents a general framework for object correspondence in distributed surveillance. Before the object
correspondence process is implemented, the server must ï¬nish decoding the video; performing video
preprocessing tasks (feature extraction or segmentation), and warping the view. The video preprocessing
tasks prevent object correspondence in real-time in real-world distributed surveillance systems. Fig. 4.9
shows the runtime proï¬ling of the preprocessing elements for two-camera object correspondence. As
indicated by the proï¬ling, video decoder and segmentation consume over 80% of CPU resources in all
cases. When the frame size of the camera reaches 1280 Ã— 720, the processing speed is less than 1 fps.
Therefore, a real-time object correspondence approach at the server end should reduce the number of
preprocessing tasks as much as possible. Fig. 4.8(b) shows a possible framework. Each smart IP camera
uses a hardware accelerator to extract and segment features. The server receives the compressed bit stream
and the information on the extracted features. The server only uses information on the extracted features
and no further video preprocessing or video decoding steps is required to perform object correspondence.
In the experiment with a general-purpose processor that is running at 3GHz, the EMD function runs at
3,381 fps for PETS07S02-12, whose pair/frame rate is about 26. For PETS07S07-13, the pair/frame rate
is approximately 3.3, and the EMD function runs at 40,100 fps. As a result, the EMD-based method
consumes few resources for object correspondence. Finally, the overall system runs about 43 fps. The
last column in Table 4.1 presents these data.
56
Server end
IP camera 
encoder
Video 
decoder
Feature extract
Object 
corresponding
IP camera 
encoder
Video 
decoder
Feature extract
N
e
t
w
o
r
k
(a)
Server end
Smart IP camera
Feature extract
Video encoder
Smart IP camera
Feature extract
Video encoder
N
e
t
w
o
r
k
Storage
Storage
Object 
corresponding
(b)
Figure 4.8: Two hypothetical frameworks for object correspondence in distributed surveillance systems.
The server in Fig. 4.8(a) should complete all tasks, including video decoding, image preprocessing, and
object correspondence. The server in Fig. 4.8(b) stores the video bit stream and only performs object
correspondence using features captured by smart IP cameras.
58
Part II
Human Action Recognition
60
Figure 5.1: Example of an orientation histogram.
object, the box is partitioned into non-overlapped nw Ã— nh tiles. The concept of the partitioning is to
model the motion of diï¬€erent body parts to multiple tiles. The motion vector information in each tile is
then accumulated separately. In the experiment, a 3Ã— 3 tiles is adopted.
The feature vector FVi in a tile Ti comprises the following motion information: a direction mean
(xÂ¯i,yÂ¯i), a magnitude mean mi, a direction variance (vari(x),vari(y)), a magnitude variance vari(m), and
an orientation histogram of the motion vectors. An orientation histogram is formed by the following
steps. Let nmag and nori represent the number of bins used by the magnitude and orientation of motion
vectors respectively. All motion vectors in tile Ti are translated into the form (magnitude, orientation)
followed a quantization along each axis. A bin B(p, q) in the orientation histogram represents the number
of motion vectors that their (magnitude, orientation) are quantized into (p, q). Note that the maximum
of the magnitude is restricted by the size of the search range in ME, and the orientation is in the range
of [0, 2Ï€). Figure 5.1 shows an example of an orientation histogram with nmag = 4 and nori = 6. The
ï¬nal motion-vector-based feature is obtained by concatenating all feature vectors. Figure 5.2 shows an
example on the construction of the motion vector pattern. The red lines are the bounding box of the
performer and the blue lines are the tile boundary lines. The vector at the center of each tile indicates
the mean direction.
Next, a salient feature vector from training and testing action clips is selected for the use of recognizing
behaviors. Currently, in the experiment, such selection is based on sum of all magnitude of the motion
vectors in all tiles, since a larger sum of magnitude indicates more motion occurred in an object. Other
methods combining more than one feature vectors can also be experimented. Once the feature is obtained,
the action label of the testing clip is determined in by
label = argminCidfvi,fvt (5.1)
where i is the training sample index; Ci is the action name of index i; fvi is the feature vector of index i;
fvt is the feature vector of the testing clip; and dfvi,fvt is the distance between the training feature vector
and the testing feature vector. In the implementation, this distance is computed by L1 similarity metric.
Efros et al. also built a feature based on optical ï¬‚ow [82], which can be considered as a dense
motion vector. However, their pattern is formed from entire frame, which may include unrelated motion
information from background. Moreover, they considered the dynamics of the performer as a whole,
62
Figure 5.3: Example of motion energy image and motion history image.
Seven Hu moments [83] are adopted on MEI/MHI to classify actions as in [7]. Speciï¬cally, the mean
and the covariance of the moments of MEI and MHI in each action class are computed from all training
samples in this class. To recognize an unknown action in a clip, the MEI/MHI representation of the clip
and the according moments are calculated. The Mahalanobis distance is then computed, given the vector
of moments of each training class and that of the unknown actions.
5.4 Experimental Results
The motion-vector-based approach is evaluated using the following parameters. The frame size of test
sequences is 720Ã— 480; the block sizes in ME is set to 2Ã— 2, 4Ã— 4, and 8Ã— 8, and the search ranges are
(Â±8,Â±8), (Â±16,Â±16), and (Â±32,Â±32). The following results are based on the setting that the block size
is 4Ã— 4 and the search range is (Â±16,Â±16), unless explicitly mentioned.
Table 5.1 shows confusion matrices of the recognition results by the motion-vector-based approach
with the directional discrimination (Right, Left, Center). In most cases, the algorithm performs well.
The worst results are in â€œVB Spikeâ€. The action â€œSpike Râ€ and â€œSpike Lâ€ canâ€™t be distinguished from
â€œSpike Câ€ eï¬€ectively, even if the block size is 2Ã— 2.
Figure 5.4 displays the overall classiï¬cation result of the motion-vector-based approach; the classes
consist of all VB/GK actions with all three directions. The bars with diï¬€erent colors indicate the diï¬€erent
sizes of block size used in the experiment. Except the â€œVB Spikeâ€ case discussed before, the correction
rate in most cases is higher than 80%.
Table 5.2 displays the confusion matrices of the classiï¬cation result by the temporal-template-based
approach also with the directional discrimination (Right, Left, Center). In this table, the correction rate
in most cases is higher than 80%. However, the classiï¬cation results of actions performed in frontal view
(C) are better than the actions performed in left view (L) and right view (R). That means the algorithm
has better discriminating ability for the cases performed in frontal view.
Table 5.3 summarizes the result of action classiï¬cation by the temporal-template-based method; the
upper part of this table shows the results of discriminating ability of the actions performed in diï¬€erent
views, and the lower part shows the results of discriminating ability of all actions. In the experiments,
64
Table 5.2: Confusion matrices generated by the temporal-template-based approach.
(a) GK Header by MEI
Header R Header L Header C
Header R 8/10 2/10 0/10
Header L 0/10 8/10 2/10
Header C 1/12 0/12 11/12
(b) GK Kick by MEI
Kick R Kick L Kick C
Kick R 8/10 2/10 0/10
Kick L 1/9 7/9 1/9
Kick C 0/10 1/10 9/10
(c) GK Header by MHI
Header R Header L Header C
Header R 8/10 2/10 0/10
Header L 0/10 9/10 1/10
Header C 0/12 0/12 12/12
(d) VB Blocking by MEI
Block R Block L Block C
Block R 7/10 2/10 0/10
Block L 0/8 7/8 1/8
Block C 0/10 0/10 10/10
(e) GK Kick by MHI
Kick R Kick L Kick C
Kick R 9/10 1/10 0/10
Kick L 1/9 7/9 1/9
Kick C 0/10 1/10 9/10
(f) VB Blocking by MHI
Block R Block L Block C
Block R 7/10 2/10 0/10
Block L 1/8 7/8 0/8
Block C 1/10 1/10 8/10
MHI outperforms MEI with an average gain of 1.64%, and the classiï¬cation results deteriorate when the
number of test cases increase. For example, the average classiï¬cation rate of MHI in the upper part of
the table is 85.57%, but drops to 75.29% while including all actions.
Table 5.4 summarizes the overall recognition results by the motion-vector-based and the temporal-
template-based approaches. This table indicates that, with increasing number of actions (performed in
diï¬€erent views), the classiï¬cation accuracy decays much more slower in the motion-vector-based approach
than in the temporal-template-based approach. Also, both approaches do not model the body movement
explicitly by joints, the proposed motion-vector-based approach outperforms the temporal-template-based
approach in all cases.
5.5 Summery
This chapter proposes a motion-vector-based approach for recognizing simple and short period actions,
such as actions in controller-free gaming systems or human-computer interfaces. The approach uses the
concept that the movement of diï¬€erent body parts may represent a meaningful motion in a speciï¬c action.
Accordingly, the foreground is partitioned into tiles and the motion descriptor of each tile is generated
for action recognition. This approach is compared with the temporal-template-based approach using the
GK/VB physical gaming actions as test cases. Because feature vectors of the temporal-template-based
approach are generated by the entire foreground of one people, no local information of the foreground
is gathered. Therefore, the proposed motion-vector-based approach outperforms the temporal-template-
based approach in all cases.
66
Chapter 6
Parametric Action Recognition
6.1 Introduction
Parametric time-series approaches build a model on the temporal dynamics of motions. The particular
parameters for a class of actions is then estimated from training data. Examples of parametric approaches
include hidden Markov models (HMMs) and linear dynamical systems (LDSs). Parametric approaches
are better suited for more complex actions that are temporally extended. Examples of such actions
include the steps in a ballet dancing video and a music conductor conducting an orchestra using hand
gestures.
This chapter proposes a parametric action recognition approach for the same controller-free gaming
(VB/GK) applications in chapter 5, by using trajectory models of joints of a human, which are generated
by the particle ï¬lter in chapter 3.
6.2 Joint-Trajectory-Based Action Recognition
6.2.1 Overview
In psychology, Johansson studied the human perception of movement by experimenting with well-known
moving light displays (MLD) attached to human body part [1]. He discovered that people can recognize
diï¬€erent actions with only these moving dots without full structural information [3]. Accordingly, an
action recognition method using joint trajectories is proposed in this chapter. The purpose of joint-
trajectory modeling is to model the movement of various body parts and use the model to classify human
actions. The joint trajectories are generated by the proposed particle ï¬lter algorithm, which tracks
speciï¬c color markers preset on all joints.
Let nJ be the number of body joints used to model the movement. Because the address of each joint
is expressed in the form of (x, y) in a 2D space, an action is represented by 2nJ 1D trajectories.
Figure 6.1 illustrates the steps to model the joint trajectories and classify actions. The proposed
method mainly follows the trajectory modeling in [17]. The concept of the joint trajectory modeling is
to transform the continuous high dimensional data into discrete symbols (words) from symbol dictionary.
A symbol represents a salient segment in a trajectory, and a symbol dictionary consists of all segments
used to construct the joint trajectories in training databases.
68
(a) Fixed-size segments.
(b) Adaptive-size segments.
Figure 6.3: Illustrations of ï¬xed-size and adaptive-size segments.
to zero before the training stage.
6.2.3 Training Stage
In the training stage, all segments are clustered to build a symbol dictionary. Instead of using K-means
clustering algorithm as in [17], the K-medoids clustering algorithm are applied to keep original shapes of
the segments. In the implementation, the distance metric is the L1 distance, as follows
dist =
nâˆ‘
i=1
|tp(i)âˆ’ tq(i)| (6.1)
where n is the total number of points in each segment, and tp, tq are two compared segments. The ï¬nal
clustered segments form the symbol dictionary.
6.2.4 Matching Stage
In the matching stage, all trajectories are converted into segments and then the symbol sequences.
Speciï¬cally, each segment chooses the closest matching segment in the dictionary, and one trajectory
can be converted into an equivalent symbol sequence. All symbol sequences from all joints are then
concatenated into a single motion descriptor. To classify a descriptor into one action, a histogram-
based method and a sequence-alignment-based method are adopted to train and compare actions. In
the following discussion, the total number of actions are nact, the actions to classify are labeled as ak,
1 â‰¤ k â‰¤ nact, and there are nak training samples for action ak.
70
Table 6.1: Testing Similar Actions
Game Name Similar Actions
GK Header (R, L, C)
GK Blocking (R, L, C)
GK Kick (R, L, C)
GK SingleHand (LL, RR, UL, UR)
VB Blocking (R, L, C)
VB Setting (R, L, C)
VB Spike (R, L, C)
VB Underhand (R, L, C)
Table 6.2: Testing All Actions
Game Name Actions
GK Header, Kick, SingleHand, Down, Blocking
VB Blocking, Setting, Spike, Underhand
6.3.1 Testing Similar Actions
As illustrated in subsection 6.2.2 and 6.2.4, there are two methods to build the dictionary â€” ï¬xed segment
size and adaptive segment size. Also, there are two methods to compare the label sequences â€” histogram-
based and sequence-alignment-based matching. Therefore, the joint trajectories modeling approach are
tested with four combinations.
Table 6.3 gives the confusion matrices of similar actions by using histogram distance descriptor with
ï¬xed segment size. The table indicates the performance to discriminate actions with diï¬€erent direction
are high, only action â€œSpikeâ€ in game â€œVBâ€ has wrongly recognized actions, which cannot discriminate
the direction left and center eï¬€ectively. Others actions can be discriminated with diï¬€erent directions well.
Table 6.4 lists the action recognition result of the trajectory modeling method for discriminating ac-
tions with diï¬€erent directions under four combinations. In general, if the histogram-based distance is
adopted, the method using a ï¬xed-segment-size dictionary outperforms that using an adaptive-segment-
size dictionary with the increase of recognition rate as 14.2%. This table also indicates the performance
of the histogram-based method is better than the sequence-alignment-based method in the implementa-
tion. The reason is that training procedures of these two methods are diï¬€erent. The histogram-based
method can eï¬€ectively merge the training samples in each action by fusing histograms. However, the
sequence-alignment-based method computes the distances between the testing and the training samples
individually. No information merge between the training samples in the same action. Accordingly, an
eï¬€ective fusion method of training samples by the sequence-alignment-based method needs to be further
explored. If the adaptive-segment-size dictionary is adopted, the sequence-alignment-based method has
smaller variation in recognition rate compared with the histogram-based method.
72
 "	&
	

	

	

		
		
		



	

	

	




	






	






	




	


	


	


	


	


	















	




	




	


7F
47F
67F
;7F
?7F
377F
Figure 6.5: Confusion matrix of all games in game â€œVBâ€.
Table 6.5: Recognition accuracy using histogram distance descriptors.
Fixed Dict. Adaptive Dict.
Three views All Three views All
GK 100% 100% 86.89% 59.60%
VB 97.5% 96.67% 82.21% 72.31%
6.3.2 Testing All Actions
Figure 6.5 shows the confusion matrix discriminating all actions in game â€œVBâ€, with a ï¬xed-segment-size
dictionary and the histogram-based method. In the test with 12 actions, only action â€œSpike Lâ€ will be
confused by action â€œSpike Câ€, with the recognition rate equal to 60%. All other actions in â€œVBâ€ can be
recognized correctly. The results of recognizing actions in â€œGKâ€ are all correct. Therefore, the confusion
matrix is not shown.
Table 6.5 and table 6.6 summarizes the recognition results of the proposed joint-trajectory-based
approach. In general, the combination of a ï¬xed-segment-size dictionary and using histogram distance
descriptors achieves best results. However, if the adaptive-segment-size method is adopted, sequence-
alignment-based distance descriptors can provide higher accuracy.
6.4 Summery
This chapter proposes a parametric time-series approach using joint trajectories extracted by particle
ï¬lter as the action descriptors. Each trajectory is converted into a symbol sequence. The action recog-
nition is accomplished using all combinations of two distance measuring methods and two dictionaries
completed by ï¬xed-size or adaptive-size segments. In the experiments, the histogram-based method with
74
Chapter 7
Knowledge-based Action Recognition
7.1 Introduction
Intelligent and automatic security surveillance systems have recently become an active research focus due
to continuously growing public demand for such systems. Terrorist attacks frequently employ bombs,
such as car bombs, suicide bombs, and luggage bombs. Modern technology cannot fully prevent such
attacks, and security oï¬ƒcers can easily miss their targets. However, compared with the two previous
forms, luggage bombs are relatively diï¬ƒcult to hide and furthermore there is generally ample time to
either deal with the bombs or organize an evacuation. Human thus have a better chance to prevent
destruction arising from luggage bombs. Therefore, to achieve early detection of these threats with the
assistance of automatic security systems, the ability to reliably detect suspicious items and identify their
owners is urgently necessary in various venues such as airports and train stations.
Previous studies have given several deï¬nitions of a luggage abandonment event [87â€“91]. This study
follows three similar but slightly diï¬€erent rules [91]: (1) Contextual rule: luggage is considered unattended
after the person who entered the area in possession of that luggage concerned is no longer in close proximity
to it. (2) Spatial rule: luggage is considered unattended when its owner is outside of a small neighborhood
around the luggage. (3) Temporal rule: If the owner of a luggage leaves the area without the luggage, or
if the luggage has been left unattended for more than 30 consecutive seconds, the luggage is considered
abandoned.
7.1.1 Related Works
The task of abandoned luggage detection in surveillance video generally comprises three stages: The ï¬rst
stage localizes candidate abandoned luggage items in the video. The second stage locates and tracks the
luggage owner(s), providing a trajectory for subsequent probabilistic reasoning. The ï¬nal stage assesses a
probability or conï¬dence score for the luggage-abandonment event based on information obtained during
previous stages. The three stages all represent distinct research areas with their own rich literature.
Various existing algorithms may employ diï¬€erent methods for diï¬€erent stages.
The ï¬rst stage of locating candidate abandoned luggage items within the video frame is performed
using two types of techniques: Those that utilize the technique of background subtraction [92â€“94], and
those that do not [95, 96]. As is generally acknowledged, object detection and recognition is an instinc-
tive and spontaneous process for human visual system. However, implementing a robust and accurate
76
	@	


	
%

&%	

	"	#	
	,'
)%	%	"	
-	57	
	*'0	
*	


	
%
#	"
#	
	"
	
%
G	57	
"			
H	57	
Figure 7.1: System work ï¬‚ow.
78
Figure 7.2: AVSS 2007 video dataset. Images captured via a typical surveillance camera are looking
down, causing the lower part of objects to appear larger and the upper part to appear smaller.
pixel has a value of 1 and a background pixel a value of 0; let Mk(i, j) represent the k foreground masks,
Mk(i, j) =
{
1, |Fk(i, j)âˆ’ B(i, j)| â‰¥ w(i, j) Â· Std(i, j)
0, |Fk(i, j)âˆ’ B(i, j)| < w(i, j) Â· Std(i, j)
These n foreground masks are merged and their intersection taken as the static foreground object mask
S =
â‹‚n
k=1Mk. Filtering is then conducted on S to remove irrelevant and sporadic noisy pixels, connected
component analysis is subsequently performed. A white (valued 1) block on S indicates a region that has
remained in the foreground of all the n sample frames over the previous 30-second period, and therefore
this region should likely correspond to either a static abandoned luggage item or a stationary human.
The tracking module, which is detailed in the next section, then analyzes the region and further localizes
it if it is determined to be a static luggage item. Figure 7.3 shows an example. S presents candidate
abandoned luggage items. The localized targets provide search regions for subsequent tracking and higher
level event reasoning.
7.3 Selective Tracking Modules
The system presents information on the locations of suspicious items after obtaining S. All static fore-
ground objects are assumed to be either humans or luggage items. Each foreground region in S is checked
to determine whether it is a human via a combination of skin color information and body contours. If
the region is identiï¬ed as a human, it is discarded because the object of the search is abandoned luggage
items. If the region is identiï¬ed as not a human, it is assumed to be a luggage item. A local search
region is constructed around the detected luggage to see whether its owner is in close proximity in the
present frame at time t. If the owner is found, the region is again discarded because the owner exhibits no
80
Figure 7.4: Left: input video frame with localized search region indicated by red circle. Right: the Cr
detection result within the search region.
7.3.1 Cr Color Channel with Human Skin
Human skin signal response is signiï¬cantly larger in the YCbCr color space than the commonly used
RGB color space. Due to signiï¬cant blood ï¬‚ow, human skin responds strongly to the Cr channel in the
YCbCr space, irrespective of skin color [101]. Accordingly, the Cr channel of skin color is used for human
face localization because in situations involving severe occlusion (crowded scenes with people overlapping
one another), human face is the most visible body part when viewed with a typical surveillance camera
positioned looking downwards from a height.
To ï¬nd the face of the owner, a search region is ï¬rst constructed around the suspicious static luggage
item. Background subtraction is then performed on RGB color space within the region. An RGB
foreground of the region is obtained and then converted to the YCbCr color space, and the Cr channel is
retained, as illustrated in Fig. 7.4. Background subtraction is performed within the search region prior to
conversion to YCbCr color space because Cr is a channel representing the diï¬€erence of red color, and thus
the face signal is stronger when background clutter is removed. The Cr channel response is then used to
locate the face of the luggage owner, while simultaneously locating human body contour information as
explained below.
7.3.2 Improved Hough Transform on Body Contour
Cr channel responds to red within the search region, which in some cases may include other reddish
objects besides the face of the owner. Therefore, a new mechanism for reliably detecting the presence
of the luggage owner is employed. The human upper-body contour, which comprises the head-shoulder
silhouette, is then adopted. The head-shoulder contour, as inspired by [95], is used under the Hough
transform (HT) to detect human upper-body within the search region. Figure 7.5 depicts the contour
and the used notations.
HT is a morphological tool which, in its simplest form, maps a straight line in normal space to a
point in parameter space [102]. A generalized version of HT is utilized to localize contour of an arbitrary
shape. The algorithm comprises two stages: template generation and contour matching.
During template generation, given a predeï¬ned head-shoulder contour, as in Fig. 7.5, the HT algo-
rithm ï¬rst establishes a center point (xC , yC) of the face for the contour template. The algorithm then
82
  


		
=1
Figure 7.6: The origin of the two red lines has a Ïˆ angle with degree m, corresponding to the red, m-th,
bin which contains two (r, Î±) pairs. Solid lines denote correct matches, while dashed lines represent noise.
Diï¬€erent points on the head-shoulder contour in the edge image converge to a local maximum on the
detection map on the right. At the bottom is the 180-bin reference table; a Gaussian weighting g is shown
below the table, with the center bin labeled in red and the neighboring ten bins in blue.
84
of the owner at time t, and the prediction for time t+ 1 can be formulated as
r(t+ 1) = r(t) + Î”r
where Î”r is generated recursively via motion prediction and given by
Î”rt â† Î± Â·Î”rtâˆ’1 + Î² Â· (r(t)âˆ’ r(tâˆ’ 1))
where Î±+ Î² = 1, Î± > 0, Î² â‰¥ 0. The fact that Î”r is calculated recursively ensures that past information
is considered and past inï¬‚uences decay exponentially with time. In the implementation, the exponential
smoothing coeï¬ƒcients Î± and Î² are empirically determined to be 0.4 and 0.6, respectively.
Three measures are used to calculate the probability score for the trajectory of the luggage owner,
which is then used to obtain a conï¬dence score for the luggage-abandonment event. The three measures
include the diï¬€erences between the prediction from the last frame and the detection on the present frame,
in location, size and color histogram of the luggage owner [96]. The probability score increases with
closeness of prediction and detection. Let PTOTAL denote the probability score combining the measures;
let PPOS, PSIZE and PCH denote the scores of the position measure, size measure and color-histogram
measure, respectively; ï¬nally, let r represent the position vector, s the size (in pixel area) and c the color
histogram. The three scores are deï¬ned as follows, with subscript P corresponding to prediction and D
to detection.
PPOS(rP, rD) = exp(âˆ’(xP âˆ’ xD)
2
Ïƒ2x
) Â· exp(âˆ’(yP âˆ’ yD)
2
Ïƒ2y
)
PSIZE(sP, sD) = exp(âˆ’(sP âˆ’ sD)
2
Ïƒ2s
)
PCH(cP, cD) =
1âˆš
2Ï€Ïƒ
exp(âˆ’D
2
2Ïƒ2
)
whereD is the Bhattacharyya distance between the two color histograms, as inD2 = 1âˆ’âˆ‘256i=1âˆšcP(i)cD(i),
and Ïƒ is the standard deviation. The total probability score is calculated by combining the above three
measures, each with a scale factor Î», so they total 1, as follows
PTOTAL = Î»POSPPOS + Î»SIZEPSIZE + Î»CHPCH (7.1)
The three probabilities serve more as comparative than absolute values. A change in the standard
deviations of these probability calculations would similarly aï¬€ect all probabilities thus calculated, with the
most probable detection still having the highest probability ranking. Empirical values thus are assigned
to the standard deviations, and parameter selections in the experiment produce insigniï¬cant eï¬€ects.
7.4 Probabilistic Event Model
The tracking module provides a trajectory and associated probability score. The module calculates the
distance from the feet of the owner to the luggage for each incoming frame, with this distance being used
to determine when the owner leaves the scene or the luggage, and for how long the luggage has been
left unattended. The feet of the owner are assumed to be positioned below the head at a distance of
about ï¬ve face-lengths. According to this study, luggage is formally declared abandoned when its owner
leaves the scene without it, or when it has been left unattended for 30 consecutive seconds. Luggage-
abandonment events are deï¬ned using a probabilistic framework [87]. Let A denote the event, and O
86
Table 7.1: Alarm Time (Second).
Sequence Ground Truth Owner Break Time Time Alarm
AVSS 2007 Easy 180.00 114.80 119.76
AVSS 2007 Medium 162.00 100.88 102.64
AVSS 2007 Hard 162.00 101.08 102.28
PETS 2006 Seq. 1 113.72 85.88 90.52
PETS 2006 Seq. 2 91.84 61.92 65.04
PETS 2006 Seq. 4 104.08 72.88 76.36
PETS 2006 Seq. 5 110.56 80.28 83.04
PETS 2006 Seq. 6 96.88 68.44 73.96
PETS 2006 Seq. 7 93.96 60.68 90.60
the owner remains continuously with the luggage, and therefore no alarm is raised. In video 7, the owner
wanders before ï¬nally leaving the scene without his luggage; the trajectory of the actively moving owner
contains too many abrupt changes in speed and direction for the present motion prediction algorithm to
successfully follow. The tracker lost the owner 34 seconds after leaving the luggage, while an alarm is
triggered at 30 seconds. Figure 7.8 shows some labeled scene shots. Using diï¬€erent system parameter
settings for all tested environments, the processing speed of the proposed approach is approximately 15
to 20 frames per second, which is suï¬ƒcient for real world applications.
7.6 Summery and Future Work
This chapter presents a localized approach for detecting abandoned luggage in surveillance environments.
Through foreground-mask sampling, only the object of interest is localized, while ï¬ltering out all irrel-
evant, interfering agents. Tracking thus can be performed in a more selective and localized manner.
An improved implementation of the HT for detecting the contours of the upper-body is also proposed
for use in tandem with skin color detection. A probabilistic framework and the MAP principle are em-
ployed to model the luggage-abandonment event. In the future, the proposed approach is extended to a
multi-camera network in which coordination of various cameras enables cues to be gathered from mul-
tiple perspectives and information to be relayed from one to another camera. Besides, the approach is
generalized to include diï¬€erent viewing angles on the human form.
88
Chapter 8
Conclusion
8.1 Principal Contributions
In this dissertation, the analysis of VLSI architecture of feature extraction and related recognition ap-
proaches are studied, which is key modules to human action/activity analysis. The design challenges
include robust feature extraction and eï¬€ective mathematical action/activity model. Three recognition
categories are chose as case studies to discuss the above design challenges in controller-free gaming in-
terfaces and abandoned luggage detection systems. For diï¬€erent application constraints, designers can
adopt proposed techniques to design a suitable system. The major contributions are summarized in the
following subsections.
8.1.1 Color Structure Descriptor
First analysis of dedicated hardware architecture design for MPEG-7 CSD descriptor is proposed. This
design runs at 30fps for real-time multimedia applications, and can be modiï¬ed to capture the trajectory
of humans. With the analysis of histogram accumulation, local histogram observing (LHO) is used to
buï¬€er local structure window for data reuse, and three parallel LHOs is implemented to support real-time
operations. The chip area is further saved from the color transformation and the non-linear quantization.
The divider in the color transformation is implemented with a lookup table, which area is 36% of that of
original divider. The 255 comparators in non-linear quantization are folded into one.
8.1.2 Tracking Using Particle Filter
A hardware architecture that implements the color-based particle ï¬lter tracking algorithm is proposed.
The design runs at 30 fps on D1 resolution. In essence, the particle ï¬lter is a stochastic algorithm.
Converting from algorithm to hardware architecture requires limiting the bit precision. Prioritized ï¬nite
word length analysis is proposed to analyze the word length requirement. Second, particle-level parallel
processing is developed; it is suitable for accelerating such a highly order-dependent algorithm and can be
easily extended to higher speciï¬cations. Third, content addressable memory is applied to reduce memory
required by the algorithm. The extent of the memory reduction depends on the characteristics of the
90
Third, event detection is formulated as a maximum a posteriori (MAP) problem. The reliability
of the tracked trajectory of the owner is used in evaluating the overall conï¬dence score of the luggage-
abandonment event. An alarm is triggered if the overall conï¬dence score exceeds a given threshold, which
is adjustable by the user to achieve varying degrees of system sensitivity.
8.2 Future Directions for Human Action Recognition
Several important issues still need to be considered for successful action recognition algorithms in terms
of their robustness to real-world conditions and real-time performance. Some of these issues include:
invariances in human action analysis, spatio-temporal action localization, feature extraction in real-world
conditions, and intention reasoning.
8.2.1 Invariances in Human Action Analysis
Invariances in human action analysis contain three main classes: viewpoint invariance, execution rate
invariance, and anthropometry invariance.
Viewpoint invariant issue means that recognition algorithms should recognize actions performed in
diï¬€erent views. However, to build statistical models of several views is extremely challenging because
of the variations in motion and structure features induced by camera perspective eï¬€ects and occlusions.
A direct method is to store templates from several canonical views and interpolate across the stored
views [7, 105]. Another approach assumes that some correspondences across views are available. Rao et
al. [106] preserved extrema in spatio-temporal curvature of trajectories across views. Parameswaran et
al. deï¬ned a view-invariant representation of actions based on the theory of 2D and 3D invariants [107].
Weinland et al. extended motion history image to 3D space by combining views from multiple cameras
to generate a 3D binary volume [108]. View-invariant features are extracted by computing circular fast
Fourier transform (FFT) of the volume.
Diï¬€erent persons or even the same person may act with diï¬€erent execution rates while performing
the same action. Minor changes in execution rates will work ï¬ne with state-space approaches. However,
these kind of methods do not explicitly model transformations of the temporal axis and are not truly
rate invariant. The variation in execution rate can be modeled as a warping function of the temporal
scale. To model highly nonlinear warping functions, the most common method is dynamic time warping
(DTW) of the feature sequence [105, 109, 110]. DTW is a promising method because it is independent
of the choice of feature. However, DTW requires the start and end time have to be aligned. Moreover,
DTW is unsuitable for long sequences involving many templates due to the distance computations.
Anthropometry invariant issue illustrates that the same action performed by diï¬€erent people must
be reliably categorized into one despite of diï¬€erent personal styles. Individual diï¬€erences in size, shape,
gender, habit, etc., generate the anthropometric variations. Some methods that normalize the features
to compensate for changes in size, scale, etc., are usually employed.
8.2.2 Spatial and Temporal Action Localization
Action localization issues can be classiï¬ed into spatial and temporal action localization. The deï¬nite range
of an action clip is important for some action recognition approaches. Besides, if the range is known,
the feature extraction steps and the following recognition steps can be limited in this data. Accordingly,
using this information can reduce computation complexity. However, to locate start time and end time
92
Bibliography
[1] G. Johansson, â€œVisual perception of biological motion and a model for its analysis,â€ Perception
and Psychophysics, vol. 14, no. 2 1973, pp. 201â€“211, 1973.
[2] Martin A. Giese and Tomaso Poggio, â€œNeural mechanisms for the recognition of biological move-
ments,â€ Nature Reviews Neuroscience, vol. 4, pp. 179â€“192, 2003.
[3] D. M. Gavrila, â€œThe visual analysis of human movement: a survey,â€ Comput. Vis. Image Underst.,
vol. 73, no. 1, pp. 82â€“98, 1999.
[4] J.K. Aggarwal and Q. Cai, â€œHuman motion analysis: a review,â€ in Nonrigid and Articulated Motion
Workshop, 1997. Proceedings., IEEE, Jun 1997, pp. 90â€“102.
[5] C. Cedras and M. Shah, â€œMotion-based recognition: A survey,â€ IVC, vol. 13, no. 2, pp. 129â€“155,
March 1995.
[6] P. Turaga, R. Chellappa, V.S. Subrahmanian, and O. Udrea, â€œMachine recognition of human
activities: A survey,â€ Circuits and Systems for Video Technology, IEEE Transactions on, vol. 18,
no. 11, pp. 1473â€“1488, Nov. 2008.
[7] A.F. Bobick and J.W. Davis, â€œThe recognition of human movement using temporal templates,â€
Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 23, no. 3, pp. 257â€“267, Mar
2001.
[8] Hongying Meng, N. Pears, and C. Bailey, â€œA human action recognition system for embedded
computer vision application,â€ in Computer Vision and Pattern Recognition, 2007. CVPR â€™07.
IEEE Conference on, June 2007, pp. 1â€“6.
[9] Haojie Liu, Shouxun Lin, Yongdong Zhang, and Kun Tao, â€œAutomatic video-based analysis of ath-
lete action,â€ in Image Analysis and Processing, 2007. ICIAP 2007. 14th International Conference
on, Sept. 2007, pp. 205â€“210.
[10] E. Shechtman and M. Irani, â€œSpace-time behavior based correlation,â€ in Computer Vision and
Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, June 2005, vol. 1,
pp. 405â€“412 vol. 1.
[11] P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie, â€œBehavior recognition via sparse spatio-temporal
features,â€ in Visual Surveillance and Performance Evaluation of Tracking and Surveillance, 2005.
2nd Joint IEEE International Workshop on, Oct. 2005, pp. 65â€“72.
94
[26] L. Cieplinski, M. Kim, J.-R. Ohm, M. Pickering, and A. Yamada, â€œIso/iec 15938-3 fcd information
technology - multimedia content description interface - part 3: Visual,â€ March 2001.
[27] Shao-Yi Chien, Wei-Kai Chan, Der-Chun Cherng, and Jing-Ying Chang, â€œHuman object tracking
algorithm with human color structure descriptor for video surveillance systems,â€ in Multimedia and
Expo, 2006 IEEE International Conference on, July 2006, pp. 2097â€“2100.
[28] Nintendo Co. Ltd., â€œWii,â€ http://wii.com/, 2006.
[29] Sony Computer Entertainment, â€œPlaystation3,â€ http://www.us.playstation.com/, 2006.
[30] Microsoft Co. Ltd., â€œProject natal,â€ http://www.xbox.com/en-US/live/projectnatal/, 2009.
[31] Microsoft Co. Ltd., â€œXbox 360,â€ http://www.xbox.com, 2005.
[32] R. E. Kalman, â€œA new approach to linear ï¬ltering and prediction problems,â€ Transactions of the
ASME Journal of Basic Engineering, , no. 82 (Series D), pp. 35â€“45, 1960.
[33] P. PeÂ´rez, C. Hue, J. Vermaak, and M. Gangnet, â€œColor-based probabilistic tracking,â€ ECCV â€™02:
Proceedings of the 7th European Conference on Computer Vision-Part I, pp. 661â€“675, 2002.
[34] M. Isard and A. Blake, â€œCondensation â€” conditional density propagation for visual tracking,â€
IJCV, vol. 29, pp. 5â€“28, 1998.
[35] X. Li and N. Zheng, â€œAdaptive target color model updating for visual tracking using particle ï¬lter,â€
Systems, Man and Cybernetics, 2004 IEEE International Conference on, vol. 4, pp. 3105â€“3109 vol.4,
Oct. 2004.
[36] Y. Zhuang, W. Wang, and R. Xing, â€œTarget tracking in colored image sequence using weighted color
histogram based particle ï¬lter,â€ Robotics and Biomimetics, 2006. ROBIO â€™06. IEEE International
Conference on, pp. 1488â€“1493, Dec. 2006.
[37] G. Liu, C. Fan, and E. Gao, â€œVisual target tracking based on multiple cues and particle ï¬lter,â€
Robotics and Biomimetics, 2006. ROBIO â€™06. IEEE International Conference on, pp. 1483â€“1487,
Dec. 2006.
[38] T. Zhang, S. Fei, X. Li, and H. Lu, â€œAn improved particle ï¬lter for tracking color object,â€ Intelligent
Computation Technology and Automation (ICICTA), 2008 International Conference on, vol. 2, pp.
109â€“113, Oct. 2008.
[39] K. Nummiaro, E. B. Koller-Meier, and L. V. Gool, â€œObject tracking with an adaptive color-based
particle ï¬lter,â€ Symposium for Pattern Recognition of the DAGM, pp. 355â€“360, 2002.
[40] J. Czyz, B. Ristic, and B. Macq, â€œA color-based particle ï¬lter for joint detection and tracking
of multiple objects,â€ Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP â€™05).
IEEE International Conference on, vol. 2, pp. 217â€“220, 18-23, 2005.
[41] A.C. Sankaranarayanan, R. Chellappa, and A. Srivastava, â€œAlgorithmic and architectural design
methodology for particle ï¬lters in hardware,â€ Computer Design: VLSI in Computers and Proces-
sors, 2005. ICCD 2005. Proceedings. 2005 IEEE International Conference on, pp. 275â€“280, Oct.
2005.
96
[56] Saad M. Khan and Mubarak Shah, â€œA multiview approach to tracking people in crowded scenes
using a planar homography constraint,â€ in European Conference on Computer Vision, 2006, pp.
133â€“146.
[57] Anurag Mittal and Larry S. Davis, â€œM2tracker: A multi-view approach to segmenting and tracking
people in a cluttered scene,â€ International Journal of Computer Vision, vol. 51, no. 3, pp. 189â€“203,
2003.
[58] Weiming Hu, Min Hu, Xue Zhou, Tieniu Tan, Jianguang Lou, and S. Maybank, â€œPrincipal axis-
based correspondence between multiple cameras for people tracking,â€ IEEE Trans. Pattern Anal.
Machine Intell., vol. 28, no. 4, pp. 663â€“671, 2006.
[59] S. Calderara, R. Cucchiara, and A. Prati, â€œBayesian-competitive consistent labeling for people
surveillance,â€ Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 30, no. 2,
pp. 354â€“360, Feb. 2008.
[60] V. Kravtchenko, â€œTracking color objects in real time,â€ M.S. thesis, University of British Columbia,
Vancouver, British Columbia, Nov. 1999.
[61] Y. Deng and B. S. Manjunath, â€œUnsupervised segmentation of color-texture regions in images and
video,â€ IEEE Trans. Pattern Anal. Machine Intell., vol. 23, no. 8, pp. 800â€“810, 2001.
[62] D. Comaniciu, V. Ramesh, and P. Meer, â€œKernel-based object tracking,â€ IEEE Trans. Pattern
Anal. Machine Intell., vol. 25, no. 5, pp. 564â€“577, 2003.
[63] Tyng-Luh Liu and Hwann-Tzong Chen, â€œReal-time tracking using trust-region methods,â€ IEEE
Trans. Pattern Anal. Machine Intell., vol. 26, no. 3, pp. 397â€“402, 2004.
[64] A. Yilmaz, Li Xin, and M. Shah, â€œContour-based object tracking with occlusion handling in video
acquired using mobile cameras,â€ Transactions on Pattern Analysis and Machine Intelligence, vol.
26, no. 11, pp. 1531â€“1536, 2004.
[65] Manuel J. Lucena, Jose M. Fuertes, and Nicolas Perez de la Blanca, â€œReal-time tracking using
multiple target models,â€ in Pattern Recognition and Image Analysis, pp. 20â€“27. 2005.
[66] Kazuyuki Morioka, Xuchu Mao, and Hideki Hashimoto, â€œGlobal color model based object matching
in the multi-camera environment,â€ in IEEE/RSJ International Conference on Intelligent Robots
and Systems, 2006, pp. 2644â€“2649.
[67] C. Schmid and A. Zisserman, â€œAutomatic line matching across views,â€ in IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, 1997, pp. 666â€“671.
[68] Zhengyou Zhang, â€œDetermining the epipolar geometry and its uncertainty: A review,â€ International
Journal of Computer Vision, vol. 27, no. 2, pp. 161â€“195, 1998.
[69] F. Schaï¬€alitzky and A. Zisserman, â€œViewpoint invariant texture matching and wide baseline stereo,â€
in IEEE International Conference on Computer Vision, 2001, vol. 2, pp. 636â€“643 vol.2.
[70] Y. Wexler, A. W. Fitzgibbon, and A. Zisserman, â€œLearning epipolar geometry from image se-
quences,â€ in IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
2003, vol. 2, pp. IIâ€“209â€“16 vol.2.
98
[87] Y.-L. Tian, R. S. Feris, and A. Hampapur, â€œReal-time detection of abandoned and removed objects
in complex environments,â€ in The Eighth International Workshop on Visual Surveillance - VS2008,
2008.
[88] N. Bird, S. Atev, N. Caramelli, R. F. K. Martin, O. Masoud, and N. Papanikolopoulos, â€œReal time,
online detection of abandoned objects in public areas,â€ in Robotics and Automation, 2006. ICRA
2006. Proceedings 2006 IEEE International Conference on, 2006, pp. 3775â€“3780.
[89] S. Ferrando, G. Gera, and C. Regazzoni, â€œClassiï¬cation of unattended and stolen objects in video-
surveillance system,â€ in AVSS â€™06: Proceedings of the IEEE International Conference on Video
and Signal Based Surveillance, 2006, p. 21.
[90] M. D. Beynon, D. J. Van Hook, M. Seibert, A. Peacock, and D. Dudgeon, â€œDetecting abandoned
packages in a multi-camera video surveillance system,â€ Advanced Video and Signal Based Surveil-
lance, IEEE Conference on, vol. 0, pp. 221â€“228, 2003.
[91] X. Song F. Lv, B. Wu, V. K. Singh, and R. Nevatia, â€œLeft luggage detection using bayesian
inference,â€ in Proceedings of the 9th IEEE International Workshop on Performance Evaluation in
Tracking and Surveillance (PETS â€™06), 2006, pp. 83â€“90.
[92] J. Martinez del Rincon, J. E. Herrero-Jaraba, J. R. Gomez, and C. Orrite-Urunuela, â€œAutomatic
left luggage detection and tracking using multi-camera ukf,â€ in Proceedings of the 9th IEEE In-
ternational Workshop on Performance Evaluation in Tracking and Surveillance (PETS â€™06), 2006,
pp. 59â€“66.
[93] L. Li, R. Luo, W. Huang R. Ma, and K. Leman, â€œEvaluation of an ivs system for abandoned
object detection on pets 2006 datasets,â€ in Proceedings of the 9th IEEE International Workshop
on Performance Evaluation in Tracking and Surveillance (PETS â€™06), 2006, pp. 59â€“66.
[94] J. Zhou and J. Hoang, â€œReal time robust human detection and tracking system,â€ in CVPR
â€™05: Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPRâ€™05) - Workshops, 2005, p. 149.
[95] B. Wu and R. Nevatia, â€œDetection of multiple, partially occluded humans in a single image by
bayesian combination of edgelet part detectors,â€ Computer Vision, IEEE International Conference
on, vol. 1, pp. 90â€“97, 2005.
[96] B. Wu and R. Nevatia, â€œTracking of multiple, partially occluded humans based on static body
part detection,â€ in CVPR â€™06: Proceedings of the 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, 2006, pp. 951â€“958.
[97] E. Auvinet, E. Grossmann, C. Rougier, M. Dahmane, and J. Meunier, â€œLeft-luggage detection
using homographies and simple heuristics,â€ in Proceedings of the 9th IEEE International Workshop
on Performance Evaluation in Tracking and Surveillance (PETS â€™06), 2006, pp. 51â€“58.
[98] S. Guler, J. A. Silverstein, and I. H. Pushee, â€œStationary objects in multiple object tracking,â€
in AVSS â€™07: Proceedings of the 2007 IEEE Conference on Advanced Video and Signal Based
Surveillance, 2007, pp. 248â€“253.
[99] K. Smith, P. Quelhas, and D. Gatica-perez, â€œDetecting abandoned luggage items in a public space,â€
in Proceedings of the 9th IEEE International Workshop on Performance Evaluation in Tracking and
Surveillance (PETS â€™06), 2006, pp. 75â€“82.
100
I 
 
è¨ˆç•«ç›¸é—œåœ‹éš›æœƒè­°ï¥æ–‡ç™¼è¡¨: 
 
1. T.-H. Wang, J.-Y. Chang, and L.-G. Chen, "Algorithm and architecture for object tracking using particle 
filter," in Multimedia and Expo, 2009. ICME '09. IEEE International Conference on, 2009, pp. 1374-1377. 
2. C.-C. Liang, J.-Y. Chang, and L.-G. Chen, "Run-Processing: A Coherence-oriented Processing Method and 
its Hardware Architecture for Real-time Video Object Segmentation," in Robotics,Vision, Signal Processing, 
and Power Applications, 2009. RoViSP '09. International Conference on, 2009. 
3. H.-H. Liao, J.-Y. Chang, and L.-G. Chen, "A Localized Approach to Abandoned Luggage 
Detection with Foreground-Mask Sampling," in Advanced Video and Signal Based Surveillance, 2008. AVSS 
'08. IEEE International Conference on, 2008, pp. 132-139. 
4. J.-Y. Chang, T.-H. Wang, S.-Y. Chien, and L.-G. Chen, "Spatial-temporal consistent labeling for 
multi-camera multi-object surveillance systems," in Circuits and Systems, 2008. ISCAS '08. IEEE 
International Symposium on, 2008, pp. 3530-3533. 
 
3. PARTICLE FILTER TRACKING ALGORITHM
This section gives the formal deï¬nition of the particle ï¬lter algo-
rithm, describes the color feature we used, and illustrates the pro-
posed dynamic model.
3.1. Basic Theory
For Bayesian tracking framework, the goal is to estimate the state
xt in each time t using current and previous observations, z1:t, and
previous state xtâˆ’1. There are two models - a motion model and an
observation model, describing how the system evolves along with
time. A motion model xt+1 = ft(xt,vt) describes how states
evolve with time, where vt is the noise introduced to the motion
model. An observation model zt = ht(xt,nt) describes the rela-
tion between current state xt and its observation zt, where nt is the
noise introduced in the observation.
The particle ï¬lter algorithm works by approximating the pos-
terior distribution with a set of particles, namely p(xt|z1:t) â‰ˆP
i
witÎ´(xâˆ’ x
i
t) , where wit is the weight of particle i at time t.
3.2. Color-based Particle Filtering
Color histograms are often used as a feature for many vision-based
applications. Here we adopt the spatial-weighted color histogram as
in [1]. Each pixel contributes its color weight according to its norm
to the center of the object as
qË†u = C
nX
i=1
k(||x||2)Î´k[b(x)âˆ’ u] (1)
where C is the normalization constant, Î´k(Â·) is the Kronecker delta
function, and k(Â·) is the chosen kernel function. For our case, we
use Gaussian kernel as k(x) = exp(âˆ’x2/2) to compute our color
histogram. We use the L1 distance as the similarity metric between
two color histograms as the L1 distance gives good results.
3.3. Second Order Dynamic Model
The constant velocity assumption used in most particle ï¬lters is not
always followed in real cases. Instead of simply estimating the ve-
locity from the position shift, we additionally estimate the accel-
eration of the object. In the following discussion, pt = {x, y}
stands for the position, qt = {vx, vy} stands for the velocity, and
rt = {ax, ay} stands for the acceleration of the target object at time
t. From the recursive form of the equal-acceleration equations we
obtain (
fpt = ptâˆ’1 + qtâˆ’1 + 12rtâˆ’1
eqt = qtâˆ’1 + rtâˆ’1 (2)
wherefpt, eqt are predicted from previous ptâˆ’1, qtâˆ’1, and rtâˆ’1.
When we obtain the estimated position pt after observation, we
need to estimate qt and rt at this frame. By solving the dual equation
in equation (2), we obtain
(
qt =
2
3
[(pt âˆ’ ptâˆ’1) +
1
2
qtâˆ’1]
rt =
2
3
[(pt âˆ’ ptâˆ’1)âˆ’ qtâˆ’1]
(3)
The equation (3) is our second order dynamic model. Note that in
[1], they also proposed a second-order dynamic model. Their model
coefï¬cients are obtained by training or manual initialization, while
in our method they are embedded in the state estimation and auto-
matically learned.
Fig. 2. Main processing diagram.
3.4. The Whole Algorithm
In our current implementation, the state variable consists of xt =
{x, y, vx, vy, ax, ay}. After manual initialization is done, at each
time step, the tracking operations are processed, as shown in ï¬gure
2.
In the propagation stage, each particle is propagated by equation
(2) mentioned above. Then we add some Gaussian noises to perturb
particle states. In the observation stage, each particle constructs its
own color histogram as described in section 3.2, spanning the size
and angle of the current tracked object. Note that we must normalize
the individual weights in each histogram bin before computing the
L1 distance.
In particle weighting stage, we compute the weight of each par-
ticle according to the following formula:
wi = exp(
âˆ’d2i
2Ïƒ2
) (4)
where i is the particle index, di is theL1 distance calculated between
the color histogram of the target and the color histogram of particle i,
and Ïƒ is to model the variance of histograms. To estimate actual state
after all weights are known, we implement the maximum a posteriori
(MAP) and mean estimation. The MAP estimation is given by
xt = argmaxxi
t
p(xt|z1:t,x
i
t) (5)
where xit is the state presented by particle index i at current time t.
In mean estimation, the ï¬nal state is simply the weighted sum of all
particles. We must resample the particles to avoid the degeneracy
problem. Here we implement the resampling algorithm as in [7].
Optionally we update the size and angle of the tracked object
after above steps. To do this, we use another 9 particles to estimate
objectâ€™s size by the MAP criterion. The selected 9 candidate sizes
are obtained by [Wâˆ’Î”W,W,W+Î”W ]Ã—[Hâˆ’Î”H,H,H+Î”H],
whereÎ”W andÎ”H are the parameters to model the increase of the
object size. To estimate objectâ€™s angle, we quantize the angle into
32 entries spanning evenly over [0, Ï€). For each angle candidate,
we apply the color histogram calculation and select the ï¬nal angle
with minimum distance. After all these steps are done, the algorithm
proceeds with another frame.
Note that in our current implementation, we separate the estima-
tion of objectâ€™s size and angle from its position and velocity. The
reason is that in typical tracking scenario, objectâ€™s position is the
most important target to the tracking system.
4. HARDWARE ARCHITECTURE DESIGN
Particle ï¬lter algorithms generally require high computation cost.
In our software implementation, we can only achieve 8 frames per
1375
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:48 from IEEE Xplore.  Restrictions apply. 
(a) Frame 28. (b) Frame 48.
Fig. 4. Example of tracking results.
Table 1. Tracking Accuracy
Category Success Targets Total Targets Success Rate
Category One 11 13 84.6%
Catetory Two 9 10 90.0%
Category Success Frames Total Frames Success Rate
Category One 1022 1277 80.0%
Catetory Two 1536 1727 88.9%
5.2. Architecture Evaluation
We adopt the 4-parallel scheme with single port SRAM as the im-
plementation. To demonstrate the effectiveness of the hardware ar-
chitecture, we implement the architecture in Verilog hardware de-
scription language and synthesize at 5ns using UMC 90nm Logic
SP-RVT Process. The synthesis result is shown in table 2, which
consists of the area and gate count of several modules as in ï¬gure
3(a). Table 3 gives the simulation result of the proposed architec-
ture. The FPS is normalized according to different object sizes in
the sequences. By carefully choosing the parallelization parameter
P and effective pipelining in implementation, real-time performance
can be achieved.
6. CONCLUSION
In this paper, we propose an algorithm and architecture for color-
based particle ï¬lter tracking. The proposed algorithm can estimate
objectsâ€™ positions, sizes and angles while tracking. The second or-
der dynamic model gives more accurate results than constant speed
dynamic model in our case, and the tracking accuracy is 85% in av-
erage. The proposed architecture can operate at 31.35 frames per
second in average, achieving real-time performance of color-based
particle ï¬lter. Different parallelization schemes can be chosen ac-
cording to the hardware resource and timing requirement.
Table 2. Synthesis Results
Module Name Cell Area (um2) Gate Count
Current Position Generator 4498 1125
Position Filter 31017 7754
Spatial Weight PE 21704 5426
Spatial Weight Normalizer 20733 5183
L1 Distance Calculator 3277 819
(a) Initialization. (b) Constant speed model.
(c) Proposed model. (d) Frame 131.
Fig. 5. Example of tracking results.
Table 3. Architecture Evaluation Results
Sequence Name Cycles/Frame Normalized FPS
move 67973 30.81
climb 127334 31.43
touch 100051 31.82
7. REFERENCES
[1] Patrick PeÂ´rez, Carine Hue, Jaco Vermaak, and Michel Gangnet, â€œColor-
based probabilistic tracking,â€ ECCV â€™02: Proceedings of the 7th Euro-
pean Conference on Computer Vision-Part I, pp. 661â€“675, 2002.
[2] M. Isard and A. Blake, â€œCondensation â€” conditional density propaga-
tion for visual tracking,â€ IJCV, vol. 29, pp. 5â€“28, 1998.
[3] A.C. Sankaranarayanan, R. Chellappa, and A. Srivastava, â€œAlgorith-
mic and architectural design methodology for particle ï¬lters in hard-
ware,â€ Computer Design: VLSI in Computers and Processors, 2005.
ICCD 2005. Proceedings. 2005 IEEE International Conference on, pp.
275â€“280, Oct. 2005.
[4] Shaohua Hong, Zhiguo Shi, and Kangsheng Chen, â€œCompact resam-
pling algorithm and hardware architecture for paticle ï¬lters,â€ Communi-
cations, Circuits and Systems, 2008. ICCCAS 2008. International Con-
ference on, pp. 886â€“890, May 2008.
[5] J. Alarcon, R. Salvador, F. Moreno, P. Cobos, and I. Lopez, â€œA new real-
time hardware architecture for road line tracking using a particle ï¬lter,â€
IEEE Industrial Electronics, IECON 2006 - 32nd Annual Conference on,
pp. 736â€“741, Nov. 2006.
[6] H. Medeiros, J. Park, and A. Kak, â€œA parallel color-based particle ï¬lter
for object tracking,â€ Computer Vision and Pattern Recognition Work-
shops, 2008. CVPRW â€™08. IEEE Computer Society Conference on, pp.
1â€“8, June 2008.
[7] M.S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, â€œA tutorial
on particle ï¬lters for online nonlinear/non-gaussian bayesian tracking,â€
Signal Processing, IEEE Transactions on, vol. 50, no. 2, pp. 174â€“188,
Feb 2002.
1377
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:48 from IEEE Xplore.  Restrictions apply. 
memory actualize high-resolution binary morphology,  the 
computation time is still limited by video resolution due to 
its pixel-by-pixel processing nature, i.e., computation cycle 
proportionally rises with video resolution. For example, 
performing binary morphology on a D1 (720x480) video 
requires about 345,600 cycles, even though the image is 
actually empty. 
Various hardware designs for CCL has been reported.  A 
classical approach is proposed by Rosenfeld and Pfaltz[7], It 
employed a two-path algorithm and a global equivalence 
table, which is later improved by Lumia et al. by storing 
equivalence table of a single-row[21].  Chien et al. 
introduced a sub-word concept which processes four pixels 
at a time to fully-utilize bus-bandwidth[10].   Similar 
multi-pixel solution is also found in [8]. A block of 2x2 
pixels is processed simultaneously and corresponding type 
of possible equivalences are analyzed to ensure its memory 
access reduction scheme.  However, increasing the size of 
block implies more complicate connectivity conditions, 
preventing the solution from higher-resolution extension. 
Moreover, pixel-by-pixel scan restrains further acceleration. 
Considering segmentation as a prior operation of intelligent 
video processing system, a better processing technique 
should have the following features: 
1. It is somehow aware of video content rather than 
blind operation and ensures efficient computation, 
thus reserving the hardware resource for other 
essential processing. 
2. It is memory-efficient and consumes bus-bandwidth 
to a limited extent. 
3. Support numerous general functions in video object 
segmentation. 
In this paper, a run-processing method is proposed to support 
high-resolution real-time video object segmentation.  
Making use of coherence property of binary masks, 
run-processing possesses content-awareness and reduces 
memory usage by 67%. Binary morphological operations, 
connected components labeling, seeded region growing, 
bitwise operation, and random-search operations are 
accelerated. Furthermore, run-processing is suitable for 
hardware implementation. Simulation shows that based on 
the proposed hardware architecture, a shadow-cancellation 
video object segmentation algorithm achieves an average 
speed of 151 frames per second at D1 (720x480) resolution. 
The structure of this paper is as follows: The succeeding 
section illustrates run-coding this paper adopts.  Next, 
Run-processing Functions section describes the methods of 
each operation with run-coding.  Then, the proposed 
architecture are demonstrated. Finally, experiment results 
are shown and conclusion is followed. 
Run-coding 
Coherence Property 
Since foreground (FG) binary mask attempts to store the 
existence of active object, â€˜1â€™ pixels (pixels at where a object 
is found) of a object tends to be spatially connected into a 
closed region. Identical characteristic applies to â€˜0â€™ pixels.  
This is called the coherence property of foreground binary 
masks. 
To achieve both coded-domain computation capability and 
storage reduction, row-based Run-coding is proposed. 
Run-coding Format 
Binary runs are of the form (X,Yb), which represents a series 
of pixel labeled as Yb starting from address X.  Fig.1 shows 
the run coded data of a binary mask.  The rows in a frame are 
then separately encoded into several runs.  The number of 
runs depends on the mask content.  (Nw,0) indicates the â€˜endâ€™ 
run of a row.  Where Nw is the length of a row; in this 
example Nw=25.  The encoding processing can be done by 
single raster scan. Moreover, independence between rows 
allows further acceleration through processing multiple rows 
in parallel. 
 
Figure 1 â€“ Example of Run-coding (a)Pixel representation 
(b) Run-coded format 
X is log2Nw bits and Yb is 1 bit. Therefore, each run is 
(log2Nw+1) bits long and the storage requirement of a binary 
mask equals Nrun x (log2Nw+1). Nrun is the number of total 
runs in a frame.  For example, run-coded representation for 
720x480 binary masks of Fig.2(a) takes 1,858 runs. During 
segmentation process, erroneous patterns usually corrupt the 
masks, leading to increased run numbers.  Noisy masks like 
Fig.2(b) contains 4,636 runs, which is equivalent to  50.9Kb 
storage.  Even so, run-coding still achieves 85.3% storage 
reduction compared with 345.6Kb, the pixel-based storage 
size of a D1 image.  Experiments have shown an average of 
78% storage reduction in segmentation processes.  
 
Figure 2 â€“ General binary mask (a)refined binary mask 
(b)noisy binary mask 
isolated â€˜1â€™ runs in (i+1)th row. Go to step 3. 
3. Scan local equivalence table for each non â€˜0â€™ runs in 
(i+1)th row; change run labels Gj into Kj. <Gj,Kj> is 
equivalent units in equivalence table.  If (i+1)th row 
is the last row, go to Bottom-up Process, otherwise i 
= i+1 and return to step 2. 
Bottom-up Process:  
1. Scan the ith and (i+1)th rows. Push <G,K> into 
equivalence table and assign G label to the upper-row 
run if lower-row G label run encounters a upper-row 
K label run; Go to step 2. 
2. Scan local equivalence table for each non â€˜0â€™ runs in 
ith row; change run labels Gj into Kj.  If ith row is the 
first row, end the process, otherwise i = i-1 and return 
to step 1. 
The adopted CCL algorithm also contributes to 
bus-bandwidth reduction, which is later explained in 
Hardware Architecture section. 
Full-frame Bitwise Operations 
Recall that in binary morphology Inter-row process,  a â€˜0â€™ 
run exists only if all of the corresponding B rows is â€˜0â€™ at the 
address.  This is in essence the â€œORâ€ operation.  
Programmable input/output label inversion of Inter-row 
process allows all of the bitwise combinations such as 
And/Or/Nand/Nor/(MaskA & MaskB & ~MaskC).  For B 
input binary masks, 2B+1 modes are supported.  This implies 
a reusable hardware module. 
Seeded Region Growing (SRG) 
SRG is done utilizing CCL and seed insertion.  First a binary 
mask to be seeded is labeled by CCL. Then the seed mask is 
inserted into the label image, i.e., runs encountered with seed 
are labeled as â€˜seedâ€™ label (a fixed new label number). After 
the insertion, a confined CCL which only propagates â€˜seedâ€™ 
label is executed.  Finally unwanted regions are erased by 
label extraction and a binary mask of seeded region is 
obtained. 
Other Functions 
Other functions such as boundary extraction, bounding box 
extraction and random search can be done in similar 
procedures, which is out of the scope of this paper. 
Hardware Architecture of Run Processor 
The architecture of Run Processor is shown in Fig.5.  
Processing Element (PE) is implemented by two 
multi-functional modules because of the highly correlation 
between functions.  Instruction Decoder module decodes the 
input instructions and assigns the process.  External Memory 
Allocation module stores external starting address of data 
frames and currently available address of external memory 
data banks. 
 
 
Figure 5 â€“ Architecture of Run Processor 
Run Data I/O 
Process Control module manages Row-to-PE linker 
according to function requirements.  Each of the 10 Row 
-storage can buffer Npagerun runs. Npagerun is set to 11.  The 
Row-storage serves as temporary buffer for read and write 
commands.  For instance, Inter-row morphology uses one 
row as output buffer and the others as input row buffer.  
After Inter-row morphology completes a row, output buffer 
is written out to internal storage and external storage and the 
next row of the B rows should be read in for the process.  In 
addition, the other rows should be rewind to the first run.  
Process Control module requests a read-in command to the 
new row and sends B-1 rewind commands to the other rows.  
Memory Access Unit reads data from external memory to 
the corresponding internal SRAM bank.  Fig.6 shows the 
internal SRAM and Row-storage structure. Each bank of 
internal SRAM is comprises 4 pages. The number is 
empirically chosen for the reuse scheme.  The first page of 
read data is then transferred to the new row buffer, which 
completes the data preparation for next process.   
If the row consists of more runs than row bank can hold, the 
address to the succeeding runs is kept in External Memory 
Allocation module for the next read-in. Using 10 Row 
Sub-storage, the maximum SE size of a single morphology is 
(Nw x 9). 
Internal Data Reuse Scheme 
Data are reused in two phasesâ€”page-reuse phase and 
bank-reuse phase.  During each operation, Process Control 
requests mostly rewind than read-in command because of 
the access pattern of windows-based functions.  If a 
Row-storage did not request a page in previous operation, 
the data in Row-storage is still the first page of the row, thus 
alleviating an internal read-in for rewind command.  
Likewise, page-reuse phase works if the row bank is not 
overwritten by further read-in, rewinding the row can be 
done simply by internal read-in instead of external memory 
access.  Furthermore, temporary data such as local 
equivalence table are not written out to external memory if 
possible.  42% reduction external transmission is reduced by 
the data-reuse scheme. 
binary masks of video object segmentation is proposed.  
Multiple widely-used functions such as binary morphology, 
connected component labeling and seeded region growing is 
supported.  A shadow-cancellation video object 
segmentation algorithm run by the presented hardware 
implementation achieves 151 fps for D1 (720x480) video. 
The presented hardware is shown to be more cost-effective 
than existing architectures. 
References 
[1]  K. I. Diamantaras, S. Y. Kung 1997. A linear systolic 
array for real-time morphological image processingIn 
Journal of VLSI Signal Processing: 43 â€“ 55. 
[2] E. N.  Malamas, A. G. Malamos, T. A.Varvarigou 2000.   
Fast implementation  of  binary  morphological  
operations  on  hardware-efficient  systolic  architectures.  
In Journal of VLSI Signal Processing: 79 â€“ 83. 
[3] S.-Y. Chien, B.-Y.  Hsieh, Y.-W. Huang, S.-Y. Ma, L.-G. 
Chen 2006. Hybrid morphology processing unit 
architecture for moving object segmentation systems. In  
Journal of VLSI Signal Processing: 275-278. 
[4] Hugo Hedberg, Fredrik Kristensen, Viktor Ã–wall 2008. 
Low-Complexity Binary Morphology Architectures 
With Flat Rectangular Structuring Elements. In IEEE 
Transactions on Circuits and Systems I, 2216-2225. 
[5] S. Fejes, F. Vajda 1994. Data-driven algorithm and 
systolic architecture for image morphology. In IEEE 
International Conference of Image Processing, 550-554. 
[6] J. Velten and A. Kummert 2002. FPGA-based 
implementation of variable sized structuring elements for 
2-D binary morphological operations.  In Proceedings of 
1st IEEE International Workshop Electronic Design, 
Test, Application, 309â€“312. 
[7] A. Rosenfeld and J. L. Pfaltz. 1966. Sequential 
Operations in Digital Picture Processing. In Journal of 
the ACM (JACM) archive. Volume 13, Issue 4. 
[8] Flatt, H., Blume, S., Hesselbarth, S., Schunemann, T., 
Pirsch, P. 2008. A Parallel Hardware Architecture for 
Connected Component Labeling Based on Fast Label 
Merging. In International Conference on 
Application-Specific Systems, Architectures and 
Processors, 144-149. 
[9] Appiah, K., Hunter, A., Dickinson, P., Owens, J. 2008. A 
Run-Length Based Connected Component Algorithm for 
FPGA Implementation. In International Conference on 
Field-Programmable Technology, 177-184. 
[10] Wei-Kai Chan, Shao-Yi Chien 2006. Subword 
Parallel Architecture for Connected Component 
Labeling and Morphological Operations. In IEEE Asia 
Pacific Conference on Circuits and Systems, 936-939. 
[11] C. J. Nicol 1995. A Systolic Approach for Real 
Time Connected Component Labeling â€œ, In Source, 
Computer Vision and Image Understanding 
archive. ,Volume 61, Issue 1. 
[12] R. M. Haralick 1981. Real time Parallel 
Computing Image Analysis, Mass: Plenum Pub Corp. 
[13] J. Park, C. G. Looney, H. Chen 2000. Fast 
Connected Component Labeling Algorithm Using A 
Divide and Conquer Technique, Technical report. 
[14] A. Rosenfeld and J. Pfaltz. 1966. Sequential 
operations in digital picture processing.  In Journal of the 
ACM, 241â€“242. 
[15] Yang, X.D. 1988. Design of Fast Connected 
Component hardware. In  Computer Society Conference 
on Computer Vision and Pattern Recognition, 937 â€“ 944. 
[16] Schwartz, J.T., Sharir, M.,  Siegel, A., 1985. An 
Efficient Algorithm for Finding Connected  Components 
in a Binary Image.  Technical Report No.  154, Courant 
Institute, NYU. 
[17] Thomas M. Breuel 2008. Binary Morphology and 
Related Operations on Run-Length Representations. In 
Proceedings VISAPP. 
[18] Lumia, R.,  Shapiro, L.,  Zuniga, O. 1983. A  New  
Connected  Components  Algorithm for  Virtual Memory 
Computers.   In IEEE Transactions on Computer Vision, 
Graphics, and Image Processing, 22, 287-300 
[19] N. Ranganathan,  R. Mehrotra,  S. Subramanian 
1995.  A high speed systolic architecture for labeling 
connected components in an image. In IEEE Trans. Syst., 
vol. 25, 415 â€“ 423. 
[20] Hugo Hedberg,  Petr Dokladal, Viktor Ã–wall 
2009. Binary Morphology With Spatially Variant 
Structuring Elements: Algorithm and Architecture. In 
IEEE Transactions on Image Processing, 562- 572. 
[21]  Parks, D.H., Fels, S.S. 2008. Evaluation of 
Background Subtraction Algorithms with 
Post-Processing. In International Conference on 
Advanced Video and Signal Based Surveillance, 192 - 
199  
Digital Object Identifier 10.1109/AVSS.2008.19 
[22] Dong Xu, Jianzhuang Liu, Zhengkai Liu, Xiaoou 
2004. Tang Indoor shadow detection for video 
segmentation. In IEEE International Conference on 
Multimedia and Expo, 41-44, Vol.1. 
background subtraction [6], [7] and [8], and those that 
donâ€™t [1] and [2]. Background subtraction works 
reasonably well when the camera is stationary and the 
change in ambient lighting is gradual, if at all. For those 
that do without background subtraction, a set of 
discriminative features of the objects of interest has to be 
learned beforehand through machine learning algorithms 
in order to be able to detect these objects in subsequent 
stages. 
The majority of existing event detection methods 
incorporates tracking algorithm of some form in their 
system, as in [2], [3], [6] and [7]. In most cases tracking is 
performed on all detected moving objects or blobs in the 
foreground. However, due to occlusion and a fixed camera 
angle, this kind of comprehensive tracking often results in 
errors such as identity switch (when two nearing objects 
switch their identities), which is difficult to avoid and can 
be seen in many PETS 2006 demonstration sequences 
such as those in [5]. 
In most cases surveyed, the final stage of determining 
whether an alarm should be issued is done in a 
deterministic fashion. In a deterministic system an event is 
declared to have occurred if some criteria are satisfied. A 
minority employs a probabilistic framework [3] to model 
events, in which case an event is deemed to have occurred 
if its confidence score exceeds a certain threshold. The 
probabilistic approach gives users more flexibility to set 
thresholds and hence system sensitivity, as well as a better 
understanding of how real the situation might actually be. 
1.2. Outline of our approach 
Our proposed approach employs a novel technique, 
which we shall refer to as foreground-mask sampling, to 
localize the candidates of abandoned luggage items in the 
scene. As the first stage of our system, the foreground-
mask sampling technique computes the intersection of a 
number of background-subtracted frames which are 
sampled over a period of time. Abandoned luggage items 
are assumed to be static foreground objects and therefore 
will show up in this intersection. Since our approach 
requires no prior learning of luggage appearance in any 
form, we can successfully localize luggage of all shapes, 
sizes, orientations, viewing angles and colors with no need 
for and no constraints from any training data. Once a 
suspicious luggage item is identified and localized, our 
algorithm attempts to search for its owner within a 
neighborhood around the detected luggage. If the owner is 
found within this neighborhood, the luggage is assumed to 
be attended by its owner and no further processing is 
required. 
However, if no owner is found in proximity to the 
luggage in this present frame at time t, our tracking 
algorithm then goes back in time (for a pre-defined length 
of Î”t seconds) to the frame at time t â€“ Î”t when the owner 
was still attending the luggage, and it starts tracking the 
owner from then. The tracking algorithm utilizes motion 
prediction in conjunction with (1) skin color information 
and (2) an improved version of generalized Hough 
Transform on human body contour as feature. Rather than 
comprehensively tracking all moving foreground objects, 
which is normally done in most existing event detection 
systems, we track only the owner of the suspicious 
luggage item which has been localized in the first stage; 
other irrelevant moving objects in the foreground are 
simply ignored. We call our method selective tracking, as 
opposed to conventional comprehensive tracking.  
The tracking module provides a trajectory of the 
luggage owner from frame t â€“ Î”t to frame t, and this 
information is used for probabilistic reasoning in the third 
stage. For the luggage to become abandoned, its owner has 
to leave the scene without it, or it has to remain unattended 
for at least 30 consecutive seconds. A probability score 
will be given by the tracking algorithm to represent the 
reliability of the ownerâ€™s tracked trajectory, and this 
probability score is used in the subsequent evaluation of 
the overall confidence score of the luggage-abandonment 
event. The event detection is formulated as a Maximum A 
Posteriori (MAP) problem. Finally an alarm will be 
Owner left scene or 
more than 30 seconds
Within 30 seconds 
Owner not found in 
present frame 
Owner 
found
Static object is luggage 
Foreground static object is human 
Input 
video 
frames 
Foreground
-mask 
sampling 
Discard track
Search for owner in neighborhood
Discard 
track 
Back-tracking for 
past 60 seconds Discard track 
Luggage abandoned; alarm 
triggered 
Figure 1. Flow chart of our proposed approach 
 
1303
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:50 from IEEE Xplore.  Restrictions apply. 
 
Binarization allows the intersection to be taken through 
simple point-wise multiplication of the 6 foreground 
masks, as indicated by the operator â€˜.*â€™. Filtering 
operation is then carried out on this static foreground 
object mask S to remove irrelevant and sporadic noisy 
pixels, and connected component analysis is performed 
afterwards. A white (valued 1) block on the static 
foreground object mask S indicates a region that has 
remained foreground in all of the 6 sample frames over the 
past 30-second period, and therefore this region should 
very likely correspond to either a static abandoned luggage 
item or a non-moving human being. Our tracking module, 
which is to be detailed in the next section, will then 
analyze the region and further localize it if it is determined 
to be a static luggage item, see Figure 3. 
The static foreground object mask S thus obtained by 
the foreground-mask sampling gives possible candidates 
for abandoned luggage items. The approach is elegant and 
robust in that it manipulates directly such low-level 
features as foreground and background image pixels. This 
provides us with a localized target and allows us to focus 
on a localized search region for later tracking and higher-
level event reasoning.  
3. Selective tracking module 
With the static foreground object mask S obtained, our 
system has localized information on where the suspicious 
objects are in the scene. It should also be pointed out that 
here we assume all static foreground objects to be either 
human or luggage item. For each white region (valued 1) 
in the static foreground object mask S, our algorithm 
checks if it is a human or luggage (i.e. not human) by a 
combination of skin color information and human body 
contour that shall be explained in detail shortly. If the 
region is determined to be a human, it is discarded because 
what we are looking for is abandoned luggage items. If it 
is a luggage item, a local search region is constructed 
around the detected luggageâ€™s neighborhood to see if its 
owner is in close proximity in this present frame at time t. 
If the owner is found, the region is again discarded 
because the owner exhibits no present intention of 
abandoning the luggage. If, however, the owner is not 
found around the luggage in the present frame, our 
algorithm then goes back in time for a pre-defined Î”t 
seconds, 60 in this case, to the frame at time t â€“ Î”t when 
the owner was still attending the luggage and starts 
tracking the owner from here (at time t â€“ Î”t). The tracking 
algorithm again employs skin color information and 
human body contour as features. 
Because suspicious luggage has already been localized 
by foreground-mask sampling in the first place, we are 
able to perform tracking solely and selectively on the 
owner of this static luggage item. This mechanism closely 
mimics the human ability to notice and track only the 
object that is of interest to us even under a highly cluttered 
background, for example humansâ€™ natural ability to 
identify familiar faces in such crowded space as an airport 
pick-up area. Our ability to track only the object we are 
interested in also reduces the risk of identity switch that is 
difficult to avoid if tracking is performed on a 
comprehensive, full-frame scale. 
The details on the implementation of detection and 
tracking using skin color information and human body 
contour are described below, as well as their integration 
into the motion prediction part of the tracking module.   
3.1. Cr color channel with human skin 
Human skin signal response is significantly larger in the 
YCbCr color space than in the commonly used RGB color 
space. Due to a large amount of blood flow, human skin 
gives high response to the Cr channel in YCbCr space, 
irrespective of skin color or race [4]. We propose to utilize 
skin color as given by the Cr channel for human face 
localization because in situations of severe occlusion 
(crowded scenes with people overlapping one another), 
human face is the most visible body part under a typical 
surveillance camera with a tilting angle looking down 
from top. 
A search region is first constructed around the 
Figure 3. Foreground-mask sampling. The first row shows input frames; the second row shows corresponding foreground images. 
Image on right is the intersection of the 6 foreground images sampled over a period of 30 seconds, which contains the abandoned 
luggage item. AVSS 2007 video dataset. 
1325
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:50 from IEEE Xplore.  Restrictions apply. 
(m+1)th bin with a width of Î”m = 5
access a total of 11 (=1+2Î”m) bins cent
bin with their respective weights giv
distribution g, where g = 1 for the ce
before and 0 < g < 1 for other 10
decreasing with respect to distance fr
these other 10 bins, (xCâ€™, yCâ€™) is also co
Î±) pair under these bins and the corresp
the detection map is incremented b
weight g (smaller than the center weigh
6 below for an illustration.  
The reason for making this modifica
angle that is computed from an edge 
edge image, there is an inherent e
quantization and angle quantization, an
obtained is at best indicative only o
neighboring angles. We model this ra
Gaussian-weighting system on a range 
range being specified by Î”m = 5. A
angle to vary within a limited rang
handle human head-shoulder contours 
of alignment with perfectly frontal imag
Following the above procedure, onc
the input edge image have been trave
center point of the contour of interes
image will correspond to a local maxim
map. See Figure 7 for the detection re
improved version of Hough Transfo
with the original implementation of 
introduced in [10] and a simple norm
Figure 6. The origin point of the two red li
m degrees; it corresponds to the red, 
contains two (r, Î±) pairs. Solid lines a
while dashed lines indicate noise. As sho
on the head-shoulder contour in the 
converge to a local maximum on the de
right. At the bottom is the 180-bin referen
weighting g is shown below it with the c
red and neighboring 10 bins in blue.  
. Specifically, we 
ered on the (m+1)th 
en by a Gaussian 
nter (m+1)th bin as 
 neighboring bins, 
om center bin. For 
mputed for each (r, 
onding location on 
y the binâ€™s given 
t of 1). See Figure 
tion is that for a Ïˆ 
point on the input 
rror due to pixel 
d thus the Ïˆ angle 
f a small range of 
nge by applying a 
of Ïˆ-angle bins, the 
lso, by allowing Ïˆ 
e, our system can 
that are slightly out 
e. 
e all edge points on 
rsed, the supposed 
t in the input edge 
um in the detection 
sults given by our 
rm, in comparison 
Hough Transform 
alized correlation 
technique for feature detection
shown to be superior to the l
contour detection operation is s
search region, interference fro
reduced to a minimum. The gen
Transform that we use here eff
contour of our choice (head-s
case) to a large-valued point (or
bright points due to pixel quanti
3.3. Integration into motion
For detection of luggage ow
color information from Cr ch
contour information from our im
algorithm are combined to pin-
the owner. 
To further exploit the temp
successive frames, motion 
Prediction of the ownerâ€™s loca
based on its location in the cur
frames with exponentially-deca
if we denote r(t) as the position 
t, the prediction for time t+1 can
 
r(t+1) = r(t) + â–³r                   
 
where â–³r is generated recursiv
and is given by 
 
â–³r = Î± â–³r + Î²( r(t) â€“ r(t-1) )  
 
Î± + Î² = 1.                               
 
nes has a Ïˆ angle of 
(m+1)th, bin which 
re correct matches, 
wn, different points 
input edge image 
tection map on the 
ce table; a Gaussian 
enter bin labeled in 
Figure 7. Upper row: (left) input v
edge image by 3x3 sobel convolu
HT detection map superimposed
implementation. Lower row: (left)
result with our improved implemen
response at the correct location
shoulder contour in the input edg
result with original implementatio
false positives; (right) simple im
correlation between the input edge
which gives the most false positive
. Our improved method is 
atter two. And since this 
olely performed within the 
m irrelevant contours is 
eralized version of Hough 
ectively maps an arbitrary 
houlder silhouette in this 
 a small region of scattered 
zation).  
 prediction 
ner in a single frame, the 
annel and the upper-body 
proved Hough Transform 
point the head location of 
oral relationship between 
prediction is employed. 
tion in the next frame is 
rent frame and in the past 
ying weights. Specifically, 
vector of the owner at time 
 be formulated as 
                                   (4) 
ely by motion prediction 
 and                            (5) 
                                   (6) 
ideo frame; (middle) the input 
tion; (right) input frame with 
 on it using our improved 
 Hough Transform detection 
tation, which gives one large 
 of the most visible head-
e image; (middle) detection 
n of HT, which gives more 
plementation of normalized 
 image and contour template, 
s, as expected. 
1347
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:50 from IEEE Xplore.  Restrictions apply. 
sequences recorded with different difficulty levels: easy, 
medium and hard. The easy sequence contains objects of 
larger appearance, activities which are closer to the 
camera and less scene clutter; as the difficulty level rises, 
objects become smaller and clutter more serious. Our 
proposed 3-stage approach has successfully detected the 
abandoned luggage in all three sequences from the AVSS 
2007 dataset. We have been able to track the owner in the 
easy sequence all the way until he leaves the scene without 
the luggage, hence resulting in an alarm event. In the 
medium and hard sequences, however, the owner passes 
behind a large pillar before leaving the scene without the 
luggage, and therefore is occluded for about 1.5 seconds, 
which translates into around 40 frames under a frame rate 
of 25 fps. Our tracking engine has not been able to follow 
the owner through the occlusion and the owner is deemed 
to be lost; therefore alarms are also triggered for these two 
sequences. The PETS 2006 datasets contains seven 
sequences. In video 1, 2, 4, 5 and 6 the luggage owner 
leaves the scene without the luggage, and our method has 
successfully issued an alarm in all these 5 cases while 
tracking the owner all the way until the owner is no longer 
within camera view.  In video 3 the owner stays with the 
luggage all the time, and therefore no alarm is issued. In 
video 7, the owner wanders about for some time before 
finally leaving the scene without luggage; the trajectory of 
the highly-maneuvering owner, however, contains too 
many abrupt changes in speed and direction for our 
present motion prediction algorithm to successfully 
follow. The owner is lost 34 seconds after he leaves the 
luggage, while an alarm is triggered at 30 seconds. In 
Figure 8, some labeled scene shots are provided.  
6. Conclusion and future work 
In this paper, we have proposed a novel approach to 
left-luggage detection in surveillance video. Through the 
use of foreground-mask sampling, we are able to emulate 
the human vision capability of localizing and focusing on 
solely the object of interest to us, while filtering out all 
other irrelevant, interfering agents. We are therefore able 
to apply tracking in a selective, more localized manner. 
We have also proposed an improved implementation of 
the Hough Transform for detecting the human upper-body 
contour from the video frames. And we have incorporated 
a probabilistic framework and employed the MAP 
principle in our modeling of the luggage-abandonment 
event and subsequent reasoning. In the future, we plan to 
extend our proposed approach to a multi-camera network 
where coordination of an array of cameras will allow cues 
to be gathered from multiple views and information to be 
relayed from one to another.   
7. References 
[1] B. Wu and R. Nevatia, â€œDetection of Multiple, Partially 
Occluded Humans in a Static Image by Bayesian 
Combination of Edgelet Part Detectorsâ€, ICCV 2005, IEEE, 
Vol I: 90-97  
[2] B. Wu and R. Nevatia, â€œTracking of Multiple, Partially 
Occluded Humans based on Static Body Part Detectionâ€, 
CVPR 2006, IEEE, Vol I: 951-958  
[3] F. LV, X. Song, B. Wu, V. K. Singh, R. Nevatia, â€œLeft-
Luggage Detection using Bayesian Inferenceâ€, 9th PETS, 
CVPR 2006, IEEE, pp. 83-90  
[4] Kumar, C. N. Ravi and Bindu. A, â€œAn Efficient Skin 
Illumination Compensation Model for Efficient Face 
Detectionâ€, IECON 2006, IEEE, pp. 3444-3449 
[5] K. Smith, P. Quelhas and D. Gatica-Perez, â€œDetecting 
Abandoned Luggage Items in a Public Spaceâ€, 9th PETS, 
CVPR 2006, IEEE, pp. 75-82 
[6] J. MartÃ­nez-del-RincÃ³n, J. ElÃ­as Herrero-Jaraba, J. RaÃºl 
GÃ³mez, and C. Orrite-UruÃ±uela, â€œAutomatic Left Luggage 
Detection and Tracking Using Multi-Camera UKFâ€, 9th  
PETS, CVPR 2006, IEEE, pp. 59-66 
[7] L. Li, R. Luo, R. Ma, W. Huang, K. Leman, â€œEvaluation of 
An IVS System for Abandoned Object Detection on PETS 
2006 Datasetsâ€, 9th PETS, CVPR 2006, IEEE, pp. 91-98 
[8] J. Zhou, J. Hoang, â€œReal Time Robust Human Detection 
and Tracking Systemâ€, CVPR 2005, IEEE, Vol III: 149-149 
[9] P.V.C Hough Method and Means for Recognizing Complex 
Patterns, US Patent 3,069,653, December 1962 
[10] R. O. Duda, R. E. Hart, Use of the Hough Transform to 
Detect Lines and Curves in Pictures, CACM(15), No. 1, 
January 1972, pp. 11-15 
Figure 8. Sequence 1, 3 and 4: (from left) static luggage detected; owner tracking starts; owner leaves the scene, alarm triggered. 
Sequence 2: static luggage detected; owner tracking starts; owner lost due to occlusion, alarm triggered. Sequence 1 and 2 are 
from AVSS 2007 dataset; Sequence 3 and 4 from PETS 2006 dataset. 
1369
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:50 from IEEE Xplore.  Restrictions apply. 
C
or
re
sp
on
de
nc
e 
R
es
ul
ts
SCL
Homography MapAll video parameters
Object
Segmentation
Connected 
Component 
Labeling
TCLVideo Sequence 0
Object
Segmentation
Connected 
Component 
Labeling
TCLVideo Sequence N
Fig. 1. Spatial-temporal consistent labeling flow.
background object. As indicated in this example, one main
issue for object tracking is how to know where objects are
during merging, splitting, and reappearing.
The main concept of the proposed consistent labeling al-
gorithm is taking each merged mask as a merged object and
recording which single objects are in it. As in Fig. 1, TCL
takes charge of the labeling problem throughout consecutive
frames for each camera. Then SCL solves the labeling problem
across different views with object information from TCL. The
two consistent labeling methods are detailed in the following
sections.
A. TEMPORAL CONSISTENT LABELING
The block diagram of TCL is shown in Fig. 2. Every mask
in the current frame needs to be compared with the objects
saved in a history database. To match a mask with objects
in history, four features are extracted to find their similarity.
Here we use two different pass thresholds for on-merging
masks and other masks. This is because an on-merging mask
should be considered as a new merged object. The mask
should not be recognized as any single object in the database.
These false recognitions happen when one object is too small
compared to the other before they are merged, the bigger mask
will dominate the on-merging mask and then the similarity
checking result. Most of the time, the on-merging mask will
match to one of single objects in history if we do not tighten
the pass threshold. Hence before object matching stage, the
merge/split condition should be detected first to make the
matching result favor whether creating a new merged object
or selecting an existed object. A merge condition is valid if
one mask in the current frame overlaps multiple objects in the
previous frame. On the other hand, a split condition is valid if
one object in the previous frame overlaps multiple masks in
the current frame.
Object matching decision tree is shown in Fig. 3. A mask
is matched to one history object with rules based on the
following features, color histogram distance, overlapped area
size, mask size ratio, and mask centroid distance. Sometimes,
there may be multiple matched candidates, and the object with
the smallest centroid distance is selected. If no candidates
pass all criteria, there would be three kinds of conditions:
one is a new object appears; another is the mask is a new
on-merging object; the other is the mask appearance changes
too fast and fails certain rules. To differentiate which situation
happens, the number of objects appearing in the last frame
which are overlapped by the current mask indicates the result.
If no previous object is overlapped, the mask is identified as
Merge/Split Condition
Object Matching
Merged Object Update
No Longer Existent Object Removal
Object Masks
Updated Object Database
Fig. 2. Temporal consistent labeling block diagram.
Yes No
Yes No 0 Multiple1
Similarity Match
Check
History
Object
Update
Overlapped 
NumberMulti-match?
Minimum 
Distance
Object 
New
Single
Object
New
Merged
Object
Fig. 3. Object matching decision tree.
a new appearing object. If multiple objects are overlapped,
the mask is identified as a new on-merging object. If mask
overlaps only one object, it is considered as an object with
fast changing appearance.
After matching all current masks with history objects, some
single objects will reappear. If the reappearing objects come
from merged objects, the record of which contained objects
in the merged objects should be updated. If the number of
merged objects of a merged object reduces to zero or one,
that means this merged object no longer exists and should
be wiped out. Although one mask may be matched to the
merged object but not the single object before merge, due to
the mask changes a lot during the merging time, this error will
be compensated with the step of removing reappearing single
objects from merged objects. Finally, to keep a database in
a reasonable size, those objects not included in any merged
objects and disappearing for a predefined length of time need
to be removed from database.
B. SPATIAL CONSISTENT LABELING
With the assumption that each view should have common
ground in most surveillance cases, the proposed algorithm for
spatial consistent labeling is based on ground plane homogra-
phy transformation. Homography transforms the coordinates
of a ground point of an object in one view to the coordinates
in the other. An example is shown in Fig. 4. Given some
matched pairs, we can derive the ground plane warping
matrix. The performance of the homography-based consistent
labeling significantly depends on the segmentation result and
the ground point decision. We still assume the bottom center
of a bounding box of an object is its ground point. However,
for a mask of a person, the shadow in the mask may shift the
true ground point, and for a mask of a vehicle, the different
view may have totally different ground points. Two concepts,
3531
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:49 from IEEE Xplore.  Restrictions apply. 
TABLE I
SPATIAL AND TEMPORAL CONSISTENT LABELING RESULTS
Test Sequences View TP OP FIT(FIO) FIP
PETS2001
(Outdoor)
1 99.32% 94.58% 0.0057
2 98.34% 81.25% 0.0500
Avg. 98.81% 87.23% 0.0279 0.2168
Ming-Da Hall
(Indoor)
1 77.43% 78.94% 0.1810
2 87.37% 84.34% 0.0076
Avg. 82.40% 81.64% 0.0943 0.0294
Barry Lam
Hall
(Outdoor)
1 100.00% 90.55% 0.0000
2 100.00% 88.57% 0.1199
3 100.00% 86.47% 0.0000
4 100.00% 96.74% 0.0000
Avg. 100.00% 90.58% 0.0300 0.0763
Fig. 6. Spatial consistent labeling results. The green car contains green and
red tags because the driver goes into the car previously. The merged mask
in the first(left) view contains green, red, and blue tag which represents the
driver, the green car, and the dark blue car in turn. If an object in the first
view is a single object, its transformed ground point with the same color tag
is shown in right view.
are generated from bad foreground masks and the situation
when objects enter the view but occluded by others. Running
with Intel Core 2 Duo processor at 2.8 GHz, the SCL speed
is about 4,638 fps per one pair of CIF size video channels.
The overall speed is about 10.24 fps.
IV. CONCLUSION
An effective TCL and SCL algorithm is proposed in this
paper. The objects within an occluded mask need not to be
separated to make our TCL and SCL work fine. Homography
ground plane warping is used for deriving SCL. EMD is
utilized to do object matching, and the concept of trusting-
former-pairs-more is applied to generate the matching pairs
across temporal and spatial domain correctly and stably. FIP
metric is also defined for an objective judgement on the
performance of the proposed SCL algorithm. As indicated in
the experimental results, the FIP is 0.1075 which means
about 89.25% of pairs are correctly identified. For online
processing applications, the proposed algorithm need not trace
back to the past frames.
REFERENCES
[1] J. Krumm, S. Harris, B. Meyers, B. Brumitt, M. Hale, and S. Shafer,
â€œMulti-camera multi-person tracking for easyliving,â€ in IEEE Interna-
tional Workshop on Visual Surveillance, 2000, pp. 3â€“10.
[2] A. Mittal and L. Davis, â€œM2tracker: A multi-view approach to segmenting
and tracking people in a cluttered scene using region-based stereo,â€ in
European Conference on Computer Vision, 2002, pp. 18â€“36.
t
First view Second view
(a)
t
First view Second view
(b)
Fig. 7. Two pair renew examples. Fig. 7(a) shows that a man is leaving a
car. Fig. 7(b) shows that a car enters left view.
[3] W. Hu, M. Hu, X. Zhou, T. Tan, J. Lou, and S. Maybank, â€œPrincipal axis-
based correspondence between multiple cameras for people tracking,â€
IEEE Trans. Pattern Anal. Machine Intell., vol. 28, no. 4, pp. 663â€“671,
2006.
[4] S. Khan and M. Shah, â€œConsistent labeling of tracked objects in multiple
cameras with overlapping fields of view,â€ IEEE Trans. Pattern Anal.
Machine Intell., vol. 25, pp. 1355â€“1360, 2003.
[5] Y. Rubner, C. Tomasi, and L. J. Guibas, â€œThe earth moverâ€™s distance as
a metric for image retrieval,â€ Stanford, CA, USA, Tech. Rep., 1998.
[6] â€œPets2001 dataset,â€ http://www.cvg.cs.rdg.ac.uk/PETS2001/pets2001-
dataset.html.
[7] K. Smith, D. Gatica-Perez, J. Odobez, and B. Sileye, â€œEvaluating multi-
object tracking,â€ in IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, vol. 3, 2005, pp. 36â€“43.
3533
Authorized licensed use limited to: National Taiwan University. Downloaded on September 18, 2009 at 04:49 from IEEE Xplore.  Restrictions apply. 
 åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«è¡ç”Ÿç ”ç™¼æˆæœæ¨å»£è³‡ï¦¾è¡¨ 
æ—¥æœŸï¼š 2011  ï¦ 10 æœˆ 31 æ—¥ 
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•« 
è¨ˆç•«åç¨±ï¼šæ™ºæ…§å‹å¤šæ”å½±æ©Ÿç›£æ§ç³»çµ±åŠæ¶æ§‹ç ”ç©¶ 
è¨ˆç•«ä¸»æŒäººï¼šé™³ï¥¼åŸº         
è¨ˆç•«ç·¨è™Ÿï¼šNSC97-2221-E-002-173-MY3     ï¦´åŸŸï¼šE1204 
ç ”ç™¼æˆæœåç¨± 
ï¼ˆä¸­æ–‡ï¼‰ä»¥è¦–è¦ºç‚ºåŸºç¤ä¹‹äººï§å‹•ä½œè¾¨ï§¼çš„æ¼”ç®—æ³•åŠæ¶æ§‹åˆ†æ 
ï¼ˆè‹±æ–‡ï¼‰Algorithm and Architecture Analysis of Video-based 
Human Action and Activity Recognition 
æˆæœæ­¸å±¬æ©Ÿæ§‹ åœ‹ï§·å°ç£å¤§å­¸ ç™¼æ˜äºº (å‰µä½œäºº) 
é™³ï¥¼åŸºï¼Œï¥ºå®¶éˆï¼Œå¼µï¨œ
ï¦®ï¼Œç‹å­æ† 
æŠ€è¡“ï¥¯æ˜ 
ï¼ˆä¸­æ–‡ï¼‰ 
ä»¥è¦–è¨Šç‚ºåŸºç¤çš„äººï§ï¨ˆç‚ºè¾¨ï§¼æŠ€è¡“æä¾›ï¦ºè¨±å¤šèˆ‡é›»è…¦è¦–è¦ºç›¸é—œçš„
é‡è¦æ‡‰ç”¨ï¼ŒåŒ…å«å¤šåª’é«”å¨›ï¤”ã€å®‰å…¨ç›£æ§ã€äº’å‹•å¼ç’°å¢ƒã€è¦–è¨Šåˆ†æ
èˆ‡ç”Ÿç‰©ï¨ˆç‚ºç‰¹å¾µç­‰ç­‰ã€‚ï¨ˆç‚ºè¾¨ï§¼æ¼”ç®—æ³•å’Œç³»çµ±ä¸­ï¼Œæœ€é‡è¦çš„æŒ‘æˆ°
ï¤­è‡ªäººï§èˆ‡æ©Ÿå™¨é–“çš„èªç¾©é´»æºï¼Œéœ€è¦æ“·å–æœ‰æ„ç¾©çš„ç‰©é«”ç‰¹å¾µå’Œè¨­
è¨ˆæœ‰æ•ˆçš„é‹å‹•æ¨¡å‹ï¼Œï¤­è®“é›»è…¦æ­£ç¢ºï§¤è§£ç•«é¢ä¸­æ‰€å‘ˆç¾çš„çœŸæ­£é‹å‹•
å«ç¾©ã€‚ç¬¬ä¸€å€‹æŒ‘æˆ°æ˜¯ç‰©é«”ç‰¹å¾µçš„é¸æ“‡ï¼Œé€™äº›ç‰¹å¾µï¥§åªè¦èƒ½æœ‰æ•ˆçš„
ç”¨ï¤­å€åˆ†å„ç¨®ï¨ˆç‚ºï¼Œé‚„å¿…é ˆèƒ½åœ¨çœŸå¯¦ç’°å¢ƒä¸­ï¼ŒåŒ…å«é›œè¨Šã€é®è”½å’Œ
é™°å½±çš„å½±éŸ¿ä¹‹ä¸‹ï¼Œï¨¦é‚„èƒ½è¢«å–å¾—ä¸¦æ­£ç¢ºè¡¨é”ï¼Œå› ç‚ºåªè¦æœ‰ä»»ä½•çš„
èª¤å·®ï¼Œï¨¦æœƒå°è‡´å¾Œé¢é‹ç®—çš„çµæœéŒ¯èª¤ã€‚ç¬¬äºŒå€‹æŒ‘æˆ°æ˜¯å»ºï§·äººï§çš„
ï¨ˆç‚ºæ¨¡å‹ï¼Œé€™å€‹æ¨¡å‹å¿…é ˆï¨æº–åœ°æè¿°ä¸”èƒ½å€åˆ¥ï¥§åŒï¨ˆç‚ºçš„å·®ï¥¢ã€‚
æ­¤å¤–ï¼Œè©²æ¨¡å‹æè¿°å¤šå°‘å±€éƒ¨å’Œå…¨åŸŸç‰¹æ€§ï¼Œæ¨¡å‹æœ¬èº«çš„ç¶­ï¨å¤§å°çš„
æ±ºå®šï¼Œï¨¦è¦è€ƒæ…®å‹•ä½œæœ¬èº«çš„æ€§è³ªå’Œæ¸¬è©¦è³‡ï¦¾çš„å¤šå¯¡ã€‚æœ¬æŠ€è¡“ä»¥ç„¡
æ§åˆ¶å™¨çš„éŠæˆ²å¹³å°å’Œéºæ£„ï¨ˆï§¡çš„åµæ¸¬ç³»çµ±ï¼Œï¤­åˆ†æå¦‚ä½•è¨­è¨ˆä»¥è¦–
è¦ºç‚ºåŸºç¤çš„äººï§å‹•ä½œè¾¨ï§¼ã€‚å…¶ä¸­åŒ…å«ï¥¸å€‹éƒ¨åˆ†ç¬¬ä¸€éƒ¨åˆ†è¨ï¥å¦‚ä½•
å–å¾—å¤§éƒ¨åˆ†ï¨ˆç‚ºè¾¨ï§¼çš„å…±åŒç‰¹å¾µï¼Œå³ç‰©é«”çš„è»Œè·¡ï¼Œè€Œç¬¬äºŒéƒ¨åˆ†é‡
å°é€™ï¥¸å€‹æ‡‰ç”¨è¨ï¥å¦‚ä½•è¨­è¨ˆï¨ˆç‚ºæ¨¡å‹ã€‚ 
 
é™„ä»¶ä¸‰ 
æŠ€è¡“ç§»è½‰å¯ï¨ˆæ€§åŠé æœŸ
æ•ˆï¨— 
ä¿ƒé€²é«˜æº–ï¨ä¹‹è¦–è¦ºå‹•ä½œè¾¨ï§¼ä¹‹ç ”ç©¶ç™¼å±•èˆ‡æŠ€è¡“é–‹ç™¼ã€‚æ­¤æŠ€è¡“å¯ç¶“
ç”±æŠ€è¡“è½‰ç§»è‡³å½±åƒæ„Ÿæ¸¬å™¨ã€æ™ºæ…§å‹ç›£æ§ç³»çµ±åŠç›¸é—œå‹•ä½œè¾¨ï§¼ç”¢æ¥­
æä¾›é«˜æº–ï¨å‹•ä½œè¾¨ï§¼æ¼”ç®—æ³•åŠç¡¬é«”æ¶æ§‹å¹³å°ã€‚ 
     è¨»ï¼šæœ¬é …ç ”ç™¼æˆæœï¥´å°šæœªç”³è«‹å°ˆï§ï¼Œè«‹å‹¿æ­ï¤¸å¯ç”³è«‹å°ˆï§ä¹‹ä¸»è¦å…§å®¹ã€‚ 
æ˜¯ Does ASR have a PHD, or is it just Piled Higher and Deeperã€‚å…§å®¹ä¸»è¦åœ¨ä»‹ç´¹è‡ª
å‹•è©±èªéŸ³è¾¨ï§¼ automatic speech recognition (ASR) çš„å›é¡§èˆ‡æœªï¤­ã€‚ä»–æåˆ°éå» 
speech recognition çš„ç™¼å±•ä¸Šé‡è¦çš„æ ¸å¿ƒæƒ³æ³•ï¼Œæ¯”å¦‚ï¥¯è²éŸ³ç‰¹å¾µæ“·å–çš„ MFFC 
PLPï¼Œ è²éŸ³æ¨¡å‹çš„ HMM ç­‰ï¼Œå¤§ï¨¦åœ¨ 1991 ä»¥å‰å°±å·²ç¶“å»ºï§·ï¦ºåŸºæœ¬çš„ï§¤ï¥ï¦ºã€‚è€Œ
ä¸€ç›´åˆ° 2011 ï¦ï¼Œé€™äº›æ ¸å¿ƒé‚„æ˜¯æ²’æœ‰æ”¹è®Šã€‚æš—ç¤ºï¦ºèªéŸ³ï¦´åŸŸåœ¨ 20 ï¦å…§æ–¹æ³•ä¸Šçš„çª
ç ´æ˜¯æœ‰é™çš„ã€‚åœ¨é€™ç›¸å°ã€Œæˆç†Ÿã€çš„ï¦´åŸŸï¼Œæœªï¤­å¯èƒ½éœ€è¦æ–°çš„æ¦‚ï¦£ï¤­å¼•å°ã€‚æå‡ºæœª
ï¤­çš„ç ”ç©¶é‡é»å¯èƒ½åœ¨æ–¼è¾¨ï§¼åŠï§¤è§£æ•´æ®µè©±æˆ–æ–‡ç« çš„å ´æ™¯ã€‚ç¬¬å››å¤©çš„ä¸»é¡Œæ¼”è¬›æ˜¯æ©Ÿ
å™¨å­¸ç¿’æ¼”ç®—æ³•ï¦´åŸŸçš„éº¥å¯å–¬ï¥ï¼ŒUC Berkeley çš„ Prof. Michael I. Jordan æ•™æˆä¸»
è¬›ã€‚é¡Œç›®æ˜¯ Bayesian Nonparametrics for Speech and Signal Processingã€‚é€™å€‹ä¸»é¡Œ
ç ”è¬›æ¯”è¼ƒåƒæ˜¯ä¸€èˆ¬çš„ Lectureï¼Œä»‹ç´¹ Bayesian Nonparametrics çš„ä¸€äº›ï¥©å­¸ï§¤ï¥å’Œä½¿
ç”¨åœ¨èªéŸ³åŠè¨Šè™Ÿè™•ï§¤ä¸Šçš„ä¸€äº›çµæœã€‚è—‰ç”±æ¼”ç®—æ³• non-parametric çš„ç‰¹è‰²ï¼Œï¥§æœƒå—
åˆ°äº‹å…ˆçµ¦å®šæ¨¡å‹ï¥«ï¥©çš„é™åˆ¶ï¼Œåœ¨ç‚ºï¤­æ¥µæœ‰å¯èƒ½åœ¨ç›¸é—œï¦´åŸŸæ‰®æ¼”é‡è¦çš„è§’è‰²ã€‚ 
 
é€™æ¬¡å¤§æœƒåœ¨æ¯å¤©çš„ä¸­åˆé¦–å‰µï¦ºä¸€å€‹ Penal List çš„æ´»å‹•ï¼Œå«åš Expert Summaries 
of Current Trendsã€‚å››å¤©ä¸­çš„å‰ä¸‰å¤©ä¸­åˆï¼Œæ¯å¤©æœ‰ä¸‰åˆ°å››å€‹ç ”ç©¶ï¦´åŸŸç‚ºä¸»é¡Œï¼Œè«‹
Technical Committees ä¸­çš„ä½¼ä½¼è€…ï¼Œé¡Œä¾›é–‹å ´ï¼Œéš¨å¾Œè®“èˆ‡æœƒè€…ç™¼å•å¼•å°ï¼Œå±•é–‹è¨
ï¥ã€‚æˆ‘æ¯å€‹ä¸­åˆï¨¦æœ‰é¸æ“‡ä¸€å€‹æœ‰èˆˆè¶£çš„å ´æ¬¡å»ï¥«åŠ ï¼Œä½†ç”±æ–¼æ™‚é–“å¾ˆçŸ­ï¼Œå¤§éƒ¨åˆ†è¬›
è€…é–‹å ´å®Œå°±æ²’å‰©å¤šå°‘æ™‚é–“ï¦ºï¼Œå› æ­¤è¨ï¥ï¨¦ï¥§å¤ªæ·±å…¥ï¼Œæ˜¯ç¨å«Œå¯æƒœä¹‹è™•ã€‚ 
 
ICASSP å‘ï¤­ä»¥ç››ï¨ˆçš„è¨ï¥é¢¨æ°£æ‰€è‘—åï¼Œæœ¬æ¬¡å¤§æœƒä¹Ÿï¥§ï¦µå¤–ã€‚åœ¨æµ·å ±å€ï¼Œè²
éŸ³åŠèªéŸ³çš„ï¥æ–‡ä¸€å¦‚å¾€å¸¸çš„ï¨¦æœƒæ“ æ»¿äººï¼Œä½œè€…å¹¾ä¹ï¨¦ï¥§æœƒæœ‰ç©ºä¸‹ï¤­çš„æ™‚é–“ã€‚é€™æ¬¡
å¤§æœƒç‰¹åˆ¥çš„æ˜¯ï¼Œcompressed sensing ç›¸é—œçš„é¡Œæï¼Œå› ç‚ºæ˜¯æœ€è¿‘å¾ˆç†±é–€çš„é¡Œç›®ï¼Œæ‰€
ä»¥å¤§çˆ†æ»¿ã€‚åœ¨æˆ‘å€‘ï¥æ–‡æ‰€å±¬çš„ç”Ÿé†«è¨Šè™Ÿè™•ï§¤æ–¹é¢ï¼Œé›–ç„¶äººæ¬¡æ²’æœ‰ä¸Šé¢ï¥¸å€‹ä¸»é¡Œé‚£
éº¼å¤šï¼Œä½†ä¹Ÿæ˜¯å¾ˆé›£æœ‰é–’ä¸‹ï¤­çš„æ©Ÿæœƒã€‚å³ä½¿æ˜¯ï¥§åŒï¦´åŸŸçš„äººä¹Ÿï¨¦æœƒï¤­è©¢å•ï¦ºè§£æˆ‘å€‘
ç ”ç©¶çš„å…§å®¹ï¼Œå¯ï¨Šé€™å€‹æœƒè­°ä¸»è¾¦çš„ç”¨å¿ƒåŠï¥«äºˆç ”ç©¶äººå“¡å¥½å­¸çš„é¢¨æ°£ã€‚ 
 
æœ¬æ¬¡ç™¼è¡¨çš„ï¥æ–‡ç‚ºâ€œDesign and Implementation of Cubic Spline Interpolation 
æœ‰å·®ï¥¢ï¼Œæ‰€ä»¥ä¸€é–‹å§‹æº–å‚™åŠæ¼”ï¦–çš„æ™‚å€™ï¼Œå°±è¦å…ˆè¨­æƒ³å¾ˆå¤šï¥§åŒçš„ï§ºæ³ï¤­åŠ ä»¥æº–
å‚™ã€‚é€™æ¬¡å ±å‘Šç”±æ–¼æœ‰å……åˆ†çš„æº–å‚™ï¼Œæ‰€ä»¥å ±å‘Šæ™‚æ™®éåæ‡‰ï¥¼å¥½ã€‚åŒæ™‚ï¥§åŒï¦´åŸŸçš„äºº
ä¹Ÿæœƒçµ¦äºˆä¸€äº›å»ºè­°ï¼Œæ¨è–¦ä¸€äº›ï¥§åŒæ‡‰ç”¨ä¸Šå¸¸ç”¨çš„è™•ï§¤æ–¹æ³•ï¼Œèªç‚ºæ‡‰è©²å¯ä»¥æœ‰æ•ˆçš„
ä½¿ç”¨æ–¼æˆ‘å€‘çš„ç ”ç©¶ä¸Šï¼Œå¯ï¥¯æ˜¯æ”¶ç©«ï¥§å°ã€‚ 
å››ã€ æ”œå›è³‡ï¦¾åç¨±åŠå…§å®¹ 
 
1. æœƒè­°ï¥æ–‡é›† USB éš¨èº«ç¢Ÿä¸€æ”¯ï¼Œå…§å«æ‰€æœ‰ç™¼è¡¨ï¥æ–‡ï¼› 
2. æœƒè­°ï¥æ–‡ç°¡ä»‹æ‰‹å†Šä¸€æœ¬ 
 
äº”ã€ èˆ‡æœƒç…§ç‰‡ 
 
å¤§æœƒé–€å£ç‰¹åˆ¥çš„å°å¼•æŒ‡æ¨™ 
Expert of current trends æ¼”è¬›å®Œå¾Œèˆ‡å­¸ç”Ÿåˆç…§ã€‚ 
 
 
å¤§æœƒ Poster Session çš„è¨ï¥é¢¨æ°£å¾ˆç›› 
 
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«è¡ç”Ÿç ”ç™¼æˆæœæ¨å»£è³‡æ–™è¡¨
æ—¥æœŸ:2011/10/23
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«
è¨ˆç•«åç¨±: æ™ºæ…§å‹å¤šæ”å½±æ©Ÿç›£æ§ç³»çµ±åŠæ¶æ§‹ç ”ç©¶
è¨ˆç•«ä¸»æŒäºº: é™³è‰¯åŸº
è¨ˆç•«ç·¨è™Ÿ: 97-2221-E-002-173-MY3 å­¸é–€é ˜åŸŸ: è¨Šè™Ÿè™•ç†
ç„¡ç ”ç™¼æˆæœæ¨å»£è³‡æ–™
'09. International 
Conference on, 2009. 
3. H.-H. Liao, J.-Y. Chang, 
and L.-G. Chen, â€™â€™A 
Localized Approach to 
Abandoned Luggage 
Detection with 
Foreground-Mask 
Sampling,â€™â€™ in Advanced 
Video and Signal Based 
Surveillance, 2008. AVSS 
'08. IEEE International 
Conference on, 2008, pp. 
132-139. 
4. J.-Y. Chang, T.-H. Wang, 
S.-Y. Chien, and L.-G. 
Chen, â€™â€™Spatial-temporal 
consistent labeling for 
multi-camera multi-object 
surveillance systems,â€™â€™ 
in Circuits and Systems, 
2008. ISCAS '08. IEEE 
International Symposium on, 
2008, pp. 3530-3533. 
 
å°ˆæ›¸ 0 0 100% ç« /æœ¬  
ç”³è«‹ä¸­ä»¶æ•¸ 0 0 100%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 100% ä»¶  
ä»¶æ•¸ 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆå¤–åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 0 0 100% 
äººæ¬¡
 
å…¶ä»–æˆæœ 
(ç„¡æ³•ä»¥é‡åŒ–è¡¨é”
ä¹‹æˆæœå¦‚è¾¦ç†å­¸è¡“
æ´»å‹•ã€ç²å¾—çé …ã€
é‡è¦åœ‹éš›åˆä½œã€ç ”
ç©¶æˆæœåœ‹éš›å½±éŸ¿åŠ›
åŠå…¶ä»–å”åŠ©ç”¢æ¥­æŠ€
è¡“ç™¼å±•ä¹‹å…·é«”æ•ˆç›Š
äº‹é …ç­‰ï¼Œè«‹ä»¥æ–‡å­—
æ•˜è¿°å¡«åˆ—ã€‚) 
æœ¬ç ”ç©¶æˆæœå·²æœ‰å››ç¯‡ç›¸é—œè«–æ–‡ç™¼è¡¨ï¼Œä»¥åŠä¸‰ç¯‡æœŸåˆŠè«–æ–‡é€å¯©ä¸­ï¼Œå…¶ä¸­åŒ…å« ICME åœ‹
éš›æœƒè­°è«–æ–‡ï¼ŒTCSVTã€TCE ç­‰è‘—ååœ‹éš›æœŸåˆŠã€‚æœ¬ç ”ç©¶æˆæœæ•´åˆé‹å‹•æ¨¡å‹ã€å¤šæ”å½±æ©Ÿ
é—œä¿‚æ¨¡å‹ã€ç‰©é«”è¿½è¹¤ç­‰ç”¨åœ¨è¡Œç‚ºè¾¨è­˜ç³»çµ±ä¸Šçš„æ ¸å¿ƒæ¨¡çµ„ï¼Œæ ¹æ“šä»–å€‘çš„é‹ç®—ç‰¹æ€§åŠæ€§
è³ªï¼Œè¨­è¨ˆé«˜æ•ˆç‡çš„ç¡¬é«”æ¶æ§‹å’Œé«˜æº–ç¢ºåº¦çš„æ¼”ç®—æ³•ã€‚å¯ä¾›å¾ŒçºŒç ”ç©¶è€…åƒè€ƒä¸¦å»¶ä¼¸å…¶æ€§
èƒ½å’Œæ‡‰ç”¨ã€‚æœ¬ç ”ç©¶æˆæœå¯å¯¦éš›æ‡‰ç”¨æ–¼ç›£è¦–ç³»çµ±åŠéœ€è¦å‹•ä½œè¡Œç‚ºè¾¨è­˜ä¹‹äººæ©Ÿäº’å‹•ã€å‹•
ä½œæ„ŸçŸ¥ç³»çµ±ï¼Œå› æ­¤å¯ä»¥å¯¦éš›æ‡‰ç”¨æ–¼å…¬å…±å®‰å…¨ç³»çµ±æˆ–æ˜¯æ¶ˆè²»æ€§é›»å­ç”¢æ¥­ä¹‹ä¸Šï¼Œæä¾›ç”¢
æ¥­ç•Œåœ¨æœªä¾†ç™¼å±•çš„å¾Œç›¾ã€‚æ­¤å¤–æœ¬ç ”ç©¶æˆæœæå‡ºä¹‹ç¡¬é«”æ¶æ§‹è¨­è¨ˆå¯ä»¥æ‡‰ç”¨æ–¼ç›¸é—œé›»å­
ç”¢æ¥­åŠ IC è¨­è¨ˆç”¢æ¥­ï¼Œæä¾›ä¸€æœ‰æ•ˆç‡ä¹‹å‹•ä½œè¾¨è­˜æ ¸å¿ƒæ¨¡çµ„ã€‚æ­¤ç ”ç©¶æˆæœæ›´æ˜¯åæ‡‰æœª
ä¾†äººæ©Ÿäº’å‹•ã€å‹•ä½œè¾¨è­˜æ„ŸçŸ¥ç­‰ç›¸é—œè¶¨å‹¢ï¼Œæœªä¾†æœƒæœ‰æ›´å¤šç›¸é—œç ”ç©¶è­°é¡Œå› æ‡‰è€Œç”Ÿã€‚æ­¤
ç ”ç©¶æˆæœå°æ–¼ç”¢æ¥­ç•Œå·²æœ‰é€²ä¸€æ­¥å½±éŸ¿ï¼Œå…¶ä¸­çš„æ™ºæ…§å‹å¤šç‰©é«”è¿½è¹¤ä¹‹å¤šæ”å½±æ©Ÿæ•¸ä½è¦–
è¨Šç›£æ§ç³»çµ±æ¨¡çµ„å·²ç¶“æŠ€è½‰æ–¼ç›¸é—œç”¢æ¥­ã€‚ 
 æˆæœé …ç›® é‡åŒ– åç¨±æˆ–å…§å®¹æ€§è³ªç°¡è¿° 
