Date:2008/07/20                 åœ¨ï¥¢è³ªç¶²ï¤·ä¸Šæ‰“é€ ä¸€å®‰å…¨ U åŒ–ä¹‹ç’°å¢ƒ(II)æˆæžœå ±å‘Šæ›¸ 
 2
ï¨ˆæ”¿é™¢åœ‹å®¶ç§‘å­¸å§”å“¡æœƒå°ˆé¡Œç ”ç©¶è¨ˆç•«æˆæžœå ±å‘Š 
åœ¨ï¥¢è³ªç¶²ï¤·ä¸Šæ‰“é€ ä¸€å®‰å…¨ U åŒ–ä¹‹ç’°å¢ƒ(II) 
Develop Securing Ubiquitous Services over Heterogeneous Networks (II) 
è¨ˆç•«ç·¨è™Ÿï¼šNSC 96-2218-E-224-003 
åŸ·ï¨ˆæœŸé™ï¼š96 ï¦Ž 8 æœˆ 1 æ—¥è‡³ 97 ï¦Ž 7 æœˆ 31 æ—¥ 
ä¸»æŒäººï¼šä¼ï¦ˆæ¨µ    åœ‹ï§·é›²ï§´ç§‘æŠ€å¤§å­¸è‡ªç”±è»Ÿé«”ç ”ç©¶ä¸­å¿ƒ 
å…±åŒä¸»æŒäººï¼šå¼µå‚³è‚²ã€çŽ‹æ–‡æ¥“ã€é»ƒèƒ¤å‚…ã€æ–½æ±æ²³ã€å¼µæ…¶ï§„ 
Email: wuulc@yuntech.edu.tw 
 
ä¸€ã€ä¸­æ–‡æ‘˜è¦ 
åœ¨ï¦Œç¶“ E åŒ–ã€M åŒ–ä¹‹ï¥©ä½é©å‘½å¾Œï¼Œç¾
ä»Šå…¨ä¸–ç•Œèˆˆèµ·ï¦ºä¸€è‚¡ U åŒ–ä¹‹ç†±æ½®ï¼Œæ‰€è¬‚ U 
åŒ–å³æ˜¯å»ºï§·ä¸€å€‹æœ‰ç·šèˆ‡ç„¡ç·šé€šè¨Šæ•´åˆçš„ç¶²
ï¤·ç’°å¢ƒï¼Œè®“ä»»ä½•è³‡è¨Šèˆ‡æœå‹™é€éŽæ­¤ç„¡æ‰€ï¥§
åœ¨çš„ç¶²ï¤·ï¤­å‚³è¼¸ã€‚æ¯«ç„¡ç–‘å•ã€Œç„¡æ‰€ï¥§åœ¨çš„
è¨Šæ¯ç¤¾æœƒã€å°‡å¸¶çµ¦äººï§æ–°ä¸€ä»£å¶„æ–°çš„ç”Ÿæ´»
æ–¹å¼ï¼Œä½†å‰ææ˜¯å®‰å…¨å•é¡Œå¿…éœ€èƒ½å¤ å¾—åˆ°å¦¥
å–„çš„è§£æ±ºã€‚æœ¬è¨ˆç•«çš„ç›®æ¨™å³æ˜¯åœ¨ï¥¢è³ªç¶²ï¤·
ä¸Šæ‰“é€ ä¸€å®‰å…¨ U åŒ–ä¹‹ç’°å¢ƒ[1-9]ï¼Œæœ¬è¨ˆç•«ä»¥
application-oriented çµ åˆ sensor-oriented 
çš„æž¶æ§‹å¯¦ç¾ä¸€å®‰å…¨ U åŒ–ä¹‹å¹³å°ï¼Œç›®çš„æ˜¯è®“
å„ç¨®çš„ applications å¯ä»¥è¿…é€Ÿã€æœ‰å½ˆæ€§çš„æ•´
åˆè‡³æœ¬è¨ˆç•«çš„å®‰å…¨ U åŒ–ç’°å¢ƒã€‚åœ¨ U åŒ–ç’°å¢ƒ
ä¸­ï¼Œæä¾›æœå‹™çš„å‰ææ˜¯è¦ä¿è­‰é›™æ–¹çš„èº«ä»½
æ˜¯åˆæ³•ä¸”çœŸå¯¦çš„ï¼Œæœ¬è¨ˆç•«å¯¦ä½œï¥¸ç¨®ç”Ÿç‰©è¾¨
ï§¼æ–¹å¼(è‡‰éƒ¨è¾¨ï§¼åŠéµæ“Šè¾¨ï§¼)ï¤­ç¢ºèªä½¿ç”¨
è€…çš„èº«ä»½ã€‚æ­¤å¤–ï¼Œæƒ³è¦èƒ½å¤ éš¨æ™‚éš¨æ„åœ°æ“·
å–è³‡è¨Šï¼Œç›®å‰æœ€æ–°çš„æŠ€è¡“ä»¥å°„é »ï§¼åˆ¥
ï¼ˆRadio Frequency Identification, RFIDï¼‰ç‚º
æ ¸å¿ƒï¼Œæœ¬è¨ˆç•«æ•´åˆ RFIDã€Ethernetã€ZigBee
èˆ‡ Environment Sensor ç­‰æŠ€è¡“ï¼Œå¯¦ä½œ U åŒ–
è³‡è¨Šæ“·å–å™¨åŠç„¡ç·šé–˜é“å™¨ï¼Œä»¥æä¾›ä½¿ç”¨è€…
ç„¡æ‰€ï¥§åœ¨çš„ï¦šç¶²ç’°å¢ƒï¼›å¦å¤–è€ƒæ…®æ–¼ï¥¢è³ªç¶²
ï¤·ç’°å¢ƒæœƒæœ‰ï¥§åŒçš„é »å¯¬éœ€æ±‚ï¼Œå¯¦ä½œä¸€å…·æœ‰
æœå‹™å“è³ªä¿è­‰ä¹‹å®‰å…¨ç¾¤æ’­æ©Ÿåˆ¶[10-11]ï¼›æœ¬
è¨ˆç•«ä¸¦å¯¦ç¾ä¸‰å€‹ Applications: U åŒ–é›»å­æŠ•
ç¥¨ç³»çµ±ã€U åŒ–é¤å»³ã€U åŒ–é»žåç³»çµ±ï¼Œä¸¦é”
åˆ°æ¯å€‹ Application æ‰€æ‡‰å…·å‚™ä¹‹å®‰å…¨ç‰¹æ€§ã€‚ 
é—œéµè©žï¼š U åŒ–ç’°å¢ƒã€æœå‹™å“è³ªã€å®‰å…¨ç¾¤æ’­ã€
äººè‡‰è¾¨ï§¼ã€ZigBee/RFID/Ethernet
è³‡ï¦¾æŽ¢å‹˜ã€é–˜é“å™¨ã€éµæ“Šè¾¨ï§¼ 
äºŒã€ç·£ç”±èˆ‡ç›®çš„ 
åœ¨æœªï¤­ï¼Œå¯¬é »çš„ç„¡ç·šä¸Šç¶²æœå‹™èˆ‡ï¥§æ–·
æŽ¨é™³å‡ºæ–°çš„ï¨ˆå‹•é€šè¨Šå·¥å…·å°‡å¾¹åº•é¡›è¦†äººï§
ç¾æœ‰çš„ç”Ÿæ´»æ–¹å¼ï¼›ä¹Ÿå°±æ˜¯æœªï¤­åœ¨äººå€‘çš„ç”Ÿ
æ´»ç’°å¢ƒï§¨ï¼Œè—‰è‘—ä½ˆå»ºæ™ºæ…§åž‹ç¶²çµ¡ï¼ˆå¦‚ IPv6ã€
Ubiquitous Sensor Networkï¼‰ä¸¦çµåˆå¦‚ RFID 
ç­‰çš„æŠ€è¡“ï¼Œäººå€‘å°‡å¯ä»¥éš¨æ™‚éš¨åœ°äº«æœ‰ç§‘æŠ€
æ‰€å‰µé€ çš„ç„¡æ‰€ï¥§åœ¨çš„ç¶²ï¤·æœå‹™ï¼Œç¨±ç‚º U åŒ–
ç’°å¢ƒã€‚é›–ç„¶ U åŒ–ç’°å¢ƒå¸¶ï¤­è¨±å¤šï¥¥ï§æœå‹™ï¼Œ
èƒŒå¾Œå»ä¹Ÿéš±è—è‘—ä¸€äº›æ½›åœ¨çš„å±æ©Ÿï¼šå¦‚å€‹äºº
è³‡ï¦¾é­å—ç›œç”¨ã€å€‹äººéš±ç§å—åˆ°ä¾µçŠ¯ã€é§­å®¢
èˆ‡é›»è…¦ç—…æ¯’çš„å…¥ä¾µç­‰ï¼Œï¨¦æ˜¯äººå€‘æ†‚å¿ƒçš„è­°
é¡Œã€‚åœ¨æœªï¤­ U åŒ–ç’°å¢ƒï§¨ï¼Œå¦‚ä½•ç¢ºä¿å€‹äººéš±
ç§æ¬Šèˆ‡è³‡ï¦¾å‚³é€çš„å®‰å…¨æ€§ï¼Œå‹¢å¿…æ˜¯å„åœ‹æŽ¨
å‹• U åŒ–æ”¿ç­–æˆåŠŸèˆ‡å¦çš„é‡è¦é—œéµã€‚æœ‰é‘‘æ–¼
æ­¤ï¼Œæœ¬è¨ˆåŠƒçš„ç›®æ¨™å³æ˜¯æŽ¢è¨Ž U åŒ–ç’°å¢ƒæ‰€æœƒ
é¢å°åˆ°çš„å®‰å…¨å•é¡Œä¸¦é–‹ç™¼å„é …é—œéµæ€§æŠ€
è¡“ã€‚ 
ä¸‰ã€ç ”ç©¶çµæžœèˆ‡è¨Žï¥ 
æœ¬è¨ˆåŠƒä¹‹ç³»çµ±ç‚ºåœ¨ï¥¢è³ªç¶²ï¤·ä¸Šæ‰“é€ ä¸€
å®‰ å…¨ U åŒ– ä¹‹ ç’° å¢ƒ (Develop Securing 
Ubiquitous Services on Heterogeneous 
Networksï¼ŒSUSHN)ï¼Œæ­¤ç³»çµ±ç¸½å…±åˆ†ç‚ºäº”å€‹
å­ç³»çµ±ï¼Œåˆ†åˆ¥ç‚ºï¤…ï§ŠåŠ å¯†åŠå®‰å…¨ç¾¤æ’­å­ç³»
çµ± [Secure Stream Cipher Multicast 
Subsystemï¼ŒSSCMS 1.1.0]ã€è‰²å½©ç©ºé–“è‡ªå‹•
ï¨€ æ› ä¹‹ å¤š æ” å½± æ©Ÿ å” åŒ åµ æ¸¬ å­ ç³» çµ±
[Multi-camera Collaborated Surveillance 
Subsystemï¼ŒMCSS 1.2.0]ã€è¼•ï¥¾åŒ–ä½¿ç”¨è€…
è³‡è¨Šæ“·å–å™¨å­ç³»çµ±[Thin of UIA module 
Subsystemï¼ŒTUS 1.3.0]ã€ä½¿ç”¨è€…ï¨ˆç‚ºæŽ¢å‹˜
å­ ç³» çµ± [Mining User Behaviors 
Date:2008/07/20                 åœ¨ï¥¢è³ªç¶²ï¤·ä¸Šæ‰“é€ ä¸€å®‰å…¨ U åŒ–ä¹‹ç’°å¢ƒ(II)æˆæžœå ±å‘Šæ›¸ 
 4
 
z ä½¿ç”¨è€…ï¨ˆç‚ºæŽ¢å‹˜å­ç³»çµ±(MUBS)ï¼šæœ¬ç ”ç©¶
è¨ˆç•«æ—¨åœ¨ U åŒ–ç’°å¢ƒä¸­å»ºæ§‹ä¸€å€‹è³‡è¨Šæä¾›
å¹³å°ï¼Œè—‰ä»¥æä¾›å¯¦é«”ç‰©ä»¶è³‡è¨Šå…§å®¹ä¾›æŽˆ
æ¬Šé€šéŽçš„ç”¨æˆ¶ç€è¦½[25-27]ã€‚æˆ‘å€‘åœ¨æœ¬è¨ˆ
åŠƒæ•´åˆéŽåŽ»ç ”ç©¶çš„æŠ€è¡“èˆ‡è§€ï¦£ï¼Œï¥§ï¥æ˜¯
è³‡ï¦¾çš„æä¾›ã€ç®¡ï§¤ï¼Œç”¨æˆ¶çš„èªè­‰ã€æŽˆæ¬Šã€
åŠå­˜å–æŽ§åˆ¶çš„è®Šï¤ï¨¦è¦ç¬¦åˆå®‰å…¨æ€§åŠéš±
ç§æ€§ã€‚å¦å¤–æˆ‘å€‘ä¹Ÿå¸Œæœ›å°‡ç”¨æˆ¶ç€è¦½å„å¯¦
é«”ç‰©ä»¶çš„è¹¤è·¡è¨˜ï¤¿èµ·ï¤­ï¼Œä»¥ï¥¥é€²ï¨ˆç”¨æˆ¶
ï¨ˆç‚ºçš„æŽ¢å‹˜ï¼Œè©²æŽ¢å‹˜å¾Œä¹‹è³‡è¨Šå¯æä¾›æœª
ï¤­å°å¯¦é«”ç‰©ä»¶çš„æ“ºè¨­æˆ–ç”¨æˆ¶ï¨ˆç‚ºçš„åˆ†æž
æä¾›ï¤æº–ç¢ºä»¥åŠå¤šæ¨£æ€§ä¹‹ï¥«è€ƒä½¿ç”¨ï¼Œæœ¬
å­ç³»çµ±å…±åŒ…å«ï¦ºå››å€‹éƒ¨ä»½ï¼Œå…¶ç³»çµ±æž¶æ§‹
åœ–å¦‚åœ–äº”æ‰€ç¤ºã€‚ 
 
åœ–äº” ä½¿ç”¨è€…ï¨ˆç‚ºæŽ¢å‹˜å­ç³»çµ±æž¶æ§‹åœ– 
z ä½¿ç”¨è€…é›™é‡é©—è­‰å­ç³»çµ±(UDAS)ï¼šæœ¬å­ç³»
çµ±å˜—è©¦ä»¥ç„¡ç·šå°„é »æŠ€è¡“çš„ï¥¥ï§æ€§çµåˆå‹•
æ…‹éµæ“Šç‰¹å¾µç³»çµ±çš„å¯é æ€§æ‡‰ç”¨åœ¨ç„¡æ‰€ï¥§
åœ¨çš„ç’°å¢ƒä¸­åŠ å¼·é›»è…¦ä½¿ç”¨è€…é©—è­‰æž¶æ§‹çš„
å®‰å…¨æ€§ï¼Œçµåˆï¥¸è€…çš„ç‰¹é»ž[28-29]ï¼Œæœ¬å­
ç³»çµ±ï§ç”¨å¤šé‡ä½¿ç”¨è€…èº«ä»½è¾¨ï§¼æ©Ÿåˆ¶ï¼Œæ”¹
å–„å–®ä¸€ä½¿ç”¨è€…é©—è­‰æ©Ÿåˆ¶å¯èƒ½å­˜åœ¨çš„å®‰å…¨
æ€§å•é¡Œï¼Œä¸¦å¸Œæœ›è—‰ä»¥æå‡é›»è…¦ç³»çµ±åœ¨èº«
åˆ†é©—è­‰ä¸Šçš„å®‰å…¨æ€§ï¼Œä½¿å¾—éžæ³•ä½¿ç”¨è€…ç„¡
æ³•é€éŽå„ç¨®æ–¹å¼å…¥ä¾µæˆ–æ˜¯å–å¾—ä½¿ç”¨è€…çš„
å¸³è™Ÿã€å¯†ç¢¼è€Œè¼•ï§ çš„å…¥ä¾µæˆ–ç™»å…¥ç³»çµ±å–
å¾—ä½¿ç”¨è€…çš„å€‹äººè³‡è¨Šï¼Œå› ç‚ºæœ¬å­ç³»çµ±çš„
èº«ä»½é©—è­‰æ©Ÿåˆ¶éœ€è¦å®Œå…¨é€šéŽé©—è­‰å¾Œæ‰å¯
é€²ï¨ˆå­˜å–ã€‚æœ¬å­ç³»çµ±æž¶æ§‹åœ–å¦‚åœ–ï§‘æ‰€ç¤ºã€‚ 
 
åœ–ï§‘ ä½¿ç”¨è€…é›™é‡é©—è­‰å­ç³»çµ±æž¶æ§‹åœ– 
æœ¬è¨ˆç•«ä»Šï¦Žï¨ä¹‹ Application ç‚º U åŒ–é¤
å»³ï¼ŒU åŒ–é¤å»³ç³»çµ±å…±åˆ†ç‚ºï§‘å€‹å ´æ™¯ï¼Œåˆ†åˆ¥
ç‚ºç€è¦½è¨‚ä½ã€é€²å…¥é¤å»³ã€å®¢æˆ¶é»žé¤ã€å»šæˆ¿
é€é¤ã€æ’­æ”¾å½±ç‰‡ã€çµå¸³ï¼Œæ•´å€‹ç’°å¢ƒæž¶æ§‹åœ–
å¦‚åœ–ä¸ƒæ‰€ç¤ºã€‚ä»¥ä¸‹å°‡ä¾ç…§ U åŒ–é¤å»³çš„å ´æ™¯
é€²ï¨ˆï¥¯æ˜Žã€‚ 
 
åœ–ä¸ƒ U åŒ–é¤å»³ç³»çµ±ç’°å¢ƒæž¶æ§‹åœ– 
z ç€è¦½è¨‚ä½ï¼šé¡§å®¢é€éŽå®¶ï§¨çš„å®¢æˆ¶ç«¯é›»è…¦
ä¸Šç¶²è‡³è³‡ï¦¾åº« MUBS[1.4.0]ç€è¦½é¤å»³ç¶²
é  ( ï¥´ ç‚º æœƒ å“¡ å‰‡ å¯ ç¶“ å®‰ å…¨ é€š é“
SSCMS[1.1.0]ç™»å…¥è³‡ï¦¾åº«)ï¼Œä¸¦å–å¾—è¨‚ä½
è³‡è¨Šï¼Œå‡ï¥´é¤å»³ç¾å­˜ç©ºä½ï¼Œå‰‡å¯é€²ï¨ˆè¨‚
ä½å‹•ä½œã€‚ 
z é€²å…¥é¤å»³ï¼šé¡§å®¢åœ¨é€²å…¥é¤å»³é–€å£å‰ï¼Œåœ¨
é–€å£æœƒæœ‰ä¸€è¼•ï¥¾åŒ–è³‡è¨Šæ“·å–å™¨ç³»çµ±
TUS[1.3.0]æ‰€æä¾›ä¹‹æ”å½±æ©Ÿè² è²¬æ“·å–å½±
åƒä¸¦é€éŽå®‰å…¨é€šé“ SSCMS[1.1.0]å‚³é€è‡³
Date:2008/07/20                 åœ¨ï¥¢è³ªç¶²ï¤·ä¸Šæ‰“é€ ä¸€å®‰å…¨ U åŒ–ä¹‹ç’°å¢ƒ(II)æˆæžœå ±å‘Šæ›¸ 
 6
[3]. Moving from Security to Distributed Trust in 
Ubiquitous Computing Environments (http:// 
www.cs.umbc.edu/~finin//paNs/ieee01/ieee01.p
df) 
[4]. Provably Secure Ubiquitous Systems : 
Universally Composable RFID Authentication 
Protocols (http://eprint.iN.org/2006/131.pdf) 
[5]. Security and trust issues in ubiquitous 
environments - the business-to-employee 
dimension (http://ieeexplore.ieee.org/search/ 
wrapN.jsp?arnumber=1268723) 
[6]. M. Hecker et al., â€œA Testbed for Ubiquitous 
Computing using Next Generation Mobile 
Networksâ€, IEEE HSNMC Toulouse., Jun 2004. 
[7]. Jean-Marc Seigneur , Christian Damsgaard 
Jensen,â€Trust enhanced ubiquitous payment 
without too much privacy lossâ€, Proceedings of 
the 2004 ACM symposium on Applied 
computing, pp. 1593 â€“ 1599. 
[8]. D. Llewellyn-Jones, M. Merabti, Q. Shi, and 
B.Askwith, "An Extensible Framework for 
Practical Secure Component Composition in a 
Ubiquitous Computing Environment," in 
International Conference on Information 
Technology, Las Vegas, USA, 2004.  
[9]. D. Llewellyn-Jones, M. Merabti, Q. Shi, and B. 
Askwith.,â€A security framework for executables 
in a ubiquitous computing environmentâ€, 
Globecom 2004, Dallas, USA, 2004. 
[10]. Zhang, Z., Chen, S., Ling, Y., Chow, R., 
â€œCapacity-aware multicast algorithms on 
heterogeneous overlay networksâ€, Parallel and 
Distributed Systems, IEEE Transactions, 
Volume 17, Issue 2, Feb. 2006, pp. 135 â€“ 147. 
[11]. L.C. Wuu, L.S. Lin and S.C. Shiao, "Efficient 
Distributed Multicast Tree Construction in 
High-Speed Networks", International 
Conference on Next Decades of High 
Technologies, 1997 Taiwan, pp. 40-45 
[12]. NIST,A Statistical Test Suite For Random And 
Pseudorandom Number Generators For 
Cryptographic Applications, May 2001 
[13]. N. Galbreath, â€Cryptography for Internet and 
Database Applicationsâ€, Wiley Publishing, Inc., 
2002. 
[14]. Klaus Wehrle, Frank Pahlke, Hartmut Ritter, 
Daniel Muller, Nc Bechler, â€œThe LinuxR 
Networking Architecture: Design and 
Implementation of Network Protocols in the 
Linux Kernelâ€, Prentice Hall, 0-13-177720-3, 
August 01, 2004, page 648. 
[15]. B.D. Zarit, B.J. SuN, F. K. H. Quek, 
â€œComparison of Five Color Models in Skin 
Pixel Classificationâ€. 
[16]. P. Viola and M.J. Jones. Rapid Object Detection 
using a Boosted Cascade of Simple Features. 
IEEE Computer Vision and Pattern Recognition, 
vol. 1, 511-518, 2001. 
[17]. W. Zhao, and R. Chellappa, â€œRobust Face 
Recognition using Symmetric 
Shape-from-Shading,â€ Center for Automation 
Research University of Maryland, College Park, 
Technical Report CAR-TR-919,1999. 
[18]. P. N. Belhumeur, J. P. Hespanha and D. J. 
Kriegman, Â« Eigenfaces vs. Fisherfaces: 
Recognition Using Class Specific Linear 
Projection,â€œ IEEE Trans. on Pattern Analysis 
and Machine Intelligence, Vol. 19, pp. 711-720, 
1997. 
[19]. Rieback, M.R.; Crispo, B.; Tanenbaum, A.S., 
"The Evolution of RFID Security," Pervasive 
Computing, IEEE Volume 5, Issue 1, 
Jan.-March 2006 Page(s):62 - 69. 2006. 
[20]. Ondrej, S. Zdenek, B. Petr, F. Ondrej. H., 
"ZigBee Technology and Device Design," 
International Conference on Systems and 
International Conference on Mobile 
Communications and Learning Technologies, 
pp.129 â€“ 129, April 2006. 
[21]. UPnPâ„¢ Forum; "http://www.upnp.org/"; 
retrieved 2006/6/12 
[22]. Jennic Ltd, â€œJN5121 IEEE802.15.4 / ZigBee 
Wireless Microcontrollers,â€ http://www.jennic. 
com, retrieved 2006/4/10 
[23]. JEAN J. LABROSSE, â€œMicroC/OS-â…¡ The 
real-time kernel 2rdED,â€ CMPBooks, 1992. 
[24]. CMMI (Capability Maturity Model Integration), 
http://www.sei.cmu.edu/cmmi/. 
[25]. Mi-Jeong Kim, Eunkyu Lee, MNoo Kim, and 
Inhak Joo, â€œDevelopment of information 
platform server for telematics service Provider,â€ 
Proc. IEEE International Geoscience and 
Remote Sensing Symposium, 2005, pp. 
1598-1601. 
[26]. A. Schuck and T. Green, â€œNetwork for a 
European forest information service - 
developing a metadata schema as a collaborative 
action of data providers, users and system 
developers,â€ Proc. Sixteenth International 
Workshop on Database and Expert Systems 
Applications, 2005, pp. 674-678. 
[27]. H.G. Enns and S.L. Huff, â€œImplementation of 
information technology in developing countries: 
experiences of a Mongolian Internet service 
Provider,â€ Proc. 32nd Annual Hawaii 
International Conference on System Sciences, 
1999. 
[28]. J.K.W. Lee, D.W. Cheung, B. Kao, J. Law, and 
T. Lee, â€œIntelligent agents for matching 
information providers and consumers on the 
World-Wide-Web,â€ Proc. 30th Annual Hawaii 
International Conference on System Sciences, 
1997, pp. 189-199. 
[29]. S. Markovits, M. Lam, and R. Braun, 
â€œInformation modeling of trouble: a service 
Provider view,â€ Proc. 8th International 
Conference on Telecommunications. 
å ±å‘Šå…§å®¹æ‡‰åŒ…æ‹¬ä¸‹ï¦œå„é …ï¼š 
ä¸€ã€ï¥«åŠ æœƒè­°ç¶“éŽ 
ç¬¬å››å±†ç¶²çµ¡è³‡è¨Šç³»çµ±èˆ‡æŠ€è¡“åœ‹éš›ç ”è¨Žæœƒæ–¼ä¹åä¸ƒï¦Žäº”æœˆå››æ—¥è‡³äº”æœˆä¸ƒæ—¥åœ¨è‘¡è„ç‰™
é¦¬å¾·ï¤¥å³¶ Tivoli Ocean Park ï¨ªåº—å¬é–‹ã€‚æœƒè­°åœ°é»žå°±åœ¨ Funchal å¸‚å€ï¼Œï§ª Funchal åœ‹éš›
æ©Ÿå ´ï¤‚ç¨‹ç´„ 40 åˆ†é˜ï¼Œäº¤é€šï¥§æ˜¯å¾ˆï¥¥ï§ã€‚æœƒè­°åœ°é»žåœ¨ï¨ªåº—ï§‘ï¥Œäº”å€‹æœƒå ´é€²ï¨ˆï¼Œæœƒå ´é‚„æ
ä¾›å…è²»æœ‰ç·šåŠç„¡ç·šç¶²ï¤·ä¾›èˆ‡æœƒè€…ä¸Šç¶²ã€‚æœƒè­°ç¬¬ä¸€å¤©æœ‰å¤§æœƒé–‹å¹•ã€ã„§å ´ Keynote 
Presentationã€å’Œäº”å€‹ Sessions åˆ†ç‚ºä¸‰å€‹æœƒå ´åŒæ™‚é€²ï¨ˆã€‚ç¬¬äºŒå¤©ï¥¸å ´ Keynote 
Presentationsã€å’Œä¹å€‹ Sessions åˆ†ç‚ºä¸‰å€‹æœƒå ´åŒæ™‚é€²ï¨ˆã€‚ç¬¬ä¸‰å¤©ï¥¸å ´ Keynote 
Presentationsã€å’Œä¹å€‹ Sessions åˆ†ç‚ºä¸‰å€‹æœƒå ´åŒæ™‚é€²ï¨ˆã€‚æœ€å¾Œä¸€å¤©æœ‰ä¸€å ´ Keynote 
Presentationã€å’Œä¸‰å€‹ Sessions åˆ†ä¸‰å€‹æœƒå ´åŒæ™‚é€²ï¨ˆã€‚é€™æ¬¡ç™¼è¡¨çš„ï¥æ–‡å…± 154 ç¯‡ï¼Œå…¶
ä¸­å£é ­å ±å‘Šï¥æ–‡ 96 ç¯‡ã€æµ·å ±ï¥æ–‡ 58 ç¯‡ï¼Œï¤­è‡ªå°ç£çš„ï¥æ–‡æœ‰ 3ç¯‡ã€‚æœ¬äººæ–¼ç¬¬äºŒå¤©ç™¼è¡¨ï¥
æ–‡ï¼Œè·Ÿåœ‹å¤–å­¸è€…äº’å‹•ç†±çµ¡ã€‚ 
 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
ï¥«åŠ æœ¬æ¬¡åœ‹éš›æœƒè­°æ„Ÿè§¸ï¥¼å¤šï¼Œå°¤å…¶èƒ½èˆ‡ï¤­è‡ªä¸–ç•Œå„åœ°çš„çŸ¥åå­¸è€…åŠå°ˆå®¶é€²ï¨ˆå­¸è¡“äº¤
ï§Šï¼Œç²ï¨—ç”šå¤šã€‚é€™å€‹ç ”è¨Žæœƒé¦–å±†åœ¨ç¾Žåœ‹èˆ‰è¾¦ï¼ŒæŽ¥è‘—ç¬¬äºŒå±†åœ¨è‘¡è„ç‰™ï¼Œç¬¬ä¸‰å±†åœ¨è¥¿ç­ç‰™ï¼Œ
ç¬¬å››å±†åˆå›žåˆ°è‘¡è„ç‰™ã€‚ç”±æ–¼ä¸»è¾¦åœ‹æ˜¯è‘¡è„ç‰™ï¼Œæ‰€ä»¥ï¥«èˆ‡çš„å­¸è€…ä»¥æ­æ´²äººå±…å¤šï¼Œæœªï¤­æœƒè­°
åœ°é»žå¯èƒ½é‚„æ˜¯åœ¨æ­æ´²å„åœ°ï¼Œå› ç‚ºæ˜Žï¦Žé–‹æœƒåœ°é»žæ˜¯åœ¨è‘¡è„ç‰™ï§©æ–¯æœ¬ã€‚ä¸»è¾¦å­¸æ ¡é¦¬å¾·ï¤¥å¤§å­¸
ç›¸ç•¶ç†±èª æ¬¾å¾…ï¼Œèˆ‰å¦‚ç”¨é¤çš„æº–å‚™ã€é¤˜èˆˆç¯€ç›®çš„å®‰æŽ’ï¼Œï¨¦ç›¡åˆ°åœ°ä¸»ä¹‹èª¼ï¼Œè®“èˆ‡æœƒçš„å­¸è€…ï¨¦
è¦ºå¾—ï¥§è™›æ­¤ï¨ˆã€‚é€™æ¬¡æœ¬äººé™¤ï¦ºä¸»æŒä¸€å€‹æœƒå ´å¤–ï¼Œä¹Ÿï¥«èˆ‡ï¦ºå¹¾å€‹æœ‰èˆˆè¶£çš„å ´æ¬¡ï¼Œå¦‚ Web 
Interfaces and Applicationsï¼›å…¶ä¸­æœ‰å¹¾å€‹ç ”ç©¶ä¸»é¡Œç”šä½³ï¼Œå¯è€ƒæ…®ä½œç‚ºæœªï¤­ç ”ç©¶çš„æ–¹
å‘ã€‚æœƒå ´å¤§éƒ¨åˆ†çš„ç™¼è¡¨è€…ï¨¦æœ‰å‡ºå¸­ï¼Œè€Œä¸”åæ‡‰ç†±çµ¡ï¼Œå€¼å¾—åœ‹å…§å­¸è¡“å–®ä½å­¸ç¿’ã€‚ 
 
ä¸‰ã€å»ºè­° 
é¦–å…ˆæ„Ÿè¬åœ‹ç§‘æœƒåœ¨ç¸½ç ”ç©¶è¨ˆåŠƒä¸‹æ ¸çµ¦å‡ºå¸­åœ‹éš›æœƒè­°çš„è£œåŠ©ï¼Œå¾—èˆ‡åœ‹éš›çŸ¥åå­¸è€…é€²ï¨ˆ
å­¸è¡“äº¤ï§Šã€‚å„˜ç®¡åœ¨ç¸½ç ”ç©¶è¨ˆåŠƒä¸‹æ ¸çµ¦å‡ºå¸­åœ‹éš›æœƒè­°çš„é ç®—ï¥§å¤šï¼Œåœ¨æ­¤é‚„æ˜¯å»ºè­°åœ‹ç§‘æœƒæ‡‰
å¤šé¼“ï¥¿åœ‹å…§å­¸è€…ï¥«èˆ‡åœ‹éš›æœƒè­°ä»¥å¢žå»£ï¨Šèžä¸¦å¸æ”¶æ–°çŸ¥ï¼›å¦å¤–åœ‹å…§å­¸è¡“å–®ä½ä¹Ÿæ‡‰ç©æ¥µçˆ­å–
èˆ‰è¾¦çŸ¥åçš„åœ‹éš›æœƒè­°ä»¥ææ˜‡åœ‹éš›å­¸è¡“çŸ¥åï¨ï¼Œç•¶ç„¶æœƒå ´åœ°é»žçš„é¸æ“‡æ˜¯ç›¸ç•¶é‡è¦ï¼Œåœ‹å…§èˆ‰
è¾¦éŽçš„åœ‹éš›æœƒè­°åœ°é»žå¤§å¤šä¾·é™åœ¨å¤§å­¸æ ¡é™¢ï¼Œå¦‚èƒ½åœ¨åå‹é¢¨æ™¯å€èˆ‰è¾¦ï¼Œå‰‡è¼ƒèƒ½å¸å¼•åœ‹éš›çŸ¥
åå­¸è€…ï¥«èˆ‡ã€‚ 
 
å››ã€æ”œå›žè³‡ï¦¾åç¨±åŠå…§å®¹ 
å¤§æœƒï¥æ–‡é›†ã€å…‰ç¢Ÿç‰‡ã€åŠæœªï¤­åœ‹éš›æœƒè­°å®£å‚³è³‡ï¦¾ 
 
äº”ã€å…¶ä»– 
ç„¡ 
 
2 RELATED PRELIMINARY 
WORK 
Although there have been some researches exploring 
on ontology, most of them focused on using specific 
ontology to assist their work, rather than on building 
ontology. On the other hand, other researches (Trent, 
2002, Rowena, 2005, Dave 2001, Sin-Jae, 2001, 
Yan-Hwang, 2005, Alexander, 2000, Riichiro, 2003, 
Thanh Tho, 2006, Prieto-Diaz, 2003, Yuri A., 2003 
and Ju-in Youn, 2004) addressed building ontology. 
They could be classified into two categories in 
building ontology (strictly speaking, some of them 
are just to propose a schema of object entities). The 
first one is to classify documents into their domain 
based on key terms which are organized by several 
words in documents (Florian, 2002, Dave, 2001, 
Weipeng, 2001, Yin-Fu, 2007, Thanh Tho, 2006 and 
Ju-in, 2004). The other one is to classify keywords 
to construct a taxonomy structure based on 
belonging documents, thesauri, or pre-built ontology 
(Trent, 2002, Rowena, 2005, Sin-Jae, 2001, Yan-
Hwang, 2005, Alexander, 2000, Prieto-Diaz, 2003, 
Vaclav, 2005 and Yuri A., 2003). 
Youn et al. (Ju-in Youn, 2004) first constructed 
the ontology by fuzzy function and relations, and 
then classifies documents based on this ontology. In 
fact, the ontology constructed here is just a word 
relation tree similar to that proposed (Yin-Fu Huang, 
2007). Besides, two papers (Florian, 2002 and Yin-
Fu, 2007) also provide schemas of documents, and 
the classification on documents has the same 
characteristics, since each cluster of documents (or 
each tree node in word relation tree) implies the 
same term feature. However, their methodologies 
are different where one is how to select term features 
to do clustering, and another is how to stretch the 
current level to the next one. 
Since building ontology is so tremendous, it 
should be maintained incrementally, rather than 
building from scratch. Some learning techniques to 
refine the built ontology were proposed (P. Buitelaar, 
2005, AsunciÃ³n, 2003 and Alexander, 2001), and 
even general relationship learning (not focusing on 
Is-A or Parts-of relationships) has been discussed 
(M. Kavalec, 2004, David, 2006 and A. Schutz, 
2005). In our framework, new incremental 
documents could be imported periodically, and then 
the learning process uses them to refine word 
relationships in the same way. 
2.1 Key Terms for Generating 
Ontology 
Term-Document-Matrix (TDM) records the 
frequency that each key term appears in documents, 
and it is also called weighted word histogram 
(Weipeng, 2001). Key terms and documents are two 
dimensions in TDM. If we take the dimension of 
documents as our classified target, key terms can be 
viewed as feature (Florian, 2002, Dave, 2001, 
Weipeng, 2001 and Teuvo, 2000), and vice versa. 
Usually, it is necessary to build ontology to present 
the overall context structure on web pages. Tijerino 
et al. developed an information-gathering engine, 
TANGO, to exploit tables and filled-in forms to 
generate domain-specific ontology (Yuri A., 2003). 
In our framework, TDM is treated as the implicit 
feature to evaluate word correlations. 
FOLDOC (http://foldoc.org/) is an online 
computing dictionary, in which each keyword and 
its relatives are tagged to show their relationships. 
Apted and Kay followed its original relationships 
between words, and transferred the whole keywords 
in the dictionary into a clear relation graph of 
keywords (Trent Apted, 2002). Although it has 
stored about 14,000 computing terms till now, many 
computing terminologies are not yet stored inside. 
2.2 Features of Key Terms 
Besides the documents as the input source, 
additional dictionaries are required to build ontology 
(Sin-Jae, 2001 and Alexander, 2000). The features 
of key terms retrieved from documents and 
dictionaries help to build ontology, which could be 
generalized as three kinds; i.e., document vectors, 
sememes, and the meaning coming from 
dictionaries. 
Sememes are defined as the smallest basic 
semantic unit in HowNet (K. W. Gan, 2002). Some 
papers (Yi, 2002 and Yan-Hwang, 2005) took 
sememes as feature roles to do further processing. 
However, many computing terms are special 
terminologies, the meanings of which could be 
different from their original words. Thus, viewing 
sememes in computing terms as features could not 
be feasible here. Finally, since FOLDOC does not 
have enough computing terms for our work, the 
instruction inside it is somewhat inadequate to 
provide further features. Therefore, we choose The 
Free Dictionary instead as the explicit feature 
provider. 
 
A FRAMEWORK AUTOMATING DOMAIN ONTOLOGY CONSTRUCTION
17
Relationship Summary 
Extractor
Incremental T-D Matrix, 
Incremental Relative Matrix
Concept Hierarchy
Constructor
Synonyms,
Antonym,
Acronyms 
Relatives
OntologySophisticated  Relationship Extractor
Correlation Matrix 
Generator
Unique Acronyms
INSPEC 
Bibliographic Info.
The Free 
Dictionary
WordNetWikipedia
Concept Hierarchy
Key Terms
Learning Phase
Correlation Matrix 
Local Matrix 
Database
T-D Matrix, 
 Relative Matrix
 
Figure 1: System framework. 
Figure 2: Relative-Matrix where KT and T represent terms, and KTi is the same as Ti. 
sophisticated relationship extractor retrieves the 
Is-A and Parts-of relationships from individual 
input and specific external knowledge bases 
(e.g., Wikipedia and WordNet). 
 
The built ontology in the learning phase could be 
refined periodically where the whole processes 
would be executed again, given incremental 
documents as input. 
3.1 Relationship Summary Extractor 
The functionality of Relationship Summary 
Extractor is to extract vector spaces used to evaluate 
the correlations between key terms in the ontology. 
We have two kinds of data sources; i.e., document 
vector extracted from INSPEC bibliographic 
information, and relative vector retrieved from sub-
dictionaries in The Free Dictionary. 
For document vector, index terms (or keywords) 
in every document could be distinguished into two 
categories, subject headings and key phrase 
identifiers that are also called controlled and 
uncontrolled indexing respectively in IEL 
(http://ieeexplore.ieee.org/Xplore/guesthome.jsp.). 
Only the index terms filtered through the filtering 
process would be the key terms in the ontology. 
According to the definition of indexing in INSPEC 
(http://www.lib.nus.edu.sg/lion/slb/d/SD/inspec/insc
ont.html), since uncontrolled indexing contains free-
language words or phrases assigned by INSPEC 
indexers, and has a wider range of terms, it has less 
weighting than controlled one. Thus, the Term-
Document-Matrix is defined as follows. 
where wij is the weight of key term i in document 
j, m is the no. of key terms, n is the no. of 
documents, KT is a key term, S is a set of key terms 
with subject heading, and K is the one of key terms 
with key phrase identifier. 
For relative vector, the terms could be synonyms 
or relatives. There are ten sub-dictionaries in The 
Free Dictionary, but only four of them, called 
Dictionary/thesaurus, Acronyms, Computing 
Dictionary, and Wikipedia Encyclopedia, are used as 
references. Since we aim to construct computer 
science ontology, Computing Dictionary is chosen 
as the target. The synonyms and antonyms of terms 
are collected from Dictionary/thesaurus, the 
acronyms from Acronyms, and the relatives from 
Dictionary/thesaurus, Computing Dictionary, and 
Wikipedia Encyclopedia. The Relative-Matrix 
organized by relative vectors is shown in Fig. 2. 
In summary, Relationship Summary Extractor 
extracts each individual vector for each key term, 
and finally produces Term-Document-Matrix and 
Relative-Matrix. 
3.2 Correlation Matrix Generator 
The next step is to combine two matrices produced 
in the last step into one matrix. However, since there 
are some duplicate terms in these two matrices, such 
as â€œdatabaseâ€ and â€œdatabasesâ€, a merging process 
should be done before combining the matrices. 
Key Terms Synonyms Antonyms Acronyms Relatives 
KT1 T2, T15, T16 â€¦ â€¦ â€¦ T3, T4, T6, T8, T10, T11 
KT2 T1, T14, T15 â€¦ â€¦ â€¦ T4, T5, T7, T9, T10, T11, T12, T13 
... â€¦ â€¦ â€¦ â€¦ 
A FRAMEWORK AUTOMATING DOMAIN ONTOLOGY CONSTRUCTION
19
3.3 Concept Hierarchy Constructor 
To generate a concept hierarchy, we first apply a 
hierarchical clustering technique to build a binary 
clustering tree, and then reorganize the tree into a 
taxonomy tree. 
3.3.1 Hierarchical Clustering 
For the hierarchical clustering algorithm used to 
build the binary clustering tree, the clustering 
criterion is based on Distance-Matrix transferred 
from Correlation-Matrix according to Equation (4). 
 
( , ) 1 ( , ),  
where 0 1.
i j i jDistance KT KT Correlation KT KT
Distance
= âˆ’
â‰¤ â‰¤  (4) 
3.3.2 Reorganizing Binary Clustering Tree 
Since each internal node in the binary clustering tree 
built using the hierarchical clustering algorithm 
contains exactly two children, we would reorganize 
the tree into a taxonomy tree to represent a concept 
hierarchy. Here, we also use the merging technique 
to further reduce the height of the tree where a 
threshold is defined as the merging condition. If the 
distance between parent and child nodes is less than 
the threshold, the child node would be merged into 
the parent node. For the example as shown in Fig. 3, 
child node N1 contains two leaf nodes workflow 
modeling and workflow analysis. For the threshold 
0.005, since the distance (or gap) between node N1 
and N2 is 0.002, less than the threshold, N1 would be 
merged into N2. In other words, there is only one 
internal node N2 left, which contains three leaf nodes 
workflow specification, workflow modeling, and 
workflow analysis. 
 
Figure 3: Enlarged binary clustering tree. 
3.4 Sophisticated Relationship 
Extractor 
Although both Is-A and Parts-of relationships 
between the key terms could be retrieved from 
WordNet, only a few words are for Is-A relationships 
in the computer science domain in WordNet, and 
therefore Wikipedia Categories covering more and 
wider words, instead of WordNet, is used to generate 
Is-A relationships. 
For extracting Is-A relationships, we first label 
each key term (or leaf node) in the concept hierarchy 
generated beforehand, by finding the categories of 
each key term in Wikipedia Categories. As the 
example shown in Fig. 4, â€œComputer hardware 
stubsâ€ and â€œComputer busesâ€ are two categories for 
â€œCompactPCIâ€ and four categories are for â€œISA 
busâ€. Then, each key term in the concept hierarchy 
can be labeled. Next, we try to label internal nodes 
in the concept hierarchy. Since each internal node 
(or cluster) consists of key terms or sub-clusters as 
its children, the label should be the least common 
categories to cover the children. Therefore, we use 
breadth-first search to find the nearest ones among 
the common categories in Wikipedia Categories. As 
shown in Fig. 4, finding from two categories for 
â€œCompactPCIâ€ and four categories for â€œISA busâ€, 
their least common category would be â€œComputer 
busesâ€. In Wikipedia Categories, since each term 
could have multiple categories, the label for an 
internal node might be more than one. By the way, if 
the common categories cannot be found or the key 
terms do not exist in Wikipedia, some internal nodes 
might have no labels. For this case, these internal 
nodes are ignored and the process keeps on finding 
the least common categories till the root is reached. 
For extracting Parts-of relationships, we can 
look up a word in WordNet where not only each 
sense of the word, but also the relevant domains, is 
indicated. Here, the six kinds of Parts-of 
relationships of all key terms in WordNet, as 
mentioned in Section 2.4, are retrieved as the Parts-
of relationships. During the extraction, we also give 
the specified domain to increase the accuracy of 
collected Parts-of relationships. 
A FRAMEWORK AUTOMATING DOMAIN ONTOLOGY CONSTRUCTION
21
As mentioned in Section 3.2.2, since Relative-
Matrix implies more explicit correlations between 
terms than Term-Document-Matrix, Î±  value in 
Equation (1) is set as 0.67 (i.e., the former is double 
weighting than the latter). For the same reason, Î²  
value in Equation (2) is also set as 0.67. The 
statistics of the correlation matrix are presented in 
Table 2. Then, we can use the correlation matrix to 
construct the computer science ontology. 
Table 2: Correlation distributions. 
Correlation values Number of correlations 
=1.0 10,933 
>0&<0.1 3,019,716 
>=0.1&<0.2 1,058 
>=0.2&<0.3 36,294 
>=0.3&<0.4 5,702 
>=0.4&<0.5 485 
>=0.5&<0.6 4 
>=0.6&<0.7 225 
>=0.7&<0.8 11 
>=0.8&<0.9 0 
>=0.9&<1.0 0 
Total 3,074,428 
 
In the Concept Hierarchy Constructor step, we 
employ the software tool - Matlab to do hierarchical 
clustering on Distance-Matrix transferred from 
Correlation-Matrix. Furthermore, seven kinds of 
methods computing hierarchical clustering are 
tested, and then cophenetic correlation coefficient in 
Matlab is used to evaluate how well the generated 
clustering trees are. The cophenetic correlation for a 
clustering tree is defined as the linear correlation 
coefficient between the cophenetic distances 
obtained from generated clustering trees, and the 
original distances (or dissimilarities) used to 
construct the tree. In other words, it is a measure of 
how faithfully a tree represents the dissimilarities 
among observations. The cophenetic correlation 
coefficient for each generated clustering tree is 
shown in Table 3. For the last three methods, 
because of memory limitation, the hierarchical 
clustering algorithm cannot generate their 
corresponding trees. Finally, since the more the 
cophenetic correlation coefficient is and the better 
the clustering tree is, we choose the clustering tree 
constructed by method unweighted average distance 
as the results. 
Table 3: Cophenetic correlation coefficients for generated 
clustering trees. 
Methods Cophenetic correlation 
Shortest distance (default) 0.0984 
Furthest distance 0.3993 
Unweighted average distance 0.4965 
Weighted average distance 0.4732 
Centroid distance No tree generated 
Weighted center of mass 
distance 
No tree generated 
Inner squared distance No tree generated 
 
The gap distribution in the binary clustering tree 
is shown as Table 4. We take the threshold 0.005 as 
the merging condition, and finally 7,358 clusters are 
left to form the concept hierarchy. 
Table 4: Gap distribution in the binary clustering tree. 
Gap/heig
ht 
Merged 
clusters 
Clusters after 
reorganizing 
Reduce 
ratios 
<=0.01 5,381 5,551 0.49222 
<=0.005 3,574 7,358 0.32693 
<=0.002 2,053 8,879 0.1878 
<=0.001 1,362 9,570 0.12459 
<=0.0005 887 10,045 0.08114 
<=0.0002 523 10,409 0.04784 
<=0.0001 331 10,601 0.03028 
=0 38 10,894 0.00348 
 
In the Sophisticated Relationship Extractor step, 
55 records of Parts-of relationships are extracted 
from WordNet, after specifying the computer science 
domain (if no specifying, we would have 657 
records of Parts-of relationships). For Is-A 
relationships, we label each node of the concept 
hierarchy using collected categories from Wikipedia 
Categories. Finally, 180,681 categories are 
collected, and the partial concept hierarchy with Is-A 
relationships is shown in Fig. 5. 
Memorization
1.Central processing unit
2.Computer data
3.Computer memory
Computing
Workstation UNIX workstation
Computer 
workstations
Computer 
storage
1.Incomplete lists
2.Computer storage devices
3.Rotating disc computer storage media
4.Non-volatile memory
SCSI
Computer 
hardware
Computing
â€¦â€¦â€¦
â€¦â€¦â€¦           â€¦â€¦â€¦ â€¦â€¦â€¦       â€¦â€¦â€¦ â€¦â€¦â€¦        â€¦â€¦â€¦  
Figure 5: Partial concept hierarchy with Is-A relationships. 
A FRAMEWORK AUTOMATING DOMAIN ONTOLOGY CONSTRUCTION
23
Conference on Machine Learning and Cybernetics, 
Vol. 1, pp. 234-239. 
Yin-Fu Huang and Chun-Hao Hsu, 2007. â€œPubMed 
smarter: searching the papers with implicit words 
based on Gene Ontology,â€ Proc. 4th International 
Conference on Information Technology and 
Applications, Vol. 1, pp. 339-343. 
Sin-Jae Kang and Jong-Hyeok Lee, 2001. â€œSemi-
automatic practical ontology construction by using a 
thesaurus, computational dictionaries, and large 
corpora,â€ Proc. Workshop on Human Language 
Technology and Knowledge Management, pp. 1-8. 
M. Kavalec, A. Maedche, and V. Svatek, 2004. 
â€œDiscovery of lexical entries for non-taxonomic 
relations in ontology learning,â€ SOFSEM 2004: 
Theory and Practice of Computer Science, LNCS 
2932, pp. 249-256. 
Latifur Khan and Lei Wang, 2002. â€œAutomatic ontology 
derivation using clustering for image classification,â€ 
Proc. Workshop on Multimedia Information Systems, 
pp. 56-65. 
Teuvo Kohonen, Samuel Kaski, Krista Lagus, Jarkko 
alojÃ¤rvi, Jukka Honkela, Vesa Paatero, and Antti 
Saarela, 2000. â€œSelf-organization of a massive 
document collection,â€ IEEE Transactions on Neural 
Networks, Vol. 11, No. 3, pp. 574-585. 
Yan-Hwang Kuo, Chang-Shing Lee, Shu-Mei Guo, and 
YingHsu Chen, 2005. â€œApply object-oriented 
technology to construct Chinese ontology on the 
internet,â€ Journal of Internet Technologies, Vol. 6, No. 
4, pp. 385-394. 
Jim Z. C. Lai and Wen-Feng Wu, 2002. â€œDesign and 
implementation of a classifier for Chinese e-mails,â€ 
Proc. 7th Conference on Artificial Intelligence and 
Applications, pp. 368-373. 
Alexander Maedche and Steffen Staab, 2000. â€œMining 
ontologies from text,â€ Proc. 12th European Workshop 
on Knowledge Acquisition, Modeling and 
Management, pp. 189-202. 
Alexander Maedche and Steffen Staab, 2001. â€œOntology 
learning for the semantic web,â€ IEEE Intelligent 
Systems, Vol. 16, No. 2, pp. 72-79. 
Riichiro Mizoguchi, 2003. â€œTutorial on ontological 
engineering - part 1: introduction to ontological 
engineering,â€ New Generation Computing, Vol. 21, 
No. 4, pp. 365-384. 
Thanh Tho Quan, Siu Cheung Hui, and Tru Hoang ao, 
2006. â€œAutomatic fuzzy ontology generation for 
semantic web,â€ IEEE Transactions on Knowledge and 
Data Engineering, Vol. 18, No. 6, pp.842-856. 
Prieto-Diaz Ruben, 2003. â€œA faceted approach to building 
ontologies,â€ Proc. IEEE International Conference on 
Information Reuse and Integration, pp. 458-465. 
David SÃ¡nchez and Antonio Moreno, 2006. â€œDiscovering 
non-taxonomic relations from the web,â€ Proc. 7th 
International Conference on Intelligent Data 
Engineering and Automated Learning, pp. 629-636. 
A. Schutz and P. Buitelaar, 2005. â€œRelExt: a tool for 
relation extraction in ontology extension,â€ Proc. 4th 
International Semantic Web Conference, pp. 593-606. 
Vaclav Snase, Pavel Moravec, and Jaroslav Pokorny, 2005. 
â€œWordNet ontology based model for web retrieval,â€ 
Proc. International Workshop on Challenges in Web 
Information Retrieval and Integration, pp. 220-225. 
Yuri A. Tijerino, David W. Embley, Deryle W. Lonsdale, 
and George Nagy, 2003. â€œOntology generation from 
tables,â€ Proc. 4th International Conference on Web 
Information Systems Engineering, pp. 242-249. 
Ju-in Youn, He-Jue Eun, Cheol-Jung Yoo, and Yong-Sung 
Kim, 2004. â€œAdaptive documents classification system 
based on ontology constructed by fuzzy function and 
fuzzy relations,â€ Proc. International Conference on 
Cyberworlds, pp. 182-187. 
A FRAMEWORK AUTOMATING DOMAIN ONTOLOGY CONSTRUCTION
25
Yi Guan, Xiao-Long Wang, Xiang-Yong Kong, and Jian 
Zhao, 2002. â€œQuantifying semantic similarity of 
Chinese words from HowNet,â€ Proc. International 
 ç³»çµ±ï§¼åˆ¥è™Ÿ 
å…¬å‹™å‡ºåœ‹å ±å‘Šæè¦ 
 
å‡ºåœ‹å ±å‘Šåç¨±ï¼šWCCI2008å‡ºå¸­å¾Œæ„Ÿæƒ³ 
                                            é ï¥©     å«é™„ä»¶ï¼šâ–¡æ˜¯â–¡å¦ 
 
å‡ºåœ‹è¨ˆç•«ä¸»è¾¦æ©Ÿé—œ/ï¦—çµ¡äºº/é›»è©± 
The Chinese University of Hong Kong 
å‡ºåœ‹äººå“¡å§“å/æœå‹™æ©Ÿé—œ/å–®ä½/è·ç¨±/é›»è©± 
å¼µå‚³è‚²/åœ‹ï§·é›²ï§´ç§‘æŠ€å¤§å­¸/é›»è…¦èˆ‡é€šè¨Šå·¥ç¨‹ç³»/å‰¯æ•™æŽˆ/05-5342601 ext. 4337 
å‡ºåœ‹ï§åˆ¥ï¼šâ–¡1è€ƒå¯Ÿâ–¡2é€²ä¿®â–¡3ç ”ç©¶â–¡4å¯¦ç¿’55å…¶ä»–ï¼ˆå¦‚å‡ºå¸­æœƒè­°ã€ï¥«è§€è¨ªå•ã€
æ¯”è³½ã€æ¥­å‹™è¦–å¯Ÿç­‰ï¼‰ 
 
å‡ºåœ‹æœŸé–“ï¼š97/6/2/~97/6/4         å‡ºåœ‹åœ°å€ï¼šé¦™æ¸¯ 
 
 
å ±å‘Šæ—¥æœŸï¼š97/6/30 
 
 
åˆ†ï§è™Ÿ/ç›®ï¼š 
 
 
é—œéµè©žï¼š 
WCCI 2008ã€neural networkã€‚ 
 
å…§å®¹æ‘˜è¦ï¼šï¼ˆ200~300å­—ï¼‰ 
 
æœ¬å‡ºåœ‹å ±å‘Šæ•˜è¿°æœ¬äººåˆ°é¦™æ¸¯å‡ºå¸­WCCI2008æœƒè­°ï¼Œä¸¦ç™¼è¡¨ï¥æ–‡çš„ç¶“éŽåŠæ”¶
ç©«ã€‚æ­¤æ¬¡æœƒè­°æœ‰ 2228 ç¯‡ï¥æ–‡çš„æŠ•ç¨¿ï¼Œæœ€å¾ŒåªæŽ¥å— 1622 ç¯‡ï¥æ–‡çš„ç™¼è¡¨ï¼ŒæŽ¥å—ï¥¡
ç´„ç‚ºä¸ƒæˆã€‚ç”±æ–¼ WCCI2008 æ˜¯ä¸€å€‹ç”±ä¸‰å€‹å¤§åž‹ç ”è¨Žæœƒ(IJCNN2008ã€Fuzzy-IEEE 
2008ã€CEC2008)æ‰€çµ„æˆçš„è¶…å¤§åž‹åœ‹éš›æœƒè­°ï¼Œï¥«èˆ‡è€…ï¤­è‡ªä¸–ç•Œ 70å¤šå€‹åœ‹å®¶ï¼Œï¥«èˆ‡
æœƒè­°çš„äººï¥©ç›¸ç•¶å¤šã€‚ä¹Ÿå› æ­¤æ•´å€‹æœƒè­°çš„èˆ‡æœƒäººå“¡äº’å‹•ç›¸ç•¶ï¥¼å¥½ï¼Œå½¼æ­¤å¾ˆå¤šçš„æ©Ÿ
æœƒå¯ä»¥æ·±å…¥çš„ï¦ºè§£æœ€æ–°çš„ç ”ç©¶æˆæžœã€‚ç¶œåˆè€Œè¨€ï¼Œè—‰ç”±æ­¤æœƒè­°çš„ç ”è¨Žï¼Œç²è‡´åº•ä¸‹
å¹¾é»žæ”¶ç©«ï¼š(1)ï¦ºè§£ç›¸é—œï¦´åŸŸæœ€æ–°çš„ç™¼å±•ï§ºæ³ã€‚(2)èªï§¼ç›¸é—œï¦´åŸŸçš„ç ”ç©¶å­¸è€…ï¼Œå¢ž
ä¸»æŒäººï¼Œåœ¨è©² sectionä¸­å…±æœ‰ï§‘ç¯‡ï¥æ–‡ï¼Œä¸»è¦å‡æ˜¯æŽ¢è¨Žæ™ºæ…§è¨ˆç®—æŠ€è¡“æ‡‰ç”¨åœ¨é†«å­¸
åŠè¼”åŠ©è¨ºæ–·ç­‰ç›¸é—œå•é¡Œï¼Œæ‰€æœ‰ä½œè€…å‡å‡ºå¸­è©² sessionä¸¦ç™¼è¡¨ï¥æ–‡ï¼Œæœƒä¸­è¨Žï¥ç›¸ç•¶
ç†±ï¦Ÿï¼Œæ˜¯ä¸€å€‹æˆåŠŸçš„ï¥æ–‡ç™¼è¡¨æœƒã€‚ 
 
å¿ƒå¾—ï¼š 
WCCI2008 æ˜¯ä¸€å€‹ç”±ä¸‰å€‹å¤§åž‹ç ”è¨Žæœƒ(IJCNN2008ã€Fuzzy-IEEE 2008ã€
CEC2008)æ‰€çµ„æˆçš„è¶…å¤§åž‹åœ‹éš›æœƒè­°ï¼Œï¥«èˆ‡è€…ï¤­è‡ªä¸–ç•Œ 70å¤šå€‹åœ‹å®¶ï¼Œæœƒå ´ä¸­äººå™¨
ç›¸ç•¶æ—ºç››ï¼Œç‰¹åˆ¥æ˜¯åœ¨å¤§æœƒæ‰€å®‰æŽ’çš„å¹¾å ´ Keynoteå‡å¸å¼•çˆ†æ»¿çš„äººæ½®ï¼Œæ˜¯ä¸€å€‹ç›¸ç•¶
æˆåŠŸçš„æœƒè­°ã€‚ 
åœ¨æœƒè­°é€²ï¨ˆéŽç¨‹ï¼Œé™¤ï¦ºå¯ä»¥ï¦°è½ï¤­è‡ªä¸–ç•Œå„åœ‹æœ€æ–°çš„ç ”ç©¶æˆæžœå¤–ï¼Œï¤å¯ï§
ç”¨ä¼‘æ¯çš„æ™‚é–“èˆ‡ç›¸é—œå­¸è€…å°ˆå®¶é€²ï¨ˆï¦—èª¼äº¤ï§Šã€‚æ­¤å¤–ç¶“ç”±é€™æ¬¡æœƒè­°ï¼Œèªï§¼ï¦ºè¨±å¤š
å­¸è€…å°ˆå®¶ï¼Œå°æ–¼åœ‹å®¶åŠå€‹äººåœ¨åœ‹éš›èƒ½ï¨Šï¨çš„æå‡ç¢ºå¯¦æœ‰é¡¯è‘—çš„å¹«åŠ©ã€‚ 
 
å»ºè­°ï¼š 
1. åœ‹ç§‘æœƒæ‡‰å¤šé¼“ï¥¿ï¦Žè¼•å­¸è€…å‡ºå¸­åœ‹éš›æœƒè­°ï¼Œçµ¦äºˆè¼ƒå¤šçš„å‡ºåœ‹è£œåŠ©ã€‚ 
2. ï¥´åœ‹å…§æœ‰å¤šäººåŒæ™‚ï¥«åŠ ä¸€å€‹æœƒè­°ã€‚å¯çµ„åœ˜å‰å¾€ï¼Œä¸€æ–¹é¢ç¯€ï¥­ï¦ƒè²»ï¼Œä¸€æ–¹é¢å¯
ç›¸äº’ç…§æ‡‰ã€‚ 
 
æ´»å‹•ç›¸ç‰‡ï¼š 
Abstract Ñ§ Most of the thyroid nodules are
heterogeneous with various internal components, which
confuse many radiologists and physicians with their
various echo patterns in thyroid nodules. A lot of texture
extraction methods were used to characterize the
thyroid nodules. Accordingly, the thyroid nodules could
be classified by the corresponding textural features. In
this paper, five support vector machines (SVM) were
adopted to select the significant textural features and to
classify the nodular lesions of thyroid. Experimental
results showed the proposed method classifies the
thyroid nodules correctly and efficiently. The
comparison results demonstrated that the capability of
feature selection of the proposed method was similar to
the sequential floating forward selection (SFFS) method.
However, the proposed method is faster than the SFFS
method.
I. INTRODUCTION
odular lesions of the thyroid are very common among
the general populations. The incidence of palpable
thyroid nodules of the adult population is about 4% to 8%.
The incidence of thyroid nodules is much higher (50%) by
autopsy [1]. Because of sensitivity and convenience,
sonography is the modality of choice for diagnosis and
management of thyroid nodules [1], [2]. Most of the thyroid
nodules tend to have various internal echogenicities in the
sonogram, which makes the definite diagnosis of them
difficult. If the characteristic echogenicities for the major
components of the thyroid nodule can be realized, the
interpretation of thyroid sonography would be more
realistic, the misdiagnosis rate of thyroid cancer would be
decreased and management facilitated.
Sonographic findings of nodular lesions, such as nodular
goiter and thyroid tumors are well documented in textbooks
and many articles [3]-[5]. In nodular goiter, the sonographic
changes are usually heterogeneous. In follicular adenoma,
changes are either isoechoic or hyperechoic. In carcinoma,
the echogenicities vary [6]. Several ultrasound features
have been found to be associated with an increased risk of
thyroid cancer, including hypoechogenicity, predominantly
solid compositionÂ¡etc. No ultrasound feature has both a
high sensitivity and a high positive predictive value for
thyroid cancer [1].
Fine-needle aspiration (FNA) represents a critical
C.Y. Chang and M.F. Tsai are with the Institute of Computer Science
and Information Engineering, National Yunlin University of Science &
Technology, Douliou, Yunlin, Taiwan (e-mail: chuanyu@yunteh.edu.tw).
S.J. Chen is with the Department of Medical Imaging, Buddhist Tzu Chi
General Hospital, Dalin, Chia-Yi, Taiwan.
diagnostic test in determining proper management of
thyroid nodular disease following thyroid ultrasonography.
Except for almost entirely cystic change, nodules larger
than 1 cm should be evaluated, because they have the
potential to be clinically significant cancers [1], [2].
However, many physicians are confused about the nature of
various echo patterns of thyroid nodules, or about target
selection of multiple thyroid nodules during ultrasound
guided FNA. Approximately 10% to 20% of thyroid
biopsies by FNA are nondiagnostic [7]. Nondiagnostic
FNAs of the thyroid may be associated with a high
probability of thyroid malignancy and need reaspiration [7].
Target selection of thyroid nodules disease for improvement
of diagnostic FNA should be emphasized here.
The support vector machine (SVM) has the capability of
generating a hyperplane to separate two sets and providing
good generalization performance. Nowadays, SVM had
been widely used for classification and prediction in many
fields.
Recently, various feature extraction methods were
proposed, i.e., we can obtain a lot of features from medical
images. However, it is difficult to select significant features
from the extracted features. Thus, in this paper, five SVMs
are applied to select the significant features.
Figure 1 shows six types of thyroid nodules with ROI,
which were outlined by radiologist and recognized by
biopsy. To recognize these thyroid nodules, we extracted 78
textural features from each ROI. The SVMs are then
applied to select the significant features from the extracted
78 features. Each SVM is trained by specific features,
which having more discrimination between two types of
thyroid nodules. In the experiments, the SVMs were
implemented by the libsvm [9].
This paper is organized as follows. In Section II, the
approach for thyroid nodules classification is described and
the corresponding procedures including the feature
extraction stage, the feature selection stage, and the
classification of this approach are presented. Section III,
shows the experimental results obtained by the proposed
method. Finally, conclusions are given in Section IV.
II. THYROID NODULES CLASSIFICATION
APPROACH
Figure 2 shows the flowchart of the proposed method.
There are three major parts including feature extraction,
feature selection, and thyroid nodules classification. Details
of these processes are described as follows:
Classification of the Thyroid Nodules Using Support Vector
Machines
Chuan-Yu Chang, Ming-Feng Tsai, and Shao-Jer Chen
N
3092
978-1-4244-1821-3/08/$25.00 cÂ©2008 IEEE
Finally, we convolute them with the image and use the
statistics of the results. The features calculated from LawsÂ¡
texture energy measures are given by the following:
F20) LE mean, F21) EL mean, F22) SL mean, F23) EE
mean, F24) LS mean, F25) LE variance, F26) EL variance,
F27) SL variance, F28) EE variance, and F29) LS variance.
5) Neighboring Gray Level Dependence Matrix: This is
a two-dimensional matrix constructed by the gray level
relationship between every pixel and its neighbors in an
image [15]. Each element within the matrix is computed by
a mask, which defined by the distance parameter. For
example, if the distance = 1, we can obtain a 3Â¡ 3 mask,
when the distance = 2, we can obtain a 5Â¡ 5 mask and so on.
By using the mask, we compute each difference of gray
level which is equal to or less than a factor between center
pixel and its neighbors in an image. If the highest gray level
is g and the distance parameter is d, we can construct a
g-by-(d2-1) neighboring gray level dependence matrix. The
following features are calculated from this matrix:
F30) small number emphasis, F31) large number
emphasis, F32) number nonuniformity, F33) second
moment, and F34) entropy.
There are many combinations of the two parameters to
make up this matrix. In this study, the distance is 1 and the
difference of gray level is 0.
6) Wavelet Features: By using the one-dimensional
lowpass and highpass filter, the image would be
decomposed as a low and a high frequency sub-band,
respectively. Once we perform the decomposition with them
again, we obtain four sub-bands named as LL, LH, HL, HH.
Because of the LL sub-band is the approximation of the
image, so, we calculate the textural features from it.
In this paper, the filter coefficients were set by Antonini
et al. [16]. The mean, standard deviation, and LawsÂ¡
features of the LL sub-band image are calculated as wavelet
features. They are given by the following:
F35) mean, F36) standard deviation, and F37~F46) are
LawsÂ¡ features of the LL sub-band.
7) Fourier Feature based on Local Fourier Coefficients:
The Fourier transform is applied to obtain the local Fourier
coefficients maps of the image [17]. Each coefficient
consists of two parameters: magnitude and phase angle.
Accordingly, the histograms of these two parameters are
obtained. Finally, the mean and standard deviation of the
magnitude and phase angle of local Fourier coefficients
maps are computed as textural feature. They are F47~F54)
are means of 8 magnitudes, F55~F62) are means of 8 phase
angles, F63~F70) are standard deviations of 8 magnitudes,
and F71~F78) are standard deviations of 8 phase angles.
B. Feature Selection
According to the previous stages, we extract totally 78
features from each ROI. We can use them to classify the
thyroid nodules directly, but it will be time-consuming for
the subsequent work when the size of ROI is large.
Thus, a wrapper-type method is carried out for feature
selection. This method introduces a classifier to show us
which combination of features has the best performance, i.e.,
the highest accuracy of classification that using this
combination. In our approach, we use five binary-SVMs to
perform the feature selection. Figure 3 shows the flowchart
of each binary-SVM.
Firstly, all feature vectors have been normalized with each
feature. Then we can calculate F-score [8] of each feature
from different category. Given feature vectors xk , k=1,Â¡ ,m,
where m is total number of feature vectors, and number of
positive and negative instances such as thyroid nodules of
follicles and fibrosis base are n+ and n-, respectively. The
F-score of the ith feature is calculated as:
)(
1
1)(
1
1
)()()(
1
2)()(
,
1
2)()(
,
2)(2)(
Â¦Â¦ 
 

 




 
n
k
iik
n
k
iik
iiii
xx
n
xx
n
xxxxiF (1)
where ix ,
)(
ix ,
)(
ix represent the average of the ith
feature of the whole, positive, and negative data sets,
respectively. )(
,

ikx is the ith feature of the kth positive
instance, and )(
,

ikx is the ith feature of the kth negative
instance.
After obtaining F-score of each feature, all feature vectors
were used to train SVM. A 3-fold cross validation method is
applied to evaluate the performance of each combination of
features [8]. After each process of cross validation, the
validation rate is recorded and the feature with the lowest
F-score will be eliminated. The remaining features are used
to train SVM and perform cross validation again. These
No
Yes
Feature set
Training
feature set
Validating
feature set
f = f - 1 f = 0?
Results
Fig. 3. The feature selection flowchart of each binary-
SVM.
Calculate F-score
Generate new data set with the
number of feature f, where f is
the total number of features
SVM training and
k-fold cross validation
3094 2008 International Joint Conference on Neural Networks (IJCNN 2008)
F-scores of two major bases of thyroid nodules. To illustrate
the selected features have significant discriminability, Fig. 5
shows the distribution of Follicles and Fibrosis base in the
LL-mean (F35) and sum entropy (F5) feature space.
Observing Fig. 5, we can clearly find the two groups can be
separated by a hyperplane easily.
To validate the performance of each SVM, the data set is
randomly partitioned into a training set and a test set. A
3-fold cross validation method is adopted, ie., the ratio of
training and test data is 2:1. For the generalization, we
perform the validation 10 times.
The average accuracy with the most discriminative
feature combination of each SVM was shown in Table II.
From Table II, we find the thyroid nodules can be classified
effectively by one, two or six features in System 1 and one
to three features in System 2. The average accuracies are all
higher than 96%. Table II also indicates the proposed
approach is available for different operation setups.
To demonstrate the effectiveness and efficiency of the
proposed method, the proposed method was compared with
a widely used approach proposed by Pudil et al., called
sequential floating forward selection (SFFS) [21]. The
classification results were shown in Table III. Owing to the
termination condition of SFFS is the number of features,
and it was decided by users. However, in practice, users
usually have no notion of how many features will result in a
better performance. Thus, we set the number of features
equals to that of our method. Observing Table II and III, we
find that the SFFS and the proposed method have similar
performance in average accuracies. However, in the case of
one selected feature, SFFS required less time than ours, but
as the number of selected features is equal to or larger than
two, the proposed method is faster than SFFS.
IV. CONCLUSION
Nodular lesions of the thyroid are very common among
the populations in Taiwan. According to the pathology,
thyroid nodules were categorized as the enlarged follicles,
the follicular cells with follicles, the papillary cells with
follicles, the follicular cells with fibrosis, the papillary cells
with fibrosis, and the fibrosis. Since the low resolution of
sonography, many physicians are confused about the nature
of various echo patterns of thyroid nodules.
To provide a helpful way to decrease the erroneous
diagnosis, a SVM-based thyroid nodules classification
method was proposed. Firstly, we extract 78 textural
features from ROIs, which were outlined by radiologist and
recognized by biopsy. To select the significant features, we
applied the SVMs for feature selection and obtain the more
discriminative feature set of different categorizes of thyroid
nodules. Finally, each SVM was trained by the sifted
features of corresponding category. The experimental
results showed the proposed classification approach
successfully identifies six kinds of thyroid nodules with
good performance. These results are very helpful in
interpretation of thyroid ultrasound and in enhancement of
diagnostic performance of ultrasound guided needle
aspiration.
ACKNOWLEDGEMENTS
This work was supported by Research Grants (NSC
94-2213-E-303-002 and NSC 96-2221-E-303-001) from
National Science Council, R.O.C and grants (DTCRD
96(2)-13) from Buddhist Dalin Tzu Chi General Hospital,
Chia-Yi, Taiwan, R.O.C. The authors would like to thank
department of pathology, Buddhist Dalin Tzu Chi General
Hospital, Chia-Yi, Taiwan, R.O.C. for the support and
guidance.
REFERENCES
[1] Frates MC, Benson CB, Charboneau JW, Cibas ES,
Clark OH, Coleman BG, Cronan JJ, Doubilet PM,
Evans DB, Goellner JR, Hay ID, Hertzberg BS,
Intenzo CM, Jeffrey RB, Langer JE, Larsen PR,
Mandel SJ, Middleton WD, Reading CC, Sherman SI,
Tessler FN, Â¡Management of Thyroid Nodules
Detected at US: Society of Radiologists in Ultrasound
Consensus Conference StatementÂ¡, Radiology
Vol.237, pp.794-800, 2005.
TABLE III
The Average Accuracies and their Corresponding Features
Selected by the SFFS Method
SVM
No.
Avg. Acc. (%) Features Time (sec)
S1 99.80198 F1, F4 1538.423
S2 99.54544 F1, F17 1198.576
S3 100 F1, F2,
F3, F4,
F5, F6
3233.349
S4 100 F1, F18 1118.36
Sy
st
em
1
S5 97.36841 F17 525.515
SVM
No.
Avg. Acc. (%) Features Time (sec)
S1 99.8077 F1, F2, F9 2763.515
S2 100 F1, F3 1321.843
S3 99.04109 F4 731.907
S4 100 F4 589.796
Sy
st
em
2
S5 100 F11 620.032
TABLE II
The Average Accuracies and their Corresponding Features
Selected by the Proposed Method
SVM
No.
Avg. Acc. (%) Features Time (sec)
S1 100 F5, F35 1149.765
S2 99.84848 F1, F24 875.501
S3 100 F5, F21,
F35, F48,
F54, F63
684.61
S4 99.56522 F30, F58 747.608
Sy
st
em
1
S5 96.31578 F34 597.189
SVM
No.
Avg. Acc. (%) Features Time (sec)
S1 100 F10, F65,
F69
1684.42
S2 100 F17, F34 984.984
S3 99.45205 F9 914.171
S4 100 F33 808
Sy
ste
m
2
S5 100 F34 846.983
3096 2008 International Joint Conference on Neural Networks (IJCNN 2008)
å°é¢æ ¼å¼ 
  å…¬å‹™å‡ºåœ‹å ±å‘Š 
  ï¼ˆå‡ºåœ‹ï§åˆ¥ï¼šå…¶ä»–ï¼šå‡ºå¸­æœƒè­°ï¼‰ 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ICICIC2008å‡ºå¸­å¾Œæ„Ÿæƒ³ 
 
 
 æœå‹™æ©Ÿé—œï¼š åœ‹ï§·é›²ï§´ç§‘æŠ€
å¤§å­¸ 
å‡º åœ‹ äººè·    ç¨±ï¼š å‰¯æ•™æŽˆ 
 å§“    åï¼š å¼µå‚³è‚² 
å‡ºåœ‹åœ°å€ï¼šä¸­ åœ‹  
å‡ºåœ‹æœŸé–“ï¼š 9 7 / 6 / 1 6 ~ 9 7 / 6 / 2 2
å ±å‘Šæ—¥æœŸï¼š 9 6 / 6 / 3 0  
 
 
æ ¡
é•·  
å‰¯
æ ¡
é•· 
 
ä¸»
ä»»
ç§˜
æ›¸ 
 
äºº
äº‹
å®¤
 
å–®
ä½
ä¸»
ç®¡
 
å‡º
åœ‹
äºº 
 
æ³ã€‚(2)èªï§¼ç›¸é—œï¦´åŸŸçš„ç ”ç©¶å­¸è€…ï¼Œå¢žé€²åœ‹å®¶åŠå€‹äººåœ¨åœ‹éš›çš„èƒ½ï¨Šï¨ã€‚ 
 
æ­£æ–‡ï¼ˆ1.ç‰ˆé¢ç‚º A4ç›´å¼æ©«å°ï¼Œä¸¦åŠ è¨»é ç¢¼ã€‚2.ç‰ˆé¢è¨­å®šé‚Šç•Œç‚ºä¸Šä¸‹å·¦å³å„è¨­ 2cmã€‚3.å­—é«”ç‚º
æ¨™æ¥·é«”ã€å¤§å°ç‚º 14ï¼Œï¨ˆè·ç‚º 1.5 å€é«˜ã€‚4.å…§å®¹æ‡‰åŒ…å«ã€Œç›®çš„ã€ã€ã€ŒéŽç¨‹ã€ã€ã€Œå¿ƒå¾—ã€ã€ã€Œå»ºè­°ã€åŠ
å…¶ä»–ç›¸é—œäº‹é …ã€‚ï¼‰ 
 
ç›®çš„ï¼š 
æ­¤æ¬¡å‡ºåœ‹çš„ç›®çš„åœ¨æ–¼å‡ºå¸­ The Third International Conference on Innovative 
Computing, Information and Control (ICICIC2008)æœƒè­°ï¼Œç™¼è¡¨ï¥¸ç¯‡ï¥æ–‡â€ Apply an 
Adaptive Center Selection Algorithm to Radial Basis Function Neural Network for 
Face Recognitionâ€åŠâ€ A Figure Extraction and Synthesis System by Learning Vector 
Quantization Neural Networksâ€ï¼Œè—‰ç”±æœƒè­°çš„ç ”è¨Žï¼Œï¦ºè§£ç›¸é—œï¦´åŸŸæœ€æ–°çš„ç™¼å±•ï§º
æ³ã€‚æ­¤å¤–ç¶“ç”±æ­¤æ¬¡æœƒè­°çš„ï¥«èˆ‡ï¼Œäº¦å¯èªï§¼ç›¸é—œçš„ç ”ç©¶å­¸è€…ï¼Œå¢žé€²åœ‹å®¶åŠå€‹äººåœ¨
åœ‹éš›çš„èƒ½ï¨Šï¨ã€‚ 
 
éŽç¨‹ï¼š 
97ï¦Ž 6æœˆ 16æ—¥ä¸Šåˆ 8:30æ­ä¹˜ï§·æ¦®èˆªç©ºç­æ©Ÿ(B7052)å‰å¾€éŸ“åœ‹é¦–ï¨¦é¦–çˆ¾å¸‚ï¼Œ
ç”±æ–¼ç•¶å¤©é«˜é›„å¤§é›¨ï¼Œèƒ½ï¨Šï¨æœªé”é£›æ©Ÿèµ·ï¨‰æ¨™æº–ï¼Œç­æ©Ÿå»¶èª¤è¿‘ 30åˆ†é˜å¾Œèµ·é£›ï¼Œç¶“
ï¦Œç´„ 2.5å€‹å°æ™‚çš„é£›ï¨ˆï¼Œæ–¼ç•¶åœ°æ™‚é–“ 6 æœˆ 16æ—¥ä¸­åˆ 13:25å·¦å³æŠµé”éŸ“åœ‹é¦–çˆ¾ã€‚
å‡ºé—œéš¨å³æ­ Buså‰å¾€é¦–çˆ¾ï¼Œã€Žç„¡æ‰€ï¥§åœ¨å¤¢æƒ³å±‹ã€ (Ubiquitous Dream hall) é€²ï¨ˆï¥«
è¨ªï¼Œï¦ºè§£éŸ“åœ‹åœ¨ Ubiquitousæ‡‰ç”¨çš„æŠ€è¡“ç™¼å±•æƒ…æ³ã€‚ 
6æœˆ 17æ—¥åœ¨é¦–çˆ¾å¸‚å€é€²ï¨ˆçŸ­æš«çš„åœï§ï¥«è§€å¾Œæ­æ©Ÿå‰å¾€ ICICIC2008çš„æœƒè­°
åœ°é»žã€Œå¤§ï¦šã€ã€‚å°±åœ¨é£›æ©Ÿï¨‰ï¤˜å‰ï¼Œæ©Ÿé•·å»£æ’­ï¥¯æ˜Žå› å¤§ï¦šæ©Ÿå ´å¤§é›¨åŠæ¿ƒéœ§çš„é—œä¿‚ï¼Œ
é£›æ©Ÿæœ€å¾Œè½‰ï¨‰å¤©æ´¥ã€‚ç¶“éŽæ¼«é•·çš„ç­‰å¾…ï¼Œçµ‚æ–¼æ–¼ 6/18ï¥•æ™¨ 1:30åˆ†å·¦å³æŠµé”å¤§ï¦šæ©Ÿ
å ´ã€‚ 
æ­¤æ¬¡æœƒè­°æœ‰ä¸Šåƒç¯‡ï¥æ–‡çš„æŠ•ç¨¿ï¼Œæœ€å¾ŒåªæŽ¥å— 604ç¯‡ï¥æ–‡çš„ç™¼è¡¨ï¼ŒæŽ¥å—ï¥¡ç´„
ç‚ºï§‘æˆã€‚æœƒè­°çš„èˆ‡æœƒäººå“¡äº’å‹•ç›¸ç•¶ï¥¼å¥½ã€‚ 
Apply an Adaptive Center Selection Algorithm to Radial Basis Function
Neural Network for Face Recognition
Abstract
In general, the principal component analysis
(PCA) technique is applied to reduce the feature
dimensions. In this paper, different from traditional
PCAs, the PCA is used to select adequate centers for
the classifier of radial basis function neural networks
(RBFNN). In addition, a novel weights updating
method is also included in the RBFNN for face
recognition. The specific design, not only increases
the convergent speed, but also retains generalization
ability. Experimental results show the proposed
method has high recognition rate with a short
training time.
Keywords: principal component analysis, radial
basis function neural network, face recognition.
1. Introduction
Face recognition is an important issue in security
system, film processing, identification card
recognition, and criminal identification system [4].
The most challenge arisen in face recognition is
various variations exist in the face image, such as
illumination, pose, facial expression, aging, hair, and
glasses [1,3,6].
Generally speaking, face recognition approaches
can be divided into feature-based, template-based,
statistics-based and neural network-based categories
[4,6]. Feature-based approaches are based on the
geometrical relationships of invariant salient features
of the face, such as eyes, eyebrows, mouth, nose, and
chin [4]. The recognition rate of the featureâ€“based
techniques is highly depended on the correctness of
the detected invariant salient features [6].
Unfortunately, the variations of illumination and
facial expression will affect the detection of invariant
salient features [3]. Template-based approaches are
based on similarity measurement of two feature sets,
which can be calculated without the pairing of the
invariant salient features. The drawback of the
template-based method is the recognition results are
highly depended on the variation of scale, pose and
shape [1,4]. The principal component analysis (PCA)
is a well-known statistics-based approach, which is
often applied to pre-processing of face recognition. In
general, PCA was used to determine an optimal linear
transformation matrix. The input data are projected
from original n-dimensional feature vector space onto
m-dimensional eigenvector space through the matrix
[2]. The drawback of the PCA is that the PCA cannot
attenuate the influences of variation of lighting, facial
expression, and other factors [4,5]. Whereas, neural
networks have capabilities of robustness and
generalization, thus they had been widely used in
face recognition [5,6]. Neural network-based
approaches are learning from the sample images to
find the relevant characteristics of face images.
Neural networks can be trained to capture more
knowledge about the variation of face patterns, and
thereby achieving good generalization [4]. Among
the neural networks models, the radial basis function
neural network (RBFNN) has the capability of
cure-fitting in high dimensional space. Being a
function approximator, the training time of RBFNN
is shorter than multilayer perceptron (MLP).
Therefore, the RBFNN is used in this paper. In
addition, to alleviate the variation of illumination, a
Gabor wavelet transform procedure is also used [8].
The block diagram of the proposed face recognition
system is shown in Fig. 1.
Gabor wavelet
Transform
PCA-based
Center Selection
Neural NetworkFace Image Result
Fig. 1 Block diagram of the proposed method.
2. The proposed method
2.1 Feature extraction based on Gabor
wavelet transform
Gabor wavelet transformation in spatial and
frequency domain had been applied to various pattern
recognition problems for feature extraction [8]. The
Gabor filter has two characteristics including scale
normalization and grayscale equalization. Scale
normalization is depended on the size of the Gabor
filter. The Gabor filter transforms the image into a
fixed normalization size and eliminates the influence
of illumination variance to make post-processes more
easily and conveniently. The general function form of
the Gabor Transform in spatial and frequency domain
is given by
Chuan-Yu Chang Hung-rung Hsu
Institute of Computer Science and Information Engineering,
National Yunlin University of Science & Technology
Emailï¼šchuanyu@yuntech.edu.tw Emailï¼šg9517724@yuntech.edu.tw
(person), ïƒ¥ï€½ï€½ ni ijj InI 1/1 is the averaged face
image of j-th class. j
i
j
i
j IIZ ï€­ï€½ is a zero mean.
Accordingly, the initial center of j-th hidden neuron is
obtained by:
kjxc jj ,...,2,1, ï€½ï€¢ïƒ—ï€½M (8)
where Mj is the transformation matrix of the j-th
hidden neuron in the RBF neural network.
Furthermore, the feature vector (cj) with n-dimension
obtained by Eq. (8) is similar to the original input
vector (x) with m-dimension, nâ‰¦m. In this paper, let
kjniIS ijj ,...2,1},,...,2,1|{ ï€½ï€½ï€½ be the j-th class
image set, i.e. Sj consists of face images of the j-th
person.
2.2.3 Estimation of the parameters. There are three
factors affecting the training of RBFNN:
1.Adjustment of the weights between output layer
and hidden layer. 2. The number of centers. 3. The
initial value of centers. Among them, the second
factor can be ignored, because we set the number of
centers equal to the number of persons. To eliminate
other affecting factors, a specific competitive learning
mechanism is applied to the hidden layer. In
competitive learning, only a single neuron is active at
any one time [7]. The specific competitive learning is
applied in the hidden layer rather than the output
layer. The schematic diagram of the specific
competitive learning is shown in Fig.2. If the hidden
neuron k has won the competition, the synaptic
weights between output node j and hidden neuron k is
updated. The error cost function J of the neural
network is defined as follows:
2
2
1
eï€½J , (9)
where e is the error term defined as:
yye ï€­ï€½ d , (10)
where yd and y is the desired and the actual output of
the network, respectively. The winner neuron, ï¡, is
determined by:
kjj
j
,...,2,1maxarg ï€½ï€½ ï¦ï¡ (11)
According to the specific competitive learning,
the weights between the winning hidden neuron, ï¡ï€¬
and output neuron can be updated by
ïƒ®
ïƒ­
ïƒ¬ ï€½ïƒ—ïƒ—ï€«ï€½ï€«
otherwise)(
if)(
)1(
nw
inw
nw
i
i
i
ï¡ï¦ï¨ ï¡e
(12)
where ï¡is the index of the hidden neuron array that
specifically identifies the winning neuron, and ï¨is
the learning rate parameter[2].
Finally, the center is updated according to the
maximum value of the neuron in output layer which
is defined as follows:
mjy j
j
,...2,1maxarg ï€½ï€½ï¢ (13)
where ï¢is the index of the winner output neuron. If
the input vector x is not correctly recognized, the
input vector is then append to the face images set.
This can be described as following.
ïƒ¯ïƒ®
ïƒ¯ïƒ­
ïƒ¬ ï‚¹
ï€½ï€«
otherwise)(
if)(
)1(
nS
CxnS
nS
x
ï¢
ï¢
ï¢
ï¢ï•
(14)
where the Cx is the class label of the input vector x.
Accompany with the modification of the face images
set S, the Mï¢is re-calculated.
3. Experimental results
In order to evaluate the proposed method, three
face databases including the ORL, the PICS, and the
Yale face database are adopted. In ORL, there are 400
face images taken from 40 persons in 10 different
situations. The situations are varying light intensity,
facial expressions, and facial details. The images are
256 gray level with size of 92 Ã— 112. Samples of
ORL face database are shown in Fig. 3(a). The PICS
face database contains 690 color images. The selected
images contain 300 face images taken from 30
persons in different situations. The total number of
images for each person is 10. Samples of PICS face
database are shown in Fig. 3(b). The Yale face
database contains 165 face images of 15 persons, i.e.
each one has 11 images. Images of the individuals
have been taken in varying light intensity, facial
expressions, and facial details. Samples of Yale face
database are shown in Fig. 3(c).
(a) (b)
(c)
Fig. 3 Samples of face database in the (a) ORL
databaseã€(b) PICS database and (c) Yale database.
3.1 Determine the spread parameter and
learning rate
There are two parameters including spread
parameter and learning rate in the RBFNN. They
should be chosen appropriately to obtain a better
recognition rate. Thus, the PICS face database is
adopted to tune the parameters. Since there are 30
individuals in the PICS face database, the number of
output node and hidden node of the RBFNN are all
set as 30. The training set is selected 30Ã—5 images
A Figure Extraction and Synthesis System by Learning Vector Quantization
Neural Networks
Chuan-Yu Chang, Zong-Yu Tsai and Chun-Hsi Li
Institute of Computer Science and Information Engineering,
National Yunlin University of Science and Technology, Taiwan
chuanyu@yuntech.edu.tw
Abstract
Extracting complete figures from videos with
complicated environments is difficult. A new figure
extraction and synthesis system with capability of
extracting figures from consecutive frames in a messy
environment is proposed in this paper. A figure
template is constructed based on the face detection
results and some image processing techniques. Figural
and non-figural features are extracted from the figure
images. By means of these features, a learning vector
quantization neural network (LVQNN) is applied to
classify the uncertain regions into figural and non-
figural objects. The extracted figure can be further
synthesized into an optional cinestrip. Experimental
results showed the proposed method successfully
extract the figure object from a complex background
environment.
1. Introduction
Moving object extraction is an important issue in
video system which is widely used in many
applications, such as video compression, sports
reporting, surveillance, and traffic management system.
Researchers have proposed many methods to solve the
problems of the moving object extraction [1-2].
However, two potential problems exist in these
methods: (1) how to specify the initial moving object
and (2) objects extraction from a complicated
background is difficult. To solve these problems, a face
tracking method proposed by Chang et al. [3] is
applied to locate the possible figural regions in this
paper. The technique detects possible face-images and
circumscribes them as labeled figure-images. The
initial objects are selected according to the figure-
images. The probable figural region locating decreases
the cost of operating the whole image, and increases
the performance of figure extraction. However, the
figure images here contain both figural portions, and
non-figural portions. To extract figural region from the
figure images efficiently, we average the figural regions
obtained by the preprocessing procedure to construct a
figure template. By means of two thresholds, the figure
template can be roughly separate as figure, non-figure,
and unknown region. In this paper, a learning vector
quantization neural network (LVQNN) is further
applied to classify the unknown region into figural and
non-figural regions. The LVQNN developed by
Kohonen et al. [6-7] is a useful tool for pattern
classification. The network is trained with the
coefficients of discrete cosine transform (DCT) of
regions acquired from figural and non-figural regions.
Once the training process completed, the trained
networks can be used to classify the unknown region.
The classified figural regions cooperating with figural
region are taken as the figure-object extracted from the
video of moving object.
Experimental results showed the proposed method
successfully extract the figure object in a complex
background environment.
This paper is organized as follows. The pre-process
is briefly described in Section 2. The proposed method
is presented in Section 3. Finally, the experimental
results and conclusions are drawn in the Section 4 and
5, respectively.
2. Pre-processing Procedure: figure
template construction
At the beginning, we apply a robust face detection
method [3] to locate the possible figure image from
frames (images) sequence of basketball-shooting
videos. For real-time requirement, the color figure
images of pixel-size 200Ã—240 are transformed to 256-
gray-level images. The pre-processing consists of two
major procedures. First, a four-step operation is used to
acquire figural regions from the figure-images. Second,
is larger than the threshold thf the pixel is regarded as
the figure region. If the intensity of a pixel is smaller
than the threshold thn the pixel is regarded as the non-
figure region. If the intensity of a pixel is between thf
and thn the pixel will be regard as the unknown region.
To obtain the features for further classification, the
figural and non-figural regions are divided into 8Ã—8
blocks. The blocks are categorized by
ï€¨ ï€©
ï€¨ ï€©
,1,...,0and
;7,...,0;7,...,0
,
otherwise,
,allif,
,allif,
,
,
,
ï€­ï€½
ï€½ï€½
ïƒ¯
ïƒ®
ïƒ¯
ïƒ­
ïƒ¬
ï€¼
ï€¾
Ni
yx
B
thyxbB
thyxbB
ui
nini
fifi
(7)
where bi(x,y) denotes the intensity of pixel (x,y) in the
i-th block, N is the number of blocks; Bi,f, B i,n, and B i,u
are the block i that belongs to figure, non-figure, and
unknown region, respectively.
Training and Testing
LVQ NNFeature
Extraction
and
Fusion
Frame tn
DCTTraining
Template
Image
Sequence
â€¦.
Testing
intensity of template < tn
intensity of template > tf
Figure 3. The schematic flow diagram of the proposed
method
Discrete cosine transform (DCT) is adopted to
acquire the features in each block. The DCT formula is
defined as
ï€¨ ï€© ï€¨ï€©ï€¨ï€© ï€¨ ï€© ï€¨ ï€© ï€¨ ï€©
ï€¨ï€© ï€¨ï€©
ï€¨ï€© ï€¨ï€©
ï€¨ ï€©
ï€¨ ï€© outputoftcoefficien:,
inputofintensity:,
,0for1,0for1
,0for
2
1
,0for
2
1
:where
(8)
16
12
cos
16
12
cos,
4
1
,
7
0
7
0
vuD
yxb
vvcuuc
vvcuuc
vyux
yxbvcucvuD
i
i j
i
ï‚¹ï€½ï‚¹ï€½
ï€½ï€½ï€½ï€½
ï€«ï€«ï€½ ïƒ¥ïƒ¥
ï€½ ï€½
ï°ï°
3.2 Categorization with Learning Vector
Quantization neural network (LVQNN)
The LVQNN [6-7] is a supervised learning neural
network that can classify input vectors based on vector
quantization. The architecture of the LVQNN is shown
in Fig. 4. The LVQNN is adopt to classify the unknown
region Bi,u into figure and non-figure region. In addition,
the lower bands in DCT generally contain most of
information and the corresponding coefficients are
lager than other bands. Thus, the lower-band
coefficients of DCT in blocks of Bi,f and Bi,n are
extracted and used to training the LVQNN. After the
training process, the unknown region Bi,u can be
explicitly classified by the LVQNN.
â€¦
w11
w21w12
w22
wN1
wN2
y1
y2
x1
x2
xN
Output Classes
Input vector:
xi
i=1,2,â€¦,N
â€¦
Figure 4. The architecture of LVQNN
We use the first ten elements of zigzag scan on the
DCT coefficients of Bi,f and Bi,n as the input vectors. In
the LVQNN, the input vectors are consisted of thirty
DCT coefficients of colors R, G and B in each block;
and the output consists of two classes figure and non-
figure. Then let the synaptic weights between the
output neuron j and the inputs wj as
ï› ï 2,1,...,, 21 ï€½ï€½ jwww jNjjjw (9)
where N is the dimension of the input vector (N=30).
For each input vector xi, wj is determined by the
following equation:
ï€¨ ï€© 2
2
,min,min ji
j
ji
j
d wxwx
ï€¢ï€¢
ï€½ (10)
Let Cwj be the class that is associated with the weight
vector wj, and Cxi be the class label of input vector xi to
the network. The weight vector wj is adjusted in the
following manner:
If Cwj = Cxi, then
ï€¨ ï€© ï€¨ï€© ï€¨ï€© ï€¨ï€©ï› ïkkkk jijj wxww ï€­ï€«ï€½ï€« ï­1 (11)
If Cwj ï‚¹Cxi, then
ï€¨ ï€© ï€¨ï€© ï€¨ï€© ï€¨ï€©ï› ïkkkk jijj wxww ï€­ï€­ï€½ï€« ï­1 (12)
where 0<Î¼(k) <1 (the learning rate parameter)
4. Experimental Results
Five basketball-shooting videos with different
players and different situations are used to evaluate the
performance and robustness of the proposed method.
The proposed method was implemented by C++
Builder 6.0 on an Intel Pentium IV 2.8GHz processor
with 1GB RAM. In addition, the resolution of the video
frames is 640Ã—480 pixels (bitmap image).
In the experiments, the parameters thd, thf, thn, and
the learning rate of LVQNN are set as 35, 254, 20, and
0.05, respectively.
å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
                                                             
è¨ˆç•«ç·¨è™Ÿ NSC 96-2218-E-224-005 
è¨ˆç•«åç¨± åœ¨ï¥¢è³ªç¶²ï¤·ä¸Šæ‰“é€ ä¸€å®‰å…¨ U åŒ–ä¹‹ç’°å¢ƒ 
å‡ºåœ‹äººå“¡å§“å 
æœå‹™æ©Ÿé—œåŠè·ç¨± 
ä¼ï¦ˆæ¨µ 
é›²ï§´ç§‘æŠ€å¤§å­¸é›»é€šç³»   æ•™æŽˆ 
æœƒè­°æ™‚é–“åœ°é»ž 6 æœˆ 17 æ—¥è‡³ 6 æœˆ 21 æ—¥, 2008, Dalian, China 
æœƒè­°åç¨± The 2008 International Conference on Innovative Computing, Information and Control 
ä¸»æŒæœƒè­°åç¨± Innovative Computing for Image Analysis 
 
ä¸€ã€ï¥«åŠ æœƒè­°ç¶“éŽ 
IEEE ICICIC2008åœ‹éš›æœƒè­°æ–¼ 6æœˆ17æ—¥è‡³6æœˆ21æ—¥åœ¨Chinaçš„Dalian University
èˆ‰ï¨ˆï¼Œï¥«åŠ æœ¬å±†å¤§æœƒçš„ï¨ˆç¨‹ï¼Œç°¡è¿°å¦‚ä¸‹ï¼š 
æ™‚é–“ å‡ºç™¼é»ž ç›®çš„åœ° ï¨ˆç¨‹æ¦‚è¦ 
6/16 ä¸­æ­£æ©Ÿå ´ Korea 
é›†åˆæ–¼ä¸­æ­£æ©Ÿå ´ï¼Œæ­ä¹˜ä¸­è¯èˆªç©ºå…¬å¸ç­æ©Ÿå‰å¾€
Koreaã€‚ 
6/17 Korea Dalian 
ï¥«è§€ Korea çš„ Ubiquitous Dream Hullï¼Œäº¤ï§Š U
åŒ–ç™¼å±•æŠ€è¡“ä¹‹å¾Œï¼Œæ­ä¹˜å—æ–¹èˆªç©ºå…¬å¸ç­æ©Ÿå‰å¾€
China æ™šé–“ä½å®¿ä¸­å±±ï¨ªåº—ã€‚ 
6/18 ä¸­å±±ï¨ªåº— 
Dalian 
University 
æ“” ä»» æœƒ è­° â€Innovative Computing for Image 
Analysisâ€çš„ session chairpersonã€‚ 
6/19 ä¸­å±±ï¨ªåº— 
Dalian 
University 
ï¥«åŠ  IEEE ICICIC2008 åœ‹éš›æœƒè­°ã€‚ 
6/20 ä¸­å±±ï¨ªåº— 
Dalian 
University 
ï¥«åŠ  IEEE ICICIC2008 åœ‹éš›æœƒè­°ã€‚ 
6/21 ä¸­å±±ï¨ªåº— 
Dalian 
University 
ï¥«è§€ Dalian University çš„åšç‰©ï¨¬ï¼Œäº¤ï§Š U åŒ–ç™¼
å±•æŠ€è¡“ã€‚ 
6/22 ä¸­å±±ï¨ªåº— ä¸­æ­£æ©Ÿå ´ 
ä¸Šåˆ 6 é»žç”±ä¸­å±±ï¨ªåº—å‡ºç™¼ï¼Œå‰å¾€ Dalian æ©Ÿå ´ï¼Œ
æ­ä¹˜å—æ–¹èˆªç©ºå…¬å¸ç­æ©Ÿç¶“ç”± Korea è½‰æ©Ÿï¼Œæ­ä¹˜
ä¸­è¯èˆªç©ºå…¬å¸ç­æ©Ÿå›žåˆ°æ¡ƒåœ’ä¸­æ­£æ©Ÿå ´ï¼ŒçµæŸï¨ˆ
ç¨‹ã€‚ 
 
 åœ– 3. U-home å†°ç®± 
 
åœ– 4. U-city äº¤é€šå·¥å…· 
 
åœ– 5. Ubiquitous Dream Hull 
å››ã€ï¥«èˆ‡ IEEE ICICIC2008 åœ‹éš›æœƒè­° 
IEEE ICICIC2008åœ‹éš›æœƒè­°æ–¼ 6æœˆ17æ—¥è‡³6æœˆ20æ—¥åœ¨Chinaçš„Dalian University
èˆ‰ï¨ˆï¼Œæœ¬å±†å¤§æœƒçš„è­°ç¨‹å®‰æŽ’ï¦º 70 å ´çš„ Technical Sessionsï¼Œåˆ†ç‚ºä¸‰å€‹ Tracks åŒæ™‚é€²ï¨ˆï¼Œ
å¦‚ä¸‹æ‰€ç¤º: 
 
 
 
 
 
æœ¬å±†å¤§æœƒæ”¶åˆ° 1000 å¤šç¯‡çš„æŠ•ç¨¿ï¥æ–‡ï¼Œä½†åªæŽ¥å— 604 ç¯‡ï¥æ–‡ï¼Œï¥æ–‡çš„æŽ¥å—ï¥¡å¦‚ä¸‹æ‰€ç¤ºã€‚ 
 
 åœ– 6. å¤§æœƒä¹‹é‚€è«‹å‡½åŠï¥æ–‡æŽ¥å—ï¥¡ 
 
åœ– 9. ç­‰ï§ªå­çƒ(ç‰©ï§¤ï¨¬) 
 
åœ– 10. æŽ’çˆ†æœºå™¨äºº 
 
åœ– 11. å¤§ï¦šå¤§å­¸å…¬é—œä¸»ä»»è¦ªè‡ªè§£ï¥¯ 
 
 
â€¢ Service Oriented Computing  
â€¢ VLSI and Design Methodology  
â€¢ Video Surveillance  
ä¸»è¦è´ŠåŠ©å–®ä½ï¼š 
University of Technology, Sydney, Australia  
IEEE Computer Society 
IEEE Technical Committee on Scalable Computing 
Research Institute for Information and Communication Technology, Korea University, Korea  
BK 21 Information Technology Division, Korea University, Korea  
ARC Research Network in Enterprise Information Infrastructure (EII), Australia  
Federation of Chinese Scholars in Australia (FOCSA)  
Australian Chinese ICT Professional Society
æœƒè­°è³‡è¨Šï¼š 
æ­¤æœƒè­°æœ‰550ç¯‡ï¥æ–‡æŠ•ç¨¿ï¼ŒæŽ¥æ”¶æˆregular papersåƒ… 27%. æ¯ç¯‡ï¥æ–‡è‡³å°‘æœ‰2ä½fully 
reviewer. æ­¤æ¬¡ï¥«èˆ‡æœƒè­°ä¹‹äººï¥©ç´„400äºº, å…¶ä¸­å°ç£éŽåŽ»èˆ‡æœƒä¹‹äººå£«ç´„æœ‰20å¤šäººï¼Œå¦‚ï¼šä¸­
æ­£å¤§å­¸ ï§‰ å‰¯æ ¡é•· ï¤Šç« , å—è¯å¤§å­¸ è³‡å·¥ç³» è”¡åŠ æ˜¥ä¸»ä»», å‹¤ï¨—ç§‘å¤§ è³‡å·¥ç³» çŽ‹åœ³æœ¨ä¸»
ä»»ç­‰ç­‰. 
 
æœƒè­°å…§å®¹ï¼š 
æ­¤æœƒè­°ç‚ºæœŸå››å¤©ï¼Œåˆ†æˆå¤šå€‹sessionèˆ‰ï¨ˆï¼Œä¸”æ¯å¤©æ—©ä¸Šçš†æœ‰keynotesæ¼”è¬›ï¼ŒåŠåŒ…å«ï¥¸å ´special 
talksèˆ‡ä¸€å ´tutorialã€‚ 
Keynotes Topicæœ‰ï¼š 
1ã€Detection and Traceback of DDoS attacks, Prof. Wanlei Zhou, Deakin University, Australia 
2ã€Ubiquitous/Pervasive Intelligence: Visions and Challenges, Prof. Laurence Tianruo Yang, St 
Francis Xavier university, Candada 
3ã€High dimensional data analysis in computer vision, David Suter, monash University, Australia 
4ã€Dynamic Mobility Management, Albert Y. Zomaya, University of Sydney, Australia. 
 
Tutorialä¸»é¡Œç‚ºï¼š 
Information Fusion in Wireless Sensor Networks, Prof. Antonia A.F. Loureiro, Federal University 
of Minas Gerais, Brazil 
 
Special Talksï¼š 
1ã€ Die-hard Sensor Network: Its Concept and Applications, Prof. Toshiaki Miyazaki, The 
University of Aizu, Japan 
2ã€ Research on Visual Tracking and 3D Face Recognition,  Prof. Jie Yang, Shanghai Jiaotong 
University, China. 
  
æœ¬æ¬¡æ–¼æ­¤ï¥æ–‡æ‰€ç™¼è¡¨ä¹‹ï¥æ–‡â€The Research of zero packet loss hand-off mechanism in SIP-
based wireless networks,â€æ˜¯è¢«å®‰æŽ’åœ¨ Ubiquitous and Sensor Network Session ç™¼è¡¨ (7æœˆ10æ—¥ä¸‹
åˆ)ã€‚é™¤ï¦ºï¥æ–‡ç™¼è¡¨å¤–ï¼Œäº¦æ“”ä»»æ­¤Session ä¹‹Session Chair. 
 
 
 
 
 
 
