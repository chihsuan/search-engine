 1
ä¸­æ–‡æ‘˜è¦ 
è‡ªé¦–å°åƒåœ¾éƒµä»¶å‡ºç¾é–‹å§‹ï¼Œå…¶ï¥©ï¥¾ï¥¥ï¥§æ–·æˆé•·ã€‚è¿„ä»Šï¼Œåƒåœ¾éƒµä»¶æ°¾ï¤¢ç¾è±¡å·²ç„¶æˆç‚ºæ¯
ä¸€ä½é›»å­éƒµä»¶ä½¿ç”¨è€…æœ€è¿«ï¨€å¸Œæœ›è§£æ±ºçš„å•é¡Œã€‚ç‚ºæ­¤ï¼Œéƒ¨åˆ†ç ”ç©¶è€…æ²¿ç”¨æ—¢æœ‰æ–‡ä»¶åˆ†ï§æ¼”ç®—æ³•
é€²ï¨ˆåƒåœ¾éƒµä»¶åˆ†ï§ã€‚å¤šï¥©æ–‡ä»¶åˆ†ï§æ–¹æ³•è‘—é‡æ–¼ç™¼æ˜æ­£å¸¸éƒµä»¶æˆ–åƒåœ¾éƒµä»¶å…±åŒç‰¹å¾µï¼Œçˆ¾å¾Œä»¥
æ­¤ç‚ºä¾æ“šé€²ï¨ˆåˆ†ï§ã€‚ç„¶è€Œä¸¦éæ‰€æœ‰ç‰¹å¾µï¨¦å°åƒåœ¾éƒµä»¶åˆ†ï§æœ‰æ‰€åŠ©ï¨—ï¼Œæœ‰äº›ç”šè‡³å¯èƒ½å°è‡´åˆ†
ï§æ­£ç¢ºï¥¡ä¸‹ï¨‰æˆ–å»¶é²åˆ†ï§æ©Ÿåˆ¶ä½œå‹•æ™‚é–“ã€‚é‡å°è©²å•é¡Œï¼Œæœ¬è¨ˆç•«ä»¥ï¥¸éšæ®µçš„æ–¹å¼ï¼Œä»¥æ¼”åŒ–å¼
è¨ˆç®—çš„æŠ€è¡“çµåˆæ–‡ä»¶åˆ†ï§çš„æ–¹æ³•ï¼Œå¾äº‹åƒåœ¾éƒµä»¶çš„ç‰¹å¾µåˆ†æå¾Œé€²è€Œç”¢ç”Ÿæœ€é©çš„ç‰¹å¾µé›†åˆï¼›
ç„¶å¾Œå¾æ­¤ç‰¹å¾µé›†åˆä¸­ï¼Œå˜—è©¦å»ºï§·ç”¨ä»¥åˆ†ï§åƒåœ¾éƒµä»¶çš„è¦å‰‡ã€‚ 
åœ¨ç¬¬ä¸€éšæ®µä¸­ï¼Œæˆ‘å€‘ä»¥æ¼”åŒ–å¼è¨ˆç®—æŠ€è¡“ä¸­çš„ï§¹å­ç¾¤å°‹å„ªæ¼”ç®—æ³•ç‚ºè¦è§’ï¼Œé…åˆæ”¯æ’å‘ï¥¾
æ©Ÿçš„åˆ†ï§æŠ€è¡“ï¼Œå¾ä¸€ç¾¤åƒåœ¾éƒµä»¶ä¸­ï¼Œç¯©é¸å‡ºæœ€èƒ½ä»£è¡¨é€™ç¾¤éƒµä»¶çš„é—œéµå­—é›†åˆã€‚ç”±æ–¼ï§ç”¨æ–‡
ä»¶åˆ†ï§æŠ€è¡“äº¦èƒ½æŒ‘é¸å‡ºç‚ºï¥©ç›¸ç•¶çš„ç‰¹å¾µï¼›å› æ­¤ï¼Œæ­¤éšæ®µçš„ç›®çš„ä¹ƒè¨­å®šç‚ºç‰¹å¾µé›†åˆç¶­ï¨çš„ç¸®
æ¸›ï¼Œé€éæˆ‘å€‘çš„æ–¹æ³•å¯ä»¥é”åˆ°ä»¥è¼ƒå°‘çš„ç‰¹å¾µï¥©ï¥¾ï¼Œé”åˆ°æœ€å¤§çš„åƒåœ¾éƒµä»¶åˆ†ï§æ•ˆæœã€‚è‡³æ–¼ç¬¬
äºŒéšæ®µï¼Œæ˜¯å°‡ç¬¬ä¸€éšæ®µæ‰€å¾—åˆ°çš„çµæœï¼Œè¼”ä»¥å¸ƒï§´é‹ç®—å…ƒé€²ï¨ˆç›¸çµåˆï¼Œä»¥é€ å°±å‡ºé©åˆä¸€èˆ¬ä½¿
ç”¨è€…é–±ï¥šä¸”å®¹ï§ è¨­å®šçš„åˆ†ï§è¦å‰‡ã€‚é€™å€‹å°‡é—œéµå­—èˆ‡å¸ƒï§´é‹ç®—å…ƒç›¸çµåˆçš„å‹•ä½œï¼Œå‰‡éœ€ä»°è³´æˆ‘
å€‘æ‰€è¨­è¨ˆçš„å·¢ï§ºå‹å¼çš„æ¼”åŒ–å¼æ¼”ç®—æ³•ã€‚ç¶“ç”±ä¸Šè¿°ï¥¸å€‹éšæ®µçš„è™•ï§¤ï¼Œæˆ‘å€‘ï¥¥å¯ä»¥å»ºç½®ä¸€å¥—ç¬¦
åˆä½¿ç”¨è€…æœ€ä½³éœ€æ±‚çš„å€‹äººåŒ–åƒåœ¾éƒµä»¶åˆ†ï§ç³»çµ±ã€‚ 
é—œéµè©ï¼šåƒåœ¾éƒµä»¶ã€æ¼”åŒ–å¼è¨ˆç®—ã€ï§¹å­ç¾¤å°‹å„ªæ¼”ç®—æ³•ã€æ”¯æ’å‘ï¥¾æ©Ÿã€æ–‡ä»¶åˆ†ï§ 
 
Abstract 
The increasing volume of unsolicited bulk e-mail (also known as spam) has generated a need 
for reliable anti-spam filters. Using a classifier based on machine learning techniques to 
automatically filter out spam e-mails has drawn many researchersâ€™ attention. Some text 
categorization methods were used to select common features of spam e-mails and to classify 
spam e-mails according to these features. Unfortunately, in some cases, there are too many 
features to decrease the classification accuracy. In this project, we try to design a two-stage 
approach to overcome the problem above mentioned. The proposed approach integrates 
evolutionary computation technique and text categorization method, and the purposes of it are to 
reduce the dimension of feature space and to automatically generate the classification rules. 
In the first stage, the particle swarm optimization (PSO) is used to select the proper features 
in spam e-mails. PSO is a relatively new population-based evolutionary computational model. In 
contrast to other evolutionary computational techniques which exploit the competitive 
characteristics of biological evolution, PSO is motivated from the simulation of cooperative and 
social behavior. The integration of PSO and support vector machine (SVM) can produce proper 
and representative features in the spam e-mails corpora. In the second stage, we will use the 
results from the first stage to automatically generate the classification rules for anti-spam filtering. 
Here we propose a novel technique, called nested evolutionary algorithm (NEA), and the Boolean 
operators are also considered simultaneously to accomplish the task. NEA is a special model in 
which there is an EA embedded in another EA. We believe that the proposed approach can 
provide a good way to construct a personalized and adaptive anti-spam filtering system. 
Keywords: spam, evolutionary computation, particle swarm optimization, support vector 
machine, text categorization 
 
 3
åˆä¸­ï¼Œï§ç”¨å·¢ï§ºå¼æ¼”åŒ–å¼è¨ˆç®—(nested evolutionary computation, NEC)çš„æ–¹å¼ï¼Œå°‡é—œéµ
å­—æ­é…å¸ƒï§´é‹ç®—å…ƒï¼Œè‡ªå‹•å»ºæ§‹å‡ºå®¹ï§ é–±ï¥šä¸”é©åˆè¨­å®šçš„åˆ†ï§è¦å‰‡(classification 
rules)ï¼Œä»¥ï§ä½¿ç”¨è€…åœ¨è‡ªå·±çš„éƒµä»¶è»Ÿé«”ä¸­é€²ï¨ˆè¨­ç½®ã€‚ 
 
2. The Proposed Approach 
ç¬¬ä¸€éšæ®µå°‡ä»¥æ¼”åŒ–å¼è¨ˆç®—ä¸­çš„ï§¹å­ç¾¤å°‹å„ªæ¼”ç®—æ³•èˆ‡æ”¯æ’å‘ï¥¾æ©ŸæŠ€è¡“ï¼Œé€²ï¨ˆåƒåœ¾éƒµä»¶ç‰¹
å¾µä¹‹åˆ†æï¼Œä»¥å°‹æ±‚æœ€é©(æˆ–æœ€å°)åˆ†ï§çš„ç‰¹å¾µç¶­ï¨ã€‚åœ¨ç¬¬äºŒéšæ®µä¸­ï¼Œå‰‡ï§ç”¨ç¬¬ä¸€éšæ®µæ‰€å»º
ï§·çš„æœ€é©åƒåœ¾éƒµä»¶åˆ†ï§ç”¨ä¹‹ç‰¹å¾µé›†åˆï¼Œæ‡‰ç”¨å·¢ï§ºå¼æ¼”åŒ–å¼æ¼”ç®—æ³•(nested evolutionary 
algorithm, NEA)[74]æŠ€è¡“ï¼Œè½‰æ›æˆåƒåœ¾éƒµä»¶åˆ†ï§æ‰€ç”¨çš„è¦å‰‡(classification rules)ã€‚é‡é»åœ¨
æ–¼ï¼Œå¦‚ä½•å°‡åˆ†ï§ç”¨çš„ç‰¹å¾µ(feature)ï¼Œä¹Ÿå°±æ˜¯ä¸€äº›ç¨ï§·çš„é—œéµå­—é›†ï¼Œè®Šæˆä¸€äº›å¯ç”¨çš„åˆ†ï§è¦
å‰‡ã€‚ 
(1) é¦–å…ˆï¼Œæˆ‘å€‘å°‡é‡å°å–å¾—çš„æ¸¬è©¦éƒµä»¶é›†é€²ï¨ˆå‰è™•ï§¤(pre-processing)ï¼Œæ­¥é©Ÿå¦‚ä¸‹ï¼š 
Step 1. éƒµä»¶é›†å‰ç½®è™•ï§¤ï¼šå°‡åƒåœ¾èˆ‡æ­£å¸¸éƒµä»¶é›†é€å°é€²ï¨ˆ stemmingã€stoppingã€å»é™¤ï¥©
å­—ã€ç‰¹æ®Šç¬¦è™Ÿèˆ‡å…¨å­—å°å¯«ç­‰ã€‚ 
Step 2. é—œéµå­—é¸å–ï¼šå°æ–¼ç¶“éå‰ç½®è™•ï§¤ä¹‹éƒµä»¶é›†ï¼Œä½¿ç”¨æ–‡å­—åˆ†ï§ä¸Šå¸¸ä½¿ç”¨çš„ç‰¹å¾µé¸å–
æ–¹æ³•ï¼Œï¦µå¦‚ï¼šdocument frequency (DF), term frequency-inverse document frequency 
(TF-IDF), information gain (IG), mutual information (MI)ç­‰æ–¹æ³•é€²ï¨ˆé—œéµå­—é¸å–ã€‚ 
Step 3. é—œéµå­—å¾ŒçºŒè™•ï§¤ï¼šå°æ–¼é¸å‡ºä¹‹æ‰€æœ‰é—œéµå­—é€²ï¨ˆåƒåœ¾èˆ‡æ­£å¸¸éƒµä»¶ç›¸åŒé—œéµå­—æ¯”
å°ï¼Œå»é™¤ç„¡æ„ç¾©ä¹‹é—œéµå­—(ï¦µå¦‚ï¼šé¸å‡ºä¹‹å­—å½™ç‚º xxxyyyï¼Œæ­¤ç¨®åœ¨è‹±èªä¸­ï¥§å…·æ„
ç¾©ä¹‹å­—)ç­‰å¾ŒçºŒè™•ï§¤ã€‚ 
Step 4. ç”¢ç”Ÿå¯ä¾›å¯¦é©—ä½¿ç”¨ä¹‹é—œéµå­—ã€‚ 
(2) ç„¶è€Œå¦‚å‰æ‰€è¿°ï¼Œä¸¦éæ¯ä¸€å€‹é—œéµå­—å°æ–¼åˆ†ï§ï¨¦æœ‰æ‰€åŠ©ï¨—ï¼›å†å‰‡ï¼Œç¶“ç”±ä½œæ³•(1)ä¸­ç¯©é¸è€Œ
å¾—çš„é—œéµå­—ï¥©ï¥¾å¯èƒ½ç›¸ç•¶é¾å¤§ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘å€‘å¯ä»¥å°‹æ‰¾å‡ºæœ€é©ç”¨(æˆ–æœ€ä½³)çš„ç‰¹å¾µçµ„
åˆï¼Œå¯ä»¥ç¸®æ¸›åˆ†ï§æ™‚çš„æ™‚é–“ï¼Œäº¦èƒ½ç²å–æœ€å¤§çš„åˆ†ï§æ•ˆèƒ½ã€‚æˆ‘å€‘å°‡æ¡å–å°æœ€ä½³åŒ–å•é¡Œæœ‰
ï¥¼å¥½æ•ˆèƒ½ä¹‹ DPSOï¼Œè¼”åŠ©ç¯©é¸æ—¢æœ‰ç‰¹å¾µï¼›æ­¥é©Ÿå¦‚ä¸‹æ‰€è¿°ã€‚ 
Step 1. ç”¢ç”Ÿå¯èƒ½çš„ç‰¹å¾µçµ„åˆï¼šç”±æ–¼é—œéµå­—çš„ï¥©ï¥¾å¯èƒ½å¾ˆå¤šï¼Œå› æ­¤æ‰‹å‹•èª¿æ•´çµ„åˆæ–¹å¼çµ•
å°ï¥§å¯ï¨ˆã€‚è—‰åŠ©é©ç•¶çš„æ¼”ç®—æ³•å‰‡é€²ï¨ˆé¸å–ï¼Œæ‰èƒ½é”æˆè‡ªå‹•åŒ–çš„ç›®çš„ã€‚æˆ‘å€‘åœ¨æ­¤
è¨ˆç•«ä¸­ï¼Œé¸å®šï¦º DPSO æ¼”ç®—æ³•ï¼Œä»¥å…¶é€²ï¨ˆåˆé©é—œéµå­—é›†çµ„åˆçš„é¸å–ã€‚å°‡ï§¹å­çš„
ä½ç½®å€¼è¦–ç‚ºç‰¹å¾µï¼Œç”¨æ„ç‚ºæ˜¯å¦æ¡ç”¨è©²é …ç‰¹å¾µç‚ºç”¢ç”Ÿç‰¹å¾µçµ„åˆä¹‹ä¾æ“šï¼Œæ„å³ç•¶æŸ
ï¤ä½ï¥©å€¼ç‚º 1 æ™‚ï¼Œä»£è¡¨æ‰€å°æ‡‰ä¹‹ç‰¹å¾µå°‡è¢«ä½¿ç”¨ï¼Œï¥©å€¼ç‚º 0 æ™‚å‰‡ï¥§ä½¿ç”¨ã€‚ 
 
 
 
 
 
åœ– 1 Particle swarm è¡¨ç¤ºåƒåœ¾éƒµä»¶ä¹‹ç‰¹å¾µçµ„åˆ 
0 1 0 0 1...
1 0 0 1 1...
1 1 0 1 0...
keywords:   money      sex       friend                                love  Viagra     
population: 
k particles
 5
å…¶ä¸­ï¼Œ S Sn â†’ ç‚ºè¢«æ­£ç¢ºåˆ†ï§çš„åƒåœ¾éƒµä»¶ï¥©ï¥¾ï¼Œ S Ln â†’ ç‚ºè¢«èª¤åˆ¤çš„åƒåœ¾éƒµä»¶ã€‚ 
Â„ å–®ç´”ä½¿ç”¨ Precision æˆ– Recall ï¤­è©•ä¼°åˆ†ï§ç³»çµ±çš„å¥½å£ï¼Œæœ‰æ™‚å€™æœƒæœ‰ï¥§å¤ªå®¢è§€
çš„æƒ…å½¢ã€‚å¦‚æœå¯ä»¥èª¿å’ŒäºŒè€…çš„çµæœï¼Œï¥¥å¯ä»¥å……åˆ†é¡¯ç¤ºå‡ºè©²åˆ†ï§ç³»çµ±çš„ç‰¹é»æ‰€
åœ¨ã€‚å› æ­¤ï¼Œæœ‰å­¸è€…æå‡ºï¦ºæ‰€è¬‚çš„ F-Measure å¦‚ä¸‹å¼æ‰€ç¤º 
SLLSSS
SS
nnn
n
â†’â†’â†’
â†’
++
â‹…=+
Ã—Ã—=
2
2
Recall Precision 
RecallPrecision2  Measure-F  (4) 
Â„ é«˜å¬å›ï¥¡å’Œé«˜æº–ç¢ºï¥¡ä¸€ç›´æ˜¯æ¯ä½ç ”ç©¶è€…æ‰€æœŸæœ›çš„ç›®æ¨™ï¼Œä½†äº‹å¯¦ä¸Šä¸€é«˜ä¸€ä½ï¼Œ
æˆ–äºŒè€…çš†ä½ä¹‹æƒ…æ³å‡æœ‰å¯èƒ½ç™¼ç”Ÿã€‚å› æ­¤å¯ç”±è©² 2 å€‹é …ç›®å¾—çŸ¥å¯¦éš›åˆ†ï§æƒ…å½¢ï¼Œ
ä¸¦åˆ†æå¯èƒ½åŸå› ã€‚Androutsopoulos et al. [68, 69, 70, 71]æå‡ºé«˜åˆ†ï§ï¥¡èˆ‡ä½éŒ¯
èª¤ï¥¡æœƒé€ æˆå°åˆ†ï§å¯¦é©—çš„èª¤è§£ï¼Œè¡¡ï¥¾åˆ†ï§ç³»çµ±å„ªï¦çš„åŸºç¤ï¼Œæ‡‰è©²å»ºï§·åœ¨éƒµä»¶
è¢«èª¤åˆ¤ï¥¡(åƒåœ¾éƒµä»¶åˆ¤å®šæˆæ­£å¸¸éƒµä»¶ï¼Œæˆ–æ­£å¸¸éƒµä»¶è¢«è¦–ç‚ºåƒåœ¾éƒµä»¶)çš„ï¨‰ä½ã€‚
å› æ­¤ Androutsopoulos ç­‰äººå¼•å…¥æ¬Šé‡æ¦‚ï¦£æ–¼æ­£ç¢ºï¥¡åŠéŒ¯èª¤ï¥¡å…¬å¼ï¼Œä»¥åŠ é‡è¨ˆ
åˆ†æ–¹å¼å¼·èª¿åˆ†ï§æ­£ç¢ºçš„é‡è¦æ€§ä»¥åŠåˆ†ï§éŒ¯èª¤çš„åš´é‡æ€§ã€‚ 
æ­¤å¤–ï¼Œä»–å€‘äº¦æå‡ºåŒ…å«æ¬Šé‡å€¼æ¦‚ï¦£çš„æ­£ç¢ºï¥¡(WAcc)èˆ‡éŒ¯èª¤ï¥¡(WErr)ã€‚ä»¥åŠå¯
æ¸…æ¥šå¾—çŸ¥éƒµä»¶èª¤åˆ¤æƒ…æ³ï¼ŒåŒ…å« bWAcc èˆ‡ bWErr çš„åŸºæº–ç·š(baseline)å…¬å¼ã€‚æœ€çµ‚ï¼Œ
ä»–å€‘çµåˆ WErr èˆ‡ bWErr ï¼Œæå‡ºç¸½å€¼æ¯”ï¦µ(total cost ratio, TCR)å…¬å¼ï¼Œå¯å°± TCR
å€¼çš„é«˜ä½è©•ä¼°åˆ†ï§ç³»çµ±çš„å„ªï¦ã€‚ä»¥ä¸‹å°‡å° WErr, bWErr èˆ‡ TCR é€²ï¨ˆï¥¯æ˜ã€‚ 
WErr ç‚ºè€ƒæ…®æ‰€æœ‰éƒµä»¶ï¥©ï¥¾ä¸­ï¼Œè¢«èª¤åˆ¤éƒµä»¶æ‰€ä½”çš„æ¯”ï¦µï¼›å…¶ä¸­ï¼Œå±¬æ–¼æ­£å¸¸éƒµ
ä»¶çš„éƒ¨åˆ†å¿…é ˆæœ‰æ¬Šé‡ä¸Šçš„è€ƒï¥¾ã€‚ 
SL
LSSL
NN
nnWErr +â‹…
+â‹…= â†’â†’Î»
Î» , (5) 
å…¶ä¸­ï¼Œ LNÎ» â‹… ç‚ºåŠ é‡è¨ˆåˆ†ä¹‹æ­£å¸¸éƒµä»¶ç¸½ï¥©ï¥¾ï¼Œ SN åƒåœ¾éƒµä»¶ç¸½ï¥©ï¥¾ï¼Œ L SnÎ» â†’â‹… ç‚º
åŠ é‡è¨ˆåˆ†ä¹‹è¢«èª¤åˆ¤æ­£å¸¸éƒµä»¶ä¹‹ï¥©ï¥¾ï¼Œ S Ln â†’ ç‚ºè¢«èª¤åˆ¤åƒåœ¾éƒµä»¶ä¹‹ï¥©ï¥¾ã€‚ 
bWErr åœ¨æ–¼è¡¡ï¥¾åŠ é‡æ­£å¸¸éƒµä»¶çš„è¨˜åˆ†è¨­å®šä¸‹ï¼Œåƒåœ¾éƒµä»¶ä½”æ‰€æœ‰éƒµä»¶çš„æ¯”ï¦µã€‚
å…¶ï¥©å€¼è¶Šå°ï¼Œä»£è¡¨åˆ†ï§å‡ºä¹‹åƒåœ¾éƒµä»¶æ¯”ï¦µè¶Šé«˜ã€‚ 
SL
Sb
NN
N WErr +â‹…= Î» , (6) 
å…¶ä¸­ LNÎ» â‹… ç‚ºè¢«æ­£ç¢ºåˆ†ï§ä¸”åŠ é‡è¨ˆåˆ†çš„æ­£å¸¸éƒµä»¶ï¥©ï¥¾ï¼Œ SN ç‚ºåƒåœ¾éƒµä»¶ç¸½ï¥©
ï¥¾ã€‚ 
ç¸½å€¼æ¯”ï¦µçµåˆ WErr èˆ‡ bWErr ï¼Œå…¶ç›®çš„åœ¨æ–¼è¨ˆç®—æ‰€æœ‰åƒåœ¾éƒµä»¶ï¥©ï¥¾èˆ‡æ‰€æœ‰èª¤
åˆ¤éƒµä»¶ï¥©ï¥¾ä¹‹æ¯”ï¦µï¼Œä»¥æ‰€å¾—ï¥©å€¼ä¹‹é«˜ä½ï¼Œè¡¡ï¥¾åˆ†ï§æ•ˆèƒ½çš„å„ªï¦ã€‚ç¸½å€¼æ¯”ï¦µè¶Š
é«˜ï¼Œä»£è¡¨è©²ç³»çµ±è¨­è¨ˆè¶Šï¨ï¥¼ã€‚ 
Step 5. é€²ï¨ˆ DPSO çš„ç›¸é—œï¥«ï¥©ï¤æ–°ï¼šä¾æ“š Step 4 æ‰€å¾—åˆ°é©æ‡‰å€¼(fitness)ï¼Œæª¢æŸ¥æ˜¯
å¦æ»¿è¶³å¯¦é©—è¨­å®šï¼Œä»¥é€²ï¨ˆ DPSO ç›¸é—œï¥«ï¥©çš„ï¥©å€¼ï¤æ–°ç¨‹åºã€‚ï¤æ–°æ­¥é©Ÿå¦‚
ä¸‹æ‰€ç¤ºï¼š 
 7
æ–°è¨­è¨ˆäº¤é…é‹ç®—å…ƒï¼Œä»¥ç™¼æ®æœ€å¤§çš„åŠŸæ•ˆã€‚ 
Step 6. è‡³æ–¼çªè®Šé‹ç®—å…ƒå‰‡éœ€åˆ†ä¸Šä¸‹å±¤å€‹åˆ¥è€ƒæ…®ï¼šåœ¨ä¸Šå±¤çš„æ¼”åŒ–å¼æ¼”ç®—æ³•ä¸­ï¼Œç”±æ–¼æŸ“è‰²
é«”çš„çµ„æˆå–®å…ƒæ˜¯ï¤­è‡ªæ–¼ç¬¬ä¸€éšæ®µæ‰€ç²å¾—çš„é—œéµå­—é›†ï¼›å› æ­¤ï¼Œæœ€ç°¡å–®çš„çªè®Šé‹ç®—
å…ƒå¯è¨­è¨ˆæˆé‡æ–°å¾é—œéµå­—é›†ä¸­ä»»æ„æ‰¾å°‹ä¸€å€‹å­—ï¤­é€²ï¨ˆæ›¿æ›å³å¯ã€‚è‡³æ–¼ä¸‹å±¤çš„æ¼”
åŒ–å¼æ¼”ç®—æ³•ä¸­ï¼Œæœ€ç°¡å–®çš„æ–¹æ³•äº¦å¯ä»¿æ•ˆä¸Šå±¤çš„å‹•ä½œï¼Œå¾æ‰€è€ƒæ…®çš„å¸ƒï§´é‹ç®—å–®å…ƒ
ä¸­æ“‡ä¸€å–ä»£åŸæœ‰çš„é‹ç®—å…ƒï¼›æˆ–è€…ï¼Œäº¦å¯è¨­è¨ˆæˆä»»æ„é¸æ“‡çªè®Šä½ç½®å¾Œï¼Œå°‡è©²ä½ç½®
ä¸Šçš„å¸ƒï§´é‹ç®—å…ƒäº’ç›¸äº¤æ›ã€‚ 
Step 7. ç¶“éä¸Šè¿° Step 1 ~ 6 çš„å‹•ä½œå¾Œï¼Œæˆ‘å€‘å°‡å¾—åˆ°ä»¥æœ€é©é—œéµå­—é›†ä¸­çš„é—œéµå­—æ­é…å¸ƒ
ï§´é‹ç®—å…ƒçš„åˆ†ï§æ³•å‰‡ã€‚ç„¶è€Œæˆ‘å€‘å¯ä»¥é€éä¸€å¾ŒçºŒè™•ï§¤å‹•ä½œ(post-processing)ï¼Œï¤­
æ§‹å»ºå‡ºï¤å…·æ„ç¾©ä¸”å®¹ï§ è¨­å®šçš„åˆ†ï§è¦å‰‡ã€‚æˆ‘å€‘å°‡æª¢æŸ¥è¦å‰‡ä¸­çš„é—œéµå­—ï¼Œåˆ†æå…¶
ç©¶ç«Ÿå‡ºç¾åœ¨éƒµä»¶ä¸­çš„é‚£å€‹ä½ç½®(Header, Subject, or Body)ï¼Œå¦‚æ­¤ï¼Œå°±å¯ä»¥ç”¢ç”Ÿå‡º
å¦‚ä¸‹çš„åƒåœ¾éƒµä»¶åˆ†ï§æ³•å‰‡ï¼š 
Rule_k: IF (Subject contains "Information" AND Body contains "message" AND 
"job") THEN (give the mail a high rating for spam) 
3. Experiments 
å¯¦é©—ä¸­æ‰€ä½¿ç”¨ä¹‹éƒµä»¶é›†åˆ†åˆ¥ï¤­è‡ª spamassassin, i-config èˆ‡ E. M. Canada ç¶²ç«™æ‰€æä¾›ä¹‹
éƒ¨åˆ†å…¬é–‹æ­£å¸¸èˆ‡åƒåœ¾éƒµä»¶é›†(public corpus)ï¼Œä¾æ“šå¾ŒçºŒå¯¦é©—çš„éœ€æ±‚ï¼Œæˆ‘å€‘è¦åŠƒï¦ºä»¥ä¸‹ 4 å€‹
éƒµä»¶é›†ï¼Œå¦‚è¡¨ 1 æ‰€ç¤ºã€‚ 
è¡¨ 1 å¯¦é©—ç”¨éƒµä»¶é›†ç›¸é—œç´°ç¯€è¡¨ 
ç·¨è™Ÿ éƒµä»¶å±¬æ€§ éƒµä»¶ï¥©ï¥¾ éƒµä»¶å‡ºè™• 
æ­£å¸¸éƒµä»¶ 2,501 spamassassin 2003_0228_easy_hamã€‚ C1 åƒåœ¾éƒµä»¶ 501 spamassassin 2003_0228_spamã€‚ 
æ­£å¸¸éƒµä»¶ 1,401 spamassassin 2003_0228_ easy_ham_2ã€‚ C2 åƒåœ¾éƒµä»¶ 1,397 spamassassin 2003_0228_spam_2ã€‚ 
æ­£å¸¸éƒµä»¶ 2,412 Ling-Spam éƒµä»¶é›†ä¸­çš„æ­£å¸¸éƒµä»¶ã€‚ C3 åƒåœ¾éƒµä»¶ 481 Ling-Spam éƒµä»¶é›†ä¸­çš„åƒåœ¾éƒµä»¶ã€‚ 
æ­£å¸¸éƒµä»¶ 2,501 spamassassin 2003_0228_easy_hamã€‚ C4 åƒåœ¾éƒµä»¶ 4,441 E. M. Canada 2005 ï¦ 5 æœˆä»½åƒåœ¾éƒµä»¶é›†ã€‚ 
æœ¬å¯¦é©—ä¸»è¦ç”¨æ„ï¼Œåœ¨æ–¼è­‰æ˜æ‰€ææ–¹æ³•ç¢ºå¯¦èƒ½æœ‰æ•ˆç¸®æ¸›é—œéµå­—ç‰¹å¾µçš„ç¶­ï¨ï¼Œä¸¦ç”¢ç”Ÿèˆ‡ä½¿
ç”¨æ‰€æœ‰é—œéµå­—é€²ï¨ˆåˆ†ï§æ™‚ï¼Œç›¸åŒç”šè‡³ï¤é«˜ä¹‹æ•ˆèƒ½ã€‚é¦–å…ˆç”± DF, TF-IDF èˆ‡ IG ç­‰æ—¢æœ‰é—œéµå­—
ç‰¹å¾µé¸å–æ–¹æ³•ï¼Œå°æ–¼ C1-C4 ç­‰ 4 å€‹éƒµä»¶é›†é€²ï¨ˆé—œéµå­—ç‰¹å¾µé¸å–ï¼Œæ¥è‘—ä½¿ç”¨ä¸Šè¿° 3 å€‹æ–¹æ³•é¸
å‡ºå‰ä¹‹ 100 å€‹é—œéµå­—ï¼Œé…åˆ SVM é€²ï¨ˆéƒµä»¶åˆ†ï§ï¼Œä¸¦ä»¥æ­£ç¢ºï¥¡ã€æº–ç¢ºï¥¡ã€å¬å›ï¥¡èˆ‡ç¸½å€¼æ¯”ï¦µ
ç­‰é …ç›®ï¼Œé…åˆé—œéµå­—ï¥©ï¥¾é€²ï¨ˆæ•ˆèƒ½è©•ä¼°ã€‚DPSO å¯¦é©—å‰‡å…¨éƒ¨é€²ï¨ˆ 200 æ¬¡ç–Šä»£ï¼Œä¾ä¸Šè¿°è©•ä¼°
é …ç›®å–å…¶ä¸­è¡¨ç¾æœ€ä½³ï§¹å­é€²ï¨ˆæ¯”è¼ƒã€‚ 
è¡¨ 2 ç‚ºå¯¦é©—çš„çµæœï¥©æ“šï¼Œå…¶ä¸­ DF, TF-IDF èˆ‡ IG ï¤ä½è¡¨ç¤ºï§ç”¨å‰è¿°æ–¹æ³•é¸å‡ºä¹‹å‰ 100
å€‹é—œéµå­—é€²ï¨ˆéƒµä»¶åˆ†ï§ä¹‹çµæœï¼ŒDPSO ï¤ä½å‰‡è¡¨ç¤ºæ‡‰ç”¨æ‰€ææ–¹æ³•é€²ï¨ˆé—œéµå­—å†æ¬¡é¸å–çš„çµ
æœã€‚æ ¹æ“šå°æ‰€æœ‰ï¥©æ“šçš„è§€å¯Ÿç™¼ç¾ä»¥ä¸‹ç¾è±¡ï¼š 
z 4 å€‹éƒµä»¶é›†ä¸­ï¼Œä»¥æ‰€ææ–¹æ³•å°æ–¼ä»¥ C1 èˆ‡ C3 é€²ï¨ˆä¹‹åˆ†ï§å¯¦é©—ï¼Œå…¶æ­£ç¢ºï¥¡ã€æº–ç¢ºï¥¡ã€
å¬å›ï¥¡èˆ‡ç¸½å€¼æ¯”ï¦µç­‰ 4 æ–¹é¢çš„è¡¨ç¾å‡ç›¸ç•¶å„ªï¥¼ï¼ŒC2 èˆ‡ C4 å‰‡å·®å¼·äººæ„ã€‚ï¥´å¼•é€²åƒ
åœ¾éƒµä»¶æ¯”ï¦µ(å³ç‚ºæ‰€æœ‰éƒµä»¶ä¸­åƒåœ¾éƒµä»¶ä½”çš„æ¯”ï¦µ)æ¦‚ï¦£ï¼Œç¶“è¨ˆç®—å¾Œç™¼ç¾ C1 ç‚º
16.69%ã€C2 ç‚º 49.93%ã€C3 ç‚º 16.63%ï¼ŒC4 ç‚º 63.97%ã€‚å› æ­¤ï¼Œåƒåœ¾éƒµä»¶æ¯”ï¦µçš„é«˜
 9
ç¶²ï¤·åŒ–çš„æ™®åŠï¼Œä½¿å¾—é›»å­éƒµä»¶å·²æˆç‚ºæ¯ä½ä½¿ç”¨è€…ï¥§å¯ç¼ºå°‘çš„æºé€šå·¥å…·ä¹‹ä¸€ï¼›ç„¶è€Œï¼Œåƒåœ¾éƒµ
ä»¶çš„å•é¡Œï¼Œå»ä½¿å¾—ä½¿ç”¨è€…é¢ï§¶å‰æ‰€æœªæœ‰çš„å¤§å•é¡Œï¼Œå„åœ‹ç„¡ï¥§çµç›¡è…¦æ±ä»¥å› æ‡‰æ­¤ç¾è±¡ã€‚æœ¬è¨ˆ
ç•«ï§ç”¨æ¼”åŒ–å¼æ¼”ç®—æ³•çš„ç‰¹æ€§ï¼Œçµåˆç›®å‰å»£ç‚ºä½¿ç”¨çš„æ”¯æ’å‘ï¥¾æ©Ÿä½œç‚ºæ ¸å¿ƒæŠ€è¡“ï¼Œå¯ä»¥æœ‰æ•ˆåœ°
ç¸®æ¸›ç”¨ä»¥ç¯©é¸åƒåœ¾éƒµä»¶çš„ç‰¹å¾µç¶­ï¨ï¼Œä¸¦ä¸”è‡ªå‹•åœ°ç”¢ç”Ÿå°æ‡‰çš„åˆ†ï§è¦å‰‡ã€‚å› æ­¤ï¼Œæˆ‘å€‘å¾ˆè‚¯å®š
æ­¤ä¸€æ–¹æ³•ç¢ºå¯¦å¯ä»¥æ‡‰ç”¨åœ¨æ­¤å¯¦ç”¨æ€§ç”šé«˜çš„ç ”ç©¶é¡Œç›®ä¸Šã€‚ 
 
5. Self-evaluation 
å…±ç™¼è¡¨ä¸‹ï¦œï¥æ–‡: 
1. Chih-Chin Lai* and C.-H. Wu, "Particle swarm optimization-aided feature selection for 
spam email classification," in Proc. The Second International Conference on Innovative 
Computing, Information and Control, Kumamoto City International Center, Kumamoto, 
Japan, Sep. 5-7, 2007. (NSC95-2221-E-024-015) 
2. Chih-Chin Lai* and C.-H. Wu, "Feature selection using particle swarm optimization with 
application in spam filtering," submitted to Information Processing and Management, 
2007. (NSC95-2221-E-024-015) 
3. Chih-Chin Lai* and C.-Y. Chang, "A hierarchical evolutionary algorithm for automatic 
medical image segmentation," submitted to Expert Systems with Applications, 2007. 
(NSC95-2221-E-024-015) 
 
References 
[1] Gfi Mail Essentials http://www.gfi.com/mes/anti-spam.htm?adclickid=182636 
[2] SpamCatcher, http://www.mailshell.com/spamcatcher/desktop_fd2.html 
[3] Inboxer, http://www.inboxer.com/index.shtml 
[4] SpamKiller, http://us.mcafee.com/root/package.asp?pkgid=156 
[5] Kill The Spams, http://www.zipstore.com/page7.html 
[6] SpamInspector, http://www.spamfilterreview.com/ 
[7] SpamEater Pro, http://www.hms.com/spameater.asp 
[8] Spam Buster, http://www.contactplus.com/products/spam/spam.htm 
[9] Email Protect, http://www.spamfilterreview.com/ 
[10] Choice Mail One, http://www.spamfilterreview.com/ 
[11] SpamAssassin, http://www.spamassassin.org/ 
[12] SpamPal, http://www.spampal.org 
[13] POP3 Agent, http://pop3agent.sourceforge.net/ 
[14] Spamless, http://www.qoosoft.com/ 
[15] MailWasher, http://www.mailwasher.net/ 
[16] K9, http://keir.net/k9.html 
[17] Bogofilter, http://bogofilter.org/ 
[18] ASSP, http://assp.sourceforge.net/ 
[19] SpamBayes, http://www.spambayes.org 
[20] E-Mail Relay, http://emailrelay.sourceforge.net/ 
[21] Mailfilter, http://mailfilter.sourceforge.net/ 
[22] Cohen, W., â€œLearning rules that classify e-mail,â€ in AAAI Spring Symposium on Machine 
Learning in Information Access, pp.18-25, 1996. 
[23] Brutlag, C. and J. Meek, â€œChallenges of the email domain for text classification,â€ 
Proceedings of Seventeenth Int'l Conf. on Machine Learning, 2000. 
[24] Ng, H. T., W. B. Goh and K. L. Low, â€œFeature selection, perception learning, and a usability 
case study for text categorization,â€ ACM SIGIR Forum, Vol. 31, Issue SI, pp. 67-73, 1997. 
[25] Payne, T. and P. Edwards, â€œInterface agents that learn: an investigation of learning issues in 
a mail agent interface,â€ Applied Artificial Intelligence, Vol. 11, pp. 1-32, 1997. 
[26] Boone, G., â€œConcept features in re:agent, an intelligent email agent,â€ Proceedings of 
Second Int'l Conf. on Autonomous Agents, 1998. 
[27] Pantel, P. and D. Lin, â€œSpamcop: A spam classification & organization program,â€ 
 11
ACM, vol. 35, pp.48-63, 1992. 
[51]Yang, Y., "Expert network: effective and efficient learning from human decisions in text 
categorization and retrieval," in 17th Ann Int'l ACM SIGIR Conf. on Research and 
Development in Information Retrieval, pp.13-22, 1994. 
[52] Lewis, D. D. and M. Ringuette, "Comparison of two learning algorithms for text 
categorization," in Proc. the Third Annual Symposium on Document Analysis and 
Information Retrieval, 1994. 
[53] Tzeras, K. and S. Hartman, "Automatic indexing based on bayesian inference networks," in 
Proc. 16th Ann Int'l ACM SIGIR Conf. on Research and Development in Information 
Retrieval, pp.22-34, 1993. 
[54] Wiener, E., J. O. Pedersen and A. S. Weigend, "A neural network approach to topic 
spotting," in Proc. the Fourth Annual Symposium on Document Analysis and Information 
Retrieval, 1995. 
[55] Apte, C., F. Damerau, and S. Weiss, "Towards language independent automated learning of 
text categorization models," in Proc. the 17th Annual ACM/SIGIR Conf., 1994. 
[56] Cohen, W. W. and Y. Singer, "Context-sensitive learning methods for text categorization," 
in SIGIR'96: Proc. the 19th Annual Int'l ACM SIGIR Conf. on Research and Development 
in Information Retrieval, pp.307-315, 1996. 
[57] Moulinier, I., G. Raskinis, and J. Ganascia, "Text categorization: a symbolic approach," in 
Proc. the Fifth Symposium on Document Analysis and Information Retrieval, 1996. 
[58] Lewis, D. D., R. E. Schapire, J. P. Callan, and R. Papka, "Training algorithms for linear text 
classifiers," in SIGIR'96: Proc. the 19th Annual Int'l ACM SIGIR Conf. on Research and 
Development in Information Retrieval, pp.298-306, 1996. 
[59] Yang, Y. and J. O. Pedersen, "A comparative study on feature selection in text 
categorization," in Proc. 14th Conf. Machine Learning, 1997. 
[60] Kennedy, J. and R. C. Eberhart, â€œParticle swarm optimization,â€ Proceedings of IEEE 
International Conference on Neural Networks, Vol. IV, pp. 1942-1948, 1995. 
[61] Katirai, H., â€œFiltering junk e-mail: A performance comparison between genetic 
programming & naive bayes,â€ Available: http://members.rogers.com/ 
hoomank/papers/katirai99filtering.pdf, 1999. 
[62] Sousa, T., A. Silva and A. Neves, â€œA Particle Swarm Data Miner,â€ Proceedings of 11th 
Portuguese Conference on Artificial Intelligence, pp. 43-53, 2003. 
[63] Sousa, T., A. Silva and A. Neves, â€œParticle swarm based data mining algorithms for 
classification tasks,â€ Parallel and Nature-inspired Computational Paradigms and 
Applications, Vol. 30, Issue 5-6, pp. 767-783, 2004. 
[64] Fukuyama, Y., â€œFundamentals of particle swarm techniques,â€ Modern Heuristic 
Optimization Techniques with Applications to Power Systems, IEEE Power Engineering 
Society, pp. 45-51, 2003. 
[65] Kennedy, J. and R. Eberhart, "A discrete binary version of the particle swarm optimization 
algorithm," in Proc. of the 1997 Conf. on Systems, Man, and Cybernetics, pp.4104-4109, 
1997. 
[66] Crstianini, N. and J. Shawe-Taylor, An Introduction to Support Vector Machines and Other 
Kernel-based Learning Methods, Cambridege Univ. Press. 
[67] Scholkopf, B. and A. J. Smola, Learning with Kernels â€“ Support Vector Machines, 
Regularization, Optimization, and Beyond, The MIT Press, 2002. 
[68] Androutsopoulos, I., J. Koustantinos, K. V. Chandrinos, G. Paliouras and C. D. Spyropoulos, 
â€œAn evaluation of NaÃ¯ve Bayesian anti-spam filtering,â€ in Proc. of the Workshop on 
Machine Learning in the New Information Age, 11th European Conference on Machine 
Learning, pp. 9-17, 2000. 
[69] Androutsopoulos, I., G. Paliouras, V. Karkaletsis, G. Sakkis, C. D. Spyropoulos and P. 
Stamatopoulos, â€œLearning to filter spam e-mail: A comparison of a Naive Bayesian and a 
memory-based approach,â€ in Proc. of the Workshop on Machine Learning and Textual 
Information, 4th European Conference on Principles and Practice of Knowledge Discovery 
in Databases, pp. 1-13, 2000. 
[70] Androutsopoulos, I., J. Koutsias, K. V. Chandrinos and C. D. Spyropoulos, â€œAn 
experimental comparison of Naive Bayesian and keyword-based anti-spam filtering with 
 13
 
å¯ä¾›æ¨å»£ä¹‹ç ”ç™¼æˆæœè³‡ï¦¾è¡¨ 
â–¡ å¯ç”³è«‹å°ˆï§  ; å¯æŠ€è¡“ç§»è½‰                                      æ—¥æœŸï¼š96 ï¦ 8 æœˆ 27 æ—¥ 
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•« 
è¨ˆç•«åç¨±ï¼šæ‡‰ç”¨æ¼”åŒ–å¼è¨ˆç®—æ–¼ç¶²ï¤·ï¥§ç•¶è³‡è¨Šçš„ç‰¹å¾µåˆ†æèˆ‡åˆ†ï§è¦å‰‡
ä¹‹ç”¢ç”Ÿ 
è¨ˆç•«ä¸»æŒäººï¼šè³´æ™ºéŒ¦ 
è¨ˆç•«ç·¨è™Ÿï¼šNSC 95-2221-E-024-015- 
å­¸é–€ï¦´åŸŸï¼šWeb æŠ€è¡“ 
æŠ€è¡“/å‰µä½œåç¨±  
ç™¼æ˜äºº/å‰µä½œäºº  
ä¸­æ–‡ï¼š 
åœ¨æœ¬æ¬¡çš„è¨ˆç•«ä¸­ï¼Œæˆ‘å€‘ä»¥ï¥¸éšæ®µçš„æ–¹å¼ï¼Œä»¥æ¼”åŒ–å¼è¨ˆç®—çš„æŠ€è¡“
çµåˆæ–‡ä»¶åˆ†ï§çš„æ–¹æ³•ï¼Œå¾äº‹åƒåœ¾éƒµä»¶çš„ç‰¹å¾µåˆ†æå¾Œé€²è€Œç”¢ç”Ÿæœ€é©çš„
ç‰¹å¾µé›†åˆï¼›ç„¶å¾Œå¾æ­¤ç‰¹å¾µé›†åˆä¸­ï¼Œå˜—è©¦å»ºï§·ç”¨ä»¥åˆ†ï§åƒåœ¾éƒµä»¶çš„è¦
å‰‡ã€‚ 
æŠ€è¡“ï¥¯æ˜ è‹±æ–‡ï¼š 
In this project, we try to design a two-stage approach to overcome the 
spam emails problem. The proposed approach integrates evolutionary 
computation technique and text categorization method, and the 
purposes of it are to reduce the dimension of feature space and to 
automatically generate the classification rules. 
å¯ï§ç”¨ä¹‹ç”¢æ¥­ 
åŠ 
å¯é–‹ç™¼ä¹‹ç”¢å“ 
ç¶²éš›ç¶²ï¤·ç›¸é—œç”¢æ¥­ã€ç¶²ï¤·å®‰å…¨ã€åƒåœ¾éƒµä»¶ï¦„é™¤è»Ÿé«” 
æŠ€è¡“ç‰¹é» 
æœ¬ç ”ç©¶è¨ˆç•«å¾äº‹"æ‡‰ç”¨æ¼”åŒ–å¼è¨ˆç®—æ–¼ç¶²ï¤·ï¥§ç•¶è³‡è¨Šçš„ç‰¹å¾µåˆ†æèˆ‡åˆ†
ï§è¦å‰‡ä¹‹ç”¢ç”Ÿ"çš„æŠ€è¡“ç ”ç©¶ã€‚å…¶ç ”ç©¶çš„ä¸»è¦ç›®çš„æ˜¯ï§ç”¨æ¼”åŒ–å¼æ¼”ç®—
æ³•ä¸­çš„ï§¹å­ç¾¤å°‹å„ªæ¼”ç®—æ³•ï¼ŒåŠå·¢ï§ºå‹å¼æ¼”åŒ–å¼æ¼”ç®—æ³•ç­‰å…·æœ‰å¼·éŸŒä¸”
æœå°‹å…¨åŸŸæœ€ä½³è§£ç­”çš„èƒ½ï¦Šï¼Œå¯ä»¥æä¾›ä½¿ç”¨è€…ï¤ï¨ç¢ºçš„è³‡è¨Šã€‚åƒåœ¾éƒµ
ä»¶çš„å•é¡Œï¼Œå»ä½¿å¾—ä½¿ç”¨è€…é¢ï§¶å‰æ‰€æœªæœ‰çš„å¤§å•é¡Œã€‚æœ¬è¨ˆç•«ï§ç”¨æ¼”åŒ–
å¼æ¼”ç®—æ³•çš„ç‰¹æ€§ï¼Œçµåˆç›®å‰å»£ç‚ºä½¿ç”¨çš„æ”¯æ’å‘ï¥¾æ©Ÿä½œç‚ºæ ¸å¿ƒæŠ€è¡“ï¼Œ
å¯ä»¥æœ‰æ•ˆåœ°ç¸®æ¸›ç”¨ä»¥ç¯©é¸åƒåœ¾éƒµä»¶çš„ç‰¹å¾µç¶­ï¨ï¼Œä¸¦ä¸”è‡ªå‹•åœ°ç”¢ç”Ÿå°
æ‡‰çš„åˆ†ï§è¦å‰‡ã€‚ 
æ¨å»£åŠé‹ç”¨çš„åƒ¹å€¼ 
å¯è¦–ç‚ºä¸€ç¨®æ–°å‹æ…‹çš„åƒåœ¾éƒµä»¶ï¦„é™¤æŠ€è¡“ 
â€» 1.æ¯é …ç ”ç™¼æˆæœè«‹å¡«å¯«ä¸€å¼äºŒä»½ï¼Œä¸€ä»½éš¨æˆæœå ±å‘Šé€ç¹³æœ¬æœƒï¼Œä¸€ä»½é€ è²´å–®ä½ç ”
ç™¼æˆæœæ¨å»£å–®ä½ï¼ˆå¦‚æŠ€è¡“ç§»è½‰ä¸­å¿ƒï¼‰ã€‚ 
â€» 2.æœ¬é …ç ”ç™¼æˆæœï¥´å°šæœªç”³è«‹å°ˆï§ï¼Œè«‹å‹¿æ­ï¤¸å¯ç”³è«‹å°ˆï§ä¹‹ä¸»è¦å…§å®¹ã€‚ 
â€» 3.æœ¬è¡¨ï¥´ï¥§æ•·ä½¿ç”¨ï¼Œè«‹è‡ªï¨ˆå½±å°ä½¿ç”¨ã€‚ 
 
 
ä¸‰ã€å»ºè­°èˆ‡çµèª 
 
 
å››ã€æ”œå›è³‡ï¦¾ 
SCIS & ISIS 2006 æœƒè­°è­°ç¨‹è³‡ï¦¾ä¸€ä»½ã€‚ 
SCIS & ISIS 2006 Proceedings å…‰ç¢Ÿä¸€ä»½ã€‚ 
ç›¸é—œåœ‹éš›æ€§ç ”è¨æœƒè³‡ï¦¾ï¥©ä»½(Call-for-Paper)ã€‚ 
 
ä¸‰ã€å»ºè­°èˆ‡çµèª 
 
 
å››ã€æ”œå›è³‡ï¦¾ 
SCIS & ISIS 2006 æœƒè­°è­°ç¨‹è³‡ï¦¾ä¸€ä»½ã€‚ 
SCIS & ISIS 2006 Proceedings å…‰ç¢Ÿä¸€ä»½ã€‚ 
ç›¸é—œåœ‹éš›æ€§ç ”è¨æœƒè³‡ï¦¾ï¥©ä»½(Call-for-Paper)ã€‚ 
Section 2, the basic concept of the hierarchical differential
evolution algorithm is introduced and then the proposed ap-
proach is described. The experimental results and discussions
are given in Section 3. The conclusions are summarized in
Section 4.
II. THE PROPOSED INTELLIGENT CLUSTERING APPROACH
Clustering process is ubiquitous in many real-world prob-
lems, especially those labeled as pattern recognition problems
at an abstract level. When we applied any clustering approach
to partition a data set, determining the proper or optimal
number of clusters for a data set is one of the more difficult
aspects to the clustering process. The hierarchical differential
evolution algorithm with the properties of exploration and
exploitation capabilities is properly used to simultaneously
determine the numbers of clusters as well as properly partition
a data set. In this section, we first present the hierarchical dif-
ferential evolution algorithm, and then introduce the proposed
clustering approach.
A. Hierarchical differential evolution
Differential evolution (DE) [22] is a relatively new type of
Evolutionary algorithms (EAs), which are a set of randomized
search and optimization techniques guided by the principles
of evolution and natural genetics, and have a large amount of
implicit parallelism. They provide near optimal solutions of an
objective or fitness function in complex, large, and multimodal
landscapes. The DE algorithm resembles the structure of an
EA, but differs from traditional EA in its mutation scheme and
the â€˜greedyâ€™ selection process. The former makes DE has self
adaptive property and the latter provides significant advantage
of converging performance over genetic algorithms.
The HIDE algorithm is a comparatively simple variant of
the DE algorithm. The main difference between the former
and the latter is the structure of the chromosome. In HIDE
algorithm, the chromosome consists of two types of genes â€“
the control genes and the parametric genes. The purpose of
control genes is designed to determine which parameter gene
should be utilized and which one can be disabled during the
process by HIDE. Generally, the control genes are coded as
binary digits, while the parametric genes can be coded as any
type of data structure. If the value of a control gene is â€œ1â€,
then the associated parametric gene is activated; otherwise, the
associated parametric gene is disabled.
B. The proposed approach
When we apply the HIDE algorithm to the automatic clus-
tering problem, we must consider the following components:
(1) a genetic representation of solutions to the problem, (2)
one way to create the initial population of solutions, (3) an
evaluation function that rates all candidate solutions according
to their â€œfitnessâ€, (4) genetic operators that alter genetic
composition of children during reproduction.
  Solution representation: In the proposed approach, the
chromosome is made up of binary digit (the total number
of â€œ1â€ implicitly represents the number of clusters) as
well as real numbers (representing the estimated coordi-
nates of the cluster centers). The number of control genes
is decided by the soft estimation of the upper bound of
the number of clusters. An example of the hierarchical
chromosome structure in our approach is illustrated as
follows, where it represents a partition of a give data set
with two clusters, and the associated cluster centers are
     and     	, respectively.

 

 
 	
   
    

  	
   
              	  	   
  Initial population: A HIDE algorithm requires a pop-
ulation of potential solutions to be initialized at the
beginning of the evolution process. Here we adopt the
random generation scheme to create the initial population
until all chromosomes in a population are created.
  Fitness function: A fitness function is the survival arbiter
for chromosomes. Three different cluster validity indices
used in our approach are described in the next section. For
each chromosome, the centers encoded in it are extracted,
and then a partition is obtained by assigning the data
points to a cluster corresponding to the closest center.
Given the above partition, and the number of clusters,
the value of the cluster validity index is computed and
used as the fitness of the selected chromosome.
  Mutation: The mutation operator is needed to explore
new areas of the search space and helps the search
procedure avoid sticking in local optima. Here we just
apply the mutation scheme to the parametric genes.
Assume the parametric genes in a chromosome can
be viewed as a 	-dimensional vector 

 
, here  de-
notes the generation number of the algorithm. Accord-
ing to the mutation scheme, for each vector 

 
  

          (population size), a new mutated vector
is created by

 
 

 



 
 




  




 (1)
where 

 

 

           are mutually different
random indices and they also differ from the current index
.  is a real and constant factor which controls the
amplification of the different variation [22].
  Crossover: Following the mutation phrase, the crossover
operator is applied to the population. The crossover
operator randomly pairs chromosomes and swaps parts of
their genetic information to produce new chromosomes.
A trial vector is

 
 

  
 
 
 
 
 
        
 
 
 (2)
is generated, where

 
 




 
 
 if (   CR ) or  
  



 
 if (   CR ) and  
   (3)
where  
          	;      is the th evaluation
of a uniform number generator; CR is the crossover
- 1919 -
TABLE I
DESCRIPTION OF THE DATA SETS
Data Set No. of points No. of clusters Points per cluster
D1 540 3 120, 120, 300
D2 480 4 120
D3 275 5 55
D4 500 10 50
Iris 150 3 50
Cancer 683 2 444, 239
TABLE II
ACTUAL AND COMPUTED NUMBER OF CLUSTERS FOR THE DATA SETS
Data Set Actual No. DB-index Dunnâ€™s index CH-index
of Clusters
D1 3 3 6 3
D2 4 4 6 4
D3 5 5 5 5
D4 10 8 2 10
Iris 3 2 2 2
Cancer 2 2 2 2
of the Iris data, 2 or 3 clusters are preferred depending on the
validity index used [16].
On the other hand, to show that the proposed approach
can be applied to other application, a CT head image in
which there exist a number of tiny tissues was used for the
experiment. Figure 2 is the segmented result of our method.
From the result, we can see that the proposed approach
can correctly segment the image into three distinct regions
(clusters) and obtain a clear and complete segmentation image.
V. CONCLUSION
In this paper, an intelligent clustering approach using hi-
erarchical differential evolution algorithm was proposed to
solve the automatic clustering problem. Clustering plays an
important role for knowledge discovery and is being applied in
a wide variety of engineering and scientific disciplines. Many
clustering algorithms can be found in the literature. However,
most of the clustering algorithms suffer from the drawback
that the number of clusters has to be known in advance. In
many applications, this knowledge is not available. In the
proposed clustering approach, it can automatically search for a
proper and correct number of clusters. On the other hand, the
centers of clusters are also determined, and then a partition of
given data set is done by assigning the points to corresponding
center. The hierarchical differential evolution algorithm is the
conventional differential evolution algorithm with hierarchical
genetic structure. The proposed clustering approach has been
applied to the several artificial and real-life data sets. Another
interesting real life application is that HIDE clustering method
has the ability to automatically segment a CT image.
ACKNOWLEDGMENT
This work was partially supported by the National Science
Council, Taiwan, R.O.C. under grant NSC 94-2213-E-024-
005.
REFERENCES
[1] C.L. Blake, C.J. Merz, UCI Repository of machine learning databases
[http://www.ics.uci.edu/ mlearn/MLRepository.html]. Irvine, CA: Uni-
versity of California, Department of Information and Computer Science
(1998).
[2] J.C. Bezdek, N.R. Pal, â€œSome new indexes of cluster validity,â€ IEEE
Trans. Syst., Man Cybern., vol.28, no.3, pp.301â€“315, 1998.
[3] S. Bandyopadhyay, U. Maulik, â€œNonparametric genetic clustering: com-
parison of validity indices,â€ IEEE Trans. Syst., Man Cybern., vol.31,
no.1, pp.120â€“125, 2001.
[4] S. Bandyopadhyay, U. Maulik, â€œGenetic clustering for automatic evolu-
tion of clusters and application to image classification,â€ Patt. Recong.,
vol.35, pp.1197â€“1208, 2002.
[5] R.B. Calinski,J. Harabasz, â€œA dendrite method for cluster analysis,â€
Comm. in Statistics, vol.3, pp.1â€“27, 1974.
[6] L. Davis, Handbook of Genetic Algorithm, Van-Nostrand, New York,
1991.
[7] D.L. Davies, D.W. Bouldin, â€œA cluster separation measure,â€ IEEE Trans.
Patt. Anal. MAch. Intell., vol.1, pp.224â€“227, 1979.
[8] K.A. DeJong, W.M. Spears, An analysis of the interacting roles of
population size and crossover in geneitc algorithms. in Parallel Problem
Solving from Nature, H.-P. schwefel and R. Manner, Eds., Springer-
Verlag, Berlin, 1990, p.38.
[9] J.C. Dunn, â€œA fuzzy relative of the ISODATA process and its use in
detecting compact well-separated clusters,â€ J. Cybern., vol.3, pp.32â€“57,
1973.
[10] R.A. Fisher, The use of multiple measurements in taxonomic problems,
Ann. Eugen. 3 (1936) 179â€“188.
[11] D.E. Goldberg, Genetic Algorithms in Search, Optimization, and Ma-
chine Learning, Addison-Wesley:Reading, MA, 1989.
[12] D.E. Goldberg, K. Deb, â€œA comparison of seletion schemes used in
genetic algorithms,â€ in Foundations of Genetic Algorithm 1, pp.69â€“93,
1991.
[13] J.J. Grefenstette, Optimization of control parameters for genetic algo-
rithms. IEEE Trans. Systems, Man, and Cybern. 16(1) (1986) 122â€“128.
[14] M. Herbin, N. Bonnet, P. Vautrot, â€œEstimation of the number of clusters
and influence zones,â€ Pattern Recognition Letters, vol.22, pp.1557â€“1568,
2001.
[15] A.K. Jain, M.N. Murty, P.J. Flynn, â€œData clustering,â€ ACM Computing
Surveys, vol.31, no.3, 1999. (1999) 264â€“323.
[16] R. Kothari, D. Pitts, â€œOn finding the number of clusters,â€ Patt. Recog.
Lett., vol.20, pp.405â€“416, 1999.
[17] D.J.C. MacKay, Bayesian Methods for Adaptive Models, Thesis, Cali-
fornia Institute of Technology, Pasadena.
[18] K.F. Man, K.S. Tang, S. Kwong, Genetic Algorithms: Concepts and
Designs, Springer-Verlag:London, 1999.
[19] U. Maulik, S. Banyopadhyay, â€œGenetic algorithm-based clustering tech-
nique,â€ Pattern Recognition., vol.33, pp.1455â€“1465, 2000.
[20] M. Michalewicz, Genetic Algorithms + Data Structure = Evloution
Program, Springer-Verlag:Berlin, 1996.
[21] S.K. Pal, P.P. Wang (Eds.), Genetic Algorithms for Pattern Recognition,
CRC Press, Boca Raton, 1996.
[22] R. Storn and K. Price, â€œDifferential evolution â€“ a simple and efficient
adaptive scheme for global optimization over continuous spaces,â€ Tech-
nical report, International Computer Science Institute, Berkely, 1995.
[23] M. Sarkar, B. Yegnanarayana, D. Khemani, â€œA clustering algorithm
using an evolutionary programming-based approach,â€ Patt. Recog. Lett.,
vol.18, pp.975â€“986, 1997.
[24] J.T. Tou, R.C. Gonzalez, Pattern Recognition Principles, Reading,
MA:Addison-Wesley, 1974.
[25] L.Y. Tseng, S.B. Yang, â€œA genetic approach to the automatic clustering
problem,â€ Patt. Recog., vol.34, pp.415â€“424, 2001.
- 1921 -
