             
     The genes screened by SCAD SVM and WEPO will 
be used as the features for SVM [3] and SOM [4]. 
Section 2 introduces the basic concepts of SCAD SVM 
and WEPO while SVM and SOM are reviewed in 
Section 3. Section 4 presents the experimental results 
and Section 5 gives the conclusion. 
 
2. Gene selection methods 
 
2.1. SCAD SVM 
 
     Given the input data 
1{( , ),..., ( , )}1 nx x nS y y=                        
(1) 
 The SCAD SVM [1] is proposed as equation (2). 
     
,
1 1
1
min [1 ( )] (| |)w w x w
n d
b i i j
i j
y b p
n
Î»
= =
âˆ’ + â‹… +âˆ‘ âˆ‘             (2) 
where ix  is the i-th input vector and iy  is the label 
corresponding to ix . 
The penalty function is defined according to the 
following formulas 
( ) ( )
( )
2 2
2
                                   if 
2
       if 
2( 1)
1
                         if 
2
a
p a
a
a
a
Î»
Î» Î»
Î» Î»
Î» Î»
Î» Î»
ï£± â‰¤ï£´
ï£´
âˆ’ +ï£´
= âˆ’ < â‰¤ï£²
âˆ’ï£´
ï£´ +
ï£´ >
ï£³
w w
w w
w w
w
      (3) 
 
If some jw  in the calculated weight vector w is 
very close to zero or smaller than a certain threshold, 
then the j-th dimension (feature) is regarded as a 
redundant feature which will not be used 
 
2.2. WEPO 
 
     The concept of WEPO [2] is to use the punishment 
as the score for each gene. 
Define the different categories are 1C  and 2C , 
respectively, and each vector ig  contains the i-th gene 
expression of all the patients. 
 For each element in ig , the punishment 
of the i-th gene is defined according to the 
following equations.  
 
  
1 2
1
z, z > 0( ),     where     W(z)=
0, z 0p C q C
s W p q
âˆˆ âˆˆ
ï£±
= âˆ’ ï£² â‰¤ï£³
âˆ‘ âˆ‘
                
(4) 
2 1
2
z, z > 0( ),      where     W(z)=
0, z 0q C p C
s W q p
âˆˆ âˆˆ
ï£±
= âˆ’ ï£² â‰¤ï£³
âˆ‘ âˆ‘
              
(5)  
where p and q are two components in ig  
whose corresponding patients belong to 1C and 
2C , respectively. 
     The score of each gene is acquired from 
1 2min( , )s s  and the genes with lower scores are 
considered more important. 
 
3. Classification methods 
 
     In this section, we review SVM [3] and SOM [4] 
which are used for classification in our experiments. 
 
3.1. SVM 
 
     SVM projects high dimensional vectors to lower 
dimensional space and searches the hyperplane which 
has the maximum distance to the two nodes closest to 
each other between different classes. 
     The training data S is given from equation (1), we 
have to find out the optimal separating hyperplane in 
the following figure. 
 
Figure 1 The maximum margin hyperplane 
    The geometric margin Î³  is then defined as 
21/ || ||w . 
We can maximize the geometric margin Î³  by 
minimizing 
2|| ||w  as 
,
minimize
w b  ,w w  
operation treatment for hepatocellular carcinoma (HCC) 
at National Taiwan University Hospital.      
In our experiment, the 4 samples without HBV and 
HCV and one sample with both HBV and HCV in 
HCC55 are not used for classification. We will use 20 
of the remaining 50 samples as the training dataset 
consisting of 10 HBV and 10 HCV while other 19 
HBV and 11 HCV will be used for testing. 
Besides this, we used five datasets from the 
websites [6]. They are named ALL-AML_Leukemia 
[7], CentralNervous [8], ColonTumor [9], DLBCL [10], 
and Prostate [11] 
   The dataset ALL-AML_Leukemia contains 72 
samples of two classes, acute lymphoblastic leukemia 
(ALL) and acute myeloid leukemia (AML). Each 
sample has 7129 genes. There are 47 ALL and 25 
AML types respectively. We use 12 ALL and 12 AML 
as a training dataset, the remaining samples are used 
for testing. 
 The dataset CentralNervous contains 60 
samples of two classes, class 1 and  class 0. Samples 
labeled as class 1 are patients who are alive after 
treatment while samples labeled as class 0 are those 
who succumbed to their disease. Each sample contains 
7129 genes. There are 21 class 1 patients and 39 class 
0 patients in this dataset. We use 10 class 1 and 10 
class 0 patients as a training dataset, the remaining 
samples are used for testing. 
 The dataset ColonTumor contains 62 samples 
of two classes, colon-cancer and normal. Each sample 
has 2000 genes. They were computed from 40 colon-
cancer biopsies and 22 normal biopsies from tumors 
and healthy parts of the colons of the same patients, 
respectively. We use 11 cancer and 11 normal types as 
a training dataset, the remaining samples are used for 
testing. 
 The dataset DLBCL contains 47 samples of 
two classes, germinal centre B-like and activated B-like. 
Each sample has 4026 genes. There are 22 germinal 
centre B-like and 25 activated B-like types in this 
dataset. We use 11 germinal centre B-like and 11 
activated B-like types as a training dataset, the 
remaining samples are used for testing. 
 The dataset Prostate contains 136 samples of 
two classes, prostate tumor and normal. Each sample 
contains 12600 genes. There are 77 prostate tumor and 
59 normal samples in this dataset. We use 29 tumor 
and 29 normal as a training dataset, the remaining 
samples are used for testing. 
 
5. Experimental results 
 
      We apply SCAD SVM and WEPO to find out the 
important genes and used these genes as the features 
for SVM and SOM. Our experiments are running on a 
windows based system with Pentium4 3.00GHz CPU. 
SCAD SVM screens 22 features for ALL-AML_ 
Leukemia, 41 features for CentralNervous, 12 features 
for ColonTumor, 22 features for DLBCL, 21 features 
for Prostate, and 46 features for HCC55. Since WEPO 
just ranks features by their scores, we take the features 
which have minimum scores in each dataset, and the 
number of token features of each dataset is the same as 
the number of features selected by SCAD SVM. 
Figure 2 shows the performance of classifying the 
microarray data using SVM and SOM. We use the 
suitable kernel for each dataset in SVM while the 
proper lattices are used in SOM.  
Except for Prostate, SVM performs better than 
SOM when using the genes selected by WEPO as 
features. For all the datasets, SVM outperforms SOM if 
we use the genes selected by SCAD SVM. 
 
Figure 2 The performance of SVM and SOM with the 
two different gene selection methods 
 
6. Conclusion  
 
We can find that SVM has a stable and great 
performance of classifying microarray data. Except for 
Prostate, SOM also has great classification rate. 
For the two datasets CentralNervous and Prostate, 
WEPO seems not able to select the discriminative 
features for classification. We find that the WEPO 
scores of these two datasets are very high. This is the 
probable reason which affects the efficacy of WEPO. 
Although WEPO and SOM have nice performance 
for gene selection and classification, respectively, SVM 
with linear kernel and features selected by SCAD SVM 
is the best composition for most microarray datasets.  
 
7. References 
 
ï¨ˆæ”¿é™¢åœ‹å®¶ç§‘å­¸å§”å“¡æœƒè£œåŠ©åœ‹å…§å°ˆå®¶å­¸è€…å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å ±å‘Š 
                                                                 95ï¦ 08æœˆ 25æ—¥ 
å ±å‘Šäººå§“å: é™³æœæ¬½           æœå‹™æ©Ÿæ§‹: åœ‹ï§·æ¸…è¯å¤§å­¸      è·ç¨±: æ•™æˆ 
æœƒè­°æ™‚é–“:   95ï¦ 08æœˆ 21~24æ—¥   åœ°é»:  Hong Kong, China 
 
æœ¬æœƒæ ¸å®šè£œåŠ©ç·¨è™Ÿ: 95-2221-E-007-222 
æœƒè­°åç¨±: The 18th International Conference on Pattern Recognition 
ç™¼è¡¨ï¥æ–‡é¡Œç›®: A Comparison of Texture Features Based on SVM and SOM 
ä¸€ ã€ï¥«åŠ æœƒè­°ç¶“é: 95ï¦ 08æœˆ 20æ—¥ 14:00æ­ä¹˜é•·æ¦®èˆªç©º BR0855ç­æ©Ÿæ–¼ 20æ—¥
ç•¶åœ° 15:40 pmæŠµ Hong Kong Airportï¼Œè½‰ä¹˜æ©Ÿå ´å¿«æ·åˆ°ä¹ï§„ï¤‚ç«™ï¦šæ¥å·´å£«é€²
ä½ Kowloon Hotelã€‚8æœˆ 21æ—¥å…«é»åŠå ±åˆ°éš¨å³ï¥«åŠ é–‹å¹•å¼ï¼Œå¤§æœƒå…±åŒä¸»å¸­
Professor Yuan Yan Tangå ±å‘Šæœ¬æ¬¡æœƒè­°å…±æœ‰ 1071äººå ±åï¥«åŠ ï¼Œå«ä¸€èˆ¬å­¸è€…
å°ˆå®¶ 620äººåŠå­¸ç”Ÿ 451äººï¼Œæ‰“ç ´ï¦Œï¦ï¤­çš„è¨˜ï¤¿ã€‚ç·Šæ¥è‘—ï¼Œç”± K.S. Fu award 
receiver Professor J. Kittleræ¼”è¬› â€On Context, Modelling, Dimensionality, and 
Small Sample Size in Pattern Recognitionâ€ã€‚æœƒè­°å…±åˆ†æˆäº”å€‹ TracksåŠä¸€å€‹
Poster SessionåŒæ™‚é€²ï¨ˆï¼Œæ¯æ—¥ä¸Šåˆä¸€æ®µï¼Œä¸‹åˆï¥¸æ®µï¼Œè€Œæ¯å¤©ä¸Šåˆ 8~9é»éƒ½
é‚€è«‹ä¸€ä½å°ˆå®¶æ¼”è¬›ç›¸é—œå­ï¦´åŸŸçš„ History, Present, Perspectiveï¼Œå…¶ä¸­ç”±å¯†è¥¿
æ ¹å·ï§·å¤§å­¸ Professor A.K. Jainæœ‰é—œæŒ‡ç´‹èªè­‰çš„æ‡‰ç”¨æ¼”ï¥¯æœ€å¸å¼•äººã€‚æˆ‘å€‘çš„
ï¥æ–‡å®‰æ’æ–¼ 22æ—¥ 13:30-15:10è²¼æµ·å ±ï¼Œå¼•èµ·ï¥©ä½å­¸è€…çš„å¥½å¥‡èˆ‡è¨ï¥ã€‚ä¸¦ï¥«
åŠ  Conference Reception and Banquetå’Œå…¶å®ƒåœ‹å…§å¤–å­¸è€…å°ˆå®¶è¨ï¥æ•™å­¸ã€
Researching Fundingã€Research Topics, and etc.ã€‚æœ¬æ¬¡æœƒè­°ç´„æœ‰è¶…é 15ä½å°
ç£ï¤­çš„æ•™æˆå’Œï¥©ä½ç ”ç©¶ç”Ÿï¥«åŠ ï¼Œå¤§å®¶éƒ½è¦ºå¾—å—ç›Šï¥¼å¤šã€‚ç”±æ–¼é¦™æ¸¯ï§ªå°ç£å¾ˆ
è¿‘ï¼Œä¸€æ–¹é¢æ²’æœ‰æ™‚å·®ï¼Œå¦ä¸€æ–¹é¢ï¼Œæ–‡åŒ–ã€èªè¨€ç’°å¢ƒä¹Ÿæ¥è¿‘ï¼Œç¶“è²»èƒ½è¼ƒæœ‰æ•ˆ
ä½¿ç”¨ã€‚8æœˆ 24æ—¥ 13:45æ­ä¹˜ BR0868æ–¼ 15:30 pmè¿”æŠµå°ç£ã€‚ 
äºŒã€èˆ‡æœƒå¿ƒå¾—: æœ¬æœƒè­°ç´„æœ‰ï¤­è‡ª 55å€‹åœ‹å®¶è¿‘ 1100ä½å­¸è€…å°ˆå®¶ï¥«åŠ ï¼Œæ˜¯å€‹å¤§å‹æœƒ 
è­°ç”± Baptist University of Hong Kongä¸»è¾¦ï¼Œå°ç£å¹¾å€‹ä¸»è¦å¤§å­¸åŠä¸­åœ‹è¨±å¤šå­¸ 
è€…éƒ½å‡ºå¸­ï¦ºï¼Œå¹¾ä¹æ‰€æœ‰æ´»å‹•å…¨ç…§è­°ç¨‹æº–æ™‚é€²ï¨ˆï¼Œå€¼å¾—åœ‹å…§ï¥«è€ƒï¼Œè€Œä¸”èƒ½å°±è¿‘ 
å’Œå­¸è€…å°ˆå®¶è¨ï¥å‹éè‹¦ï¥š 10ï¦æ›¸ã€‚æœ€å¾Œï¼Œç«­èª æ„Ÿè¬åœ‹ç§‘æœƒçš„ç¶“è²»è£œåŠ©ã€‚ 
ä¸‰ã€è€ƒå¯Ÿï¥«è§€æ´»å‹•: ç„¡ã€‚ 
å››ã€å»ºè­°: åœ‹ç§‘æœƒç·¨ï¦œç¶“è²»é¼“ï¥¿éƒ¨ä»½å°ç£çš„ä¸­å°å‹(100~200äºº)åœ‹å…§æœƒè­°åœ‹éš›åŒ– 
ä»¥æå‡çŸ¥åï¨ã€‚æ”¿åºœç©æ¥µç·¨ï¦œé©ç•¶ç¶“è²»é¼“ï¥¿å­¸è€…å¤šï¥«åŠ é‡è¦åœ‹éš›æœƒè­° (ç¸±ä½¿ 
æ²’æœ‰ç™¼è¡¨ï¥æ–‡) ä»¥æå‡å°ç£å­¸è¡“æ°´æº–èˆ‡çŸ¥åï¨ã€‚ 
äº”ã€æ”œå›è³‡ï¦¾åç¨±åŠå…§å®¹: DVD for the contents of papersåŒ…å«è¿‘ 1000ç¯‡ï¥æ–‡ã€‚ 
ï§‘ã€å…¶ä»–: ç„¡ã€‚ 
 
ï¦—çµ¡é›»è©±: (Tel/Fax) (03) 573-1078 / 572-3694      E-mail: cchen@cs.nthu.edu.tw  
ç°½åè“‹ç« : é™³æœæ¬½                            æ—¥æœŸ:  95ï¦ 8æœˆ 25æ—¥ 
so there are twenty features derived from a 3-scale 
wavelet transform. The matrices corresponding to the 
four wavelet transforms are given as follows. 
Figure 1.  A 3-scale wavelet transform.
2.2.1. Haar Wavelet Transform:  The Haar transform 
is the simplest type of the wavelet transforms. In brief, 
the concept of Haar wavelet can be explained by 
moving average and moving difference whose matrix 
form is given below. 
Haar
1 1 0 0
2 2
1 10 0 0
2 2
1 10 0
2 2
1 1 0 0
2 2
1 10 0 0
2 2
1 10 0
2 2
W
Â  Â¯
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°Â¢ Â±



=
"
"
# % #
"
"
# % #
.                 (4)
2.2.2. Daub4 Wavelet Transform:  As in the Haar 
transform, the Daub4 transform is expressed as 
D aub4
0 1 2 3
0 1 2 3
2 3 0 1
3 2 1 0
3 2 1 0
1 0 3 2
0 0
0 0 0
0 0
0 0
0 0 0
c c c c
c c c c
c c c c
W
c c c c
c c c c
c c c c
Â  Â¯
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°Â¡ Â°Â¢ Â±
=  
 
 
"
"
%
"
"
"
%
"
(5)
where
1 3 3 3 3 3
0 1 24 2 4 2 4 2
, , ,c c c     and 1 33 4 2c
 .
2.2.3. 5/3 Wavelet Transform: The 5/3 wavelet 
transform is a reversible transform for lossless image 
data compression in JPEG2000 [11] whose matrix 
form is given in (6). 
2.2.4. 9/7 Wavelet Transform: Besides reversible 5/3 
wavelet transform, a floating bi-orthogonal 9/7 wavelet 
transform, is presented to compress images more 
efficiently. The forward 9/7 wavelet transform is given 
in (7). Based on (3), the texture features can be derived 
by using the 9/7 wavelet matrix. 
5 3
3 1 1 1 10 0
4 4 8 8 4
31 1 1 1 0 0
8 4 4 4 8
31 1 1 10 0
8 8 4 4 4
1 11 0 0
2 2
1 10 1 0 0
2 2
1 10 0 1
2 2
W
Â  Â¯ Â¡ Â°
Â¡ Â°
Â¡ Â° Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â° Â¡ Â°
Â¡ Â° Â¡ Â°
Â¡ Â°
Â¡ Â° Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â°
Â¡ Â° Â¡ Â°Â¡ Â°Â¢ Â±
=
" "
# % #
# % #
"
"
# "
# % #
# %
" "
(6)
97
0 00 1 2 3 4 4 3 2 1
0 02 1 0 1 2 3 4 4 3
0 02 3 4 4 3 2 1 0 1
0 00 1 2 3 4 2 1
0 02 1 0 1 2 3 4
0 02 3 4 2 1 0 1
W
D D D D D D D D D
D D D D D D D D D
D D D D D D D D D
E E E E E E E
E E E E E E E
E E E E E E E
   
  
   
 
 
 
Âª Âº
Â« Â»
Â« Â»
Â« Â»
Â« Â»
Â« Â»
Â« Â»
Â« Â»
Â« Â»
Â« Â»
Â« Â»
Â« Â»
Â¬ Â¼
 
" "
" "
# " % % #
# " % % #
" "
" "
" "
# % % #
# % % #
" "
(7)
where the coefficients for the W97 are given in Table 1. 
Table 1. The coefficients of 9/7 wavelet transform.
2.3. Features derived from Gabor and wavelets 
To implement a Gabor transform on an image X, 
we need to compute inverseFT(FT(X)8FT(G)), where 
8 is a pointwise operation and FT represents Fourier 
transform. For other wavelet transforms, a matrix 
computation WXWt is applied. For either Gabor or 
other four wavelet transforms, the mean and standard 
deviation of corresponding transformed coefficients in 
each subband are computed as features. 
3. Texture classification
3.1. Support Vector Machine (SVM)
Support Vector Machine (SVM) investigated by 
Vapnik has recently been proposed as new machine 
learning system based on statistical learning theory [6]. 
SVM designs the classifier functions by constructing 
hyperplanes in a multidimensional space that separate 
different categories of the training data. The main idea 
is to build the hyperplanes as the decision boundaries 
by using the fitting kernel such as radial basis function 
(RBF), polynomial or linear classifiers [6] [5]. Then, 
the hyperplanes try to split the positive examples from 
the negative examples and maximize the distance of 
the marginal separation between classes. 
In practice, there are various algorithms to train 
SVM. We adopt the Plattâ€™s SMO algorithm to train the 
input data in our experiments [7] [8]. For multi-class 
classification, the one-against-one approach is chosen 
to classify the test data. 
0-7695-2521-0/06/$20.00 (c) 2006 IEEE
