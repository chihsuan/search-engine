-1- 
 
 
ABSTRACT 
 
åœ¨æœ¬è¨ˆåŠƒä¸­ï¼Œæˆ‘å€‘ç ”ç™¼äº†ä¸€å¥—æ–°ç©çš„é—œéµå½±æ ¼
çš„åµæ¸¬æ–¹æ³•ï¼Œå…¶åˆ©ç”¨è¦–è¦ºä¸Šçš„é¡¯è‘—ç‚ºåŸºç¤çš„é—œæ³¨å¾µ
åŠæ–‡æ„ä¸Šè³½ç¨‹ç‹€æ…‹è³‡è¨Šï¼ŒæˆåŠŸæ‡‰ç”¨æ–¼é‹å‹•ç¯€ç›®ä¸Šã€‚
åœ¨æ­¤å…©å€‹é‡è¦çš„è­°é¡Œè¢«æ·±å…¥æ¢è¨ï¼Œç¬¬ä¸€ã€äººé¡è¦–è¦º
ä¸Šçš„é—œæ³¨ç‰¹æ€§ï¼šç•¶ä¸€å€‹ä½¿ç”¨è€…åœ¨è§€çœ‹è¦–è¨Šç‰‡æ®µæ™‚çš„
è¦–è¦ºç‰¹æ€§ï¼›ç¬¬äºŒã€è³½ç¨‹çš„æ¯”åˆ†ç‹€æ…‹ï¼šæ“·å–èˆ‡è³½ç¨‹ç²¾
é‡‡äº‹ä»¶ç›¸é—œçš„æ„Ÿèˆˆè¶£æŒ‡æ•¸ã€‚æˆ‘å€‘ç ”ç™¼äº†ä¸€å€‹ä»¥ç‰©ä»¶
ç‚ºå°å‘çš„è¦–è¦ºé—œæ³¨åœ°åœ–åŠç”¨ä¾†è§£é‡‹èªæ„ä¸Šä¹‹æ„Ÿèˆˆè¶£
æŒ‡æ›²ç·šã€‚èªæ„ä¸Šçš„æ¨æ¼”å¯ä»¥ç”¨ä¾†æ¨¡æ“¬åˆ†æå“ªä¸€ç¨®çš„
è¦–è¨Šå…§å®¹æœƒå¼•èµ·è§€çœ¾çš„èˆˆè¶£ã€‚å†è€…ï¼Œæˆ‘å€‘æå‡ºä¸€å€‹
èåˆçš„æ©Ÿåˆ¶ä¾†å°è¦–è¦ºåŠèªæ„ç‰¹å¾µåŸ·è¡Œåˆ†æåŠæ¨æ¼”äºº
é¡æ„Ÿèˆˆè¶£ç‰¹æ€§ï¼Œæœ€å¾Œï¼Œå¯¦é©—éšæ®µåˆ©ç”¨æ£’çƒç¯€ç›®ä¾†èªª
æ˜äº†ç³»çµ±çš„æ•ˆç‡åŠå¼·å¥æ€§ã€‚å°æ–¼æœªä¾†çš„è¦–è¨Šå…§å®¹åˆ†
ææŠ€è¡“åŠå³æ™‚é©æ‡‰æ€§çš„æ’­æ”¾æ©Ÿåˆ¶æœ‰ç›¸ç•¶çš„åŠ©ç›Šã€‚ 
 
 
Index Termsâ€” key-frame detection, content analysis, 
contextual modeling, visual attention model, content-based 
video retrieval 
 
1. INTRODUCTION 
 
The need to access the most representative information 
and reduce its transmission cost, have made the development 
of video indexing and a suitable retrieval mechanism 
popular research topics [1]-[3]. One effective approach is 
the use of key-frames, i.e. a very small subset of frames 
representing the whole video. Many multimedia retrieval 
systems have been proposed [4], [5]. Most of them are based 
on key selections (e.q., key-frame and key shot) as an index 
for users to select and browse. From a content semantics 
point of view, a video content can be divided into four 
categories based on their semantic significance, including 
video clip, object, action, and conclusion.  
A key-frame not only represents the entire video content, 
but also reflects the amount of human attention the video 
attracts. There are two clues that help us to understand the 
viewerâ€™s attention, visual cue and contextual information. 
Visual information is the most intuitive feature for the 
human perception system. Modeling the visual attention 
when watching a video [6] is provides a good understanding 
of the video content. In intelligent video applications, visual 
attention modeling [7] combines several representative 
feature models into a single saliency map which is then 
allocated to those regions that are of interest to the user. The 
saliency feature map can be used as an indication of the 
attention level given to the key-frame. On the other hand, in 
sports videos, the game status is the information that 
concerns the subscriber the most. The embedded captions on 
the screen represent condensed key contextual information 
of the video content. Taking advantage of prior implicit 
knowledge about sports videos, Shih et. al [8] proposed an 
automatic system that can understand and extract context 
that can then be used to monitor an event.  
The number of key-frames in a shot is one of the most 
important issues in the key-frame extraction scheme. 
Whether it is predefined or chosen dynamically, either way 
it is affected by the video content. In fact, frames in a shot 
that undergo a strong visual and temporal uncertainty 
complicate the problem even more. To balance this, we let 
the number of key-frames be depending on the level of 
excitement of the context. This means that when the 
excitement in the on-going game is high, more key-frames 
should be extracted from that video shot. 
We proposed a novel attention-based keyframe 
detection system by integrating the object-oriented visual 
attention maps and the contextual on-going game outcomes. 
Using an object-based visual attention model combined with 
contextual attention model not only precisely determines the 
human perceptual characteristics but also effectively 
determines the type of video content that attracts the 
attention of the user. 
 
2. VISUAL ATTENTION 
 
In human-attention-driven application, video object (VO) 
extraction becomes necessary. Visual attention 
characteristics of every VO can be computed by using the 
saliency-based feature map extraction. We modified the 
visual attention model [7] which is frame-based manner to 
object-based approach, called the object-based visual 
attention model which provides more accurate information 
to the viewer. Based on the types of features, the object-
based visual attention model can be determined by four 
types of feature maps, i.e., spatial, temporal (local), facial, 
and (global) camera motion. 
2.1. Spatial Feature Maps 
 
We selected three spatial features to be extracted and to be 
used as the spatial feature map: intensity, color contrast (red-
green, blue-yellow), and orientation. The intensity image I 
was obtained as I = (r + g + b)/3 where r, g, and b indicate 
the red, green, and blue channels of the input image, 
respectively. Four broadly-tuned color channels were 
obtained, R = r - (g + b)/2 for red, G = g - (r + b)/2 for green, 
B = b - (r + g)/2 for blue, and Y = min{[(r + g)/2 - (|r - g|/2 - 
b], 0} for yellow. The local orientation feature was acquired 
-3- 
 
        [ ],t, text/digi,i CBSVPosCT ,=                     
where Pos indicates the relative location in the SCB. The 
text/digit bit indicates that the caption is either text or digit, 
SV indicates a set of support vectors for this character, and 
CB is the corresponding character block. The caption 
template can be used to identify the characters in the 
succeeding videos. 
 
Table 1. The reachable contextual information of the SCB 
Context Annotation Description #States 
INNS Î»1 The current innings 9 
RUNNERS Î»2 The base that are occupied 
by  runners 
8 
RUNS Î»3 The  score difference nsn Î–âˆˆ  
OUTS Î»4 The number of outs 3 
BALLS Î»5 The number of balls 4 
STRIKES Î»6 The number of strikes 3 
 
3.3. Modeling the Contextual Attention 
 
Generally speaking, contextual attention is defined as the 
probability of a userâ€™s interest in a specific game status 
which can be formulated with a context vector. However, 
different domains contain different linguistic information. 
Thus, it is hard to use a generic model for all types of video 
data, since a great deal of domain-specific features are 
involved. Different from visual attention which varies frame-
by-frame, contextual attention varies depending if it is shot-
based or event-based. 
Unfortunately, we were unable to obtain all of the 
statistical information from video the frames. Therefore, in 
this report, we not only adopted the available information 
from the SCB, but also employed the historical statistics data. 
Normally, the contextual information embedded in the SCB 
consists of six classes (Î›={Î»i|i=1,2,â€¦,6}) for the baseball 
game, as listed in Table 1. 
 
3.3.1. Implicit Factors 
 
In this report, the contextual information is divided into 
three classes. These three classes are based on the 
relationship between the value of the context and the degree 
it excites the viewer, proportionally, specifically, or 
inversely. After classifying the contextual description, a 
group of implicit factors {Fl |l=1,2,â€¦, lâ€™,â€¦,L-1, L} are 
used to model the human excitement characteristics. Then, 
each of these implicit factors can be classified as one of the 
three classes as follows 
â„¦(Flâ€™) âˆˆ{Ï‰p, Ï‰s, Ï‰i} 
where â„¦(.) denotes the classifier, and Ï‰p, Ï‰s, and Ï‰i  
represent the corresponding factor as a proportional type, 
specific type, or an inverse type respectively.  It not only 
uses the implicit factor in the current moment (i.e., Ft), but it 
also takes into account the implicit factor from the historic 
statistics (i.e., F0:t-1). Let Ïˆc(fi) indicate the viewerâ€™s 
contextual attention score of frame f, which can be 
contributed to the implicit factors Ft={Fk|k=1,2,â€¦,K} which 
consist of all the probable annotations of the SCB at that 
moment. Also, the historical statistics are taken into 
consideration, in which the implicit factors from the 
historical statistics are represented by F0:t-1={Fq|q=1,2,â€¦,Q}. 
Thus 
Ïˆc
t(fi)= F t+ F 0:t-1. 
Four implicit factors from the current observation are 
considered in determining the contextual attention score, 
which include 
 
â€¢ t
1F : The score difference 
The scoring gap between two teams greatly attracts the 
attention of the viewer. When the runs scored are the same 
or are very close, it indicates that the game is very intense. 
Therefore, the game can be formulated as 
t
1F =exp(-Î±1Î»3), 
where Î±1 denotes the normalization term 
 
â€¢ t
2F : The number of BALLS and STRIKES pairs 
The ratio between BALLS and STRIKES can be applied 
to model user attention. In a baseball game that is being 
broadcasted, the number of balls is repeatedly updated from 
0 to 3 and the number of strikes and outs are updated from 0 
to 2. When Î»5 or Î»6 reaches 3 or 2, it indicates that the game 
is getting a high level of attention, because the current player 
will soon be struck out or get to walk soon. Thus, 
t
2F = exp(Î»5-3)*exp(Î»6-2). 
â€¢ t
3F : The number of the inning being played 
Generally speaking, the less the number of remaining 
innings, the more attention the game will attract. Therefore, 
we can design an implicit factor from the number of the 
inning Î»1. In general, the maximum number of innings is 9. 
Thus, this factor can be expressed as 
t
3F =exp(Î»1-9). 
â€¢ t
4F : The number of outs 
(4) 
(5) 
(6) 
(7) 
(8) 
(9) 
-5- 
 
representative. It is obvious that combining all attention 
feature maps can meet the first rule. In the present study we 
adopted the camera motion characteristics and treated them 
as the balancing coefficient to support the second rule. A 
numerical value was then derived from the visual 
characteristics of all segmented objects within the frame. 
The denominator of each part is the normalization term, 
which is the sum of the attention maps for all frames 
belonging to the same video shot. Based on the predefined 
number of key-frames, the Ri key-frames {Fk*} with the 
largest visual attention score Ïˆvt, is calculated as follows 
[ ]âˆªi
i
R
i
icamerai
t
vfk
fMfF
1
*
,)()(Ïˆmaxarg
=
ï£¾
ï£½
ï£¼
ï£³
ï£²
ï£± Ã—=  
where 
,
)(
)(
)(
)(
)(
)()(Ïˆ
1
3
1
2
1
1
âˆ‘âˆ‘âˆ‘
===
Ã—+Ã—+Ã—=
sss N
j
jfacial
ifacial
N
j
jtemporal
itemporal
N
j
jspatial
ispatial
i
t
v
fFM
fFM
fFM
fFM
fFM
fFMf Î³Î³Î³
 
where Î³1, Î³2, and Î³3 denote the weighting coefficients among 
the visual feature maps.  
 
5. EXPERIMENTAL RESULTS 
 
Our proposed object-based attention model is used as an 
effective scheme to measure the attention score. Table 3 
shows the attention scores in different modules for six 
representative frames shown in Fig. 1. 
The values of each column indicate the attention score, 
and accurately reflect the attention to the content via spatial, 
temporal, facial and global motion. Frames #650 and #1716 
are globally static, so they have low temporal attention 
scores and low camera motions resulting in decreased visual 
attention. Frame #931 zooms in for a close-up, but the object 
itself is stable, resulting in a high visual attention. Frame 
#4001 is a mid-distance view with local motion and the 
camera zooming. However, the face it zooms into is clear 
and near the center of the frame, resulting in a high attention 
score. Frame #3450 is a mid-distance view with middle face 
attention, and with the camera panning, which also increases 
attention. Frame #5763 has high attention due to the rapid 
panning to capture the pitcher throwing the ball.  
Figs. 2 and 3 show the results of key-frame detection of 
the sample. It is evident that the proposed scheme is capable 
of extracting a suitable number of key-frames. Based on the 
visual attention score, the extracted key-frames are highly 
correlated with human visual attention. However, in the third 
and fourth rows of Fig. 2, there are two redundant key-
frames extracted. This is because the shot boundaries were 
incorrectly determined as a result of the fast panning or 
tilting by the camera. In addition, all the frames within the 
shot had near-identical attention scores. Furthermore, the 
method we proposed can also be applied to determine the 
key-frame for slow-motion replay clips as the last selected 
key-frames in Fig. 2. We assume that from each of these 
shots at least one key-frame was selected based on the 
framesâ€™ attention scores. That is why a few of the key-frames 
look like normal average play. Nevertheless, Fig. 3 shows 
that an exciting moment can be properly preserved by means 
of our proposed method. 
Table 3. The examples with different Attention Scores 
#Frame 
Integral 
Visual 
Attention 
Spatial Temporal Facial 
Camera 
Motion 
DVv DVH 
573 0.243 0.053 0.688 0.107 144 241 
650 0.093 0.051 0.110 0.039 0 11 
931 0.691 0.506 0.672 0.562 82 242 
1716 0.051 0.069 0.041 0.091 0 11 
3450 0.186 0.038 0.253 0.038 24 129 
4001 0.286 0.056 0.394 0.047 26 41 
 
#573                        #650                        #931 
 
#1716                       #3450                      #4001 
 
Fig. 1. The representative frames for testing. 
 
 
 
 
 
(13) 
 !"#$%&'()*+,-. /01"234567
7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 789:;<; = <; > ;? 87
@"34A7
BCDE 'F"GHIJKLMNOP /"2QRST=UP"2VWXYGZ
[\]^_`SaWbcG defghijfklk7mlnoe7pqrqq sZ[\'(HIJtuK
LvwKxXcPN0QRyT=z{|}R~Â€ÂÂ‚ÂƒÂ„Â…yÂ†Â‡ÂˆÂ€Â‰ÂŠÂ‹
P'(0ÂŒ@ÂÂÂ@yÂ{Â‘Â’c /Â“Â”P0ÂŒÂ•Â–Â—Â˜PÂ™ÂšSÂ›ÂœÂÂH
cÂŸÂ '(Â¡Â¢DfÂ£iojoÂ¤k7mhjhliÂ£Â¥Â¦P'(Â§SÂ‘Â¨Kx /P'(Â©ÂªÂ«Â¬Â­"
2ÂœÂ® SÂ¯Â°Â±s{u Â²fehÂ£k7jhejoi Ptu Â³Â´ ÂµÂ¶Â·Â¸Â¹ÂºSsÂ»Â¼'(NÂ½Â¾y
Â­Â¿Ã€ÂŠ Ãojkhi PÃ‚ÃƒÃ„Ã…Â‚ÂƒyÃ†4RÃ‡PsSÂ­Â¿"2P Ãojkhi7jhÂ£kfoe Ãˆ
Ã‰ÃŠÃ‹ÃŒÃÃˆS{tÃH /'(0ÂŒÃÂ¼Ã‡ÃSÃ‘Ã‰Ã’PÂšÃ“Ã”Ã•Â‘Â¨Ã–Ã‘Ã—y
Ã˜T=P"2Ã™Ã‘ÃšÃ›PsÂ‰Â’ÃœÃÃÃŸSÃ RÃ¡P ihÂ£hÃkfoeSÃ ÂˆÃ¡P Ã¢leÃ£rhk
PÂ‰Â’{Ã¤Ã¥SÃ¦Ã§Ã‡Ã5Â”SÃ¨Ã©5Â”ÃªÃ«Ã¬Ã­Ã®Ã¯{ ihÂ£hÃkfoe Ã° Ã¢leÃ£rhk P
)*Ã±Ã²7 Ã³Ã´CÃµÃµÃ¶Ã·Ã·;Ã¸Ã¶EÃ¶;Ã¹Ã¹Ã¶<;Â³7
)*Â”Ãº7 ÂŠÂ‰Ã»Ã¼Ã½Ã¾Â“Â°Ã¿G !P"#$%&'Â¹Ã¿IJ()*+Q'(7
- Â‰Â§
,Â”7
-./7
0123
Ã°Ã€Ãº7
45N07 Âµ2670*89:7
"2ÃˆÃ‰7
7 ;<<=?>;;8;
;<< = ? > ;Ã¹ 87 "2XY7 Z[\]^_`77
"2Â”Ãº7
Â¢ÂœÂƒÂ¦Ã·<;;  /HIJ<=>'F"7
Â¢?ÂƒÂ¦2011 IEEE International Conference on Multimedia & Expo(ICME2011)7
Ã„Ã…Â‚Âƒ
&@7
Â¢ÂœÂƒÂ¦AÂºÂ‰Ã»uBCDQuEFGHIÃ°uEFGJKL7
Â¢?ÂƒÂ¦ Key-frame extraction and key-frame rate determination using human attention modeling7
	


 !"#$$ $! 
%&'()$$! $!
*+,-./.01.1.2.3,2-.-,+
.	0./.+./.+0-45..
2+-%+.6+1
78*91./.1..2.-
+..9/.:-.1..-,.8
+,+.	.,20,,.,.,3:,;.00,.1.
8$.,,	<+1./+,93:,;.00,.
.:	1./+8
!)$ $ (!)$! 
'.0
-10,2+.--.,20,,.,21+
8
,<-0,2.0-:/++
	,.8+,/,1.	,.2/.+=8.,:./+
.	,21--+,9		0.3-/.00.	08
=%,.9./:2,./,0./.0.+-	
3.00.-.,-3,<0,:++2.,-	++1.,
,>0./.0.+-	.0++.	0:+++,.2
3+2+,..8
?/,5++-..	0::.+-+,,&1.-+1
9,-/.+,8*.	1../++	./
+.	0.9-3	./..//	8
	.	+.,>:+..9/.:-.:+.0	1.%+.6+1@
-,+.	0.00
.+./.+0-45.
 !
	

 ! "# $! # $$ $! % & 
 !"#$%&'()*+,-. /01"234567
7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 789:;<; = <; > ;? 87
@"34A7
BCDEF)GFHIJKL'M"NOPQRSTUVWDXSU0YZ4[
\D]Y^=D'M"_`aR b?< cYdefghijk ;blYmn]opqrD
stYdughDefvuwxy zz{|7}~7|Â€ÂÂ‚ÂƒÂ„Â€ F Â…{Â†Â‡CÂˆÂ‰ÂŠÂ‹ÂŒD][
cÂÂÂÂÂ‘RÂ’DÂ“Â”Â•Â–Â—Â˜Â™DÂšÂ›ÂœÂÂ›ÂÂŸÂ YÂ¡Â¢RÂ£Â¤Â¥Â¦Â€Â§Â¨Â©56Y
@"ÂªÂ«vÂ¬Â­kÂ®Â¯P\-Â°HÂ±&YÂ’Â²]Â³mÂKLyÂ´ÂµÂÂÂÂ‘Â¶Â·Â¸Â¹Â†
Â¡Â¢\ÂºÂ»Â°HÂ¼ÂªDÂ½2Â†7
)*Â¾Â¿7 Ã€|zÃÃÃ‚ÃƒÃƒ;Ã„Ã‚Â…Ã‚;bbÃ‚<;Ã…7
)*Ã†Ã‡7 ÂÂ“Â”ÃˆÃ‰ÃŠÃ‹ÃŒÃRÃÃDÃÃ‘mÃ’Ã“Ã”KÃIJÃ•Ã–Ã—Ã˜Ã™'(7
- Â“Ãš
Ã›Ã†7
ÃœÃÃ7
ÃŸÃ Ã¡Ã¢
FÃ£Ã‡7
Ã¤Ã¥U07 Â·Ã¡Ã¦Â›0Ã—Ã§ÂŠÃ¨7
"2Ã©Ãª7
7 ;<<=Ã«>;Ã…8Ã¬
;<< = Ã« > ;b 87 "2PQ7 STUVWÃ­XSU077
"2Ã†Ã‡7
Â¥Ã®fÂ©Ã¯Ã°Ã±EF)GFHIJKL'M"7
Â¥Ã²fÂ©7Ã³Ã´Â„7ÃƒÂ‚Ãµ7{Â‚Ã¶Â„Â€Â‚Â§Ã¶ÂÂ¦Â‚Â§Â¨7zÂ¦Â‚Ã·Â„Â€Â„Â‚Ã¸Â„7Ã¹}ÂÃºÃ»ÂÃ¶Â¦Ã»Ã¼7zÂ¦Ã½Ã»Ã¶ÂÂ‚Âƒ7
Â§Â‚Ãµ7Ã¾Ã»Â¨Ã¶ÂÃ½Â„ÃµÂÂ§7Ã¿Â¨ÂÃ¸Â§Ã¶ÂÂ¦Â‚Ã¼7
Â‹ÂŒef
&Â‘7
Â¥Ã®fÂ©ÂÂÂÂ‘Ã™ÂšÂ› !Â›ÂDÂ•Â–Â—Â˜Â™7
Â¥Ã²fÂ©Ã¿Ã¶Ã¶Â„Â‚Ã¶ÂÂ¦Â‚7Ã¾Â¦ÃµÂ„Â¨ÂÂ‚Âƒ7Â¦Ã·7"Â§Ã½Â„7Â…#Ã¸ÂÃ¶Â„Ã½Â„Â‚Ã¶7Ã·Â¦Â€7|Â¦Â€Ã¶Ã¼7$ÂÃµÂ„Â¦Ã¼7
Attention Modeling of Game Excitement for Sports Videos 
Huang-Chia Shih 
Department of Electrical Engineering, Yuan Ze University,  
135 Yuandong Rd., Chungli, Taiwan, R.O.C. 
hcshih@saturn.yzu.edu.tw
Abstract. In this paper, we proposed a novel key-frame detection method by integrating 
the object-oriented visual attention model and the contextual game-status information. We 
have illustrated an approximate distribution of a viewerâ€™s level of excitement through 
contextual annotations using the semantic knowledge and visual characteristics. The 
number of key-frames was successfully determined by applying the contextual attention 
score, while the key-frame selection depends on the combination of all the visual attention 
scores with bias. Employing the object-based visual attention model integrates with the 
contextual attention model not only produces precise human perceptual characteristics, but 
it will also effectively determine the type of video content that will attract the attention of 
viewers. The proposed algorithm was evaluated using commercial baseball game 
sequences and showed promising results. 
Keywords: attention modeling, highlight detection, content analysis, contextual modeling, 
excitement curve, content-based video retrieval
99 å¹´åº¦å°ˆé¡Œç ”ç©¶è¨ˆç•«ç ”ç©¶æˆæœå½™æ•´è¡¨ 
è¨ˆç•«ä¸»æŒäººï¼šæ–½çš‡å˜‰ è¨ˆç•«ç·¨è™Ÿï¼š99-2218-E-155-013- 
è¨ˆç•«åç¨±ï¼šä»¥äººé¡ç”Ÿç†æ„ŸçŸ¥ç‰¹æ€§ç‚ºåŸºç¤çš„å…§å®¹å¯ä¼¸ç¸®é©æ‡‰æ€§åª’é«”æ’­æ”¾ç³»çµ±ä¹‹ç ”ç©¶ 
é‡åŒ– 
æˆæœé …ç›® å¯¦éš›å·²é”æˆ
æ•¸ï¼ˆè¢«æ¥å—
æˆ–å·²ç™¼è¡¨ï¼‰
é æœŸç¸½é”æˆ
æ•¸(å«å¯¦éš›å·²
é”æˆæ•¸) 
æœ¬è¨ˆç•«å¯¦
éš›è²¢ç»ç™¾
åˆ†æ¯” 
å–®ä½ 
å‚™ è¨» ï¼ˆ è³ª åŒ– èªª
æ˜ï¼šå¦‚æ•¸å€‹è¨ˆç•«
å…±åŒæˆæœã€æˆæœ
åˆ— ç‚º è©² æœŸ åˆŠ ä¹‹
å° é¢ æ•… äº‹ ...
ç­‰ï¼‰ 
æœŸåˆŠè«–æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒè«–æ–‡ 0 0 100% 
ç¯‡ 
 
è«–æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100%   
ç”³è«‹ä¸­ä»¶æ•¸ 1 0 100%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 100% ä»¶  
ä»¶æ•¸ 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 2 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å…§ 
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆæœ¬åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 0 0 100% 
äººæ¬¡ 
 
æœŸåˆŠè«–æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒè«–æ–‡ 3 0 100% 
ç¯‡ 
 
è«–æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100% ç« /æœ¬  
ç”³è«‹ä¸­ä»¶æ•¸ 0 0 100%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 100% ä»¶  
ä»¶æ•¸ 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å¤– 
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆå¤–åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 0 0 100% 
äººæ¬¡ 
 
