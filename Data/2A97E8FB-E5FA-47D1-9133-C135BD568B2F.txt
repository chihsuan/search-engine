assessment, sparse representation 
 
Ô®àÊîøÈô¢ÂúãÂÆ∂ÁßëÂ≠∏ÂßîÂì°ÊúÉÂ∞àÈ°åÁ†îÁ©∂Ë®àÁï´ÊàêÊûúÂ†±Âëä 
Èõ≤Á´ØÁí∞Â¢É‰∏≠Â£ìÁ∏ÆÊÑüÊ∏¨ÊñºÂ§öÂ™íÈ´î‰πãÊáâÁî® 
Compressive Sensing and Its Applications to Multimedia in Cloud 
Computing Environments 
Ë®àÁï´Á∑®ËôüÔºöNSC Ôºç100 Ôºç 2221 Ôºç E Ôºç 468 Ôºç 021 
Âü∑Ô®àÊúüÈôêÔºö100 Ô¶é 8 Êúà 1 Êó•Ëá≥ 101 Ô¶é 7 Êúà 31 Êó• 
‰∏ªÊåÅ‰∫∫ÔºöÔß¥Êô∫Êèö   
Âü∑Ô®àÊ©üÊßãÂèäÂñÆ‰ΩçÂêçÁ®±: ‰∫ûÊ¥≤Â§ßÂ≠∏Ë≥áË®äÂ∑•Á®ãÂ≠∏Á≥ª 
 
[Ë®ª]:Êú¨Ë®àÁï´ÂÖßÂÆπÂ∑≤ÁôºË°®ÊñºÂúãÈöõÊúüÂàä: Li-Wei Kang, Chao-Yung Hsu, Hung-Wei Chen, Chun-Shien Lu, Chih-Yang Lin, 
and Soo-Chang Pei, ‚ÄúFeature-based Sparse Representation for Image Similarity Assessment,‚Äù IEEE Transactions on 
Multimedia, vol. 13, no. 5, pp. 1019-1030, 2011. (EI, SCI, Impact Factor: 1.935, Journal Ranking: 7/103=7%) 
 
ÂÖ∂‰ªñ‰ª£Ë°®Á†îÁ©∂ÊàêÊûú 
[1]: Chih-Yang Lin, Li-Wei Kang, Kahlil Muchtar, Jyh-Da Wei, and Chia-Hung Yeh, ‚ÄúImage Copy Detection via 
Dictionary Learning and Sparse Coding,‚Äù Proceedings of the Third International Conference on Information Security 
and Intelligent Control (ISIC 2012), Yunlin, Taiwan, pp. 244-247, 2012. 
[2]: Chih-Yang Lin, Panyaporn Prangjarote, Li-Wei Kang, Wei-Lun Huang, Tzung-Her Chen, ‚ÄúJoint Fingerprinting and 
Decryption with Noise-resistant for Vector Quantization Images,‚Äù Signal Processing, vol. 92, no. 9, pp. 2159-2171, 
2012. (EI, SCI, Impact Factor: 1.503, Journal Ranking: 79/244=33%) 
[3]: Chin-Chen Chang, Chih-Yang Lin, and Yi-Pei Hsieh, ‚ÄúReversible Steganographic Method for VQ images Using 
Multiple Bases and Declustering,‚Äù Information Sciences, vol. 201, pp. 70-79, 2012. (EI, SCI, Impact Factor: 2.833, 
Journal Ranking: 9/133=7%) 
[4]: Chih-Yang Lin, Chi-Shiang Chan, Li-Wei Kang, and Kahlil Muchtar, ‚ÄúLeft-object Detection through Background 
Modeling,‚Äù International Journal of Innovative Computing, Information and Control (IJICIC), vol. 9, no. 2, pp. 1-
11, 2013. (EI, SCI, Impact Factor: 1.667, Journal Ranking: 12/60=20%) 
[5]: Chih-Yang Lin, Li-Wei Kang, Jau-Hong Kao, Chun-Shien Lu, and Yi-Ta Wu, ‚ÄúInvariant Appearance Modeling for 
Non-Rigid Object Identification in a Real-Time Multi-Camera Environment,‚Äù Journal of Visual Communication and 
Image Representation, accepted, 2012/02. (EI, SCI, Impact Factor: 1.122, Journal Ranking: 34/103=33%) 
 
 
ÔÅÆ Abstract 
Assessment of image similarity is fundamentally important to 
numerous multimedia applications. The goal of similarity 
assessment is to automatically assess the similarities among 
images in a perceptually consistent manner. In this paper, we 
interpret the image similarity assessment problem as an 
information fidelity problem. More specifically, we propose a 
feature-based approach to quantify the information that is 
present in a reference image and how much of this information 
can be extracted from a test image to assess the similarity 
between the two images. Here, we extract the feature points 
and their descriptors from an image, followed by learning the 
dictionary/basis for the descriptors in order to interpret the 
information present in this image. Then, we formulate the 
problem of the image similarity assessment in terms of sparse 
representation. To evaluate the applicability of the proposed 
feature-based sparse representation for image similarity 
assessment (FSRISA) technique, we apply FSRISA to three 
popular applications, namely, image copy detection, retrieval, 
and recognition by properly formulating them to sparse 
representation problems. Promising results have been obtained 
through simulations conducted on several public datasets, 
including the Stirmark benchmark, Corel-1000, COIL-20, 
COIL-100, and Caltech-101 datasets. 
IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 5, OCTOBER 2011 1019
Feature-Based Sparse Representation
for Image Similarity Assessment
Li-Wei Kang, Member, IEEE, Chao-Yung Hsu, Hung-Wei Chen, Chun-Shien Lu, Member, IEEE,
Chih-Yang Lin, Member, IEEE, and Soo-Chang Pei, Fellow, IEEE
Abstract‚ÄîAssessment of image similarity is fundamentally
important to numerous multimedia applications. The goal of sim-
ilarity assessment is to automatically assess the similarities among
images in a perceptually consistent manner. In this paper, we in-
terpret the image similarity assessment problem as an information
fidelity problem. More specifically, we propose a feature-based
approach to quantify the information that is present in a reference
image and how much of this information can be extracted from a
test image to assess the similarity between the two images. Here,
we extract the feature points and their descriptors from an image,
followed by learning the dictionary/basis for the descriptors in
order to interpret the information present in this image. Then,
we formulate the problem of the image similarity assessment in
terms of sparse representation. To evaluate the applicability of
the proposed feature-based sparse representation for image simi-
larity assessment (FSRISA) technique, we apply FSRISA to three
popular applications, namely, image copy detection, retrieval, and
recognition by properly formulating them to sparse representation
problems. Promising results have been obtained through simula-
tions conducted on several public datasets, including the Stirmark
benchmark, Corel-1000, COIL-20, COIL-100, and Caltech-101
datasets.
Index Terms‚ÄîFeature detection, image copy detection, image
recognition, image retrieval, image similarity assessment, sparse
representation.
I. INTRODUCTION
I MAGE similarity assessment is fundamentally importantto numerous multimedia information processing systems
and applications, such as compression, restoration, enhance-
ment, copy detection, retrieval, and recognition/classification.
The major goal of image similarity assessment is to design al-
gorithms for automatic and objective evaluation of similarity in
Manuscript received October 31, 2010; revised March 03, 2011 and May 14,
2011; accepted May 31, 2011. Date of publication June 09, 2011; date of current
version September 16, 2011. This work was supported in part by the National
Science Council, Taiwan, under Grants NSC97-2628-E-001-011-MY3, NSC98-
2631-H-001-013, NSC98-2811-E-001-008, NSC99-2218-E-001-010, NSC99-
2811-E-001-006, and NSC 99-2221-E-468-023. A preliminary version of this
manuscript was presented in the 2010 IEEE International Conference on Mul-
timedia and Expo [8]. The associate editor coordinating the review of this man-
uscript and approving it for publication was Prof. Ming-Ting Sun.
L.-W. Kang and C.-S. Lu are with the Institute of Information Science,
Academia Sinica, 115 Taipei, Taiwan (e-mail: lcs@iis.sinica.edu.tw).
C.-Y. Hsu and H.-W. Chen are with the Institute of Information Science,
Academia Sinica, and Graduate Institute of Communication Engineering, Na-
tional Taiwan University, Taipei, Taiwan.
C.-Y. Lin is with the Department of Computer Science and Information En-
gineering, Asia University, Taichung, Taiwan.
S.-C. Pei is with the Graduate Institute of Communication Engineering, Na-
tional Taiwan University, Taipei, Taiwan.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TMM.2011.2159197
a manner that is consistent with subjective human evaluation.
A simple and popularly used metric is the peak signal-to-noise
ratio (PSNR) or the corresponding mean-squared error (MSE),
whose correlation with human judgment has been shown to be
not tight enough for most applications [1], [2]. Some advanced
approaches, based on the human visual system (HVS), natural
scene statistics (NSS), and/or some image distortion model, also
have been proposed to improve the PSNR metric. They demon-
strate that visual quality of a test image is strongly related to the
relative information present in the image and that the informa-
tion can be quantified to measure the similarity between the test
image and its reference image [1], [2].
There is no doubt that these advanced similarity metrics are
efficient to measure the ‚Äúquality‚Äù of an image compared with
its original version, especially for some image reconstruction
applications. Nevertheless, they mainly focus on assessing the
similarities between a reference image and its non-geometri-
cally variational versions, such as decompressed and bright-
ness/contrast-enhanced versions. Different from the above, in
this paper, we emphasize the ‚Äúsimilarity‚Äù between two arbi-
trary images. In several applications, assessment of the simi-
larities between a reference image and its geometrically varia-
tional versions, such as translation, rotation, scaling, flipping,
and other deformations, is required. On the other hand, one
could encounter appearance variabilities of images, including
background clutter, different viewpoints, and different orienta-
tions. Even if some advanced approaches, such as the structural
similarity (SSIM) index and visual information fidelity (VIF)
[1], [2], can tolerate slightly geometric variations, their goals
still do not devote to the consideration of more comprehensive
image variations.
In this paper, motivated by the concept addressed in Sheikh
and Bovik‚Äôs scheme [2], we interpret the image similarity
assessment problem as an information fidelity problem. More
specifically, we attempt to quantify the information that is
present in a reference image and how much of this information
can be extracted from a test image to assess the similarity
between the two images. The core of the proposed approach,
significantly different from that used in [2], can be addressed
as follows. In [2], image information is quantified using HVS,
NSS, and an image distortion model, while we propose a fea-
ture-based approach to quantify the information present in an
image, based on robust image feature extraction. That is, we de-
tect the feature points of an image, followed by describing each
feature point using a descriptor. Then, we propose to represent
all of the descriptors of an image via sparse representation and
assess the similarity between two images via sparse coding
technique. The merit is that a feature descriptor is sparsely
1520-9210/$26.00 ¬© 2011 IEEE
KANG et al.: FEATURE-BASED SPARSE REPRESENTATION FOR IMAGE SIMILARITY ASSESSMENT 1021
Fig. 1. Concept of the proposed FSRISA framework.
A bounded similarity score should be more suitable for users to
conjecture how similar two images are or for a vision system to
decide a threshold for image comparison. This metric should be
better than just using the number of matched keypoints which
may range from zero to thousands of keypoints.
E. Organization of This Paper
The rest of this paper is organized as follows. The proposed
FSRISA scheme is addressed in Section II. The applications of
FSRISA to image copy detection, retrieval, and recognition are
presented in Section III. The simulation results are shown in
Section IV, followed by the conclusion presented in Section V.
II. PROPOSED FEATURE-BASED SPARSE REPRESENTATION FOR
IMAGE SIMILARITY ASSESSMENT (FSRISA) METHODOLOGY
Sparse representation has resulted in significant impact on
computer vision and pattern recognition, usually in unconven-
tional applications where the goal is not just to obtain a com-
pact representation of the observed signal, but also to extract se-
mantic information. The selection of the dictionary plays a key
role to achieve this goal. That is, overcomplete dictionaries con-
sisting of (or learned from) the training samples provide the key
to attach semantic meaning to sparse signal representations [15].
In this paper, we utilize the sparse representation and
dictionary learning techniques to design our framework of
feature-based sparse representation for image similarity as-
sessment (FSRISA). As illustrated in Fig. 1, given a reference
image, we first apply standard SIFT to detect the keypoints and
extract the feature vector for each keypoint in this image. To
make the SIFT features more compact, we propose to learn
the dictionary consisting of the prototype SIFT atoms to form
the ‚Äúdictionary feature‚Äù of the reference image, as described in
Section II-A. Similarly, we also extract the dictionary feature
for an input test image. Then, we calculate the similarity value
between the two images using the proposed FSRISA technique,
as described in Section II-B.
A. Dictionary Feature Extraction
Here, we apply the K-SVD dictionary learning algorithm [16]
to construct the dictionary for a set of SIFT feature vectors of an
image to form its dictionary feature. To learn an overcomplete
dictionary for a set of training signals, K-SVD seeks the dictio-
nary leading to the best possible representation of each signal in
this set with strict sparsity constraints. The K-SVD algorithm,
which generalizes the K-means algorithm, is an iterative scheme
alternating between sparse coding of the training signals with
respect to the current dictionary and an update process for the
dictionary atoms so as to better fit the training signals.
Given a set of SIFT feature vectors, ,
, we apply K-SVD to find the dictionary of size
, , by formulating the problem as
(1)
where is the sparse representation coefficients of ,
, -norm of , counts the number of nonzero coefficients
of , and is the most desired number of nonzero coefficients
of . We apply K-SVD to solve (1) via an iterative manner with
two stages: 1) sparse coding stage: apply orthogonal matching
pursuit (OMP) [17] to solve for each while fixing ; and
2) dictionary update stage: update together with the nonzero
coefficients of . The two stages are iteratively performed until
convergence. It should be noted that the -minimization formu-
lation in (1) can be converted into an -minimization problem
[18] and other dictionary learning algorithm, (e.g., the online
dictionary learning algorithm [18]) can be also applied in the
dictionary feature extraction stage.
The obtained dictionary feature is an overcomplete dic-
tionary, where , contains
prototype feature vector atoms as the column vectors in .
Each original feature vector , , can
be sparsely represented as a linear combination of the atoms de-
fined in , satisfying , where is an error
tolerance.
B. Sparse Representation-Based Image Similarity Assessment
After obtaining the dictionary feature for each image, we for-
mulate the image similarity assessment based on dictionary fea-
ture matching as a sparse representation problem, described as
follows.
First, consider the two SIFT feature (column) vectors with
length , , , and , ,
extracted, respectively, from the two images, and , where
and are the numbers of feature vectors of and ,
respectively. The dictionary features of and are (of size
) and (of size ), respectively, where
and . Hence, and , where
and are the two sparse coefficient (column) vectors with
length and , of and , respectively. Obviously, if
and can be matched, can be represented sparsely
and linearly with respect to . On the other hand, can be
represented sparsely and linearly with respect to .
To assess the similarity between a reference image and
a test image , exploiting the discriminative characteristic of
sparse representation, we want to quantify how much informa-
tion present in can be extracted from . A sparse represen-
tation problem for representing each SIFT feature vector of
with respect to the joint dictionary can be
defined as
(2)
KANG et al.: FEATURE-BASED SPARSE REPRESENTATION FOR IMAGE SIMILARITY ASSESSMENT 1023
Fig. 2. Illustrated example of the proposed FSRISA framework. (a) Extrac-
tion of dictionary features for the reference and test images, respectively. (b)
Matching of the two images via sparse coding and voting.
fine a bounded score based on reconstruction error obtained by
only one dictionary.
Algorithm I: Proposed FSRISA
Input: A reference image and a test image .
Output: The similarity value between and , i.e.,
.
1. Extract the SIFT feature vectors , , from
, followed by learning the dictionary feature sparsely
representing .
2. Extract the SIFT feature vectors , , from
, followed by learning the dictionary feature sparsely
representing .
3. Perform -minimization by solving (3) for ,
, with respect to .
4. Calculate the reconstruction errors, and , for ,
, with respect to and , respectively.
5. Perform voting by comparing and , for ,
, and get the percentages of votes, and ,
with respect to and , respectively.
6. Calculate (4).
C. Computational Complexity Analysis of FSRISA
The computational complexity of the proposed FSRISA can
be analyzed as follows. The computational complexity for ex-
tracting the dictionary feature of an image includes the com-
plexities of performing SIFT feature extraction and K-SVD dic-
tionary learning. For an image with -dimensional SIFT
feature vectors, the computational complexity for learning a dic-
tionary of size , , using K-SVD [16] can
be derived to be around [23]
(5)
where is the target sparsity and is the number of training
iterations. Hence, the approximate computational complexity of
the dictionary feature extraction for an image is obtained as
(6)
where roughly denotes the computational com-
plexity (proportional to ) of SIFT feature extraction, in terms
of the size of an image.
On the other hand, the computational complexity of per-
forming -minimization using SpaRSA can be approximately
derived as [20]
(7)
where is the number of atoms in a dictionary and is a con-
stant. It has been shown that the complexity of SpaRSA is ap-
proximately linear, i.e., is very close to 1.
Based on (6) and (7), the overall computational complexity
for assessing the similarity between two images, and , by
performing the proposed FSRISA can be derived as
(8)
where and denote the number of SIFT feature vectors
for and , respectively, and denote the target sparsi-
ties for learning and (the dictionary features of and
, respectively), respectively, and denote the number of
atoms in and , respectively, and denote the number
of iterations for learning and , respectively, and de-
notes the length of a SIFT feature vector .
III. FSRISA FOR MULTIMEDIA APPLICATIONS
In this section, we introduce three multimedia applications,
including image copy detection, retrieval, and recognition, of
the proposed FSRISA.
A. Image Copy Detection via FSRISA
Digital images distributed through the Internet may suffer
from several possible manipulations, such as (re)compres-
sion, noising, contrast/brightness adjusting, and geometrical
operations. To ensure trustworthiness, image copy detection
KANG et al.: FEATURE-BASED SPARSE REPRESENTATION FOR IMAGE SIMILARITY ASSESSMENT 1025
Fig. 3. Similarity values obtained from the PSNR, VIF, and our FSRISA between the Baboon image and its manipulated images. (a) JPEG compression   
 	
       	. (b) Blurring    
     
   . (c) Brightness and contrast adjusting
   
    
 	   	. (d) Noising    

     	   . (e) Scaling and cropping
   
 
    
   . (f) Rotation    
     
   . (g) Flipping   

     
   . (h) Shearing    
        
. (i) Rippling    
 	  
    . (j) Irrelevant image     	    
   .
Then, can be also solved by performing the -minimization
based on the received measurements for (compressed ).
IV. SIMULATION RESULTS
In this section, we present simulations conducted on publicly
available benchmarks or datasets for evaluation of the proposed
FSRISA scheme in the fundamental issue of image assessment
and three multimedia applications. Then, we address some ex-
perimental comparisons between sparse coding-based and tradi-
tional approaches to demonstrate the advantage of the proposed
scheme.
A. Evaluation of FSRISA-Based Image Similarity Assessment
To evaluate the efficiency of FSRISA for assessing the sim-
ilarity between two images, we use several examples of image
manipulations (including signal processing and geometric dis-
tortions) defined in the Stirmark benchmarks [26] and com-
pare FSRISA of similarity range with well-known metrics,
PSNR of range , and VIF of range (for image con-
trast enhancement, VIF value may be larger than 1) [2]. In the
three evaluated metrics, the larger the value is, the more similar
the two evaluated images are.
In our simulation, the size of each image is 280 280. Never-
theless, it should be noted that our scheme can work for images
of different sizes. Based on the principle for tuning the KSVD
parameters described in Section II-B, we set the following pa-
rameters to ensure that is finer than . For a reference
image of size , the parameters are shown
as follows. We set the number of atoms in the dictionary fea-
ture to , where denotes the number of
SIFT feature vectors for , the number of iterations K-SVD
performs for learning to , and the target sparsity to
. For a test image of size , the
parameters are , , and . If
, we force by properly adjusting the factors
to be multiplied by and , respectively.
The similarity values obtained from the PSNR, VIF, and FS-
RISA for some examples of image manipulations for the Ba-
boon and Lena images, respectively, are shown in Figs. 3 and 4.
It can be observed from Figs. 3 and 4 that for image similarity
assessment, FSRISA is more robust to several image manipula-
tions, especially for geometrical manipulations. The similarity
values between an image and its related manipulated versions
for FSRISA are usually higher than 0.5, while the ones between
an image and unrelated versions are usually far lower than 0.5.
Nevertheless, the VIF value between an image and its manipu-
lated versions are usually lower than 0.5, except for the bright-
ness and contrast adjusting manipulation, which enhances the
image quality. For some manipulations (e.g., scaling, cropping,
and flipping), the VIF value is almost indistinguishable from
that for an unrelated image. Moreover, PSNR is obviously not
good for similarity assessment. Hence, the discriminability of
FSRISA is usually better than the other two metrics used for
comparisons. In this paper, we focus on ‚Äúsimilarity‚Äù assessment
between images and hence, the similarity scores for different
kinds of manipulations are somewhat similar. The major goal is
also the ‚Äúdiscriminability‚Äù between different images. Neverthe-
less, for the ‚Äúquality‚Äù issue, the differences between a test image
and its reference image (ground truth) becomes more critical,
which is not the focus of this paper.
B. Evaluation of FSRISA-Based Image Copy Detection
To evaluate the proposed FSRISA-based image copy detec-
tion scheme, ten 280 280 images, Baboon, Boat, Clock, Girl,
House, Lena, Monarch, Pepper, Splash, and Tiffany, were used.
Each image was manipulated by 204 manipulations defined in
the Stirmark benchmarks [26]. These image manipulations are
also very similar to the ones used to evaluate image copy or
KANG et al.: FEATURE-BASED SPARSE REPRESENTATION FOR IMAGE SIMILARITY ASSESSMENT 1027
It is particularly worth considering the flipping manipulation.
Standard SIFT keypoint matching [3] can only match a few key-
points due to the orientations of the corresponding feature vec-
tors being switched to the opposite (e.g., only six pairs of key-
point can be correctly matched for the Lena image). Neverthe-
less, even if the proposed FSRISA is based on SIFT, FSRISA
can usually provide a reasonable similarity score between an
image and its flipped version, as examples shown in Figs. 3 and
4. That is, each feature vector of a flipped image and the corre-
sponding vector of its original (unflipped) version usually only
have different signs (with the same magnitudes). Let us consider
a dictionary integrated from the two dictionary features of the
two images, respectively. When performing sparse coding for
each feature vector of the flipped image with respect to the dic-
tionary, most of the feature vectors of the image can be still well
sparsely and linearly represented by the dictionary feature of its
original version.
C. Evaluation of FSRISA-Based Image Retrieval
To evaluate the proposed FSRISA-based image retrieval
scheme, we construct an image database consisting of the ten
test images together with their respective 204 manipulations
used in Section IV-B (total 2050 images), and the Corel-1000
image dataset [27] (total 1000 images from ten classes), re-
sulting in a total of 3050 images. The parameter settings for
applying our FSRISA to each query image and each database
image are the same as those settings for reference image and
test image, respectively, used in Section IV-A. In this subsec-
tion, we conduct two kinds of experiments, including ‚Äúcopy
image retrieval‚Äù and ‚Äúgeneral image retrieval.‚Äù
In this paper, we just consider the simplest scenario for
image retrieval, where the score between a query image and
each database image is individually calculated using the pro-
posed FSRISA. Then, we retrieve the top images with the
largest scores, where is the desired number of retrieved
images. Here, we neither consider performing any indexing
or clustering techniques to re-organize an image database nor
integrating multiple features for efficient image retrieval.
The main reasons for constructing this database and con-
ducting such two kinds of experiments to evaluate our scheme
can be described as follows. We focus on investigating sparse
representation of SIFT features and finding its usefulness in
image retrieval application. Hence, the database includes sev-
eral images and their variations for ‚Äúcopy image retrieval‚Äù eval-
uation (similar to the test dataset collected in [9] and [10]). On
the other hand, without integrating multiple features, we just
want to test some ‚Äúpure‚Äù query images (without overly com-
plex scenes or with a clear background) to retrieve the images
with the same semantic meaning, with different appearance for
‚Äúgeneral image retrieval‚Äù evaluation.
For ‚Äúcopy image retrieval,‚Äù we use the ten original images as
the query images and evaluate the precision rates for retrieving
the top 205 database images with the largest scores for each
query image, as shown in Table I, where the average precision
is 99.01%. Such simulation settings are similar to the ones used
for near-duplicate image retrieval performed in [10]. It can be
observed from Table I that FSRISA is efficient for retrieving the
copy versions of a query image.
TABLE I
PRECISION RATES FOR RETRIEVING THE TOP 205 DATABASE
IMAGES WITH THE LARGEST SCORES FOR EACH QUERY IMAGE
TABLE II
AVERAGE PRECISION RATES OF THE ‚ÄúDINOSAURS‚Äù QUERY IMAGES FOR
RETRIEVING THE TOP 10, 20, AND 40 IMAGES FROM THE COREL-1000 DATASET
Fig. 7. Retrieved top ten images. (a) Query image and the retrieved 1st image
and (b)-(j) the retrieved 2nd through 10th images.
Fig. 8. Probability distributions of the FSRISA similarity values between a se-
lected ‚ÄúDinosaurs‚Äù image and the 100 images in the same class, and those be-
tween the image and the other 900 images in Corel-1000 dataset.
For ‚Äúgeneral image retrieval,‚Äù we randomly select ten im-
ages from the ‚ÄúDinosaurs‚Äù class of the Corel-1000 image dataset
and evaluate the average precision for retrieving the top 10, 20,
and 40 images, respectively, with the largest scores from the
dataset. We compare the results with the best ones reported in
[29] (denoted by ‚Äúvisually significant point feature‚Äù), as shown
in Table II. Some retrieved images for a query image are il-
lustrated in Fig. 7. Moreover, the probability distributions of
the FSRISA similarity values between a selected ‚ÄúDinosaurs‚Äù
image and the 100 images in the same class, and those be-
tween the image and the other 900 images in Corel-1000 dataset,
are displayed in Fig. 8. It can be observed from Table II and
Figs. 7 and 8, for images without overly complex scenes, FS-
RISA can be still efficient for retrieving images with similar se-
KANG et al.: FEATURE-BASED SPARSE REPRESENTATION FOR IMAGE SIMILARITY ASSESSMENT 1029
V. CONCLUSIONS
In this paper, we have proposed a scheme of FSRISA. The
core is to propose a feature-based image similarity assessment
technique by exploring the two aspects of a feature detector in
terms of representation and matching in our FSRISA frame-
work. Then, we properly formulate the image copy detection, re-
trieval, and recognition problems as sparse representation prob-
lems and solve them based on our FSRISA. The future works
may focus on reducing the computational complexities for dic-
tionary feature extraction and image matching by performing
sparse coding, which can be further reduced via novel tech-
niques, such as the online dictionary learning algorithm [18], ef-
ficient greedy algorithm [18], or fast -minimization algorithm
[40]. On the other hand, for FSRISA-based image retrieval ap-
plications, further indexing techniques should be also studied.
The proposed FSRISA-based image copy detection scheme may
be extended to video copy detection by learning the ‚Äúdictio-
nary feature‚Äù for each video sequence/clip. Moreover, incorpo-
rated with our secure SIFT techniques [41], [42], the three FS-
RISA-based applications can be performed in the encrypted do-
main and are suitable for privacy-preserving applications.
REFERENCES
[1] Z. Wang and A. C. Bovik, ‚ÄúMean squared error: Love it or leave it?‚ÄîA
new look at signal fidelity measures,‚Äù IEEE Signal Process. Mag., vol.
26, no. 1, pp. 98‚Äì117, Jan. 2009.
[2] H. R. Sheikh and A. C. Bovik, ‚ÄúImage information and visual quality,‚Äù
IEEE Trans. Image Process., vol. 15, no. 2, pp. 430‚Äì444, Feb. 2006.
[3] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant key-
points,‚Äù Int. J. Comput. Vision, vol. 60, no. 2, pp. 91‚Äì110, 2004.
[4] J. Sivic and A. Zisserman, ‚ÄúVideo Google: A text retrieval approach to
object matching in videos,‚Äù in Proc. IEEE Int. Conf. Computer Vision,
Nice, France, Oct. 2003, vol. 2, pp. 1470‚Äì1477.
[5] D. Nist√©r and H. Stew√©nius, ‚ÄúScalable recognition with a vocabulary
tree,‚Äù in Proc. IEEE Conf. Computer Vision and Pattern Recognition,
2006, pp. 2161‚Äì2168.
[6] J. Zhang, M. Marszalek, S. Lazebnik, and C. Schmid, ‚ÄúLocal features
and kernels for classification of texture and object categories: A com-
prehensive study,‚Äù Int. J. Comput. Vision, vol. 73, no. 2, pp. 213‚Äì238,
2007.
[7] J. Yang, K. Yu, Y. Gong, and T. Huang, ‚ÄúLinear spatial pyramid
matching using sparse coding for image classification,‚Äù in Proc. IEEE
Conf. Computer Vision and Pattern Recognition, Jun. 2009.
[8] L. W. Kang, C. Y. Hsu, H. W. Chen, and C. S. Lu, ‚ÄúSecure SIFT-
based sparse representation for image copy detection and recognition,‚Äù
in Proc. IEEE Int. Conf. Multimedia and Expo, Singapore, Jul. 2010.
[9] Z. Xu, H. Ling, F. Zou, Z. Lu, and P. Li, ‚ÄúA novel image copy detec-
tion scheme based on the local multi-resolution histogram descriptor,‚Äù
Multimedia Tools Appl., Jan. 2010.
[10] Y. Ke, R. Sukthankar, and L. Huston, ‚ÄúEfficient near-duplicate detec-
tion and sub-image retrieval,‚Äù in Proc. ACM Multimedia, 2004.
[11] W. L. Zhao and C. W. Ngo, ‚ÄúScale-rotation invariant pattern entropy
for keypoint-based near-duplicate detection,‚Äù IEEE Trans. Image
Process., vol. 18, no. 2, pp. 412‚Äì423, Feb. 2009.
[12] K. Mikolajczyk and C. Schmid, ‚ÄúA performance evaluation of local
descriptors,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 10,
pp. 1615‚Äì1630, Oct. 2005.
[13] V. Chandrasekhar, M. Makar, G. Takacs, D. M. Chen, S. S. Tsai, R.
Grzeszczuk, and B. Girod, ‚ÄúSurvey of SIFT compression schemes,‚Äù in
Proc. Int. Workshop Mobile Multimedia Processing, Turkey, 2010.
[14] O. Pele and M. Werman, ‚ÄúA linear time histogram metric for improved
SIFT matching,‚Äù in Proc. Eur. Conf. Computer Vision, 2008, vol. 5304,
pp. 495‚Äì508.
[15] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. Huang, and S. Yan, ‚ÄúSparse
representation for computer vision and pattern recognition,‚Äù Proc.
IEEE, vol. 98, no. 6, pp. 1031‚Äì1044, Jun. 2010.
[16] M. Aharon, M. Elad, and A. M. Bruckstein, ‚ÄúThe K-SVD: An algorithm
for designing of overcomplete dictionaries for sparse representation,‚Äù
IEEE Trans. Signal Process., vol. 54, no. 11, pp. 4311‚Äì4322, Nov. 2006.
[17] S. Mallat and Z. Zhang, ‚ÄúMatching pursuits with time-frequency dic-
tionaries,‚Äù IEEE Trans. Signal Process., vol. 41, no. 12, pp. 3397‚Äì3415,
Dec. 1993.
[18] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, ‚ÄúOnline learning for ma-
trix factorization and sparse coding,‚Äù J. Mach. Learn. Res., vol. 11, pp.
19‚Äì60, 2010.
[19] E. Candes and M. Wakin, ‚ÄúAn introduction to compressive sampling,‚Äù
IEEE Signal Process. Mag., vol. 25, no. 2, pp. 21‚Äì30, Mar. 2008.
[20] S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo, ‚ÄúSparse recon-
struction by separable approximation,‚Äù IEEE Trans. Signal Process.,
vol. 57, no. 7, pp. 2479‚Äì2493, Jul. 2009.
[21] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, ‚ÄúRobust
face recognition via sparse representation,‚Äù IEEE Trans. Pattern Anal.
Mach. Intell., vol. 31, no. 2, pp. 210‚Äì227, Feb. 2009.
[22] J. M. Fadili, J. L. Starck, J. Bobin, and Y. Moudden, ‚ÄúImage decompo-
sition and separation using sparse representations: An overview,‚Äù Proc.
IEEE, vol. 98, no. 6, pp. 983‚Äì994, Jun. 2010.
[23] R. Rubinstein, M. Zibulevsky, and M. Elad, Efficient Implementation
of the K-SVD Algorithm Using Batch Orthogonal Matching Pursuit,
Technion‚ÄîIsrael Institute of Technology, 2008, CS Tech. Rep.
[24] C. Kim, ‚ÄúContent-based image copy detection,‚Äù Signal Process.: Image
Commun., vol. 18, pp. 169‚Äì184, 2003.
[25] V. Monga and B. L. Evans, ‚ÄúPerceptual image hashing via feature
points: Performance evaluation and tradeoffs,‚Äù IEEE Trans. Image
Process., vol. 15, no. 11, pp. 3453‚Äì3466, Nov. 2006.
[26] F. A. P. Petitcolas, ‚ÄúWatermarking schemes evaluation,‚Äù IEEE Signal
Process. Mag., vol. 17, no. 5, pp. 58‚Äì64, Sep. 2000.
[27] J. Z. Wang, J. Li, and G. Wiederhold, ‚ÄúSIMPLIcity: Semantics-sen-
sitive integrated matching for picture libraries,‚Äù IEEE Trans. Pattern
Anal. Mach. Intell., vol. 23, no. 9, pp. 947‚Äì963, Sep. 2001.
[28] Y. Liu, F. Wu, Z. Zhang, Y. Zhuang, and S. Yan, ‚ÄúSparse represen-
tation using nonnegative curds and whey,‚Äù in Proc. IEEE Conf. Com-
puter Vision and Pattern Recognition, San Francisco, CA, Jun. 2010,
pp. 3578‚Äì3585.
[29] M. Banerjee, M. K. Kundu, and P. Maji, ‚ÄúContent-based image re-
trieval using visually significant point features,‚Äù Fuzzy Sets Syst., vol.
160, no. 23, pp. 3323‚Äì3341, Dec. 2009.
[30] S. A. Nene, S. K. Nayar, and H. Murase, Columbia Object Image Li-
brary (COIL-20), 1996, Tech. Rep. CUCS-005-96.
[31] S. A. Nene, S. K. Nayar, and H. Murase, Columbia Object Image Li-
brary (COIL-100), 1996, Tech. Rep. CUCS-006-96.
[32] H. Cevikalp, ‚ÄúNew clustering algorithms for the support vector ma-
chine based hierarchical classification,‚Äù Pattern Recognit. Lett., vol. 31,
pp. 1285‚Äì1291, 2010.
[33] L. Fei-Fei, R. Fergus, and P. Perona, ‚ÄúLearning generative visual
models from few training examples: An incremental Bayesian ap-
proach tested on 101 object categories,‚Äù in Proc. IEEE Conf. Computer
Vision and Pattern Recognition Workshop on Generative-Model Based
Vision, 2004.
[34] T. Zhang, K. Huang, X. Li, J. Yang, and D. Tao, ‚ÄúDiscriminative
orthogonal neighborhood-preserving projections for classification,‚Äù
IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 40, no. 1, pp.
253‚Äì263, Feb. 2010.
[35] G. A. Papakostasa, E. G. Karakasisb, and D. E. Koulouriotis, ‚ÄúNovel
moment invariants for improved classification performance in com-
puter vision applications,‚Äù Pattern Recognit., vol. 43, pp. 58‚Äì68, 2010.
[36] A. Y. Yang, M. Gastpar, R. Bajcsy, and S. S. Sastry, ‚ÄúDistributed sensor
perception via sparse representation,‚Äù Proc. IEEE, vol. 98, no. 6, pp.
1077‚Äì1088, Jun. 2010.
[37] K. Riesen and H. Bunke, ‚ÄúApproximate graph edit distance computa-
tion by means of bipartite graph matching,‚Äù Image Vision Comput., vol.
27, pp. 950‚Äì959, 2009.
[38] H. Zhang, A. Berg, M. Maire, and J. Malik, ‚ÄúSVM-KNN: Discrimina-
tive nearest neighbor classification for visual category recognition,‚Äù in
Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2006.
[39] O. Boiman, E. Shechtman, and M. Irani, ‚ÄúIn defense of nearest-
neighbor based image classification,‚Äù in Proc. IEEE Conf. Computer
Vision and Pattern Recognition, 2008.
[40] A. Yang, A. Ganesh, Y. Ma, and S. Sastry, ‚ÄúFast l -minimization al-
gorithms and an application in robust face recognition: A review,‚Äù in
Proc. IEEE Int. Conf. Image Process., Hong Kong, Sep. 2010.
[41] C. Y. Hsu, C. S. Lu, and S. C. Pei, ‚ÄúSecure and robust SIFT,‚Äù in Proc.
ACM Int. Conf. Multimedia, Beijing, China, 2009, pp. 637‚Äì640.
[42] C. Y. Hsu, C. S. Lu, and S. C. Pei, ‚ÄúHomomorphic encryption-based
secure SIFT for privacy-preserving feature extraction,‚Äù in Proc. IS&T/
SPIE Media Watermarking, Forensics, and Security, Jan. 2011.
The Report of IIHMSP2011 (The Seventh 
International Conference on 
Intelligent Information Hiding and Multimedia 
Signal Processing)  
 
Chih-Yang Lin 
Department of Computer Science & Information Engineering 
Asia University, Taichung, Taiwan 
TELÔºö(04)23323456 ext.20102 
E-mailÔºöandrewlin@asia.edu.tw 
 
1. Introduction of IIH-MSP 
IIH-MSP 2011 is the seventh International Conference on Intelligent Information 
Hiding and Multimedia Signal Processing. IIH-MSP 2011 was held at the DUT 
International Convention Center in Dalian University of Technology this October. 
Since 2004, the series of IIHMSP conferences has been held in USA, Japan, Germany 
and China. IIHMSP 2011 included a highly selective program of technical papers, 
accompanied by workshops, panel discussions and a keynote speech. Established as a 
premier venue in the area of intelligent data hiding and multimedia signal processing, 
IIHMSP 2011 offered a forum for researchers to exchange ideas and experiences. The 
conference tracks are (1) Information Hiding and Security, (2) Multimedia Signal 
Processing and Networking, and (3) Bio-inspired Multimedia Technologies and 
Systems. We focused on the first track and the details are listed below.  
 
(1) Cross-discipline Techniques in Signal Processing and Networking 
(2) Session A02: Techniques and Algorithms for Multimedia Security 
(3) Session A03: Cross-discipline Techniques in Signal Processing and Networking 
(II) 
(4) Session A04: Data Hiding, Visual Cryptography, and Secret Image Sharing (I) 
(5) Session A05: Multimedia Signal Processing and Networking 
(6) Session A06: Data Hiding, Visual Cryptography, and Secret Image Sharing (II) 
(7) Session B01: Intelligent Watermarking Techniques, Image Authentication and 
Visual Cryptography 
(8) Session B02: Multimedia Signal Processing and Stegarnography 
Abstract: Professor Volker Roth gave some constraints and side effects of 
software components at first. Next, he argued that psychological acceptability, 
conceptual simplicity and decentralization of security mechanisms, should 
receive greater attention when designing security mechanisms of the future. 
For a few selected application areas, he suggested strategies and trade-offs 
along the lines mentioned above that may help in improving overall security.  
 
   (2) Some related researches I listened are listed below. 
 
a. B02-01 (IIH-MSP-2011-IS07-01) 
Graph-based Features for Image Retrieval 
Cai-Hua Li and Zhe-Ming Lu 
China Earthquake Administration, China 
 
b. B02-02 (IIH-MSP-2011-0040) 
Development and Evaluation of Defaced Sites Automatic Detection System DICE 
Tatsuya Tanaka,Yusuke Tamura,Toshifumi Kai, and Ryoichi Sasaki 
Tokyo Denki University, Japan 
 
c. B02-03 (IIH-MSP-2011-0156) 
An Improved Algorithm for Multiple Closed Contour Detection 
Tiejun Zhang, Xufeng Bai, Xianhua Song, and Xiamu Niu 
Harbin Institute of Technology, China 
 
d. B02-04 (IIH-MSP-2011-0128) 
Power Research of JPEG Circuits in FPGA 
Xinsheng Wang and Mingyan Yu 
Harbin Institute of Technology at Weihai, China 
 
e. B02-05 (IIH-MSP-2011-0199) 
Steganography: a Class of Algorithms having Secure Properties 
Jacques M. Bahi, Jean-Fran√ßois Couchot, and Christophe Guyeux 
University of Franche-Comt√©, France 
 
f. B03-01 (IIH-MSP-2011-IS06-06) 
Low Computational Color Secret Sharing Schemes with High Shares Quality 
Color Secret Sharing 
Chin-Chen Chang, Kuo-Nan Chen, and Ying-Hsuan Huang 
Extraction and Robustness against Foreground Objects 
Kitahiro Kaneda, Yoshiaki Tachibana, and Keiichi Iwamura 
Tokyo University of Science, Japan 
 
 
 
cell phone. Therefore, the system would not decrypt the 
image, and would send emails with the attacker‚Äôs face image 
to the owner. In the following subsections, the whole process 
will be detailed. 
 
 
Fig. 1. The outline of the system. 
A. System Architecture 
In the initial stage, when the owner first uses the cell 
phone, the system will ask the user to input the e-mail 
address and also will extract the face features of the owner to 
store in the cell phone. Then the system will use the features 
as a key to encrypt all the images on the cell phone. The 
initial step can be referred to Fig. 2. After that, when the 
owner takes new pictures with the cell phone, the system will 
use the default key to encrypt them all. Therefore, the owner 
does not need to worry about whether the private pictures in 
the cell phone will ever be leaked by a malicious user. The 
whole system flowchart is detailed in Fig. 3. 
 
Start initialization
Extract the face
features of the
owner
Input e-mail address
Encrypt all the
images in the cell
phone
End
 
Start
Take pictures Show pictures
Extract the face
features of the
holder
Check feature‚Äôs
consistency
Get the default
features of the
owner
Decrypt images
Encrypt images
Send email
No
Yes
End
 
Fig. 2.  The initialization 
of the system. 
Fig. 3. The system flowchart. 
 
B. Face Detection 
In order to extract the face features, face detection is the 
first step for face recognition. In this paper, we use color 
information to detect the face in the image. The most 
common color spaces for image processing are RGB, HSV, 
and YCrCb [6]. However, RBG space is deeply sensitive to 
changes in illumination, so different illuminations for the 
same object would result in different color representations. 
HSV space has the advantage with regards to illumination 
changes, but the transformation is non-linear, causing high 
computation load. Therefore, in this paper, we adopt YCrCb 
space to detect skin color. Fig. 4 shows the skin color 
distribution in terms of HSV and YCrCb spaces [3]. 
 
    (a)                             (b) 
Fig. 4. Skin color distribution in HSV and YCrCb spaces [3]. 
 
The linear transformation from RGB to YCrCb is shown in 
Eq. (1). 
  
B
G
R
214.18786.93000.112
000.112203.74797.37
966.24553.128481.65
128
128
16





















	






















Cr
Cb
Y  
(1) 
 
From the previous research result [3], the skin color 
range based on YCrCb is 90~200 for Y, 134~165 for Cr, and 
80~128 for Cb. Based on these ranges, the result of face 
detection can be transformed into a binary image B1. 
However, this result may contain many breaks, mainly due to 
features like the eyes, brows, and mouth, so some 
morphlogical operations, such as dilation and erosion, should 
be applied to generate B2. The connected component method, 
in turn, is used to filter out small noise. Then, use B1 XOR B2 
to obtain B3, which only contains the eyes, brows, and mouth. 
Finally, partition B3 into two parts, an upper and lower part, 
and use horizontal and vertical projections to identify the 
locations of the eyes, brows, and mouth. According to these 
locations, the face region can be exactly identified. 
C. Face Recognition 
After the face region is determined, the corresponding 
color face image is transformed into a grey-level image and 
renormalized to the size 126147 [1]. The new normalized 
image is then partitioned into non-overlapping 33, 55, and 
77 blocks, as shown in Fig. 5. In this paper, a 77 
segmentation approach is applied. 
 
 
(a)                       (b)                       (c) 
Fig. 5. Face segmentation [1]. 
 
For each non-overlapping block, Ahonen et al. [1] 
proposed local binary patterns (LBP) code, which comes 
from the convolution of the block by using a 33 mask to 
generate its sub-histogram, denoted as follows: 
(, ) = ‚àë 	
 ‚àí 2.                  (2) 
In Eq(2), gc represents the grey value of the center pixel (xc, 
yc), whereas gp represents grey values of P neighborhood 
pixels of the center pixel. The function s(x) is denoted as 
follows: 
                        	()  = 1     ‚â• 0,0     < 0 .
                        (3) 
An example of an LBP value for a pixel is illustrated in Fig. 
6. Assume that the pixel value is 6 and its four surrounding 
134
horizontal projection of the mouth to get its boundary; Fig. 
11(h) is the horizontal projection of the eyes and brows in 
order to separate eyes and brows; Fig. 11(i) shows the 
vertical projection of the mouth to identify the center of the 
mouth; Fig. 11(j) is the vertical projection of the eyes and 
brows to get centers of the eyes; Fig. 11(k) shows the final 
detection result. 
 
 
Fig. 10. The result of face detection. 
 
    
(a) (b) (c) (d) 
 
   
(e) (f) (g) (h) 
  
 
(i) (j) (k) 
Fig. 11. The steps of face detection. 
 
Face recognition: 
The system of face recognition is presented in Fig. 12. 
Fig. 13(a) is the corresponding grey-level face normalized to 
the size of 126147, and Fig. 13(b) is the result of 77 
segmentation. The recognition demonstration is shown in Fig. 
14. 
 
 
(a) 
 
(b) 
Fig. 12. The face recognition 
system. 
Fig. 13. Face normalization and 
segmentation. 
 
Image encryption and decryption: 
The results of image encryption and decryption are shown in 
Fig. 15, where the left face image is shot from the camera, 
and the middle image is the encrypted image stored in the 
mobile phone. The right image shows that when the owner 
checks the image again, the system can successfully decrypt 
the image using the face features of the owner. The 
encryption and decryption operations only require less than 
0.2 sec in our environment. 
 
  
Fig. 14. The results of face recognition. 
 
 
Fig. 15. The result of mage encryption and decryption. 
IV. CONCLUSIONS 
In this paper, we propose an effective method to protect 
privacy for mobile phones. All the operations in our system 
require quite considerably few computations and can be 
efficiently applied to real cell phone systems. In sum, our 
system can achieve four main functions: (1) protecting 
privacy in the mobile phone; (2) identifying the cell phone 
holder on the fly; (3) protecting copyrights of images; (4) 
getting firsthand clues on malicious users. 
ACKNOWLEDGMENT 
This work was supported by National Science Council, 
Taiwan, under Grant NSC 99-2221-E-468-023. 
REFERENCES 
[1] T. Ahonen, A. Hadid, and M. Pietik√§inen, ‚ÄúFace Description with 
Local Binary Patterns: Application to Face Recognition,‚Äù IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 28, 
no. 12, pp. 2037-2041, 2006. 
[2]  M. U. Celik, A. N. Lemma, S. Katzenbeisser, and M. van der Veen, 
‚ÄúLookup-Table-Based Secure Client-Side Embedding for Spread-
Spectrum Watermarks,‚Äù IEEE Transactions on Information 
Forensics and Security, vol. 3, no. 3, pp. 475-487, 2008. 
[3] C. Garcia and G.Tziritas, ‚ÄúFace Detection Using Quantized Skin 
Color Regions Merging and Wavelet Packet Analysis,‚Äù IEEE 
Transactions on Multimedia, vol. 1, no. 3, pp. 264-227, 1999. 
[4] S. Lian, Multimedia Content Encryption: Techniques and 
Applications: Auerbach Publications, 2008. 
[5]  C. Y. Lin and C. S. Lu, ‚ÄúA New Joint Fingerprinting and Decryption 
Scheme Resistant to Content Leakage and Tampering Attack,‚Äù 
Proceedings of the 22th Computer Vision, Graphics and Image 
Processing Conference, pp. 10-18, 2009. 
[6]  K. H. Seo, W. Kim, C. Oh, and J.-J. Lee, ‚ÄúFace Detection and Facial 
Feature Extraction Using Color Snake,‚Äù Proceedings of the 2002 
IEEE International Symposium on Industrial Electronics, pp. 457-
462, 2002. 
[7]  W. Stallings, Cryptography and Network Security (4th Edition): 
Prentice Hall, 2005. 
 
 
136
100 Âπ¥Â∫¶Â∞àÈ°åÁ†îÁ©∂Ë®àÁï´Á†îÁ©∂ÊàêÊûúÂΩôÊï¥Ë°® 
Ë®àÁï´‰∏ªÊåÅ‰∫∫ÔºöÊûóÊô∫Êèö Ë®àÁï´Á∑®ËôüÔºö100-2221-E-468-021- 
Ë®àÁï´ÂêçÁ®±ÔºöÈõ≤Á´ØÁí∞Â¢É‰∏≠Â£ìÁ∏ÆÊÑüÊ∏¨ÊñºÂ§öÂ™íÈ´î‰πãÊáâÁî® 
ÈáèÂåñ 
ÊàêÊûúÈ†ÖÁõÆ 
ÂØ¶ÈöõÂ∑≤ÈÅî
ÊàêÊï∏ÔºàË¢´
Êé•ÂèóÊàñÂ∑≤
ÁôºË°®Ôºâ
È†êÊúüÁ∏ΩÈÅî
ÊàêÊï∏(Âê´ÂØ¶
ÈöõÂ∑≤ÈÅîÊàê
Êï∏) 
Êú¨Ë®àÁï´
ÂØ¶ÈöõË≤¢
ÁçªÁôæÂàÜ
ÊØî 
ÂñÆ‰Ωç
ÂÇôË®ªÔºàË≥™ÂåñË™™ÊòéÔºöÂ¶ÇÊï∏ÂÄãË®à
Áï´ÂÖ±ÂêåÊàêÊûú„ÄÅÊàêÊûúÂàóÁÇ∫Ë©≤Êúü
Âàä‰πãÂ∞ÅÈù¢ÊïÖ‰∫ã...Á≠âÔºâ 
ÊúüÂàäË´ñÊñá 0 0 100%  
Á†îÁ©∂Â†±Âëä/ÊäÄË°ìÂ†±
Âëä 0 0 100%  
Á†îË®éÊúÉË´ñÊñá 0 0 100% 
ÁØá
 
Ë´ñÊñáËëó‰Ωú 
Â∞àÊõ∏ 0 0 100%   
Áî≥Ë´ã‰∏≠‰ª∂Êï∏ 1 1 100%  Â∞àÂà© Â∑≤Áç≤Âæó‰ª∂Êï∏ 0 0 100% ‰ª∂  
‰ª∂Êï∏ 0 0 100% ‰ª∂  
ÊäÄË°ìÁßªËΩâ 
Ê¨äÂà©Èáë 0 0 100% ÂçÉÂÖÉ  
Á¢©Â£´Áîü 0 0 100%  
ÂçöÂ£´Áîü 0 0 100%  
ÂçöÂ£´ÂæåÁ†îÁ©∂Âì° 0 0 100%  
Âúã
ÂÖß 
ÂèÉËàáË®àÁï´‰∫∫Âäõ 
ÔºàÊú¨ÂúãÁ±çÔºâ 
Â∞à‰ªªÂä©ÁêÜ 0 0 100% 
‰∫∫Ê¨°
 
Âúã
Â§ñ Ë´ñÊñáËëó‰Ωú ÊúüÂàäË´ñÊñá 6 6 100% ÁØá 1. Chih-Yang Lin, Panyaporn 
Prangjarote, Li-Wei Kang,
Wei-Lun Huang, Tzung-Her 
Chen, ‚ÄôJoint Fingerprinting 
and Decryption with 
Noise-resistant for Vector 
Quantization Images,‚Äô 
Signal Processing, vol. 92, 
no. 9, pp. 2159-2171, 2012. 
 
2. Chih-Yang Lin, Li-Wei 
Kang, Jau-Hong Kao, 
Chun-Shien Lu, and Yi-Ta 
Wu, ‚ÄôInvariant Appearance 
Modeling for Non-Rigid 
Control (ISIC 2012), Yunlin, 
Taiwan, pp. 244-247, 2012.
 
2. Panyaporn Prangjarote, 
Chih-Yang Lin, Li-Wei Kang, 
Chia-Hung Yeh, ‚ÄôJoint 
Fingerprinting and 
Decryption for VQ Images 
through Bipartite 
Matching,‚Äô Proceedings of 
the Sixth International 
Conference on Genetic and 
Evolutionary Computing 
(ICGEC-2012), Kitakyushu, 
Japan, pp. 27-30, 2012. 
 
3. Chih-Yang Lin, Wei-Wen 
Chang, and Yi-Hui 
Chen, ‚Äô‚ÄôIntelligent 
Projector System based on 
Computer Vision,‚Äô‚Äô 
Proceedings of the Fifth 
International Conference on 
Genetic and Evolutionary 
Computing (ICGEC-2011), 
Kimen, Taiwan, pp. 176-179, 
2011. 
 
4. Chih-Yang Lin, Chao-Chin 
Chang, Yi-Hui Chen, and 
Panyaporn 
Prangjarote, ‚Äô‚ÄôMultimedia 
Privacy Protection System 
for Mobil Environments,‚Äô‚Äô 
Proceedings of The 7th 
International Conference on 
Intelligent Information 
Hiding and Multimedia Signal 
Processing (IIH-MSP 2011), 
Dalian, China, pp. 133-136, 
2011. 
Â∞àÊõ∏ 0 0 100% Á´†/Êú¨  
Áî≥Ë´ã‰∏≠‰ª∂Êï∏ 0 0 100%  Â∞àÂà© Â∑≤Áç≤Âæó‰ª∂Êï∏ 0 0 100% ‰ª∂  
‰ª∂Êï∏ 0 0 100% ‰ª∂  
ÊäÄË°ìÁßªËΩâ 
Ê¨äÂà©Èáë 0 0 100% ÂçÉÂÖÉ  
