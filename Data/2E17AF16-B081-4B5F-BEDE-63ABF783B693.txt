 I
ç›®ï¤¿ 
ç›®ï¤¿ .......................................................................  I 
ä¸­æ–‡æ‘˜è¦ ................................................................... II 
è‹±æ–‡æ‘˜è¦ .................................................................. III 
I. è¨ˆç•«ç·£ç”±åŠç›®çš„.......................................................... 1 
II. ç ”ç©¶æ–¹æ³•èˆ‡æ­¥é©Ÿ.......................................................... 1 
III. ç ”ç©¶æˆæœ............................................................... 13 
IV. ï¥«è€ƒæ–‡ç»............................................................... 21 
å¯ä¾›æ¨å»£ä¹‹ç ”ç™¼æˆæœè³‡ï¦¾è¡¨ ................................................... 25 
 
 III
è‹±æ–‡æ‘˜è¦ 
This project proposes a three-year research plan on â€œPersonalized Expressive TTS 
(Text-To-Speech) via Voice Conversionâ€. This research plan is composed of personalized term 
detection, speech synthesis of personalized speaking style and voice conversion. The aim of this 
research project is to provide speech synthesis technologies for userâ€™s communication in the 
services of digital care and educational training application. The proposed system can synthesize 
the voice for any target speaker or emotion via the voice conversion technologies, and offer the 
sound of a target person or emotion for the user. These technologies combine the dialogue system 
and provide the sound of children for the user to mold the environment of elderâ€™s â€œfamilyâ€ with 
sense of ownership for the elder care application. It can also provide the synthesized sound for 
home security or the children care application. The details are described as follows. 
In the first year, we focus on developing the approach to personalized frequent term 
extraction in the spontaneous speech. The word lattice combined with acoustic and language 
features are extracted to detect the personalized frequent terms. Pitch and duration model can be 
generated separately from the sentence, word and syllable levels via the hierarchical prosodic 
model. Linguistic features are adopted in the hierarchical prosodic model and personalized 
speaking style model. The prototype conversion system combined with the prosodic model and 
the spectral conversion technologies can be constructed.  
Prosody conversion is the main research issue of the second year. The â€œprosody hierarchy 
structureâ€ can re regarded as the basis of pitch prediction model, and apply â€œdynamic featuresâ€ to 
the unit of each hierarchical layer. We describe prosodic units as the supra-segmental units which 
occur in a hierarchy structure and reflect how brain processes speech; the latter preserve time 
correlation between adjacent units and result in more natural connection among each conjunction 
point. Applying this framework to HMM-based speech synthesis system, we can result a better, 
natural sounding speech. 
In the third year, we plan to develop a personalized expressive TTS via voice conversion. 
The conversion function for any target speaker or emotion via voice conversion is trained by a 
parallel speech corpus obtained from a source and a target speakers. Combined with the pervious 
personalized speaking style and prosodic model, the personalized expressive TTS system can be 
obtained. 
By the realization of this research project, an improvement in speech quality, including 
prosody and spectrum, of the personalized expressive speech synthesis will be obtained. The 
accomplishment of this project can be applied to other projects in the integrated project as the 
speech interface.  
 
 
Keywords: Speech Synthesis, Frequent Terms, Prosodic Model, Voice Conversion 
 
 
 2
éŸ»æ¨¡å‹(Prosodic Model)ï¼Œå…¶ä¸­åŒ…å«å€‹äººåŒ–éŸ³éŸ»ç‰¹å¾µï¥«ï¥©æ“·å–ã€éšå±¤å¼ï¤†æ³•çµæ§‹ï¼Œå…¶éšå±¤åŒ…
å«:ï¤†å­å±¤(Sentence Level)ã€è©å±¤(Word Level)ã€å–®å­—å±¤(Syllable Level)ï¼Œè—‰ç”±å„å±¤çš„ç´°åˆ†ï¼Œ
æ¡ç”¨éŸ³é«˜ç›®æ¨™æ¨¡å‹(Pitch Target Model)å°‡å€‹äººåŒ–éŸ³éŸ»çš„éŸ³é«˜ï¥«ï¥©åŠ ä»¥æ“·å–ï¼Œä¸¦ä¸”é€éå‰å€‹å·¥
ä½œé‡å°ï¤†æ³•æ‰€åšçš„åˆ†æçµæœï¼Œå»ºï§·æ–·ï¤†ä¼°æ¸¬(Break Prediction)çš„æ¨¡å‹ï¼Œæœ€çµ‚å®Œæ•´å»ºï§·å€‹äººåŒ–
éŸ³éŸ»ï¥«ï¥©æ¨¡å‹ï¼›3)ä»¥æ¬¡éŸ³ç¯€é »è­œçš„äºŒç¶­ç‰¹å¾µå€¼ (Two-Dimensional Eigen-Voice)ç‚ºåŸºæœ¬å–®å…ƒä¹‹
é »è­œè½‰æ›ï¼Œæœ‰é‘’æ–¼éå»å‚³çµ±ä»¥æ¡†æ¶(Frame)ç‚ºåŸºç¤é€²ï¨ˆä¹‹é »è­œè½‰æ›åœ¨æ™‚é–“è»¸ä¸Šæ¡†æ¶èˆ‡æ¡†æ¶ä¹‹
é–“çš„ï¥§ï¦šçºŒå•é¡Œï¼Œæœƒå½±éŸ¿æ‰€åˆæˆå‡ºï¤­çš„è²éŸ³å“è³ªï¼Œæ‰€ä»¥æœ¬ç ”ç©¶æå‡ºä¸€å€‹ä»¥æ¬¡éŸ³ç¯€(Subsyllable)
ç‚ºå–®å…ƒä¹‹ç‰¹å¾µå€¼è²éŸ³åšç‚ºåŸºæœ¬è½‰æ›å–®å…ƒå–ä»£éå»ä»¥æ¡†æ¶(frame-based)ç‚ºä¸»çš„è½‰æ›æ–¹å¼ï¼Œåœ¨æ™‚
é–“è»¸ä¸Šçš„ï¦šçºŒæ€§èƒ½ç¶­æŒï¼Œä»¥æ¬¡éŸ³ç¯€ç‚ºè½‰æ›å–®å…ƒ(Subsyllable-based)é€²ï¨ˆé »è­œè½‰æ›ï¼›4)æ•´åˆä¸Š
è¿°ä¸‰å€‹å·¥ä½œçš„æˆæœï¼Œçµåˆï¤†æ³•åˆ†æçš„çµæœå’Œèªï¦¾åº«ï¼Œä»¥åŠéšå±¤å¼çš„éŸ³éŸ»æ¨¡å‹å’Œæ¬¡éŸ³ç¯€é »è­œ
è½‰æ›æŠ€è¡“ï¼Œæ­é…ä»¥ç‰¹å¾µå€¼è²éŸ³ç‚ºåŸºæœ¬å–®å…ƒçš„é »è­œè½‰æ›æŠ€è¡“ï¼Œæœ€çµ‚å»ºæ§‹å‡ºé‡å°æ–‡å­—å…§å®¹åˆ†æ
å€‹äººèªè€…ç‰¹è‰²ï¼Œä¸¦å»ºï§·å¯ç”¢ç”Ÿå€‹äººåŒ–å«æƒ…ç·’è¡¨é”æ€§èªï§Šä¹‹èªéŸ³çš„ TTS ç³»çµ±(Expressive 
TTS)ã€‚æœ¬è¨ˆç•«å·²å®Œæˆä¸€å¥—èƒ½å°‡æ–‡å­—è½‰æ›ç‚ºå…·æœ‰å€‹äººåŒ–èªè€…ç‰¹è‰²å’Œè¡¨é”æ€§èªéŸ³åˆæˆç³»çµ±ï¼Œä»¥
ä¸‹è©³è¿°å„éšæ®µä¹‹ç ”ç©¶å·¥ä½œã€‚ 
ç¬¬ä¸€ï¦ï¨ä¸»è¦ç›®æ¨™åŒ…æ‹¬ï¼š1.)ï§ç”¨çµ±è¨ˆæ–¹æ³•è‡ªå‹•å¾è¨“ï¦–èªï¦¾ä¸­èƒå–å‡ºå€‹äººç¿’èªï¼Œä¸¦å°‡ä¹‹
åˆ†ï§ï¼›2.) è¢«æ­¸ï§ç‚ºè´…èªä¹‹å€‹äººç¿’èªï¼Œï§ç”¨è´…èªæ’å…¥æ¨¡å‹å°‡å®ƒæ”¾å…¥æ–‡ç« ä¸­æœ€åˆé©çš„ä½ç½®ï¼›3.) 
å°æ–¼éè´…èªï§çš„å€‹äººç¿’èªï¼Œè¨­è¨ˆä¸€åŒç¾©è©å°ç…§è¡¨ï¼›4.) è—‰ç”±åŒç¾©è©å–ä»£èˆ‡æ’å…¥è´…èªï¼Œå°‡åŸå§‹
è¼¸å…¥çš„æ–‡ç« ä¿®é£¾æˆå¸¶æœ‰ç›®æ¨™èªè€…ï¥¯è©±é¢¨æ ¼çš„æ–‡å­—å…§å®¹ã€‚ 
ç¬¬ä¸€éšæ®µ 
å»ºï§·å€‹äººåŒ–æ…£ç”¨èªï¤†ä¹‹ï¤†æ³•åˆ†æ(Syntactic Analysis)ç³»çµ± 
ç¬¬ä¸€éšæ®µå…ˆå»ºï§·å€‹äººåŒ–æ…£ç”¨èªï¤†ï¤†æ³•åˆ†æ(Syntactic Analysis)ç³»çµ±ï¼Œï§ç”¨æ‰€æ”¶é›†ä¹‹æ—¥å¸¸
å°è©±èªï¦¾ã€è©åº«åŠ ä»¥æ•´åˆåˆ†æï¼Œå…ˆå¾æ–‡å­—ä¸Šæ‰¾åˆ°æ¯å€‹äººï¥¯è©±çš„ç¿’æ…£æ–¹å¼ï¼Œä¸¦ä¸”åŠ ä»¥åˆ†æå¾Œ
å»ºï§·ç‰¹å®šèªè€…çš„å€‹äººåŒ–ï¤†æ³•æ¨¡å‹ï¼Œè€Œç¬¬äºŒéšæ®µä¸»è¦åœ¨æ–¼å»ºï§·éšå±¤å¼çš„éŸ³éŸ»æ¨¡å‹æ¶æ§‹ï¼Œè—‰ç”±
éšå±¤å¼çš„æ¶æ§‹ï¼Œå°‡èªï¦¾å¾ï¤†ç¾¤åˆ°è©çµ„é€²ï¨ˆåˆ†å±¤åˆ†ï§ï¼Œä¸¦ä¸”åœ¨å„éšå±¤æ“·å–åˆé©çš„éŸ³é«˜ç›®æ¨™æ¨¡
å‹(Pitch Target Model)ï¼Œå°‡å„éšå±¤çš„éŸ³éŸ»ç‰¹è‰²å’Œè®ŠåŒ–åŠ ä»¥è¨˜ï¤¿ï¼Œæœ€å¾Œå°‡å€‹äººåŒ–çš„ï¤†æ³•æ¨¡å‹å’Œ
éŸ³éŸ»æ¨¡å‹åŠ ä»¥æ•´åˆï¼Œä¸¦ä¸”åŠ ä¸Šé »è­œè½‰æ›(Spectral Conversion)æ–¹å¼ï¼Œå»ºï§·ä¸€å€‹å…·æœ‰å€‹äººåŒ–ç‰¹
è‰²èªè€…è½‰æ›çš„é››å‹ç³»çµ±ã€‚å…¶è½‰æ›çš„ï§Šç¨‹ç¤ºæ„åœ–å¦‚åœ–ä¸€ï¼š 
 
Prosodic Analysis 
Syntactic Analysis
Prosodic 
Modeling
Spontaneous Speech 
Corpus Development
Personalized 
Speaking Style 
Modeling from 
Text Prosodic 
Conversion
Spectral 
Conversion
Converted 
Speech
Spontaneous  
corpus
Text and Speech 
Data Input
Text  Data
Speech  Data
 
åœ–ä¸€ã€å€‹äººåŒ–ç‰¹è‰²èªè€…è½‰æ›ç³»çµ±ï§Šç¨‹åœ– 
 
 4
 
åœ–ä¸‰ã€æ‰¾å°‹æ…£ç”¨èªæ¼”ç®—æ³• 
 
æœ¬ç ”ç©¶æƒ³è¦èƒå–å‡ºï¤­çš„ Idiolect æœ‰ï¥¸ç¨®ï§å‹ï¼Œä¸€å€‹æ˜¯ N-gram idiolectï¼Œé€™ç¨® Idiolect æ˜¯
adjacent çš„ï¼Œå¦ä¸€å€‹æ˜¯ phrasal templateï¼Œè¡¨ç¤º not adjacent çš„ã€‚æœ¬ç ”ç©¶ï§ç”¨ Xtract é€™å€‹æ¼”ç®—
æ³•ï¤­åš Idiolect extractionã€‚Xtract æ˜¯ç¬¬ä¸€å€‹æå‡ºï¤­å¯ä»¥èƒå–å‡º adjacent å’Œ not adjacent 
collocation çš„æ–¹æ³•ã€‚å®ƒä¸»è¦çš„æ¦‚ï¦£æ˜¯å…ˆè¨ˆç®— strength å’Œ spreadï¼ŒæŠŠ two word collocation å…ˆæ‰¾
å‡ºï¤­ï¼Œç¬¬äºŒæ­¥é©Ÿæ˜¯ï§ç”¨ä¹‹å‰æ‰€è¨ˆç®—çš„ two word collocation å†æ‰¾æœ‰æ²’æœ‰ï¥¸å€‹å­—ä»¥ä¸Šçš„
collocationã€‚æœ€å¾Œï§ç”¨ parsing information æŠŠéŒ¯çš„ collocation éï¦„æ‰ã€‚Xtract çš„æ¼”ç®—æ³•çš„ç¬¬ä¸€
æ­¥é©Ÿå…ˆæŠŠä»¥ headword ç‚ºä¸­å¿ƒ window size 10 çš„ co-occurred word ï¨¦æŠ“å‡ºï¤­ï¼Œç„¶å¾Œé‡å°æ¯å€‹ 
Co-word èˆ‡ Headword pair ï¨¦ç”¨é€™ç¨®è¡¨ç¤ºæ³•è¨˜ï¤¿èµ·ï¤­ï¼ŒF æ˜¯å‡ºç¾çš„é »ï¥¡ è€Œ d æ˜¯ Co-word èˆ‡ 
headword çš„è·ï§ª(å–®ä½æ˜¯ word)ï¼Œæ‰€æœ‰çš„è¨˜ï¤¿å¯ä»¥çµ„æˆä¸€å¼µè¡¨æ ¼ã€‚å¯ä»¥ç®—å‡ºæ¯å€‹ word pair
çš„ Strengthï¼Œæ¯ä¸€å€‹ headword æœƒæŠ“å‡ºå¾ˆå¤š co-wordï¼ŒStrength ç›®çš„æ˜¯æŠŠé »ï¥¡ï¥§å¤ é«˜çš„ co-word 
ï¦„æ‰ï¼ŒFi æ˜¯ co-word èˆ‡ headword å…±åŒå‡ºç¾çš„æ¬¡ï¥©ï¼ŒF-head å‰‡æ˜¯æ‰€æœ‰èˆ‡ headword å…±åŒå‡ºç¾
çš„ co-word çš„æ¬¡ï¥©çš„å¹³å‡ï¼ŒSigma-head æ˜¯ co-word æ¬¡ï¥©çš„æ¨™æº–å·®ï¼Œé€™ï§¨è¦è¨­å€‹é–€æª»å€¼ T0ï¼Œï¼Œ
å®ƒè¦æ±‚ co-word çš„é »ï¥¡è¦é«˜æ–¼å¹³å‡çš„ T0 å€‹æ¨™æº–å·®ï¼ŒStrength æ˜¯ç‚ºï¦ºç¢ºå®šï¥¸å€‹ word æ˜¯ï¥§æ˜¯
æœ‰è¶³å¤ çš„ recurrent ç‰¹æ€§ã€‚å¤–è¦è¨ˆç®—çš„æ˜¯ Spread Measureï¼Œå› ç‚ºåŒæ¨£çš„ Co-word å¯èƒ½æœƒå‡ºç¾
åœ¨ windows çš„ä»»ä¸€å€‹ä½ç½®ï¼Œå¦‚æœå®ƒæ¯å€‹ä½ç½®å‡ºç¾æ¬¡ï¥©ï¨¦å·®ï¥§å¤šï¼Œè¡¨ç¤ºå®ƒèˆ‡ headword ä¹‹é–“
æ²’æœ‰ç‰¹å®šçš„ä½ç½®é—œç³»ï¼Œæ‰€ä»¥æˆ‘å€‘ç”¨ï§ä¼¼è¨ˆç®—è®Šï¥¢ï¥©çš„æ–¹å¼ï¼Œï¤­åˆ¤æ–·åˆ†æ•£ç¨‹ï¨ï¼Œå¦‚æœ co-word
åœ¨æŸå€‹ä½ç½å‡ºç¾é »ï¥¡éå¸¸é«˜ï¼Œè€Œåœ¨å…¶å®ƒä½ç½®ï¥§æ˜¯æ¬¡ï¥©ï¦²å°±æ˜¯éå¸¸å°ï¼Œå‰‡ spread æœƒè¼ƒé«˜ï¼Œè¡¨
ç¤ºå®ƒæœƒå‡ºç¾åœ¨ headword æ—çš„ç‰¹å®šä½ç½ï¼Œé–€æª»å€¼ T1 å°±ç”¨ï¤­åˆ¤æ–· Spread çš„é«˜èˆ‡ä½ï¼ŒSpread
æ˜¯ç‚ºï¦ºç¢ºå®šï¥¸å€‹ word é–“æœ‰è¶³å¤ çš„ Rigid combination ç‰¹æ€§ã€‚ 
Strength Measure å®šç¾©å¦‚ä¸‹: 
( , ) i headhead co i
head
f fStrength W W Ïƒâˆ’
âˆ’=        (1) 
where 
1
1 m
head i
i
f f
m =
= â‹…âˆ‘  and 2 2
1
( )
m
head i head
i
f f mÏƒ
=
â› â= âˆ’âœ âŸâ â âˆ‘  
 6
å°±æ˜¯å€‹äººç¿’èªï¼Œä¹Ÿæœ‰å€™é¸ç­”æ¡ˆï¼Œå’Œå…¶å®ƒçš„ term, ï¨ˆæ˜¯é‡å°æ¯å€‹ term çš„ä¸Šä¸‹æ–‡, ä¸­é–“æ˜¯è¨ˆç®—
Term çš„ Frequencyã€‚ 
 
 
åœ–å››ã€LSA ç¤ºæ„åœ– 
 
å†ï¤­æ˜¯è´…èªçš„è™•ï§¤ï¼Œæƒ³è¦æŠŠè´…èªæ”¾åˆ°ï¤†å­ä¸­é–“ï¼Œå¤§è‡´çš„æƒ³æ³•å°±æ˜¯å…ˆè§€æŸ¥é€™å€‹è´…èªå‡ºç¾
çš„åœ°æ–¹ï¼Œä¸¦è¨˜ï¤¿èµ·ï¤­ä¼´éš¨å®ƒå‡ºç¾çš„å„ç¨®æœ‰ç”¨çš„ featureï¼Œè¨“ï¦–å‡ºçµ±è¨ˆæ¨¡å‹ï¼Œä¹‹å¾Œè¼¸å…¥ä¸€æ®µæ–‡
ç« å°±å¯ä»¥ï§ç”¨çµ±è¨ˆæ¨¡å‹æ‰¾åˆ°æœ€å¯èƒ½çš„åœ°æ–¹ï¼ŒæŠŠè´…èªæ”¾é€²å»ï¼Œæ‰€ä»¥æœ‰å¾ˆå¤šæ–¹æ³•ï¨¦å¯ä»¥ç”¨ï¤­æ”¾
è´…èªï¼Œä½†æ˜¯æƒ³è¦å¾—åˆ°é€™äº›è´…èªçš„æ¨¡å‹ï¼Œå¾—å…ˆæƒ³å‡ºå¥½çš„ feature template æˆ–æ˜¯ decisionï¼Œç„¶è€Œå¥½
çš„ feature template æ˜¯å¾ˆé›£è¨­è¨ˆå‡ºï¤­çš„ï¼Œæ‰€ä»¥æœ¬ç ”ç©¶ä¸­æå‡ºä¸€å€‹ Idiolect Insertion Algorithm
å°‡è´…èªæ”¾é€²ï¤†å­ã€‚ 
æ¼”ç®—æ³•çš„æ­¥é©Ÿå¦‚ä¸‹ï¼š 
ç¬¬ä¸€æ­¥é©Ÿï¼Œï§ç”¨ Mining Association Rule çš„æ–¹å¼ï¼Œè‡ªå‹•æ‰¾åˆ°åœ¨ window size å…§ï¼ˆæˆ‘å€‘æ˜¯è¨‚ï§‘
ï¤†è©±ç‚ºä¸€å€‹ window çš„å¤§å°ï¼‰èƒ½ä½¿è´…èªç™¼ç”Ÿçš„äº‹ä»¶ã€‚ 
ç¬¬äºŒæ­¥é©Ÿï¼Œå‡å¦‚åœ¨ window å…§ç™¼ç¾æœ‰ Idiolect æ‡‰è©²ç™¼ç”Ÿï¼Œå°±å¾ window å…§èˆ‡ example sentences
å…§ï¼Œæ‰¾åˆ°ç›¸ï¨æœ€å¤§çš„ï¥¸å€‹ï¤†å­ï¼›å¦‚æœé€™å€‹ Idiolect æ˜¯åœ¨ï¥¸ï¤†ä¸­é–“çš„ï§å‹ï¼Œå°±æ¯”è¼ƒ sentence pair
çš„ç›¸ä¼¼ï¨ã€‚ 
ç¬¬ä¸‰æ­¥é©Ÿï¼Œå°æ–¼ï¤†ä¸­çš„ Idiolectï¼Œæ‰¾åˆ° example sentence èˆ‡ target sentence å‰å¾Œ syntactic 
structure ä¸€æ¨£çš„åœ°æ–¹ï¼ŒæŠŠ Idiolect æ”¾å…¥ target  sentence ä¸­ï¼›ï¥´æ˜¯ï¥¸ï¤†é–“çš„ Idiolect ï¼Œå°±ç›´
æ¥æ”¾åœ¨å®ƒå€‘ä¸­é–“ã€‚ 
ä»¥ä¸‹ç‚ºæ•´å€‹æ’å…¥æ¼”ç®—æ³•çš„ç¤ºæ„åœ–ï¼Œå‡è¨­æ–‡ç« æœ‰ 20 ï¤†ï¼Œæˆ‘å€‘å°±ä»¥ 6 ï¤†ç‚º windowï¼Œçœ‹çœ‹
window å…§æœ‰æ²’æœ‰è®“ Idiolect ç™¼ç”Ÿçš„ eventã€‚å‡è¨­åœ¨ç¬¬äºŒå€‹ Region æœ‰æŸ event ä½¿ Idiolect ç™¼ç”Ÿï¼Œ
å°±è®“ Region 2 ï§¨é¢æ¯ä¸€ï¤†ï¨¦èˆ‡ Example sentence æ¯”è¼ƒç›¸ä¼¼ï¨ã€‚ 
æœ€å¾Œç›¸ä¼¼ï¨æœ€å¤§çš„ example sentence 20 èˆ‡ sentence 5 ä¹‹é–“ï¼Œæ‰¾åˆ° syntactic structure ä¸€æ¨£
çš„åœ°æ–¹ï¼ŒæŠŠ Idiolect æ”¾é€²å»ï¼Œå°±å®Œæˆï¦ºé€™å€‹ region çš„ Insertionã€‚ 
 
 8
åŒçš„ï§å‹ï¼Œç”±æ–¼ï¥§åŒçš„èªç¯‡æ‰€è¦è¡¨é”çš„æ„æ€ï¥§åŒï¼Œåœ¨éŸ³éŸ»ä¸Šçš„è®ŠåŒ–ä¹Ÿæœƒæœ‰æ‰€ï¥§åŒï¼Œå› æ­¤ï¼Œ
åœ¨ï¤†å­å±¤ä¸­ï¼Œæ¯å€‹ï¤†å­ä¹Ÿæœ‰æ ¹æ“šï¥§åŒçš„ï¤†å‹è€Œæœ‰éŸ³é«˜ä¸Šçš„è®ŠåŒ–ï¼Œï¥«è€ƒè¡¨ 12-1 ï¤†å­å°‡å¯ä»¥åˆ†
ï§ç‚ºè‚¯å®šï¤†ã€ç–‘å•ï¤†ã€å‘½ï¦¨ï¤†ã€ã€ç­‰ï¼Œï¦µå¦‚:ç–‘å•ï¤†æœƒåœ¨å°¾éŸ³æœ‰å› é«˜ä¸Šæšçš„è®ŠåŒ–ã€åˆæˆ–è€…è‚¯
å®šï¤†å…·æœ‰éš¨æ­£å¸¸ï¥¯è©±å‘¼å¸æ–¹å¼éŸ³é«˜æ¼¸ä½çš„èµ°å‹¢ï¼Œï¥§åŒçš„ï¤†å‹åœ¨éŸ³éŸ»ä¸Šä¹Ÿæœƒæœ‰ç›¸ç•¶ç¨‹ï¨çš„ï¥§
åŒï¼Œè—‰ç”±ç´°éƒ¨çš„åŠƒåˆ†ï¼Œå°‡å¯ä»¥æ¯”èªç¯‡å±¤ï¤è©³ç›¡çš„æè¿°ï¤ç´°éƒ¨çš„éŸ³é«˜è®ŠåŒ–ï¼Œå¾€ä¸‹ç´°åˆ†çš„è©å±¤ã€
å­—å±¤ï¼Œå°‡ï¤ç´°è†©çš„å»æç¹ªå–®å­—åˆ°å–®éŸ³çš„éŸ³é«˜è®ŠåŒ–èµ°å‹¢ï¼Œè—‰ç”±é€™æ¨£çš„åˆ†å±¤ï§ç”¨éŸ³é«˜ç›®æ¨™æ¨¡å‹
å»æè¿°å¾Œï¼Œä¸¦ä¸”é€éå‰å€‹å·¥ä½œé‡å°ï¤†æ³•æ‰€åšçš„åˆ†æçµæœï¼Œå»ºï§·æ–·ï¤†ä¼°æ¸¬(Break Prediction)
çš„æ¨¡å‹ï¼Œæœ€çµ‚å®Œæ•´å»ºï§·å€‹äººåŒ–éŸ³éŸ»ï¥«ï¥©æ¨¡å‹ã€‚ 
 
åœ–ï§‘ã€éšå±¤å¼éŸ³éŸ»æ¨¡å‹(Hierarchical Prosodic Model)æ¶æ§‹åœ– 
  
ç¬¬äºŒï¦å·¥ä½œä¸»è¦æå‡ºï¦ºçµåˆéŸ»ï§˜éšå±¤åŠå‹•æ…‹ï¥«ï¥©çš„éŸ³é«˜é æ¸¬æ¨¡å‹ï¼Œä»¥â€œéšå±¤å¼éŸ³éŸ»ï¼ˆéŸ³
é«˜çš„éŸ»ï§˜å–®å…ƒï¼‰æ¶æ§‹ï¼‚ä½œç‚ºéŸ³é«˜é æ¸¬çš„åŸºç¤ï¼Œå„å±¤çš„éŸ³éŸ»å–®å…ƒè€ƒæ…®â€œå‹•æ…‹ï¥«ï¥©ç‰¹æ€§ï¼‚ï¼›å‰
è€…å¸Œæœ›æ”¹å–„å‚³çµ±éŸ³éŸ»æ¨¡å‹ä»¥å°å–®å…ƒåˆæˆçš„ï¥§è¶³ï¼Œå¾Œè€…ä»¥å‹•æ…‹ï¥«ï¥©ç”Ÿæˆæ¼”ç®—æ³•ï¼Œä¿ï§æ™‚é–“ä¸Š
çš„é—œï¦—æ€§ï¼Œä½¿å–®å…ƒå’Œå–®å…ƒä¹‹é–“çš„ï¦šæ¥ï¤åŠ è‡ªç„¶ï¼Œï¤ç¬¦åˆäººï§è¬›è©±æ™‚çš„ï¤†èª¿èµ°å‹¢è®ŠåŒ–ï¼Œè—‰ä»¥
æ”¹å–„åŸºæ–¼éš±è—å¼é¦¬å¯å¤«æ¨¡å‹ä¹‹åˆæˆèªéŸ³çš„è‡ªç„¶ï¨ã€‚å¦å¤–ï¼Œåœ¨èªéŸ³è¨Šè™Ÿçš„åˆ†æä¸Šï¼Œå³å°‡è¨Šè™Ÿ
ï¥«ï¥©åŒ–ä¸¦å–å¾—é »è­œä»¥åŠéŸ³é«˜ï¥«ï¥©ï¼Œä½¿ç”¨çš„æ˜¯ STRAIGHT åˆ†æåˆæˆæ¼”ç®—æ³•ï¼Œå¯ä»¥å¾—åˆ°ï¨ç¢ºçš„
åŸºé »ï¥«ï¥©ä»¥åŠé »è­œï¥«ï¥©ã€‚ç¬¬ä¸€éšæ®µå…ˆå»ºï§·å€‹äººåŒ–æ…£ç”¨èªï¤†ï¤†æ³•åˆ†æ(Syntactic Analysis)ç³»
çµ±ï¼Œï§ç”¨æ‰€æ”¶é›†ä¹‹æ—¥å¸¸å°è©±èªï¦¾ã€è©åº«åŠ ä»¥æ•´åˆåˆ†æï¼Œå…ˆå¾æ–‡å­—ä¸Šæ‰¾åˆ°æ¯å€‹äººï¥¯è©±çš„ç¿’æ…£
æ–¹å¼ï¼Œä¸¦ä¸”åŠ ä»¥åˆ†æå¾Œå»ºï§·ç‰¹å®šèªè€…çš„å€‹äººåŒ–ï¤†æ³•æ¨¡å‹ï¼Œè€Œç¬¬äºŒéšæ®µä¸»è¦åœ¨æ–¼å»ºï§·éšå±¤å¼
çš„éŸ³éŸ»æ¨¡å‹æ¶æ§‹ï¼Œè—‰ç”±éšå±¤å¼çš„æ¶æ§‹ï¼Œå°‡èªï¦¾å¾ï¤†ç¾¤åˆ°è©çµ„é€²ï¨ˆåˆ†å±¤åˆ†ï§ï¼Œä¸¦ä¸”åœ¨å„éšå±¤
æ“·å–åˆé©çš„éŸ³é«˜ç›®æ¨™æ¨¡å‹(Pitch Target Model)ï¼Œå°‡å„éšå±¤çš„éŸ³éŸ»ç‰¹è‰²å’Œè®ŠåŒ–åŠ ä»¥è¨˜ï¤¿ï¼Œæœ€å¾Œ
å°‡å€‹äººåŒ–çš„ï¤†æ³•æ¨¡å‹å’ŒéŸ³éŸ»æ¨¡å‹åŠ ä»¥æ•´åˆï¼Œä¸¦ä¸”åŠ ä¸Šé »è­œè½‰æ›(Spectral Conversion)æ–¹å¼ï¼Œ
å»ºï§·ä¸€å€‹å…·æœ‰å€‹äººåŒ–ç‰¹è‰²èªè€…è½‰æ›çš„é››å‹ç³»çµ±ã€‚å…¶è¨“ï¦–çš„ï§Šç¨‹å¦‚ä¸‹åœ–ä¸ƒï¼š 
 
 10
ä½¿ç”¨è€…è¼¸å…¥æ–‡å­—å¾Œï¼Œç¶“éå‰ç«¯çš„æ–‡å­—åˆ†æå¾Œï¼Œå¾—åˆ°èªè¨€å­¸ä¸Šçš„è³‡è¨Šä¸¦é€²ï¨ˆéŸ»ï§˜åœé “çš„
é æ¸¬ï¼Œå¾—åˆ°æ–‡å­—æ‰€å°æ‡‰çš„éŸ»ï§˜éšå±¤æ¶æ§‹ï¼Œä¸¦ç”¢ç”Ÿ(1)æ–‡å­—æ¨™è¨˜æª”ä»¥åŠ(2)å„å±¤çš„å‰å¾Œæ–‡ç›¸é—œè³‡
è¨Šï¼Œä»¥(1)æ–‡å­—æ¨™è¨˜æª”é€²ï¨ˆè²å­¸åŠéŸ³é•·ï¥«ï¥©çš„é æ¸¬ï¼Œä¸¦ä»¥(2)å„å±¤çš„å‰å¾Œæ–‡ç›¸é—œè³‡è¨Šä½œç‚ºå„å±¤
éŸ³éŸ»ï¥«ï¥©çš„é æ¸¬ï¼Œä¸¦çµåˆå„å±¤éŸ³éŸ»é æ¸¬çš„çµæœç”¢ç”Ÿå°æ‡‰çš„éŸ³é«˜è»Œè·¡ã€‚å°‡å¾—åˆ°çš„è²å­¸ï¥«ï¥©åŠ
éŸ³é«˜è»Œè·¡ï¼Œæ ¹æ“šå°æ‡‰çš„éŸ³é•·ï¼Œå¾—åˆ°å„å€‹éŸ³æ¡†çš„é »è­œåŠéŸ³é«˜å€¼ï¼Œæœ€å¾Œç¶“é MLSA filterï¼ˆMel-Log 
Spectrum Approximation filterï¼‰å¾—åˆ°åˆæˆçš„èªéŸ³è¼¸å‡ºã€‚æœ¬ï¦ï¨ç ”ç©¶ç›®æ¨™åœ¨æ–¼æå‡ºæ‡‰ç”¨éŸ»ï§˜éš
å±¤åŠå‹•æ…‹ï¥«ï¥©ä¹‹éŸ³é«˜é æ¸¬æ¨¡å‹æ–¹æ³•ï¼Œæ‡‰ç”¨åŸºæ–¼éš±è—å¼é¦¬å¯å¤«æ¨¡å‹ä¹‹ä¸­æ–‡èªéŸ³åˆæˆå™¨æ•ˆèƒ½çš„
æ”¹é€²ã€‚åŸºæ–¼ HMM çš„èªéŸ³åˆæˆå™¨ä¸»è¦æ ¸å¿ƒæŠ€è¡“åŒ…æ‹¬ï¼š(1)åŸºæ–¼æ¢…çˆ¾å€’é »è­œçš„è²éŸ³åˆæˆæŠ€è¡“
ï¼ˆvocoding techniqueï¼‰ï¼ŒåŒ…å«ï¦ºæ¢…çˆ¾å€’é »è­œä¿‚ï¥©çš„åˆ†æï¼Œä»¥åŠä½¿ç”¨ MLSA filter ç›´æ¥å°‡æ¢…çˆ¾
å€’é »è­œä¿‚ï¥©åˆæˆå›èªéŸ³è¨Šè™Ÿã€‚(2)å¾ HMM æ¨¡å‹ç”¢ç”ŸèªéŸ³ï¥«ï¥©æ™‚ï¼Œä½¿ç”¨è€ƒæ…®ï¥«ï¥©å‹•æ…‹ç‰¹æ€§çš„
ï¥«ï¥©ç”Ÿæˆæ¼”ç®—æ³•ã€‚(3)åŸºæ–¼ Multi-Space probability Distribution HMMï¼ˆMSD-HMMï¼‰ï¼Œè€ƒæ…®åŸº
é »åƒ…åœ¨æ¿éŸ³æ®µæœ‰å€¼ï¼Œè€Œåœ¨æ¸…éŸ³æ®µæ²’æœ‰å®šç¾©é€™æ¨£çš„ç‰¹æ€§ï¼Œä½¿ï¥«ï¥©çš„ç¶­ï¨åœ¨æ¿éŸ³æ®µç‚ºä¸€ï¼Œæ¸…éŸ³
æ®µç‚ºï¦²ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œä»¥â€œSegmental Tonal Phone Modelï¼‚ä½œç‚ºè¨­è¨ˆåŠå®šç¾© HMM éŸ³ç´ æ¨¡å‹çš„
æ–¹å¼ï¼Œæ­¤ç¨®æ¨¡å‹å®šç¾©çš„å¥½è™•æ˜¯èƒ½ä»¥æœ€å°‘çš„æ¨¡å‹ï¥©ï¼Œå°ä¸­æ–‡äº”è²è®ŠåŒ–ä½œè©³ç›¡çš„æè¿°ã€‚ç‚ºï¦ºé”
åˆ°æœ€ä½³çš„ï§ºæ…‹åˆä½µåˆ†ï¦ çµæœï¼Œæˆ‘å€‘å°éŸ³ç´ æ¨¡å‹ï¼Œè€ƒæ…®ï¦ºéŸ³ç´ ç›¸é—œã€å­—ç›¸é—œã€éŸ³éŸ»è©ç›¸é—œã€
éŸ³éŸ»è©çµ„ç›¸é—œèˆ‡ï¤†ç›¸é—œç­‰äº”å¤§ï§å•é¡Œï¼Œæ¥è‘—ç¶“ç”± STRAIGHT æ–¹æ³•é€²ï¨ˆï¥«ï¥©æ“·å–ï¼ŒèªéŸ³è¨Šè™Ÿ
é¦–å…ˆç¶“éå›ºå®šéŸ³æ¡†å¤§å°çš„åŸºé »åˆ†æï¼Œå–å¾—åŸºé »ï¥©å€¼ï¼Œåœ¨æå–é »è­œï¥«ï¥©æ™‚ï¼Œæ ¹æ“šå„éŸ³æ¡†å–å¾—
çš„åŸºé »ï¥©å€¼è¨ˆç®—åŸºå‘¨é€±æœŸï¼Œåšå¿«é€Ÿå‚…ï§æ¥­è½‰æ›ï¼ˆFast Fourier Transform, FFTï¼‰åŠ æ¡†é•·ï¨çš„é©
æ‡‰æ€§æ ¡æ­£ï¼Œè—‰ä»¥å»é™¤å› ç‚ºéé€±æœŸæ€§éœ‡ç›ªå°é »è­œåœ¨æ™‚é »ä¸Šç”¢ç”Ÿçš„å½±éŸ¿ï¼Œä½¿åˆ†æå¾—åˆ°ï¤æ˜ç¢ºã€
å¹³ï¤„çš„é »è­œï¥«ï¥©ï¼ˆSmoothed spectrumï¼‰ã€‚ 
æˆ‘å€‘ä½¿ç”¨ï¥¸å±¤ï¼ˆ2-tierï¼‰çš„åˆ†ï§å›æ­¸æ¨¹æ¨¡å‹ï¼Œå°‡å‰ä¸€å±¤çš„é æ¸¬çµæœï¼Œä½œç‚ºä¸‹ä¸€å±¤çš„é æ¸¬
åŸºç¤ï¼ŒéŸ»ï§˜çµæ§‹é æ¸¬æ¨¡å‹æ˜¯ä½¿ç”¨ï¥¸å±¤çš„ S-CART æ¶æ§‹ï¼Œä»¥ S-CART ä½œç‚º PWã€PP çš„é æ¸¬æ¨¡
å‹ï¼Œç‰¹æ€§å¦‚ä¸‹ï¼š(1)ä»¥èªè¨€å­¸ä¸Šçš„è³‡è¨Šä½œç‚ºæ‰€æœ‰è³‡ï¦¾é»çš„ç‰¹å¾µå‘ï¥¾ã€‚(2) supervised learningï¼š
è³‡ï¦¾é»æœ‰å…¶æ‰€å±¬ï§åˆ¥ï¼Œåœ¨æ­¤é æ¸¬æ¨¡å‹ä¸­åƒ…æœ‰ï¥¸ï§ï¼Œé‚Šç•Œæˆ–éé‚Šç•Œã€‚(3)åœ¨åˆ†ï¦ æ™‚çš„å•é¡Œé›†æ˜¯
å±¬å°æ¯ä¸€ç¶­ï¨åˆ†åˆ¥æ‰¾å‡ºæ‰€æœ‰å¯èƒ½çš„ç‰¹å¾µå€¼ï¼Œä»¥ç‰¹å¾µè³ªç•¶ä½œå•é¡Œï¼Œæ‰€æœ‰çš„å•é¡Œå½¢æˆé›†åˆï¼Œæ­¤
é›†åˆç”¢ç”Ÿæ‰€æœ‰å¯èƒ½çš„å­é›†åˆï¼Œæ¯å€‹å­é›†åˆå°±æ˜¯ä¸€å€‹å•é¡Œé›†ï¼Œæœ€å¾Œå°æ‰€æœ‰å•é¡Œé›†è©¦åšä¸€æ¬¡åˆ†
ï¦ ï¼Œä¾æ“šåˆ†ï¦ æ¨™æº–æ‰¾å‡ºå¯é”åˆ°æœ€ä½³åˆ†ï¦ çµæœçš„å•é¡Œé›†ç•¶ä½œæ­¤ç¶­ï¨ä¸Šæ‰€ä½¿ç”¨çš„åˆ†ï¦ å•é¡Œé›†ã€‚ 
(4)åˆ†ï¦ æ¨™æº–ï¼šä½¿åˆ†ï¦ å¾Œç¯€é»çš„ç´”ï¨ï¼ˆpurityï¼‰æœ€å¤§ã€ç†µå€¼ï¼ˆentropyï¼‰æœ€å°ï¼Œå³æ¯å€‹ç¯€é»ä¸­è³‡
ï¦¾é»æ‰€å±¬çš„ï§åˆ¥å‡ç›¸åŒï¼Œå³æœ€å¤§åŒ–ç†µå€¼ï¨‰ä½çš„æ¯”ï¥¡ï¼ˆReduced Entropy Ratio, RERï¼‰ã€‚ 
RER ç®—æ³•å¦‚ä¸‹ï¼š 
 ( )parent i i
i
parent
Entropy Entropy
RER =
Entropy
Ï‰ Ã—âˆ‘â€ Â           (6) 
Entropy çš„ç®—æ³•å¦‚ä¸‹ï¼š 
 
1
= - log( )
iN
j
i j jp pEntropy
=
Ã—âˆ‘       (7) 
å…¶ä¸­ = ii N NÏ‰  ï¼Œ N è¡¨ç¤ºçˆ¶ç¯€é»çš„è³‡ï¦¾ï¥©ç›®ï¼Œ iN è¡¨ç¤ºç¬¬ i å€‹å­ç¯€é»çš„è³‡ï¦¾ï¥©ç›®ï¼Œ jP ä»£è¡¨
ç¬¬ i å€‹å­ç¯€é»ä¸­çš„ç¬¬ j ç­†è³‡ï¦¾å‡ºç¾çš„æ©Ÿï¥¡ã€‚ç•¶ RER å€¼è¶Šå¤§æ™‚ï¼Œä»£è¡¨åˆ†ï¦ ä¹‹å¾Œå­ç¯€é»çš„ç´”ï¨
è¶Šé«˜ï¼Œæ‰€ä»¥æ¯æ¬¡åˆ†ï¦ ä»¥â€œä½¿å¾— RER å€¼æœ€å¤§çš„å•é¡Œé›†ï¼‚é€²ï¨ˆåˆ†ï¦ ï¼ŒæŒçºŒåˆ†ï¦ ç›´åˆ°é”åˆ°çµ‚æ­¢æ¢
ä»¶ç‚ºæ­¢ï¼ˆstopping criterionï¼‰ã€‚(5)çµ‚æ­¢æ¢ä»¶ï¼šç•¶è¦å˜—è©¦åˆ†ï¦ çš„ç¯€é»åªå‰©ä¸‹å”¯ä¸€æˆ–å°‘æ–¼ä¸€å®šï¥©
ï¥¾çš„è³‡ï¦¾é»æ™‚ï¼Œæˆ–æ˜¯ç•¶ RER å°æ–¼é–€æª»å€¼ï¼ˆthresholdï¼‰æ™‚ï¼Œé”åˆ°çµ‚æ­¢æ¢ä»¶ï¼Œæ­¤ä¸€ç¯€é»ï¥§å†é€²
ï¨ˆåˆ†ï¦ ï¼Œä¸¦ç¨±æ­¤ç¯€é»ç‚ºï¥®ç¯€é»ï¼ˆleaf nodeï¼‰ã€‚(6)æ¯ä¸€ï¥®ç¯€é»ä»£è¡¨æ±ºç­–çš„ï§åˆ¥ï¼Œå³æ±ºå®šè³‡ï¦¾
æ˜¯å¦ç‚ºé‚Šç•Œæ‰€åœ¨ã€‚ 
 12
Q çš„è§€å¯Ÿæ©Ÿï¥¡åŠäº‹å‰æ©Ÿï¥¡åœ¨åˆæˆä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦å°‡ Fn è½‰æ›å›å°æ‡‰çš„ Legendre Coefficients ï¼Œ
æ‰èƒ½é€²ä¸€æ­¥ç®—å›éŸ³é«˜è»Œè·¡ã€‚å¾—åˆ° 1 2{ , , , }N=F F F F" ï¼Œä¸¦ç”¢ç”Ÿæ•´å€‹ï¤†å­çš„æ¨¡å‹ 1 2{ , , , }NÎ» Î» Î»= "Î› ï¼Œ
æ¨¡å‹çš„å¹³å‡å€¼å‘ï¥¾ 1 2[ , , , ]Nâ€² â€² â€² â€²= "Î¼ Î¼ Î¼Î¼Î› ï¼Œå…±è®Šï¥¢ï¥©çŸ©é™£ 1 2[ , , , ]Ndiag= U U U"UÎ› ï¼Œæˆ‘å€‘å¸Œæœ›æ‰¾å‡ºä½¿
å¾—å·²çŸ¥æ¨¡å‹Î›ç”¢ç”ŸFçš„æ©Ÿï¥¡æœ€å¤§çš„Î‘ï¼Œäº¦å³ä½¿å¾— ( | )P F Î› æœ‰æœ€å¤§æ©Ÿï¥¡å€¼çš„Î‘ï¼Œå…¶ä¸­ =F WAã€‚æœ€å¾Œ
å¾—åˆ° -1=A R rï¼Œå…¶ä¸­ 1R= âˆ’â€²W U Wä¸” 1âˆ’â€²=r W U Î¼ã€‚ 
 
æ¥è‘— PW-/SYL-level éŸ³é«˜é æ¸¬æ¨¡å‹åœ¨åˆæˆç«¯çš„ï¥«ï¥©ç”Ÿæˆï¼Œï¥«ï¥©ä¹‹ç”Ÿæˆç¤ºæ„åœ–å¦‚ä¸‹ï¼š 
â€² â€²-1 -1 -1-1 W U W) (WU Î¼)A= R r = (
 
åœ–åã€ï¥«ï¥©ç”Ÿæˆç¤ºæ„åœ– 
å¦‚åœ–ï¼Œä¸€æ®µé•· N å€‹éŸ³éŸ»å–®å…ƒçš„æ–‡å­—ï¼Œåœ¨æ–‡å­—åˆ†æå¾Œå¾—åˆ°å°æ‡‰çš„æ–‡å­—è¨Šæ¯ nL ï¼Œç¶“ééŸ³
é«˜æ¨¡å‹çš„é æ¸¬å–å¾—ï¥®ç¯€é»çš„æ¨¡å‹ nÎ» ï¼Œä¸¦å¾—åˆ°æ¨¡å‹ï¥«ï¥© nF ï¼Œå³å¹³å‡å€¼å‘ï¥¾ nÎ¼ ï¼ˆmean vectorï¼‰
åŠå…±è®Šï¥¢ï¥©çŸ©é™£ nU çµ„åˆæˆçš„å‘ï¥¾ï¼Œå…¶ä¸­ nÎ¼ åŠ nU çš„ç¶­ï¨æ˜¯ä¸‰å€ Legendre Coefficients ç¶­ï¨
ï¼ˆåŒ…å«éœæ…‹ï¥«ï¥©åŠä¸€éšã€äºŒéšå‹•æ…‹ï¥«ï¥©çš„ç¶­ï¨ï¼‰ï¼Œåœ¨åˆæˆä¹‹å‰ï¼Œæˆ‘å€‘éœ€è¦å°‡ nF è½‰æ›å›å°æ‡‰
çš„ Legendre Coefficients nA ï¼Œæ‰èƒ½é€²ä¸€æ­¥ç®—å›éŸ³é«˜è»Œè·¡ï¼Œé€™æ¨£çš„è½‰æ›æ³•ç¨±ä½œï¥«ï¥©ç”Ÿæˆæ¼”ç®—
æ³•ã€‚æˆ‘å€‘å°‡éŸ³éŸ»å–®å…ƒï¥«ï¥©ï¤…æ¥æˆF ï¼Œå¾—åˆ° 1 2{ , , , }N=F F F F" ï¼Œä¸¦ç”¢ç”Ÿæ•´å€‹ï¤†å­çš„æ¨¡å‹
1 2{ , , , }NÎ» Î» Î»= "Î› ï¼Œ æ¨¡ å‹ çš„ å¹³ å‡ å€¼ å‘ ï¥¾ 1 2[ , , , ]Nâ€² â€² â€² â€²= "Î¼ Î¼ Î¼Î¼Î› ï¼Œ å…± è®Š ï¥¢ ï¥© çŸ© é™£
1 2[ , , , ]Ndiag= U U U"UÎ› ï¼Œæˆ‘å€‘å¸Œæœ›æ‰¾å‡ºä½¿å¾—å·²çŸ¥æ¨¡å‹Î›ç”¢ç”ŸFçš„æ©Ÿï¥¡æœ€å¤§çš„Î‘ï¼Œäº¦å³
ä½¿å¾— ( | )P F Î› æœ‰æœ€å¤§æ©Ÿï¥¡å€¼çš„Î‘ï¼Œå…¶ä¸­ =F WAã€‚å°‡ ( | )P F Î› å–å°ï¥©å¾—åˆ°ï¼š 
11log ( | ) ( ) ( )
2
1 3                      log log 2
2 2
P
MN Ï€
âˆ’â€²= âˆ’ âˆ’ âˆ’
âˆ’ âˆ’
F F FÎ› Î¼ U Î¼
U
Î› Î› Î›
Î›
     (9) 
æœ€å¾Œæˆ‘å€‘å¾—åˆ°ï¼š 
-1=A R r        (10) 
å…¶ä¸­ 1R= âˆ’â€²W U W è€Œ 1âˆ’â€²=r W U Î¼ã€‚ 
 
 14
Precision by Different Strength and Spread
0%
20%
40%
60%
80%
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Strength
Pr
ec
isi
on
 R
ate
s 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
 
åœ–åäºŒã€Precision rate æ¸¬è©¦çµæœ 
 
ä¸‹åœ–ä¸­ï¼Œåœ¨ strength 0.7 spread 0.2 çš„æ™‚å€™ï¼Œæ‰¾åˆ°çš„ Idiolect æœ‰é€™äº›ï¼Œé»‘é«”å­—è¡¨ç¤ºæ­£ç¢ºçš„ã€‚
æ‰€ä»¥ Precision rate æœ‰ 71.43%ï¼Œç•¶ spread å‡åˆ° 0.3-0.4 æ™‚ï¼Œæ‰¾åˆ°çš„ Idiolect ï¥©ï¥¾è®Šå°‘ï¦ºï¼Œå…¶ä¸­
noise æ¸›å°‘çš„ï¥©ï¥¾æ¯”æ­£ç¢ºçš„æ¸›å°‘çš„ï¥©ï¥¾å¤šï¼Œå› è€Œ Precision ä¸Šå‡åˆ° 83.33%ï¼Œç•¶ spread å‡åˆ° 0.5
æ™‚ï¼Œæ‰¾åˆ°çš„ Idiolect æ­£ç¢ºçš„æ¸›å°‘ï¦ºï¼Œä½† noise ä¸¦æ²’æœ‰æ¸›å°‘ï¼Œä»¥è‡³æ–¼ Precision ä¸‹ï¨‰ï¦ºã€‚æˆ‘å€‘å¯
ä»¥æ‰¾åˆ°ä¸€å€‹æœ€ä½³çš„ strength èˆ‡ spread å€¼ï¼Œï¤­ä½¿æ‰¾åˆ°çš„ Idiolect æœ€å¤šï¼Œè€Œæ­£ç¢ºï¥¡æœ€é«˜ã€‚ 
 
 
åœ–åä¸‰ã€Precision rate é‡å°ï¥§åŒï¥«ï¥©è¨­å®šä¹‹æ¸¬è©¦ç¯„ï¦µ 
 
é€™å€‹å¯¦é©—æ˜¯ç‚ºï¦ºæ¸¬è©¦ç¬¬ä¸‰å€‹ï¥«ï¥© frequency constraint å° precision å’Œ recall çš„å½±éŸ¿ã€‚æˆ‘
å€‘å›ºå®šï¦º strength èˆ‡ spread value åœ¨æœ€ä½³çš„ 0.1ã€‚ ä»¥é™³æ°´æ‰å…ˆç”Ÿçš„èªï¦¾è€Œè¨€ï¼Œéš¨è‘— Frequency 
constraint æ„ˆå°ï¼ŒRecall rate æ„ˆé«˜ï¼ŒPrecision æ„ˆä½ã€‚Frequency constraint æ„ˆå¤§ï¼ŒRecall rate æ„ˆ
ä½è€Œ Precision rate å‰‡æ„ˆé«˜ã€‚åœ¨ Frequency constraint ç­‰æ–¼ 10 çš„æ™‚å€™ï¼Œåˆ°é”ä¸€å€‹ compromise
çš„åœ°æ–¹ã€‚åŒæ¨£çš„å¯¦é©—æ­¥é©Ÿï¼Œåœ¨å°‘ï¥¾çš„é¦¬è‹±ä¹èªï¦¾ä¸Šï¼ŒPrecision å’Œ Recall åœ¨ Frequency 
constraint ç‚º 5 æ™‚ï¼Œé”åˆ° compromise çš„åœ°æ–¹ã€‚åŒæ¨£å¯¦é©—æ­¥é©Ÿåœ¨èªï¦¾ï¥¾æœ€å¤§çš„ï§¡æ•–ä¸Šï¼ŒPrecision
èˆ‡ Recall åœ¨ Frequency constraint ç‚º 20 å·¦å³é”åˆ° compromise çš„åœ°æ–¹ã€‚å› æ­¤ï¼Œæˆ‘å€‘å¯ä»¥å¾—çŸ¥ï¼Œ
åœ¨å°ï¥¾èªï¦¾ï¼ŒFrequency constraint å¯ä»¥è¨­å°ä¸€é»ï¼Œåœ¨å¤§ï¥¾èªï¦¾ï¼ŒFrequency è¨­é«˜æœƒæœ‰è¼ƒå¥½çš„
æ­£ç¢ºæ€§ã€‚ 
 16
æœ€å¾Œ t-test çš„ p-value ç‚º 0.124ï¼Œå¤§æ–¼ alphaï¼Œå›ºæ¥å—é›»è…¦èˆ‡é™³æ°´æ‰æ˜¯ä¸€æ¨£çš„å‡è¨­ã€‚ 
 
è¡¨ II: è´…èªæ’å…¥ä¹‹ä¸»è§€å¼è©•ä¼°çµæœ 
 
 
æˆ‘å€‘æœ€å¾Œæ˜¯è©•ä¼°åŒç¾©è©å–ä»£çš„æ­£ç¢ºï¥¡ã€‚æˆ‘å€‘ç‚ºæ¯ä¸€å€‹éè´…èªçš„ Idiolect æ‰¾åˆ°å®ƒçš„ä¸€å€‹åŒ
ç¾©è©ã€‚æ‰€ä»¥æ­£ç¢ºï¥¡ç®—æ³•æ˜¯ï¼šåˆ†å­æ˜¯æ‰¾åˆ°æ­£ç¢ºçš„åŒç¾©è©ï¼Œæ‰€è¬‚æ­£ç¢ºæ˜¯çœŸæ­£èƒ½æ”¾åœ¨ä¸Šä¸‹æ–‡ä¸€æ¨£
çš„ï¤†å­ä¸­æ‰ç®—ã€‚åˆ†æ¯æ˜¯éè´…èª Idiolect çš„ï¥©ç›®ï¼Œæœ€å¾Œä¹˜ä¸Š 100ã€‚æˆ‘å€‘å¯ä»¥çœ‹åˆ°ä¸‰å€‹äººå¹³å‡ä¸‹
ï¤­æ˜¯å¿«åˆ°ä¸ƒæˆçš„æ­£ç¢ºï¥¡ã€‚æ­£ç¢ºï¥¡ï¥§é«˜çš„åŸå› åœ¨æ–¼ï¼Œæœ‰äº› Idiolect æ˜¯å«æœ‰ä»£åè©çš„ï¼Œï¦µå¦‚â€¦é€™ã€
ä»–ã€é‚£æ˜¯ï¼Œç­‰ç­‰ï¼Œé€ æˆæ‰¾ï¥§åˆ°åŒç¾©è©çš„æƒ…æ³ã€‚é€™äº›å•é¡Œï¼Œå¯ä»¥ç”¨ anaphora resolution çš„ä¸€äº›
æ–¹æ³•ï¤­ä½œã€‚åœ¨æœªï¤­çš„ç ”ç©¶ä¸­ï¼Œå°‡æœƒè™•ï§¤é€™ä¸€å€‹å•é¡Œï¼Œä½¿æ­£ç¢ºï¥¡å†æå‡ã€‚ 
ç¬¬ä¸€ï¦ï¨æ‰€æä¹‹æ–¹æ³•ç¢ºèƒ½å°‡åŸå§‹ï¥§å…·æœ‰å€‹äººé¢¨æ ¼çš„æ–‡ç« ï¼Œè½‰æ›ç‚ºå«æœ‰ç›®æ¨™èªè€…ï¥¯è©±é¢¨
æ ¼ä¹‹æ–‡ç« ã€‚ä¸»è¦ç›®çš„å³é€éå€‹äººç¿’èªçš„èƒå–èˆ‡ç”Ÿæˆï¼Œå°‡åŸå§‹æ–‡ç« è½‰æ›ç‚ºå¸¶æœ‰æŸèªè€…ï¥¯è©±é¢¨
æ ¼çš„æ–‡å­—è¨Šæ¯ï¼Œé”åˆ°ï¥¯è©±é¢¨æ ¼æ¨¡å‹åŒ–çš„ç›®çš„ã€‚ï§ç”¨çµ±è¨ˆæ–¹æ³•è‡ªå‹•å¾è¨“ï¦–èªï¦¾ä¸­èƒå–å‡ºå€‹äºº
ç¿’èªï¼Œä¸¦å°‡ä¹‹åˆ†ï§ï¼Œä¸¦èƒ½å°‡è¢«æ­¸ï§ç‚ºè´…èªä¹‹å€‹äººç¿’èªï¼Œï§ç”¨è´…èªæ’å…¥æ¨¡å‹å°‡å®ƒæ”¾å…¥æ–‡ç« ä¸­
æœ€åˆé©çš„ä½ç½®ï¼Œå°æ–¼éè´…èªï§çš„å€‹äººç¿’èªï¼Œè¨­è¨ˆä¸€åŒç¾©è©å°ç…§è¡¨ï¼Œè—‰ç”±åŒç¾©è©å–ä»£èˆ‡æ’å…¥
è´…èªï¼Œå°‡åŸå§‹è¼¸å…¥çš„æ–‡ç« ä¿®é£¾æˆå¸¶æœ‰ç›®æ¨™èªè€…ï¥¯è©±é¢¨æ ¼çš„æ–‡å­—å…§å®¹ã€‚æœ€å¾Œå°‡é€™å€‹å€‹äººåŒ–æ–‡
å­—é¢¨æ ¼èªï¦¾åˆ†æç³»çµ±çš„çµæœèˆ‡ TTS ç³»çµ±é€²ï¨ˆçµåˆï¼Œå…ˆå°‡ä¸€èˆ¬æ€§çš„æ–‡ç« ï¼Œé‡å°ç›®æ¨™èªè€…ï¼Œç¶“
ç”±æ–‡å­—åˆ†æï¼Œèƒå–å‡ºè©²ç‰¹å®šä¹‹ç›®æ¨™èªè€…çš„å€‹äººé¢¨æ ¼ä¹‹æ…£ç”¨èªï¼Œå°‡ä¸€èˆ¬æ€§æ–‡ç« è½‰æ›ç‚ºå¸¶æœ‰è©²
ç›®æ¨™èªè€…å€‹äººèªè€…é¢¨æ ¼çš„æ–‡ç« ï¼Œä¸¦ä¸”å°‡å¸¶æœ‰å€‹äººèªè€…é¢¨æ ¼çš„æ–‡ç« ç¶“ç”± TTS ç³»çµ±ï¼Œæ­é…é »è­œ
å’ŒéŸ»ï§˜æ¨¡å‹è½‰æ›(Spectrum conversion and prosody conversion)ï¼Œç”¢ç”Ÿå‡ºå¸¶ç›®æ¨™èªè€…æœ‰å€‹äººé¢¨
æ ¼çš„èªéŸ³ã€‚ 
ç¬¬äºŒï¦ï¨ç ”ç©¶ä¸­ï¼Œä½¿ç”¨çš„èªï¦¾ï¤­è‡ªï¥£äº¬æ¸…è¯å¤§å­¸çš„èªéŸ³åˆæˆèªï¦¾åº«ï¼ˆTsingHuaâ€“Corpus 
of Speech Synthesisï¼ŒTHâ€“CoSSï¼‰ï¼Œæ­¤èªï¦¾åº«ä¸»è¦æ˜¯é‡å°æ¼¢èªæ™®é€šè©±èªéŸ³åˆæˆçš„ç ”ç©¶ã€é–‹ç™¼èˆ‡
è©•æ¸¬ï¼Œä»¥åŠèªéŸ³å­¸ç ”ç©¶è€Œè¨­è¨ˆçš„æ¼¢èªèªï¦¾åº«ã€‚èªï¦¾æ–‡æœ¬ä¸»è¦é¸è‡ªæ–°èï¼Œæ¥ä¸‹ï¤­å„ç¯€çš„å¯¦é©—
åŠè©•ä¼°ä¸­ä½¿ç”¨çš„èªï¦¾ç‚º THâ€“CoSS ä¸­ä¸€ä½ï¦æ€§èªè€…ï¼ˆFR00ï¼‰çš„ï¤©ï¥šèªï¦¾ã€é™³è¿°ï¤†ï¤†å‹ã€‚ï¼Œ
æˆ‘å€‘å°‡ FR00 èªï¦¾åˆ†ä½œä¸‰å€‹å­é›†åˆï¼Œåˆ†åˆ¥ç‚ºå„æ¨¡å‹çš„è¨“ï¦–èªï¦¾ï¼ˆéŸ³éŸ»çµæ§‹é æ¸¬æ¨¡å‹åŠéŸ³é«˜
é æ¸¬æ¨¡å‹ï¼‰ã€æ¬Šé‡èª¿æ•´èªï¦¾ã€ä»¥åŠæ¸¬è©¦èªï¦¾ã€‚åœ¨éŸ³éŸ»åœé “é æ¸¬æ¨¡å‹æ•ˆèƒ½çš„è©•ä¼°ä¸Šï¼Œæˆ‘å€‘ä½¿ç”¨
ä¸‰ç¨®å®¢è§€è©•ä¼°çš„æ–¹å¼ï¼ŒåŒ…æ‹¬ï¨ç¢ºï¥¡ï¼ˆprecisionï¼‰ã€å¬å›ï¥¡ï¼ˆrecallï¼‰ä»¥åŠæ­£ç¢ºï¥¡ï¼ˆaccuracyï¼‰ï¼Œ
å€‹åˆ¥çš„éŸ³éŸ»åœé “é æ¸¬æ¨¡å‹ï¼ˆPW S-CART åŠ PP S-CARTï¼‰å…¶ precision ä»¥åŠ recall å¦‚è¡¨ IIIï¼Œ
 18
 
åœ–åä¸ƒã€åŸå§‹éŸ³é«˜è»Œè·¡ï¼ˆï¤£è™›ç·šï¼‰èˆ‡ U-CART(H)ï¼ˆç´…å¯¦ç·šï¼‰ 
 
 
åœ–åå…«ã€åŸå§‹éŸ³é«˜è»Œè·¡ï¼ˆï¤£è™›ç·šï¼‰èˆ‡ U-CART(H+D)ï¼ˆç´…å¯¦ç·šï¼‰ 
 
åœ¨éŸ³éŸ»åœé “é æ¸¬æ¨¡å‹çš„è©•ä¼°ä¸Šï¼Œæˆ‘å€‘æ¡ç”¨å®¢è§€è©•ä¼°ä»¥åŠä¸»è§€è©•ä¼°ï¥¸å€‹æ–¹å‘ï¼Œåˆ†åˆ¥å°æ¨¡
å‹çš„æ•ˆèƒ½é€²ï¨ˆè©•ä¼°ï¼Œè©•ä¼°åˆ†ç‚ºä¸»è§€å’Œå®¢è§€è©•ä¼°ã€‚å®¢è§€è©•ä¼°æ¡ç”¨ RMSE ç‚ºæ¨™æº–ï¼Œå®¢è§€è©•ä¼°ç”±ä½¿
ç”¨è€…è©•åˆ†ï¼Œç”± 1~5 åˆ†ï¼Œåˆ†ï¥©è¶Šé«˜è¡¨ç¤ºè¶Šå¥½ï¼Œåä¹‹è¶Šå·®ã€‚ 
ä½¿ç”¨å‡æ–¹æ ¹èª¤å·®ï¼ˆRoot Mean Square Error, RMSEï¼‰ä½œç‚ºè©•ä¼°æ¨™æº–ï¼Œè¨ˆç®—ç›®æ¨™çš„éŸ³é«˜
è»Œè·¡å’Œé æ¸¬çš„éŸ³é«˜è»Œè·¡ä¹‹é–“çš„èª¤å·®ï¼Œå…¶ä¸­ç›®æ¨™çš„éŸ³é«˜è»Œè·¡ç‚º 1 2( , ,..., )NX x x x= ï¼Œè€Œé æ¸¬çš„
éŸ³é«˜è»Œè·¡ç‚º 1 2( , ,..., )NY y y y= ï¼ŒRMSE çš„è¨ˆç®—å¼å¦‚ä¸‹ï¼š 
2( )  
 
N
i i
i=1
x y
RMSE(X,Y)=
N
âˆ’âˆ‘
      (11) 
è¡¨ V: å®¢è§€è©•ä¼° RMSE å€¼ï¼ˆHzï¼‰ 
 
 20
æå®¢è§€è©•ä¼°èˆ‡ä¸»è§€è©•ä¼°çš„çµæœï¼Œåœ¨å®¢è§€è©•ä¼°çš„çµæœä¸Šï¼Œä¸‰ç¨®ä½¿ç”¨ CART çš„æ¨¡å‹
ï¼ˆU-CART(D)ã€U-CART(H)ã€U-CART(D+H)ï¼‰åœ¨ RMSE çš„ï¥©å€¼ä¸Šä¸¦æ²’æœ‰å¾ˆå¤§çš„å·®è·ï¼Œä½†åœ¨ä¸»è§€
è©•ä¼°æ™‚ï¼ŒU-CART(H)åœ¨ MOS åŠåå¥½æ¸¬å®šä¸Šè¡¨ç¾ï¥§ä½³ï¼Œè­‰æ˜å‹•æ…‹ï¥«ï¥©å°æ–¼æ¨¡å‹é æ¸¬çµæœã€å„å€‹
å–®å…ƒä¹‹é–“çš„ï¤…æ¥ï§Šæš¢ï¨ä¸Šï¼Œç¢ºå¯¦æœ‰å¾ˆå¤§çš„å½±éŸ¿ï¼Œå—æ¸¬è€…åœ¨èªéŸ³è‡ªç„¶ï¨çš„è©•åˆ†ä¸Šï¼Œï¤åœ¨æ„çš„
æ˜¯èªéŸ³æ•´é«”ï¤…æ¥ï§Šæš¢çš„è¡¨ç¾ï¼Œä»»ä½•åœ¨ï¤†ä¸­çªé«˜æˆ–çªä½çš„éŸ³é«˜è®ŠåŒ–ï¨¦æœƒå°è‡ªç„¶ï¨æœ‰å¾ˆå¤§çš„å½±
éŸ¿ã€‚ 
æœ¬è¨ˆç•«ç¬¬äºŒï¦ï¨ä¸»è¦å®Œæˆæ‡‰ç”¨éšå±¤å¼éŸ»ï§˜æ¶æ§‹åŠå‹•æ…‹ï¥«ï¥©ä¹‹éŸ³é«˜é æ¸¬æ¨¡å‹ï¼Œæ”¹å–„åŸºæ–¼
éš±è—å¼é¦¬å¯å¤«æ¨¡å‹ä¹‹èªéŸ³åˆæˆå™¨ï¼Œå°‡éŸ»ï§˜çµæ§‹ä»¥éšå±¤å¼çš„æ¶æ§‹æ‹†è§£ï¼Œæ ¹æ“šéšå±¤å¼çš„éŸ»ï§˜æ¶
æ§‹ä½œéŸ³éŸ»ï¥«ï¥©çš„ï¥¾æ¸¬ï¼Œä¸¦å°å…¥å‹•æ…‹ï¥«ï¥©æ–¼å„éŸ»ï§˜éšå±¤ï¥«ï¥©ç©ºé–“ï¼Œä»¥ä¿ï§çœŸå¯¦èªéŸ³åœ¨æ™‚é–“åº
ï¦œä¸Šçš„è®ŠåŒ–åŠé—œï¦šæ€§ï¼›åœ¨å„å±¤éŸ³éŸ»æ¨¡å‹çš„è¨“ï¦–ä¸Šï¼Œåœ¨éŸ³éŸ»è©åŠéŸ³ç¯€éšå±¤ä½¿ç”¨åˆ†ï§å›æ­¸æ¨¹æ¨¡
å‹ï¼Œåœ¨éŸ³æ¡†éšå±¤ä½¿ç”¨éš±è—å¼é¦¬å¯å¤«æ¨¡å‹ã€‚å„å±¤ç¶“éæ¨¡å‹çš„é æ¸¬åŠï¥«ï¥©ç”Ÿæˆæ¼”ç®—æ³•çš„è¨ˆç®—å¾Œï¼Œ
å¾—åˆ°å°æ‡‰çš„éŸ³éŸ»ï¥«ï¥©ã€‚ 
æ ¹æ“šå¯¦é©—è©•ä¼°çš„çµæœï¼Œè­‰æ˜å‹•æ…‹ï¥«ï¥©å°æ–¼æ¨¡å‹é æ¸¬çµæœã€å„å€‹å–®å…ƒä¹‹é–“çš„ï¤…æ¥ï§Šæš¢ï¨
ä¸Šï¼Œç¢ºå¯¦æœ‰å¾ˆå¤§çš„å½±éŸ¿ï¼Œå—æ¸¬è€…åœ¨èªéŸ³è‡ªç„¶ï¨çš„è©•åˆ†ä¸Šï¼Œï¤åœ¨æ„çš„æ˜¯èªéŸ³æ•´é«”ï¤…æ¥ï§Šæš¢çš„
è¡¨ç¾ï¼Œä»»ä½•åœ¨ï¤†ä¸­çªé«˜æˆ–çªä½çš„éŸ³é«˜è®ŠåŒ–ï¨¦æœƒå°è‡ªç„¶ï¨æœ‰å¾ˆå¤§çš„å½±éŸ¿ã€‚é¡¯ç¤ºæœ¬ç ”ç©¶æå‡ºçš„
æ–¹æ³•ï¼Œå°æ–¼æ”¹å–„åŸæœ‰ HMM éŸ³é«˜é æ¸¬æ¨¡å‹ä¸Šï¼Œæœ‰ï¥§éŒ¯çš„è¡¨ç¾ã€‚æœ€å¾Œï¼Œæˆ‘å€‘æœ‰ä»¥ä¸‹å¹¾é»å¯ä¾›
æœªï¤­ç¹¼çºŒæ·±å…¥æ¢è¨ä¸¦æ”¹å–„çš„æ–¹å‘ï¼Œæœ¬ç ”ç©¶ç”±æ–¼é«˜å±¤èªéŸ³è³‡è¨Šçš„ï¥§è¶³ï¼Œåœ¨éŸ³éŸ»éšå±¤æ¶æ§‹æœ€ä¸Š
å±¤åªè€ƒæ…®åˆ° PWï¼Œæœªï¤­åœ¨å¯¦é©—èªï¦¾çš„æ”¶é›†ä¸Šï¼Œå¯ä»¥ç´å…¥ï¤é•·çš„ï¤†çµ„æˆ–ç¯‡ç« ï¼Œå¾—åˆ°ï¤ä¸Šå±¤çš„
éŸ»ï§˜å–®å…ƒå°éŸ»ï§˜è¨Šæ¯çš„å½±éŸ¿ï¼Œå°éšå±¤çš„éŸ»ï§˜æ¶æ§‹æœ‰ï¤å®Œæ•´çš„æè¿°ã€‚åœ¨æ•´å€‹çš„èªéŸ³éŸ»ï§˜ä¸Šï¼Œ
ç›®å‰åªé‡å°ï¦ºæœ€å…·å½±éŸ¿çš„éŸ³é«˜è»Œè·¡åšé æ¸¬ï¼Œå…¶ä»–å¦‚éŸ³å¼·ã€éŸ³ç¯€æ™‚é•·ç­‰éŸ»ï§˜è¨Šæ¯ï¼Œäº¦æ˜¯æœªï¤­
çš„ä¸€äº›ç ”ç©¶æ–¹å‘ã€‚ 
ç¬¬ä¸‰ï¦ä¸»è¦å®Œæˆå°‡å‰ï¥¸ï¦ä¹‹éŸ³éŸ»è½‰æ› (Prosodic Conversion)å’Œé »è­œè½‰æ› (Spectral 
Conversion)ç³»çµ±æ•´åˆï¼Œä¸¦ä¸”å°éŸ³éŸ»æ¨¡å‹ (Prosodic Model)åŠ å…¥ï¤å¤šçš„å€‹äººåŒ–ï¥«ï¥©ç‰¹å¾µ
(Feature)ï¼Œæœ€çµ‚ç›®æ¨™åœ¨æ–¼å®Œæˆå»ºï§·ä¸€å€‹å…·æœ‰å€‹äººåŒ–ç‰¹è‰²èªè€…è½‰æ›çš„ç³»çµ±ï¼Œä¸»è¦åœ¨æ–¼æ•´åˆå‰ï¥¸
ï¦çš„ç³»çµ±ï¼Œå’Œæ–‡å­—è½‰èªéŸ³ç³»çµ±çµåˆ(Text-to-Speechï¼šTTS)ï¼Œé™¤ï¦ºèƒ½å¾æ–‡å­—ä¸Šå»åˆæˆå€‹äººèªè€…
ç‰¹è‰²çš„è²éŸ³å¤–ï¼Œä¹Ÿè¦èƒ½å¾ä½¿ç”¨è€…æä¾›æ‰€éœ€è¦çš„æŒ‡å®šèªè€…è²éŸ³é€²ï¨ˆè½‰æ›ï¼Œä¸¦ä¸”æ–¼æœ¬ï¦ï¨å°æ•´
åˆå¾Œçš„ç³»çµ±é€²ï¨ˆæ•ˆèƒ½è©•ä¼°ï¼Œé‡å°ç³»çµ±çš„çµæ§‹å’Œæ•ˆèƒ½é€²ï¨ˆæ”¹é€²ã€‚æ•´å€‹ï¥©ä½æ•™è‚²è¨“ï¦–åŠç…§è­·ç³»
çµ±æ•´åˆå¾Œï¼Œé‚„å¿…é ˆè©•ä¼°æ‰€åˆæˆå‡ºï¤­çš„èªéŸ³ï¼Œå°æ–¼å—ç…§è­·è€…è€Œè¨€ï¼Œè©•ï¥¾æ˜¯å¦èƒ½ç”¢ç”Ÿå¯ä»¥æ¥å—
èªéŸ³ï¼Œä»¥åŠæ‰€è¬‚å€‹äººåŒ–çš„èªè€…ç‰¹è‰²æ˜¯å¦æœ‰é”åˆ°å—ç…§è­·è€…çš„éœ€æ±‚ï¼Œé€™å¹¾é»å°‡é—œéµåˆ°æœ¬ç³»çµ±åœ¨
åˆæˆå€‹äººåŒ–çš„èªéŸ³æ™‚ï¼Œèƒ½å¦æä¾›å¯¦è³ªçš„æ•ˆæœåŠæ‡‰ç”¨ã€‚ 
è—‰ç”±ä¹‹å‰æ‰€å»ºï§·çš„éŸ³éŸ»æ¨¡å‹(Prosodic Model)ï¼Œä»¥åŠä½¿ç”¨é »è­œç©©å®šå€æ®µç‚ºåŸºæœ¬å–®å…ƒåšé »
è­œåœ–è½‰æ›çš„èªéŸ³è½‰æ›(Voice Conversion)æ–¹æ³•ï¼ŒåŠ ä»¥æ•´åˆã€‚å®Œæˆå»ºï§·æ—¥å¸¸ç”Ÿæ´»ç”¨èªçš„èªï¦¾åº«ï¼Œ
å¹³è¡¡èªï¦¾(Balanced Corpus Collection)çš„æ”¶é›†å¾Œï¼Œç”±æ–¼æ—¥å¸¸ç”Ÿæ´»çš„å°è©±ï¼Œä¸¦éä¸€èˆ¬æ–‡ç« ï¼Œæˆ–
è€…ç…§ç¨¿å®£ï¥šçš„æ¼”è¬›ï¼Œæ‰€ä»¥åœ¨æ”¶é›†ä¸Šå¿…é ˆè¨­è¨ˆéï¼Œç”±é•·æ™‚é–“çš„è¨˜ï¤¿æ–¹å¼ï¼Œé‡å°ç‰¹å®šèªè€…é€²ï¨ˆ
æ”¶é›†å…¶æ—¥å¸¸ç”Ÿæ´»çš„å°è©±ï¼Œè‡ªç„¶çš„å°è©±ä¸¦ï¥§åƒæ–‡ç« èˆ¬ï¼Œå…·æœ‰å®Œæ•´çš„ï¤†æ³•çµæ§‹ï¼Œè€Œæ˜¯å…·æœ‰å€‹äºº
åŒ–ç‰¹è‰²çš„å–®ï¤†å¤¾é›œå€‹äººçš„å£é ­ç¦ªæˆ–è€…æ–·ï¤†ç‰¹è‰²ä»¥åŠå‘¼å¸æ–¹å¼æ‰€çµ„åˆè€Œæˆï¼Œæ‰€æœ‰çš„ï¤†ç¾¤æœ‰å¯
èƒ½ä¸¦æ²’æœ‰æ˜é¡¯çš„åˆ†æ®µï¼Œåªæœ‰ç›¸åŒçš„ä¸»é¡Œå’Œæ„ç¾©ï¼Œæ‰€ä»¥åœ¨æ”¶é›†åˆ°é€™æ¨£çš„æ—¥å¸¸ç”Ÿæ´»å°è©±å¾Œï¼Œé‚„
å¿…é ˆé€²ï¨ˆäººå·¥æ¨™è¨˜(Tagging)å’Œï¤†ç¾¤åˆ†ï§çš„å‰è™•ï§¤å·¥ä½œã€‚æˆ‘å€‘ç›®å‰æ¡ç”¨çš„æ˜¯å³å¸­æ¼”è¬›çš„èª
ï¦¾ï¼Œå› ç‚ºå³å¸­æ¼”è¬›æˆ–è€…è¨ªè«‡æ€§ç¯€ç›®çš„å…§å®¹ï¼Œä¸¦éæœ‰é å®šçš„æ–‡å­—ç¨¿ï¼Œå°è©±è€…åªæœ‰æ¦‚æ‹¬çš„æ„ï¦£ï¼Œ
è€Œç„¡ç‰¹å®šè¦ï¥¯çš„æ–‡å­—ï¼Œæ‰€ä»¥æ¯å€‹äººå°‡æœƒæœ‰æ¯å€‹äººçš„è©®é‡‹æ–¹å¼ï¼Œå› æ­¤ï¼Œæˆ‘å€‘å°±å¯ä»¥è—‰ç”±é€™æ¨£
çš„èªï¦¾ï¼Œï¤­æ”¶é›†ä¸¦ä¸”æ•´ï§¤å‡ºï¼Œæ‰€è¬‚çš„â€œå€‹äººåŒ–ï¼‚èªè€…ç‰¹è‰²ï¼Œç„¶å¾Œçµåˆï¤†æ³•åˆ†æ(Syntactic 
Analysis)å’ŒéŸ³éŸ»åˆ†æ(Acoustic Analysis)çš„æ–¹æ³•ï¼Œä¸¦ä¸”åœ¨ï¤†æ³•åˆ†æä¸­ï¼Œå…ˆé‡å°æ–‡å­—éƒ¨åˆ†ï¼Œï§ç”¨
Maximum Entropy å»æ‰¾å‡ºå…·æœ‰å€‹äººåŒ–èªè€…ç‰¹è‰²çš„æ…£ç”¨èªä½ç½®ï¼Œä¸¦ä¸”è¨‚å‡º Rule Templateï¼Œå»º
æ§‹å‡ºå…·æœ‰å€‹äººåŒ–ç‰¹è‰²çš„æ–‡å­—èªè€…æ¨¡å‹å¾Œï¼Œé€²ï¨ˆé æ¸¬ï¼Œä»¥æœŸèƒ½åšåˆ°åœ¨æ‹¿åˆ°ä¸€æ®µæ­£å¸¸çš„å°è©±æ™‚ï¼Œ
 22
Using Structural Syntactic Cost,â€ IEEE Trans. Audio, Speech, and Language Processing, 
Vol. 15, No. 4, pp.1227~1235, May, 2007. 
[11] C.H. Wu, C.C. Hsia, T.H. Liu, and J.F. Wang, â€œVoice Conversion Using 
Duration-Embedded Bi-HMMs for Expressive Speech Synthesis,â€ IEEE Trans. Audio, 
Speech, and Language Processing, Vol. 14, No. 4, pp.1109~1116, July, 2006. 
[12] C.C. Hsia, C.H. Wu, and J.Q. Wu, â€œConversion Function Clustering and Selection Using 
Linguistic and Spectral Information for Emotional Voice Conversion,â€ IEEE Trans. 
Computers, Vol. 56, No. 9, pp. 1225~1233, September 2007. 
[13] R. Xu and Q. Lu, â€œMulti-Stage Chinese Collocation Extraction,â€ Proc. International 
Conference on Machine Learning and Cybernetics, pp. 3254-3259, 2005. 
[14] Q. Lu, Y. Li, and R. Xu, â€œImproving Xtract for Chinese Collocation Extraction,â€ Proc. 
IEEE Intl. Conf. NLPKE 2003, pp. 333-338, 2003. 
[15] H. Thomas, E. Charles, L. Ronald, and Stein Clifford, â€œIntroduction to Algorithms,â€ Second 
Edition, MIT Press, 2001. 
[16] K.J. Chen and M.H. Bai, â€œUnknown Word Detection for Chinese by a Corpus-based 
Learning Method,â€ International Journal of Computational Linguistics and Chinese 
Language Processing, Vol.3, No.1, pp. 27-44, 1998. 
[17] D. Kauchak and R. Barzilay, â€œParaphrasing for Automatic Evaluation,â€ Proc. HLT-NAACL, 
pp. 455-462, 2006. 
[18] ç‹å°å·, èªéŸ³è¨Šè™Ÿè™•ï§¤, å…¨è¯ç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸, 2004. 
[19] ç¨‹ï¨šå¾½, ç”°å°ç³, â€œç¾ä»£æ¼¢èªï¼‚, æ›¸ï§´å‡ºç‰ˆæœ‰é™å…¬å¸. 
[20] ï§´ç‡¾, ç‹ï§¤å˜‰, â€œèªéŸ³å­¸æ•™ç¨‹ï¼‚ äº”å—åœ–æ›¸å‡ºç‰ˆå…¬å¸. 
[21] Monaghan, A.I.C. and Ladd, D.R., â€œManipulating Synthetic Intonation for Speaker C 
haracterisationâ€, in Proc. of ICASSP, S7.11, pp. 453-456, 1991. 
[22] Lee, L. S., Tseng, C. Y. and Hsieh, C. J., â€œImproved Tone Concatenation Rules in a 
Formant-Based Chinese Text-to-Speech Systemâ€, IEEE Trans. on Speech and Audio 
processing, vol. 1, no.3, pp.287-294, July 1993. 
[23] Chen, S. H. and Wang, Y. R., â€Tone Recognition of Continuous Mandarin Speech Based on 
Neural Networksâ€, IEEE Trans. on Speech and Audio processing, vol. 3, no.2, pp.146-150, 
March 1995. 
[24] Wightman, C. W. and Ostendorf. M., â€œAutomatic Labeling of Prosodic Patternsâ€, IEEE 
Trans. on Speech and Audio Processing, vol. 2, no. 4, pp. 469-481, October 1994. 
[25] Lin, X., Chen, Y., Lim, S. and Lim, C., â€œRecognition of Emotional State From Spoken 
Sentencesâ€, IEEE 3rd workshop on Multimedia Signal Processing, pp. 469-473, 1999. 
[26] Chan, M. V., Feng, X., Heinen, J. A. and Niederjohn, R. J., â€œClassification of Speech 
Accents with Neural Networksâ€, Neural Networks, IEEE World Congress on Computational 
Intelligence., IEEE International Conference on, vol.7, pp. 4483-4486, 1994. 
[27] Andrej, L. and Frank, F., â€œSynthesis of Natural Sounding Pitch Contours in Isolated 
Utterances Using Hidden Markov Modelsâ€, IEEE Trans. on Acoustic, Speech and Signal 
Processing, vol. ASSP-34, no.5, pp.1074-1080, October 1986. 
[28] Pan, N. H., Jen, W. T., Yu, S. S., Yu, S. S., Huang, S. Y. and Wu, M. J., â€œProsody Model in a 
Mandarin Text-to-Speech System Based on a Hierarchical Approachâ€, IEEE International 
Conference on Multimedia and Expo, vol. 1, pp. 448-451, 2000. 
[29] Chen, S. H., Hwang, S. H. and Wang, Y. R., â€œAn RNN-based Prosodic Information 
Synthesizer for Mandarin Text-to-Speechâ€, IEEE Trans. on Speech and Audio Processing, 
vol. 6, no.3, pp.226-269, 1998. 
[30] Kim, S. H., and Kim, J. Y., â€œEfficient Model of Establishing Words Tone Dictionary for 
Korean TTS Systemâ€, in Proc. of Eurospeech, pp. 243-246, 1997. 
[31] Dong, M. and Lua, K. T., â€œPitch Contour Model for Chinese Text-to-Speech Using CART 
and Statistical Modelâ€, in Proc. of ICSLP, pp. 2405-2408, 2002. 
[32] Tao, J., â€œF0 Prediction Model of Speech Synthesis Based on Template and Statistical 
Methodâ€, Lecture Nodes of Artificial Intelligence, Springer, 2004. 
[33] Sun, X., The Determination, Analysis and Synthesis of Fundamental Frequency, Ph. D 
Thesis, Northwestern University, 2002. 
 24
æ¨¡å‹ï¼Œåœ¨éšå±¤å¼éŸ³éŸ»æ¶æ§‹ä¸‹ï¼Œä½œéŸ³é«˜é æ¸¬çš„åŸºç¤ç ”ç©¶ï¼Œä¸¦åœ¨å„å±¤çš„éŸ³éŸ»å–®å…ƒè€ƒæ…®å‹•æ…‹ï¥«ï¥©ï¼›
å¸Œæœ›æ”¹å–„å‚³çµ±éŸ³éŸ»æ¨¡å‹åœ¨å°å–®å…ƒä¸Šåˆæˆç´°è†©ï¨çš„ï¥§è¶³ï¼Œä¸¦ä¸”ä»¥å‹•æ…‹ï¥«ï¥©ç”ŸæˆèªéŸ³ï¥«ï¥©ï¼Œä¿
ï§æ™‚é–“ä¸Šçš„é—œï¦—æ€§ï¼Œä½¿å–®å…ƒå’Œå–®å…ƒä¹‹é–“çš„ï¦šæ¥ï¤åŠ è‡ªç„¶ï¼Œä¸¦ä¸”èƒ½ï¤ç¬¦åˆäººï§è¬›è©±æ™‚çš„ï¤†èª¿
èµ°å‹¢è®ŠåŒ–ï¼Œè—‰ä»¥æ”¹å–„åŸºæ–¼éš±è—å¼é¦¬å¯å¤«æ¨¡å‹ä¹‹åˆæˆèªéŸ³çš„è‡ªç„¶ï¨ã€‚æ­¤ä¸€å…·æœ‰å€‹äººåŒ–ç‰¹è‰²ä¹‹
æ–‡å­—åˆ†ææ¨¡å‹çš„ç ”ç©¶ä¸¦ä¸”çµåˆäººï§è¬›è©±æ™‚çš„ï¤†èª¿èµ°å‹¢è®ŠåŒ–é æ¸¬æ¨¡å‹çš„ç ”ç©¶å’Œåˆæˆå…·æœ‰è‡ªç„¶
éŸ³éŸ»çš„èªéŸ³æŠ€è¡“æœ‰ä¸€å®šä¹‹å­¸è¡“åƒ¹å€¼ï¼Œä¸¦ä¸”å·²ç¶“æ–¼ç›¸é—œä¹‹å­¸è¡“æœŸåˆŠç™¼è¡¨æœ¬ç ”ç©¶ä¹‹æŠ€è¡“ã€‚ 
 
 
 26
â–  å¯ç”³è«‹å°ˆï§  â–  å¯æŠ€è¡“ç§»è½‰                                     æ—¥æœŸï¼š98 ï¦ 5 æœˆ 27 æ—¥ 
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•« 
è¨ˆç•«åç¨±ï¼šæ‡‰ç”¨èªéŸ³è½‰æ›æ–¼å€‹äººåŒ–è¡¨é”æ€§æ–‡å­—è½‰èªéŸ³ä¹‹ç ”ç©¶ 
è¨ˆç•«ä¸»æŒäººï¼šå³å®—æ†²         
è¨ˆç•«ç·¨è™Ÿï¼šNSC96-2221-E-006-155-MY3 å­¸é–€ï¦´åŸŸï¼šè³‡è¨ŠäºŒ(å·¥ç¨‹è™•)
æŠ€è¡“/å‰µä½œåç¨± æ‡‰ç”¨éŸ»ï§˜éšå±¤åŠå‹•æ…‹ï¥«ï¥©ä¹‹éŸ³é«˜é æ¸¬æŠ€è¡“ä¹‹ç ”ç©¶ 
ç™¼æ˜äºº/å‰µä½œäºº å³å®—æ†² 
ä¸­æ–‡ï¼š 
å°æ–¼æ‡‰ç”¨éšå±¤å¼éŸ»ï§˜æ¶æ§‹åŠå‹•æ…‹ï¥«ï¥©ä¹‹éŸ³é«˜é æ¸¬æ¨¡å‹ï¼Œåˆ†ç‚ºä¸‹ï¦œå››
é …ç ”ç©¶é‡é»ï¼š(1)éšå±¤å¼éŸ»ï§˜æ¶æ§‹çš„é æ¸¬åŠç”¢ç”Ÿï¼›(2)å°å…¥å‹•æ…‹ï¥«ï¥©
ç”Ÿæˆæ¼”ç®—æ³•æ–¼å„éŸ»ï§˜éšå±¤ï¼›(3)é‹ç”¨åˆ†ï§å›æ­¸æ¨¹åŠéš±è—å¼é¦¬å¯å¤«æ¨¡å‹
å»ºï§·å„å±¤éŸ»ï§˜æ¨¡å‹ï¼› (4)ï¥«ï¥©æå–ä½¿ç”¨ STRAIGHTï¼ˆ Speech 
Transformation and Representation based on Adaptive Interpolation of 
weiGHTed spectrogramï¼‰ã€‚ 
æŠ€è¡“ï¥¯æ˜ è‹±æ–‡ï¼š 
The goal of this research is to develop a pitch prediction model using 
prosody hierarchy structure and dynamic features and to investigate the 
improvement of naturalness for synthesized speech. More specifically, 
this research is aimed to: (1) Prediction and generation of prosody 
hierarchy structure; (2) Dynamic features for each hierarchical layer; 
(3) Building the pitch prediction model for each layer: CART for 
prosodic word and syllable level, HMM for frame level; (4) Feature 
analysis using STRAIGHT (Speech Transformation and Representation 
based on Adaptive Interpolation of weiGHTed spectrogram). 
å¯ï§ç”¨ä¹‹ç”¢æ¥­ 
åŠ 
å¯é–‹ç™¼ä¹‹ç”¢å“ 
è³‡è¨Šæª¢ï¥ªç”¢æ¥­ã€ï¥©ä½å…§å®¹ç”¢æ¥­ã€ï¥©ä½æ•™å­¸ç”¢æ¥­ã€ 
æƒ…ç·’èªéŸ³åˆæˆç³»çµ± 
æŠ€è¡“ç‰¹é» 
å°‡éŸ»ï§˜çµæ§‹ä»¥éšå±¤å¼çš„æ¶æ§‹æ‹†è§£ï¼Œæ ¹æ“šéšå±¤å¼çš„éŸ»ï§˜æ¶æ§‹ä½œéŸ³éŸ»ï¥«
ï¥©çš„ï¥¾æ¸¬ï¼Œä¸¦å°å…¥å‹•æ…‹ï¥«ï¥©æ–¼å„éŸ»ï§˜éšå±¤ï¥«ï¥©ç©ºé–“ï¼Œä»¥ä¿ï§çœŸå¯¦èª
éŸ³åœ¨æ™‚é–“åºï¦œä¸Šçš„è®ŠåŒ–åŠé—œï¦šæ€§ï¼›åœ¨å„å±¤éŸ³éŸ»æ¨¡å‹çš„è¨“ï¦–ä¸Šï¼Œåœ¨éŸ³
éŸ»è©åŠéŸ³ç¯€éšå±¤ä½¿ç”¨åˆ†ï§å›æ­¸æ¨¹æ¨¡å‹ï¼Œåœ¨éŸ³æ¡†éšå±¤ä½¿ç”¨éš±è—å¼é¦¬å¯
å¤«æ¨¡å‹ã€‚ 
æ¨å»£åŠé‹ç”¨çš„åƒ¹å€¼ 
åˆæˆå…·æœ‰é«˜è‡ªç„¶ï¨çš„èªéŸ³ï¼Œæ˜¯ç›®å‰æœ€æ–°çš„ç ”ç©¶ä¸»é¡Œï¼Œæ¡ç”¨ HMM ç‚º
åŸºç¤æ­é…éŸ³éŸ»é æ¸¬çš„ç ”ç©¶å…·æœ‰é«˜ï¨å¯èª¿æ€§å’Œï¦³æ´»ï¨ï¼Œä¸”å„²å­˜ç©ºé–“
å°ï¼Œåƒ…éœ€å„²å­˜ï¥«ï¥©å°‘ï¥¾èªï¦¾è¨“ï¦–å³å¯åˆæˆï¼Œæ˜¯ç›®å‰æœ€ä¸»ï§Šçš„åˆæˆå™¨
ç ”ç©¶æ–¹å‘ã€‚é€éèª¿æ•´æ¨¡å‹ï¥«ï¥©çš„æ–¹å¼ï¼Œå°‡æ©Ÿï¥¡æ¨¡å‹èª¿æ•´æˆç›®æ¨™èªè€…
çš„æ¨¡å‹ï¼Œä¸¦åˆæˆå‡ºç›®æ¨™èªè€…çš„è²éŸ³ã€‚é æœŸå°‡ï¤­å¯ä»¥æ‡‰ç”¨åˆ°åµŒå…¥å¼ç³»
çµ± Embedded System (æ‰‹æ©Ÿï¼ŒPDA ç­‰ï¨ˆå‹•è£ç½®ä¸Š)ã€‚ 
 
 
 2
3. PLEN-3: â€œExtending Signal Processing into New Application Areasâ€ James Truchard, National 
Instruments  
4. PLEN-4: â€œRecent Advances in Radar Imaging of Building Interiorsâ€ Moeness Amin, Villanova 
University  
 
äºŒã€ï¥«èˆ‡æœƒè­° 
æœ¬äºº3æœˆ15æ—¥ä¸Šåˆå‡ºåœ‹æ­æ©Ÿé£›æŠµé”èˆŠï¤Šå±±ï¼Œè€Œå¾Œè½‰å¾€é”ï¤¥æ–¯ï¼Œä¸¦æ–¼3æœˆ15æ—¥æŠµé”ï¤¥é”ï¤¥æ–¯
ä¹‹ï¦ƒï¨¬ï¼Œä¸¦ç†Ÿæ‚‰é”ï¤¥æ–¯åŠå¤§æœƒå ´åœ°ä¹‹ï§ºæ³ï¼Œæº–å‚™ç ”è¨æœƒä¹‹ï¥æ–‡ç™¼è¡¨å·¥ä½œã€‚å¤§æœƒæ–¼3æœˆ14æ—¥åŠ15
æ—¥èˆ‰ï¨ˆTutorialèª²ç¨‹ã€‚æœ¬äººæ­¤æ¬¡ï¥«åŠ IEEEåœ‹éš›è²å­¸èªéŸ³åŠä¿¡è™Ÿè™•ï§¤åœ‹éš›æœƒè­°(IEEE Signal 
Processing Society)å­¸æœƒç›¸é—œæœƒè­°åŠï¥æ–‡ç™¼è¡¨ï¼Œåˆ†è¿°å¦‚ä¸‹ï¼š 
 
3æœˆ16æ—¥ï¼šè¨»å†ŠåŠICASSP2010å¤§æœƒè­°ç¨‹(ä¸€) 
1. ï¥«èˆ‡å¤šå ´å£é ­åŠæµ·å ±ï¥æ–‡ç™¼è¡¨ä¸¦èˆ‡åœ‹å…§å¤–å­¸è€…å°ˆå®¶è¨ï¥ç ”ç©¶ç›¸é—œå•é¡Œã€‚ 
 
3æœˆ17æ—¥ï¼šICASSP2010å¤§æœƒè­°ç¨‹(äºŒ) 
1. ï¥«èˆ‡å¤šå ´å£é ­åŠæµ·å ±ï¥æ–‡ç™¼è¡¨ä¸¦èˆ‡åœ‹å…§å¤–å­¸è€…å°ˆå®¶è¨ï¥ç ”ç©¶ç›¸é—œå•é¡Œã€‚ 
2. ï¥«åŠ å¤§æœƒæ™šå®´ã€‚ 
 
3æœˆ18æ—¥ï¼šICASSP2008å¤§æœƒè­°ç¨‹(ä¸‰) 
1. ï¥«èˆ‡å¤šå ´å£é ­åŠæµ·å ±ï¥æ–‡ç™¼è¡¨ä¸¦èˆ‡åœ‹å…§å¤–å­¸è€…å°ˆå®¶è¨ï¥ç ”ç©¶ç›¸é—œå•é¡Œã€‚ 
2. ç™¼è¡¨ï¥æ–‡ â€œPronunciation Variation Generation for Spontaneous Speech Synthesis 
Using State-Based Voice Transformationâ€ 
 
3æœˆ19æ—¥ï¼šICASSP2008å¤§æœƒè­°ç¨‹(å››)  
1. å¤§æœƒæ­£å¼è­°ç¨‹æœ€å¾Œä¸€å¤©ã€‚ 
 
3æœˆ20æ—¥ï¼šæ­æ©Ÿè¿”å°ã€‚ 
 
ä¸‰ã€æ”œå›è³‡ï¦¾ 
1. 2010 International Conference on Acoustics, Speech, and Signal Processing (ICASSP2010)ä¹‹ï¥
æ–‡é›†å…‰ç¢Ÿä¸€ç‰‡åŠå¤§æœƒæ‰‹å†Šå„ä¹™ä»½ã€‚ 
 
å››ã€å¿ƒå¾— 
1. ç•¶æˆ‘åœ‹å­¸è¡“æ°´æº–é”åˆ°æŸå±¤æ¬¡ä¹‹å¾Œï¼Œå¦‚ä½•é€²å…¥å¤§æœƒæˆ–åœ‹éš›çµ„ç¹”æ ¸å¿ƒè®Šæˆæ¯”ç™¼è¡¨ï¥æ–‡ï¤é‡
è¦ã€‚ 
2. æ­¤æ¬¡æœƒè­°è­°é¡Œç›¸ç•¶å»£æ³›ï¼Œå¤§å®¶ï¥«èˆ‡æœƒè­°æ™‚è¨ï¥äº¦éå¸¸ç†±ï¦Ÿï¼Œå¤§æœƒå®‰æ’å¤šå ´æµ·å ±å¼ç™¼è¡¨ï¼Œ
ä½¿å¾—å¤§å®¶å¯ä»¥é‡å°è‡ªå·±èˆˆè¶£ä¹‹ä¸»é¡Œå……åˆ†èˆ‡ç™¼è¡¨è€…æ·±å…¥è¨ï¥ï¼Œå› æ­¤æ”¶ç²ç›¸ç•¶å¤šã€‚ 
3. æ­¤æ¬¡æœƒè­°å°ç£ï¥«åŠ ä¹‹äººå“¡ç´„åå¹¾ä½ï¼Œå¯ï¥¯ç›¸ç•¶ç†±ï¦Ÿï¼Œä¹Ÿç”±æ–¼æœ‰å¤šäººï¥«æœƒï¼Œå¤§å®¶äº¦å¯äº¤æ›
å¿ƒå¾—åŠï¦ºè§£ç›®å‰åœ¨å„å€‹ï¦´åŸŸä¹‹ç ”ç©¶æƒ…å½¢ã€‚ 
4. ç¶“ç”±é€™æ¬¡æœƒè­°çš„ï¥«èˆ‡ï¼Œï¥§ä½†å¾—ä»¥èªï§¼ä¸€äº›ç›¸é—œï¦´åŸŸä¹‹å­¸è€…ï¼Œäº’ç›¸äº¤æ›ç ”ç©¶å¿ƒå¾—ï¼Œè€Œä¸”å¯
å¸æ”¶æœ€æ–°è³‡è¨Šï¼Œå°æ—¥å¾Œè¨ˆåŠƒä¹‹åŸ·ï¨ˆå°‡æœ‰æ‰€åŠ©ï¨—ã€‚ 
spontaneous speech; this presents the deletion effect of 
normal speech to the spontaneous speech. On the other 
hand, the horizontal lines (between the 3rd red line and 2nd
yellow line) show the insertion effect of normal speech to 
the spontaneous speech. The syllable boundaries of 
spontaneous speech with pronunciation variation can be 
identified by the aligned boundaries of normal speech 
using DTW. 
3. TRANSFORMATION FUNCTION MODELING 
The purpose of this study is to construct the 
transformation functions between the data samples of the 
read speech and spontaneous speech with pronunciation 
variations. Fig. 2 shows the flowchart of the proposed 
method.  
Figure 2: Flowchart of the proposed method. 
In the training phase, a parallel speech database is 
collected, and the feature vectors, consisting of spectrum 
and duration parameter vectors, are extracted for HMM 
training. The STRAIGHT algorithm is adopted to extract 
the spectral features. The transformation function is 
derived using the trained HMM for transforming from 
normal phone to variation phone state-by-state. The 
measure used for transformation function classification is 
estimated from the articulatory features to construct a 
transformation function classification and regression tree 
(F-CART) for spectrum transformation function selection. 
For each state, the state durations are determined and 
classified using the articulatory features to construct a 
duration classification and regression tree (D-CART) for 
duration scaling. In the synthesis phase, the linguistic and 
articulatory features of the input text are extracted for 
model selection from the F-CART and D-CART. The 
normal (read-style) speech is synthesized from the input 
text by HMM-based TTS system (HTS). Finally, 
spontaneous speech with pronunciation variation is 
synthesized from the converted mel-cepstral coefficients 
and scaled durations obtained from the F-CART and D-
CART using the MLSA filter. 
In transformation function modeling, the parallel 
observation sequences from normal and variation speech 
are used for training. The sequences X={x1, x2,â€¦, xN }and 
Y={y1, y2,â€¦, yN} are the parallel source (normal speech) 
and target (variation speech) feature (MFCC) sequences of 
N frames, respectively. A multi-dimensional linear 
regression model is adopted as the state-based 
transformation function for converting the normal speech 
X to the variation speech Y: 
 Y AX R         (1) 
where A is the linear transformation matrix and the 
residual matrix R is determined as the difference between 
the converted spectrum through transformation matrix A
and the target spectrum. In the training phase, a statistical 
modelÓ³ for the normal and variation speech features, 
which is estimated to maximize the likelihood function of 
their joint distribution, is modeled as [6]: 
   
0 1
1
, , , ( , ),
t t t
T
q q q q t t
q q t
P P q a bO O S      Â¦ Â¦ Â–X Y X Y x y       (2) 
( , ) ( ) ( )
( ) ( ; , )
( ) ( ; , )
j t t j t t j t
j t t t j t j j
x x
j t t j j
b b b
b N
b N
 
 
 
y
x y y x x
y x y A x R Èˆ
x x È Èˆ
       (3) 
where q={q1,q2,â€¦,qT} denotes the T-state sequence shared 
by the source and target feature streams;Ó¸is the initial 
probability; a is the transition probability from state qt-1 to 
qt and bj(.) means the state observation probability density 
function (PDF) for state j.  ; ,N x È Èˆ  represents the 
Gaussian distribution of x with mean vectorÊ³ Ó´ and
covariance matrixÊ³Ó¢. The parameters for each state can be 
estimated using the EM algorithm [7]. The re-estimation 
formulas for state j can be derived as [6]: 
1
1 ( )
T
x
j t t
t
r j
T  
c  Â¦È x         (4) 
1
1 ( )( )( )
T
x x x T
j t t j t j
t
r j
T  
c c c  Â¦Èˆ x È x È        (5) 
1
1 1
( )( ) ( )
T T
T T
j t t j t t t t
t t
r j r j

  
Â§ Â·Â§ Â·c  Â¨ Â¸Â¨ Â¸Â© Â¹Â© Â¹Â¦ Â¦A y R x x x       (6) 
1
1
( )( )
( )
T
t t j t
t
j T
t
t
r j
r j
 
 
c
c  
Â¦
Â¦
y A x
R
        (7) 
1
1
( )( )( )
( )
T
T
t t j t j t j t j
y t
j T
t
t
r j
r j
 
 
c c c c   
c  
Â¦
Â¦
y A x R y A x R
Èˆ
      (8) 
whereÍ‹ t(j) denotes the state occupancy probability of 
4827
For objective evaluation, the variation phone in the 
parallel speech corpus is considered as the target feature 
vector. Three different variation generation methods were 
adopted for comparison as follows:  1) MLLR adaptation 
from normal phone to variation phone; 2) duration scaling; 
3) phone-based variation transformation function. 
Experiments were conducted on the comparisons of the 
proposed state-based voice transformation method, 
adaptation method, duration scaling and phone-based 
pronunciation variation transformation methods. Fig. 3 
shows the MSE of all the collected speech data at the 
phone level for the four different methods. The analytical 
results indicate that the state-based pronunciation variation 
generation method can achieve lower MSE than the others. 
Figure 3: MSEs of different variation generation methods 
5.2 Evaluation via Formal Listening 
For subject evaluation, the speech spontaneity and 
quality of the proposed method were assessed based on 
formal listening test. Fifteen listeners were asked to 
compare each synthesized sentence with the conventional 
HTS, MLLR adaptation and the proposed method. The 
listeners were asked to give a score from one to five, with 
one for â€œbadâ€ and five for â€œexcellentâ€. Fig. 4 presents the 
results from the listening test.  
Figure 4: MOS Listening test 
The quality of the synthesized speech using HTS are 
generally better than that using adaptation and state-based 
transformation methods. The reason is because the speech 
quality directly synthesized by HTS without conversion 
was not affected by the errors from adaptation or 
transformation. However, the synthesized speech using the 
proposed state-based transformation method obtains much 
better spontaneity than the read speech synthesized by the 
HTS. In addition, the proposed state-based method only 
slightly degrades the speech quality compared to that 
generated from HTS. The MLLR-based adaptation method 
cannot obtain good speech quality as well as improve the 
spontaneity of the synthesized speech. 
6. CONCLUSIONS 
This study presented a method to improve the 
spontaneity property in conventional HMM-based speech 
synthesis. The pronunciation variation can be modeled by 
a linear transformation function trained from the parallel 
normal and variation speech data. The acoustic and 
articulatory features are considered for transformation 
function clustering to construct a function selection CART 
(F-CART) and a duration scaling CART (D-CART). The 
F-CART and D-CART are then adopted to select 
appropriate transformation function and duration 
information using the articulatory features, respectively. 
From the evaluation results, the proposed state-based 
transformation method can generate the pronunciation 
variations to effectively improve the spontaneity in 
spontaneous speech synthesis compared to the 
conventional HMM-based HTS and the MLLR-based 
adaptation methods. 
7. REFERENCES 
[1] C.-H. Wu, C.-C. Hsia, J.-F. Chen, and J.-F. Wang, 
â€œVariable-Length Unit Selection in TTS Using Structural 
Syntactic Cost,â€ IEEE Trans. Audio, Speech, and Language 
Processing, Vol. 15, No. 4, pp.1227-1235, May 2007. 
[2] H. Zen, K. Tokuda, T. Masuko, T. Kobayashi, and T. 
Kitamura, â€œHidden Semi-Markov Model Based Speech 
Synthesis System,â€ IEICE Trans. on Information Systems,
vol.E90-D, no.5, pp.825-834, May 2007. 
[3] C.-C. Hsia, C.-H. Wu., and J.-Q.Wu, â€œConversion Function 
Clustering and Selection Using Linguistic and Spectral 
Information for Emotional Voice Conversion,â€ IEEE Trans. 
Computers, Vol. 56, No. 9, pp. 1225~1233, Sep. 2007.  
[4] http://mmc.sinica.edu.tw/mcdc_e.htm The Mandarin 
Conversational Dialogue Corpus (MCDC). 
[5] H. Kawahara, â€œSpeech representation and transformation 
using adaptive interpolation of weighted spectrum: vocoder 
revisited,â€ In Proc. of ICASSP1997, pp. 1303â€“1306, 
Munich, Germany, 1997.  
[6] Z. H. Ling, K. Richmond, J. Yamagishi, and R. H. Wang. 
â€œArticulatory control of HMM-based parametric speech 
synthesis driven by phonetic knowledgeâ€œ. In Proc. 
Interspeech, pp.573-576, Brisbane, Australia, Sep. 2008.
[7] A. P. Dempster, N. M. Laird and D. B. Rubin, â€œMaximum 
Likelihood from Incomplete Data via the EM Algorithm,â€ 
J. R. Statist. Soc. B, vol. 39, pp. 1-38, 1977. 
[8] L. H. Cai, D. D. Cui, and R. Cai, â€œTH-CoSS, a Mandarin 
Speech Corpus for TTS,â€ Journal of Chinese Information 
Processing, vol. 21, no. 2, pp. 94-99, Mar. 2007.
4829

å…¶ä»–æˆæœ 
(ç„¡æ³•ä»¥é‡åŒ–è¡¨é”ä¹‹æˆ
æœå¦‚è¾¦ç†å­¸è¡“æ´»å‹•ã€ç²
å¾—çé …ã€é‡è¦åœ‹éš›åˆ
ä½œã€ç ”ç©¶æˆæœåœ‹éš›å½±éŸ¿
åŠ›åŠå…¶ä»–å”åŠ©ç”¢æ¥­æŠ€
è¡“ç™¼å±•ä¹‹å…·é«”æ•ˆç›Šäº‹
é …ç­‰ï¼Œè«‹ä»¥æ–‡å­—æ•˜è¿°å¡«
åˆ—ã€‚) 
ç„¡ 
 æˆæœé …ç›® é‡åŒ– åç¨±æˆ–å…§å®¹æ€§è³ªç°¡è¿° 
æ¸¬é©—å·¥å…·(å«è³ªæ€§èˆ‡é‡æ€§) 0  
èª²ç¨‹/æ¨¡çµ„ 0  
é›»è…¦åŠç¶²è·¯ç³»çµ±æˆ–å·¥å…· 0  
æ•™æ 0  
èˆ‰è¾¦ä¹‹æ´»å‹•/ç«¶è³½ 0  
ç ”è¨æœƒ/å·¥ä½œåŠ 0  
é›»å­å ±ã€ç¶²ç«™ 0  
ç§‘ 
æ•™ 
è™• 
è¨ˆ 
ç•« 
åŠ  
å¡« 
é … 
ç›® è¨ˆç•«æˆæœæ¨å»£ä¹‹åƒèˆ‡ï¼ˆé–±è½ï¼‰äººæ•¸ 0  
 
