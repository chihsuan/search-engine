already been segmented into semantically coherent seg-
ments. Besides, satisfying results may not be obtained
from video with a sound track containing more than
just speech (such as music and environmental sound),
or video clip which is silent. In the third category,
emphasis is put on the integration of visual and au-
dio features[6, 7, 8, 9]. This is because multi modal
features can provide more information to resolve ambi-
guities that are present in a single modality. But, it is
computationally expensive.
II. MOTIVATION
As the definition of violence is ambiguous, violence
detection is a subjective process. In this work, we de-
fine violence from an empirical viewpoint. Our defini-
tion of violence is: a series of human actions accompa-
nying with bleeding. Thus, the task of violence scene
detection can be decomposed into two processes: ac-
tion scene detection and bloody frame detection. Be-
fore describing the details of the proposed approach, we
review the hierarchical structure of video. A video is
physically formed by shots and semantically described
by scenes. A shot is a sequence of frames that was
continuously captured by the same camera. A scene
is basically a story unit and consists of a small num-
ber of interrelated shots that are consecutive or not.
Shots in video are analogous to words in language as
they convey little semantic information in isolation. On
the other hand, scenes allow event to be fully presented
and reflect the dramatic and narrative structure of a
video. It is noted that previous approaches on violent
scene detection only work on shot level and they do not
fully exploit the information contained in video struc-
ture. In this paper, we propose a novel technique based
on the construction of high-level video structure. To
be simple yet effective, only visual features of video are
used. Support vector machine (SVM) is also employed
to classify the extracted scenes into action scenes and
non-action scenes. The proposed approach is made up
of four main steps
(1) Segmentation of video into shots.
(2) Grouping of shots into scene.
(3) Action scene detection using SVM.
(4) Bloody frame detection.
where step (1) and (2) are accomplished by our previous
work[10]. The details of step (3) and (4) are described
in section III and IV respectively.
III. ACTION SCENE DETECTION USING SVM
For each scene of the video sequence, some features
are extracted. Then, a binary classifier (SVM) is em-
ployed to detect the action scene[11]. The details are
described in the following two subsections.
A. Overview of Support Vector Machines
Unlike traditional learning techniques such as neu-
ral networks which minimize the empirical training er-
ror, the support vector machines (SVMs) are based on
the structural risk minimization principle. The basic
idea is closely related to regularization[12]: for a finite
set of training samples, the search for the best model
or approximating function has to be constrained by an
appropriately small hypothesis space. If the space is
too large, functions can be found which fit exactly the
training data, but they will have poor generalization
capabilities on new test data. Instead, the minimiza-
tion of the structural risk is equivalent to minimizing
the sum of the error on the training set and the com-
plexity of the hypothesis space, expressed in terms of
VC-dimension. Consequently, the solutions obtained
with SVMs are more likely to generalize well on new
data points. We now go through the SVMs for a two-
class classification problem. For a comprehensive and
rigorous account of SVMs, please see[13].
Given a training set S = {(x1, y1), Â· Â· Â· , (xn, yn)},
where feature vector xi âˆˆ Rd and label yi âˆˆ {1,âˆ’1},
the goal of SVM is to construct a hyperplane that max-
imizes the margin while minimizing a quantity propor-
tional to the misclassification error. The optimal sepa-
rating hyperplane wâˆ— Â· x + bâˆ— = 0 can be found under
the following constraints:
min
w,b,Î¾
1
2
w Â·w +C
nâˆ‘
i=1
Î¾i
subject to
yi(w Â· xi + b) â‰¥ 1âˆ’ Î¾i, Î¾i â‰¥ 0, i = 1, Â· Â· Â· , n
where C is the penalty parameter that controls the
tradeoff between the margin and the misclassification
errors Î¾ = (Î¾1, Â· Â· Â· , Î¾n). Here training vectors xi are
mapped into a higher dimensional space by the func-
tion Ï†, and K(xi, xj) â‰¡ Ï†(xi) Â·Ï†(xj) is called the kernel
function. This is a quadratic programming problem
2
IV BLOODY FRAME DETECTION
For each detected action scene, we further check
whether it contains the bloody frame. As scene is com-
posed of several shots, key frames can be extracted from
each shot. Key frame is the frame which can represent
the salient content of the shot. Currently we choose
the middle frame of each shot as key frame. Then, we
determine whether one of these key frames has bloody
content. We first identify the existence of human and
blood in the key frame.
Face detection is the natural and convenient way
to determine whether human appear in the frame im-
age. We adopt the excellent face detection algorithm
proposed by Viola and Jones[17]. This algorithm is
capable of processing images extremely rapidly while
achieving high detection rates. Figure 1 shows some
face detection results.
There are some sophisticated techniques to detect
specific color object such as flame and skin. However,
for the blood pixel detection task, the easiest method
is to define blood-color cluster decision boundaries for
RGB color space. Ranges of threshold values for each
color space component are defined and the image pixel
with values that fall within these predefined ranges is
detected as blood pixel. According to our observation,
there are two types of blood color: bright red and dark
red. The ranges of RGB color space are defined as (1)
170 > R > 80 & G < 5 & B < 5 (2) 200 > R > 120
& G < 90 & B < 90 & abs(G âˆ’ B) < 8, respectively.
Figure 2 shows the blood detection result.
Then, a connected components analysis is per-
formed to group these detected blood pixels into several
potential blood regions. A size filter is used to elimi-
nate some potential blood regions which are too small
or too large. Small region may result from noise and
large region may correspond to some large object such
as red wall. As we observe from violent movies that
blood and human body always come up with fast mo-
tion, we integrate blood, face and motion informations
to determine whether a key frame is a bloody frame. A
key frame is detected as bloody frame if
â€¢ It contains both face and blood regions.
Table 1: Accuracy measures for four test movies.
Movie Violent Correct Missed False
ID No. Scenes Detection Detection Detection
(1) 17 14 3 0
(2) 10 8 2 0
(3) 18 17 1 0
(4) 12 10 2 0
â€¢ The distance between face and blood regions is less
than a threshold.
â€¢ The average motion intensity of each blood and
face region is greater than a threshold.
If an action scene contains at least one bloody frame
then it is detected as a violent scene.
V EXPERIMENTAL RESULTS
Four movies in MPEG-1 format are used in our
experiment: (1)â€œKill Bill: Vol. 1â€, (2)â€œGladiatorâ€,
(3)â€œThe Passion of the Christâ€ and (4)â€œFirst Blood 4:
John Ramboâ€. The ground truth of test movies, i.e.,
the decision whether a given scene is violent scene or
not, is determined by human subjects. Some violent
scenes of movies are shown in Figure 3.
The experimental results are shown in Table 1.
The missed detection is due to two factors. One is that
the actor(actress) wears mask so that the face detec-
tion algorithm fails. The other is that the face part of
actor(actress) does not appear in the key frame. Figure
4 shows two missed detection cases. Although there is
no false detection in our experiment, one scenario that
causes false detection is a group of persons in red cloth-
ing playing basketball.
VI. CONCLUSION
We have presented an effective method for auto-
matically detecting violent scene in the digital movies.
While previous approaches addressed on shot level
of video structure only, our approach construct more
4
 6
   
 
 
            Figure 1: Some face detection results. 
 
 
                     (a) 
 
                     (b) 
 
Figure 2: (a) is the original image (b) is the detected blood pixels. 
 
 
 
 
 
 8
 
 
 
 
 
Figure 4: Some missed detection cases. 
FLASHLIGHT DETECTION IN VIDEO
Liang-Hua Chen and Bi-Cheng Hsu
Department of Computer Science
and Information Engineering
Fu Jen University
Hsinchuang, Taipei, Taiwan
Hong-Yuan Mark Liao
Institute of Information Science
Academia Sinica
Nankang, Taipei, Taiwan
ABSTRACT
Shot boundary detection is a fundamental step of video in-
dexing. One crucial issue of this step is the discrimination
of abrupt shot change from flashlight, because flashlight of-
ten induces a false shot boundary. Support vector machine
(SVM) is a supervised learning technique for data classifi-
cation. In this paper, we propose a SVM based technique
to detect flashlights in video. Our approach to flashlight
detection is based on the facts that the duration of flash-
light is short and the video contents before and after a flash-
light should be similar. So we design a sliding window in
temporal domain to monitor the instantaneous video vari-
ation and extract color and edge features to compare the
visual contents between two video segments. Then, a sup-
port vector machine is employed to classify the luminance
variation into flashlight or shot cut. Experimental results
indicate that the proposed approach is effective and outper-
forms some existing techniques.
KEY WORDS
machine.
1 Introduction
The advances in low cost mass storage devices, higher
transmission rates and improved compression techniques,
have led to the widespread use and availability of digital
video. Video data offers users of multimedia systems a
wealth of information and also serves as a data source in
many applications including digital libraries, publishing,
entertainment, broadcasting and education. However, be-
cause of the large amounts of data and unstructured format,
efficient access to video is not an easy task. To make the
original video data in the database available for browsing
and retrieval, they must be analyzed, indexed and reorga-
nized. A shot is a sequence of frames that was continuously
captured by same camera. It can serve as the fundamental
indexing unit. So shot boundary detection (or video seg-
mentation) is a crucial step in many content-based video
browse and retrieval applications. In general, shot bound-
aries can be detected by employing a metric to measure the
difference between two consecutive frames. This is based
on the fact the content remains nearly the same in an iden-
tical shot. If the difference is larger than a certain thresh-
old, it is most likely that the two frames belong to different
shots. Many efforts have been devoted to shot boundary
detection and many metrics have been proposed[1]. The
main problem of these techniques is that they are sensitive
to flashing light scene where great illumination variation
lasts for a few frames. This usually leads to false detection
of shot boundary since the metric also increases abruptly.
To achieve robust shot boundary detection, there is a need
to differentiate flashlight effect from abrupt shot change.
Another motivation for flashlight detection is the
study of its role as expressive element in movies. To nar-
rate a good story that warrants repeated audience view-
ing, movie directors use many cinematic elements to en-
hance the emotional experience of the viewer. One such
element that directors manipulate is lighting. Flashlights
are often used by directors to amplify a crisis situation, in-
tensify excitement, increase a sense of foreboding or in-
dicate the presence of a supernatural power. Truong and
Venkatesh propose an algorithm to identify five types of
dramatic events in feature films that are related to the ap-
pearance of flashlights[2].
Depending on the visual features used, the existing
techniques of flashlight detection can be classified into two
categories: color based [3, 4, 5, 2, 6] and edge based
[7, 8, 9, 10]. Color histogram is the most commonly used
color feature representation, because it is easy to compute
and robust to object motion. Assuming that the luminance
and chrominance will return to their original states after a
flashlight effect, the correlation between frames before and
after a flashlight should be high. Nakajima et al.[4] use
chrominance histogram correlation as a measure to detect
flashlight. Yeo and Liu[3] propose a method that is based
on the successive frame difference and global color statis-
tic comparison. Flashlight can be detected by the presence
of two close maxima in frame differences which are much
greater than the average value of the rest within a sliding
window. Zhang et al.[5] distinguish flashlights from real
shot cut by proposing a â€˜cut modelâ€™ and a â€˜flashlight modelâ€™
which are defined based on the temporal property of av-
erage intensity value across a frame sequence in a sliding
window (see Figure 1). On the other hand, edge feature
is insensitive to illumination variation to a certain extent.
The edge-based approaches assume that the edge pixels re-
main in the approximately same position for frame pairs
with flashlight effect, and occur in random fashion between
Video content analysis, supervised learning, support vector
696-066 133
the choice of color space and quantization of color space.
HSV (hue, saturation, value) color space is chosen since
it is nearly perceptually uniform. Since the human visual
system is more sensitive to the hue than to the saturation
and the value (intensity)[19], H should be quantized finer
than S and V . In our implementation, hue is quantized into
20 bins. Saturation and value are each quantized into 10
bins. This quantization provides 20Ã— 10Ã— 10 distinct col-
ors (bins), and each bin with non-zero count corresponds to
a color object.
Since we are interested in the whole video segment
rather than single image frame, only one histogram is used
to count the color distribution of all image frames within
a video segment. Then, each bin of the resulting his-
togram is divided by the frame number of a video seg-
ment to obtain the average color histogram. Histogram in-
tersection is a popular similarity measure for color-based
image matching[20]. It yields the number of pixels that
have same color in two images. In our work[21, 22], we
extend this idea to the matching of video segments. Let
A,B be the set of all color objects in video segment S1 and
S2 respectively, for a given u âˆˆ A, its similar color ob-
ject in B is some v âˆˆ B such that ||u âˆ’ v|| < , where
||u âˆ’ v|| denotes the Euclidean distance between u and v
in the HSV color space and  is a threshold ( is set to
be 3.). Then, (u, v) is called a similar color pair. Let
â„¦ = {(u, v)|(u, v) âˆˆ AÃ—B, (u, v) is a similar color pair},
the similarity measure for color feature between S1 (with
the average color histogram HC1) and S2 (with the average
color histogram HC2) is defined as
Sim C(S1, S2) =
1
|â„¦|
âˆ‘
(u,v)âˆˆâ„¦
min(HC1(u),HC2(v))
(1)
where normalization factor |â„¦| is the size of set â„¦ and
min(.) operator corresponds to histogram intersection.
Next, several spatial features are calculated to char-
acterize the distribution state of each color object in each
image frame[21, 22]. Assuming a set of pixels S =
{(x1, y1), Â· Â· Â· , (xn, yn)} belong to color object ci, k is the
image size and m is the total number of 4-connected (non-
isolated) pixels in S. Then, we define
(i) density of distribution
fi1 =
n
k
(ii) compactness of distribution
fi2 =
m
n
(iii) scatter
fi3 =
1
n
âˆš
k
nâˆ‘
j=1
âˆš
(xj âˆ’ xÂµ)2 + (yj âˆ’ yÂµ)2
where xÂµ = 1n
âˆ‘n
i=1 xi and yÂµ = 1n
âˆ‘n
i=1 yi
After the spatial features of all image frames within a
video segment are computed, we take average of these val-
ues respectively. Let fi1, fi2 and fi3 be the average feature
values of color object ci within a video segment, for two
color objects ci and cj , the difference of spatial distribution
within a video segment is defined as
Ds(ci, cj) =
1
3
(|fi1 âˆ’ fj1|+ |fi2 âˆ’ fj2|+ |fi3 âˆ’ fj3|)
The similarity measure for the spatial distribution of
color objects between S1 and S2 is defined as
Sim S(S1, S2) =
1
|â„¦|
âˆ‘
(u,v)âˆˆâ„¦
1
1 +Ds(u, v)
(2)
Psychophysical experiments have shown that human
visual system is sensitive to the sharp intensity variation re-
gions of the image such as edges. When lighting condition
changes, the contours of the objects in the image change
little and the edge map remains nearly the same. In our
system, the Canny edge detector is used to extract edges
from an image. The detected edge image is partitioned into
a set of M Ã—M regions. Next, each region is subdivided
into N Ã—L blocks. A block is active, if the number of edge
points in this block is greater than a threshold (=30, in our
setting). The ratio of active blocks of a region is defined
as the edge density of the region. Then, we compute the
edge density of each region and its average value over the
whole video segment. Let E1i and E2i be the average edge
density of the ith region for video segment S1 and S2 re-
spectively, the edge similarity between two video segments
is determined by
Sim E(S1, S2) =
1
M2
M2âˆ‘
i=1
min(E1i, E2i) (3)
Currently, we set M = 3,N = 5 and L = 7.
4 The Proposed Algorithm
The proposed algorithm consists of two parts. The first part
locates the potential shot change frames in the video se-
quence. The second part distinguishes the flashlight from
the real shot boundary (cut) by the fact that the video con-
tents before and after flashlight are similar. Using the pro-
posed similarity measures for video segments, three clas-
sification features (in terms of color, spatial and edge) are
obtained. Then, a support vector machine is employed to
classify the potential shot change frame into flashlight or
shot boundary.
We scan a video sequence by applying a sliding win-
dow that spans 2m+1 frames. Given a sequence of frames
f1, f2, . . . , fn, a potential shot change frame is detected if
the average intensity difference of two consecutive frames
is larger than a threshold. Currently, the threshold is set to
be 5. Assuming fk is the detected potential shot change
frame, the flashlight detection process is performed as fol-
lows:
135
Table 2. Performance comparison for flashlight detection.
Method Recall Precision
Our approach 96.74% 97.8%
Zhangâ€™s approach 93.48% 94.51%
Hengâ€™s approach 88.04% 91.01%
[2] B.T. Truong and S. Venkatesh. Determining dramatic
intensification via flashing lights in movies. In Pro-
ceedings of International Conference on Multimedia
and Expo, pages 61â€“64, Tokyo, January 2001.
[3] B.L. Yeo and B. Liu. Rapid scene analysis on com-
pressed videos. IEEE Transactions on Circuits and
Systems for Video Technology, 5(6):533â€“544, Decem-
ber 1995.
[4] Y. Nakajima, K. Ujihara, and A. Yoneyama. Universal
scene change detection on MPEG-coded data domain.
In Proceedings of SPIE Conference on Visual Com-
munication and Image Processing, pages 992â€“1003,
1997.
[5] D. Zhang, W. Qi, and H.J. Zhang. A new shot bound-
ary detection algorithm. In Proceedings of IEEE Pa-
cific Rim Conference on Multimedia, pages 63â€“70,
2001.
[6] X. Qian, G. Liu, and R. Su. Effective fades and
flashlight detection based on accumulating histogram
difference. IEEE Transactions on Circuits and Sys-
tems for Video Technology, 16(10):1245â€“1258, Octo-
ber 2006.
[7] R. Zabih, J. Miller, and K. Mai. A feature-based al-
gorithm for detecting and classifying scene breaks.
In Proceedings of ACM International Conference on
Multimedia, pages 189â€“200, San Francisco, Novem-
ber 1995.
[8] D. Li and H. Lu. Lighting change problem in shot
detection. In Proceedings of IEEE International Con-
ference on Electronics, Circuits and Systems, pages
541â€“544. Volume 1, December 2000.
[9] W.J. Heng and K.N. Ngan. High accuracy flashlight
scene determination for shot boundary detection. Sig-
nal Processing: Image Communication , 18(3):203â€“
219, 2003.
[10] Y. Geng and D. Xu. A solution to illumination vari-
ation problem in shot detection. In Proceedings of
IEEE Region 10 Conference, pages 81â€“84. Volume 2,
November 2004.
[11] K. Jonsson, J. Kittler, Y.P. Li, and J. Matas. Support
vector machines for face authentication. Image and
Vision Computing , 20(5-6):369â€“375, April 2002.
[12] G. Guo, S.Z. Li, and K.L. Chan. Support vector ma-
chines for face recognition. Image and Vision Com-
puting, 19(9-10):631â€“638, August 2001.
[13] M. Pontil and A. Verri. Support vector machines for
3D object recognition. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20(6):637â€“
646, June 1998.
[14] C. Bahlmann, B. Hassdonk, and H. Burkhardt. On-
line handwriting recognition with support vector ma-
chine â€“ a kernel approach. In Proceedings of Interna-
tional Workshop on Frontiers in Handwriting Recog-
nition, pages 49â€“54, Ontario, Canada, 2002.
[15] A. Ganapathiraju, J.E. Hamaker, and J. Picone. Appli-
cations of support vector machines to speech recog-
nition. IEEE Transactions on Signal Processing,
52(8):2348â€“2355, August 2004.
[16] S. Tong and E. Chang. Support vector machine active
learning for image retrieval. In Proceedings of ACM
International Conference on Multimedia , pages 107â€“
118, 2001.
[17] T. Evgenious, M. Pontil, and T. Poggio. A unified
framework for regularization networks and support
vector machines. A.I. Memo 1654, MIT, Cambridge,
MA, 1999.
[18] V.N. Vapnik. StatisticalLearning Theory . Wiley, New
York, 1998.
[19] X. Wan and C.-C. Jay Kuo. A new approach to im-
age retrieval with hierarchical color clustering. IEEE
Transactions on Circuits and Systems for Video Tech-
nology, 8(5):628â€“643, September 1998.
[20] M.J. Swain and D.H. Ballard. Color indexing. In-
ternational Journal of Computer Vision , 7(11):11â€“32,
1991.
[21] L.-H. Chen, K.-H. Chin, and H.-Y. Liao. An inte-
grated approach to video retrieval. In Proceedings
of the 19th Australasian Database Conference , pages
49â€“55, January 2008.
[22] L.-H. Chen, Y.-C. Lai, and H.-Y. liao. Movie scene
segmentation using background information. Pattern
Recognition, 41(3):1056â€“1065, March 2008.
[23] C.W. Hsu, C.C. Chang, and C.J. Lin. A practical
guide to support vector classification. Technical re-
port, Department of Computer Science, National Tai-
wan University, Taipei,Taiwan, 2003.
137
ç„¡ç ”ç™¼æˆæžœæŽ¨å»£è³‡æ–™ 
å…¶ä»–æˆæžœ 
(ç„¡æ³•ä»¥ï¥¾åŒ–è¡¨é”ä¹‹æˆ
æžœå¦‚è¾¦ï§¤å­¸è¡“æ´»å‹•ã€ç²
å¾—çŽé …ã€é‡è¦åœ‹éš›åˆ
ä½œã€ç ”ç©¶æˆæžœåœ‹éš›å½±éŸ¿
ï¦ŠåŠå…¶ä»–å”åŠ©ç”¢æ¥­æŠ€
è¡“ç™¼å±•ä¹‹å…·é«”æ•ˆï¨—äº‹
é …ç­‰ï¼Œè«‹ä»¥æ–‡å­—æ•˜è¿°å¡«
ï¦œã€‚) 
ç„¡ 
 æˆæžœé …ç›® ï¥¾åŒ– åç¨±æˆ–å…§å®¹æ€§è³ªç°¡è¿° 
æ¸¬é©—å·¥å…·(å«è³ªæ€§èˆ‡ï¥¾æ€§) 0  
èª²ç¨‹/æ¨¡çµ„ 0  
é›»è…¦åŠç¶²ï¤·ç³»çµ±æˆ–å·¥å…· 0  
æ•™æ 0  
èˆ‰è¾¦ä¹‹æ´»å‹•/ç«¶è³½ 0  
ç ”è¨Žæœƒ/å·¥ä½œåŠ 0  
é›»å­å ±ã€ç¶²ç«™ 0  
ç§‘ 
æ•™ 
è™• 
è¨ˆ 
ç•« 
åŠ  
å¡« 
é … 
ç›® è¨ˆç•«æˆæžœæŽ¨å»£ä¹‹ï¥«èˆ‡ï¼ˆé–±è½ï¼‰äººï¥© 0  
åœ‹ç§‘æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•«æˆæžœå ±å‘Šè‡ªè©•è¡¨ 
è«‹å°±ç ”ç©¶å…§å®¹èˆ‡åŽŸè¨ˆç•«ç›¸ç¬¦ç¨‹ï¨ã€é”æˆé æœŸç›®æ¨™æƒ…æ³ã€ç ”ç©¶æˆæžœä¹‹å­¸è¡“æˆ–æ‡‰ç”¨åƒ¹
å€¼ï¼ˆç°¡è¦æ•˜è¿°æˆæžœæ‰€ä»£è¡¨ä¹‹æ„ç¾©ã€åƒ¹å€¼ã€å½±éŸ¿æˆ–é€²ä¸€æ­¥ç™¼å±•ä¹‹å¯èƒ½æ€§ï¼‰ã€æ˜¯å¦é©
åˆåœ¨å­¸è¡“æœŸåˆŠç™¼è¡¨æˆ–ç”³è«‹å°ˆï§ã€ä¸»è¦ç™¼ç¾æˆ–å…¶ä»–æœ‰é—œåƒ¹å€¼ç­‰ï¼Œä½œä¸€ç¶œåˆè©•ä¼°ã€‚
1. è«‹å°±ç ”ç©¶å…§å®¹èˆ‡åŽŸè¨ˆç•«ç›¸ç¬¦ç¨‹ï¨ã€é”æˆé æœŸç›®æ¨™æƒ…æ³ä½œä¸€ç¶œåˆè©•ä¼° 
â– é”æˆç›®æ¨™ 
â–¡æœªé”æˆç›®æ¨™ï¼ˆè«‹ï¥¯æ˜Žï¼Œä»¥ 100å­—ç‚ºé™ï¼‰ 
â–¡å¯¦é©—å¤±æ•— 
â–¡å› æ•…å¯¦é©—ä¸­æ–· 
â–¡å…¶ä»–åŽŸå›  
ï¥¯æ˜Žï¼š 
2. ç ”ç©¶æˆæžœåœ¨å­¸è¡“æœŸåˆŠç™¼è¡¨æˆ–ç”³è«‹å°ˆï§ç­‰æƒ…å½¢ï¼š 
ï¥æ–‡ï¼šâ–¡å·²ç™¼è¡¨ â– æœªç™¼è¡¨ä¹‹æ–‡ç¨¿ â–¡æ’°å¯«ä¸­ â–¡ç„¡ 
å°ˆï§ï¼šâ–¡å·²ç²å¾— â–¡ç”³è«‹ä¸­ â– ç„¡ 
æŠ€è½‰ï¼šâ–¡å·²æŠ€è½‰ â–¡æ´½è«‡ä¸­ â– ç„¡ 
å…¶ä»–ï¼šï¼ˆä»¥ 100å­—ç‚ºé™ï¼‰ 
å¾…å®Œæ•´ç‰ˆï¥æ–‡å®Œæˆï¼Œå³é€äº¤çŸ¥ååœ‹éš›æœŸåˆŠå¯©æ ¸ã€‚ 
3. è«‹ä¾å­¸è¡“æˆå°±ã€æŠ€è¡“å‰µæ–°ã€ç¤¾æœƒå½±éŸ¿ç­‰æ–¹é¢ï¼Œè©•ä¼°ç ”ç©¶æˆæžœä¹‹å­¸è¡“æˆ–æ‡‰ç”¨åƒ¹
å€¼ï¼ˆç°¡è¦æ•˜è¿°æˆæžœæ‰€ä»£è¡¨ä¹‹æ„ç¾©ã€åƒ¹å€¼ã€å½±éŸ¿æˆ–é€²ä¸€æ­¥ç™¼å±•ä¹‹å¯èƒ½æ€§ï¼‰ï¼ˆä»¥
500å­—ç‚ºé™ï¼‰ 
æœ¬ç ”ç©¶è¨ˆç•«çš„ç›®æ¨™ç‚ºè‡ªå‹•åµæ¸¬é›»å½±ä¸­çš„ï¨†ï¦Šå ´æ™¯ç‰‡æ®µã€‚æˆ‘å€‘æŠŠç ”ç™¼å‡ºï¤­çš„ç¨‹å¼æ¸¬è©¦æ–¼å››éƒ¨
é›»å½±ï¼Œå…¶ï¨ç¢ºï¨(F1 Measure)åˆ†åˆ¥ç‚º:90%ï¼Œ89%ï¼Œ97%ï¼Œ91%ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘å€‘å¤§è‡´é”æˆç›®æ¨™ã€‚
æœ¬è¨ˆç•«çš„ç ”ç©¶æˆæžœé™¤ï¦ºå¯ç”¨ï¤­é˜²æ­¢å…’ç«¥å¾žç¶²ï¤·ä¸‹è¼‰ï¨†ï¦Šå½±ç‰‡å¤–äº¦å¯æ‡‰ç”¨æ–¼ï¨é‡‡é å‘Šç‰‡çš„
è£½ä½œã€‚åœ¨ç¾æœ‰é›»å½±åˆ†ç´šåˆ¶ï¨ä¸‹ï¼Œæˆäººæ˜¯å¯ä»¥è§€è³žï¨†ï¦Šé›»å½±çš„ï¼›è€Œï¨†ï¦Šæƒ…ç¯€å¾€å¾€æ˜¯ä¸€éƒ¨é›»å½±
æœ€å¸å¼•äººæ³¨æ„çš„åœ°æ–¹ã€‚ï¥«äºˆæœ¬è¨ˆç•«çš„äººå“¡å‰‡ï¥§ï¥åœ¨åœ–å½¢ï§¼åˆ¥ï§¤ï¥çš„æ‡‰ç”¨åŠå½±åƒç³»çµ±çš„å»ºï§·
å‡èƒ½ç²å¾—ç´®å¯¦çš„è¨“ï¦–ã€‚ 
 
