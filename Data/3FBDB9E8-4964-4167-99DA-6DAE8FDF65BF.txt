components of some of these devices. 
However, existing techniques that compute 
static speeds for non-preemptive 
real-time tasks either are based on pessimistic 
feasibility tests or overestimate preemption 
times on non-preemptive tasks, leaving 
much room for improvement. In this project, 
we propose energy-efficient scheduling techniques 
which overcome the limitations of 
two representative energy-aware algorithms 
for scheduling non-preemptive tasks on variable- 
speed processor systems. The DS* algorithm, 
which computes two speeds for a 
given task set, is found to excel at low system 
utilizationÔºõ the ISA algorithm, which 
computes one speed for each individual task, 
shows its strength as the system load gets 
higher. Our preliminary experimental results 
show that both DS* and ISA obtain significant 
energy gains, compared to the existing 
techniques from which they evolve. 
In this proposed research, we will first 
investigate a selective frequency inheritance 
policy, which aims to efficiently determine if 
processor speedup can be disabled without 
jeopardizing any task deadlines whenever a 
task is blocked. We expect it to be a useful 
policy in terms of saving energy consumption, 
when combined with ISA. We will 
also consider the potential benefits of 
non-preemptive scheduling algorithms for 
saving energy on multiprocessor environments, 
particularly in those systems where 
job migration is too expensive. In addition, 
we will discuss how our approach must 
change if static power consumption is taken 
into consideration. 
Ëã±ÊñáÈóúÈçµË©ûÔºö Real-time computing,dynamic voltage scaling, fixed-
priority scheduling, dynamic and static power 
consumption, selective frequency inheritance. 
 
I 
 I 
ÁõÆÈåÑ 
‰∏Ä„ÄÅ ÊëòË¶Å‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..II 
(‰∏Ä) ‰∏≠ÊñáÊëòË¶Å‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶...II 
(‰∫å) Abstract‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.II 
‰∫å„ÄÅ Â†±ÂëäÂÖßÂÆπ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..1 
I. INTRODUCTION‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..1 
II. RELATED WORK‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.1 
III. MODEL AND ASSUMPTIONS‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....2 
IV. INDIVIDUAL-SPEED ALGORITHM‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..3 
V. PERFORMANCE EVALUATION‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶8 
VI. CONCLUSION‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..10 
‰∏â„ÄÅ ÂèÉËÄÉÊñáÁçª‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..10 
  
1 
 
‰∫å„ÄÅÂ†±ÂëäÂÖßÂÆπ 
I. INTRODUCTION 
Power consumption has become one of the most 
important issues when it comes to designing many 
mobile and embedded real-time applications. When it 
is necessary to trade system performance for reduced 
power consumption, the system can exploit Dynamic 
Voltage Scaling (DVS) to change the supply voltage 
dynamically or Dynamic Threshold Voltage Scaling 
(DTVS) by controlling the body bias voltage to 
change the threshold voltage dynamically. Different 
supply voltages or threshold voltages result in differ-
ent processor speeds. Therefore, how to choose the 
proper speeds, along with the supply voltages and 
threshold voltages, is of importance for energy reduc-
tion and meeting system timing requirements.  
In this work, we consider energy-efficient 
scheduling of non-preemptive tasks on uniprocessor 
systems that support dynamic speed adjustment. Al-
though a large body of past research has focused on 
preemptive scheduling [4], [20], [24], non-preemptive 
scheduling is still attractive in different application 
domains where properties of device hardware and 
software make preemption either prohibitively expen-
sive or just impossible [9]. Despite their inherent li-
mitations, non-preemptive scheduling algorithms are 
easier to implement, have lower run-time overhead, 
require less memory, and eliminate the need for syn-
chronization and its associated overhead 
[9],[11].Non-preemption also helps preserve program 
locality, which in turn makes programs more amena-
ble to worst-time execution time analysis [22]. As a 
result, non-preemptive scheduling has been adopted in 
many avionics applications [17] as well as in embed-
ded systems, particularly in small embedded devices 
with limited memory capacity [2], [15], [18], [30].  
To the best of our knowledge, no specific algo-
rithm has been developed for scheduling of fully 
non-preemptive fixed priority tasks for energy mini-
mization under real-time constraints in the past. The 
most relevant ones are the dual-speed (DS) algorithm 
due to Zhang and Chanson [32] and the uniform 
slowdown algorithm with frequency inheritance 
(USFI) proposed by Jejurikar and Gupta [12]. Both 
algorithms are designed for tasks that need synchro-
nization and hence have critical sections. In this paper, 
we propose ISA (Individual Speed Algorithm), a nov-
el scheme that computes one speed for each individual 
task in a non-preemptive task set. We address the 
challenging issue of figuring out the exact workload of 
higher-priority tasks on each non-preemptive task. 
The exact workload enables us to calculate optimal 
processor speeds which give optimal energy con-
sumption while satisfying the given timing constraint. 
ISA achieves considerable energy savings compared 
to existing methods with comparable quality, particu-
larly at high processor utilization. Another scheme we 
devise to co-work with ISA is selective frequency 
inheritance (SFI) which efficiently determines if pro-
cessor speedup can be disabled without jeopardizing 
any task deadlines whenever a task is blocked. We 
show that further improvement (20% to 30% reduc-
tion in energy consumption) can be obtained when 
ISA is combined with the SFI policy, at the cost of 
slight runtime overhead. Considering that the task 
actual execution time is usually less than its worst 
case execution time (WCET), we further propose a 
dynamic slack reclamation scheme based on ISA, 
which is named ISA-DR, for more energy savings. 
Experimental study shows that up to 30% energy sav-
ings can be achieved by ISA-DR compared to ISA. 
Besides their primary role in reducing processor 
energy consumption for real-time applications, we 
also demonstrate an additional use of our algorithms 
to trade off energy consumption against transmission 
delay in saving communication energy.  
The remainder of this paper is organized as fol-
lows: Section II briefly discusses related work. Sec-
tion III describes task and power models along with 
some assumptions we make. Section IV details the 
motivation and design of the ISA algorithm. In the 
same section, the SFI policy applied to ISA is also 
introduced. The experimental results are discussed in 
Section V. Finally, Section VI concludes the paper 
with some remarks. 
II. RELATED WORK 
Power consumption has become a critical prob-
lem in embedded and mobile systems with real-time 
constraint in recent years. Researchers have tackled 
energy reduction on a processor with variable speeds 
for both periodic and aperiodic real-time tasks [4], 
[20], [24], [29], as well as tasks with critical sections 
[12], [32], scheduling using a hybrid of the slowdown 
and shutdown strategies [13], [21], energy reduction 
based on slack reclamation [4], [10], [19], ener-
gy-aware scheduling with reliability requirements 
[33]‚Äì[35] (or on fault-tolerance systems [14]), ener-
gy-efficient real-time scheduling on wireless networks 
[6], [23], multiprocessor energy-efficient scheduling 
[1], [5], and energy-aware scheduling at a broader 
system level [3], [36]. Note that all the aforemen-
tioned studies focused on preemptive scheduling. 
Hong et al. [8] first reported superior results 
yielded by non-preemptive scheduling policies on 
power minimization of variable-voltage core-based 
systems. The authors proposed a synthesis approach 
that explores static scheduling algorithms, determines 
the cache size and configuration, and selects the pro-
cessor core. The effectiveness of the approach was 
demonstrated on a variety of industrial-strength mul-
timedia and communication applications. Zhang and 
3 
 
case Pd (f)/f is a convex function where fee has the 
minimum Pd (f)/f in the range of [fmin, fmax], then we 
have to set fmin as fee to ensure that only energyeffi-
cient operation frequencies are selected. 
 
IV. INDIVIDUAL-SPEED ALGORITHM 
Section IV-A introduces the motivation of ISA. 
We describe the design detail of ISA in Section IV-B. 
Section IV-C considers a policy, namely SFI, to re-
duce the frequency of processor speedup at task 
blocking times with ISA. Remarks for practical sys-
tems and a note on DS are presented in Section IV-D. 
 
A. Motivation 
Algorithm DS [32] computes two speeds for the 
task system consisting of periodic tasks with 
non-preemptive critical sections that are executed on a 
variable-speed uniprocessor, while algorithm USFI 
[12] determines a uniform slowdown for each task in 
the task set. Both DS and USFI can be applied to 
non-preemptive scheduling by treating each task as 
one critical section. However, both approaches would 
overestimate preemption times on non-preemptive 
tasks, and hence, result in task slowdown factors 
higher than necessary. We will give an example later 
to illustrate this problem. Since DS only computes two 
static speeds for the entire task set and the improve-
ment on it is trivial, a short discussion of this algo-
rithm is postponed to Section IV-D. In the following, 
we first review how USFI works, and then present an 
example to illustrate why USFI is not energy-efficient 
for fully nonpreemptive task set. 
USFI iteratively computes slowdown factors for 
all tasks, from highest to lowest priority. An index q, 
initialized to 1, is used to record the fact that at any 
instant in time the slowdown factors for tasks œÑ1 to œÑq-1, 
denoted Œ∑1, Œ∑2,‚Ä¶, Œ∑q-1 ,have been determined. During 
each iteration, it first finds candidate slowdown fac-
tors for tasks from œÑq to œÑn, i.e., Œ∑q , Œ∑q+1 ,‚Ä¶, Œ∑n. Sup-
pose Œ∑m(q ‚â¶ m ‚â¶ n) is the maximum of Œ∑q , Œ∑q+1 ,‚Ä¶, 
Œ∑n. USFI selects Œ∑m as the slowdown factor for all 
tasks from œÑq to œÑm. It then updates q to be m+1 and 
continues the next iteration until all n tasks have 
slowdown factors determined. When determining 
candidate slowdown factor Œ∑i for each task in 
{œÑq ,œÑq+1,‚Ä¶,œÑn} at each iteration, USFI uses the fol-
lowing time-demand-based feasibility test (a direct 
extension of the schedulability test in [26]): 
ÔøΩ ÔøΩ
Cr
Œ∑r1‚â§r<ùëû
ÔøΩ
Si,jTr ÔøΩÔøΩ + 1Œ∑i,j ÔøΩBi + ÔøΩ Cpq‚â§p<ùëñ ÔøΩSi,jTpÔøΩÔøΩ = Si,j 
(1) 
Si,j is a scheduling point of task œÑi and œÑi,j is the can-
didate slowdown factor corresponding to Si,j . Si,j is a 
member of Si  defined as:  Sùëñ = {(t ‚àà S) ‚àß (t < Dùëñ) ‚à™ (Dùëñ) ,where S = {ùëòTùëó|ùëó =
1, ‚Ä¶ , ùëñ; ùëò = 1, ‚Ä¶ , [Ti/Tj]}. Bùëñ = ùëöùëéùë•ùëó‚ààùëôùëù(ùëñ){ùê∂ùëó} is the 
blocking factor due to nonpreemptability, where lp(i) 
is the set of tasks having lower priority than œÑi‚Äôs. 
When there are more than one scheduling point for 
task œÑi, i.e., more than one candidate slowdown factor, 
to achieve the best energy savings, USFI chooses the 
minimum candidate slowdown factor as the slowdown 
factor for œÑi, i.e, Œ∑i = minj(Œ∑i,j). 
Whenever blocking occurs, USFI adopts the 
frequency inheritance (FI) policy to change the pro-
cessor speed immediately by employing the slowdown 
factor of the blocked job in order to ensure task dead-
lines. In all the other cases, each job will execute at its 
slowdown factor to save energy. Note that the slow-
down factors computed by USFI are in a nonincreas-
ing order. 
The following example explains why USFI has 
room for improvement when applied to fully 
non-preemptive task scheduling, and motivates us to 
push forward. 
Example 1. Consider the following three periodic 
tasks œÑ1 =(5, 5, 1), œÑ2 = (10, 10, 2), œÑ3 = (20, 20, 1) 
specified with their periods, relative deadlines, and 
worst-case execution times. The slowdown factors 
computed by USFI are Œ∑1 = 0.6, Œ∑2 = 0.45 and Œ∑3 = 
0.225, respectively. 
Now we look into how USFI works. Suppose 
we have finished the first iteration of the algorithm 
and determined Œ∑1 to be 0:6, and we continue to de-
termine Œ∑2  at the second iteration. Consider how 
USFI determines Œ∑22 for S22 (one of the scheduling 
points of œÑ2) by applying Equation (1) to get the fol-
lowing equation: 10.6 ÔøΩ10T1 ÔøΩ + 1Œ∑22 (1 + ÔøΩ Cp[10Tp ]) = 102‚â§p‚â§2       (2) 
In (2), the workload from œÑ1 has been counted 
for 2 times(i.e.,[10
5
]), which means that in the worst 
case œÑ1 can execute twice before œÑ2 starts its execution. 
But actually, this is an overestimation. To see this, we 
assume œÑ1,1 and œÑ2,1  arrive at time t = 0 while œÑ3,1 
arrives just a little earlier. It is not hard to see that this 
is a case when œÑ2 has its longest response time. Initial-
ly, USFI schedules œÑ3,1 to execute. œÑ1,1 and œÑ2,1 arrive 
immediately while œÑ3,1 is executing. Since œÑ1,1 (as well 
as œÑ2,1) is blocked, the processor speed is increased to 
Œ∑1 =0.6. At t = 53 , œÑ3,1  is finished, and œÑ1,1  begins 
execution still at speed Œ∑1 =0.6. At t = 103  , œÑ1,1 finishes 
and œÑ2,1 finally gets its chance to execute at speed Œ∑2. 
Fig. 1 shows the schedule during the first 10 time 
units (the lifetime of œÑ2,1). 
 
 
 
ENERGY EFFICIENT SCHEDULING IN NON-PREEMPTIVE SYSTEMS WITH REAL-TIME CONSTRAINTS 5
replace Œ∑Àúi by Œ∑i,k and Si,j by Si,k in (5). Hence, the new slow-
down for œÑi can be calculated simply as MinSi,k‚ààSi(Œ∑i,k).
Since Si ‚äÜ Si, this new slowdown factor for œÑi is certainly no
smaller than Œ∑Àúi, which is an intermediate slowdown computed
by USFI based on Si. On the other hand, by comparing
Equation (3) and (6), it is obvious that the computed value
Œ∑i by using our approach is not larger than Œ∑Àúi. Combining
these two facts, the lemma follows.
Note that using Œ∑i,k in (6) as œÑi‚Äôs slowdown factor can
guarantee œÑi‚Äôs deadline, but Inequality (5) may no longer hold
if we replace Œ∑Àúi by Œ∑i,k and Si,j by Si,k in (5), which means
œÑi may not begin execution some time before Si,k. This is
not permissible since in non-preemptive scheduling, once the
task instance has seized the control of processor, it will not
release it until it finishes its execution. So if œÑi does not start its
execution before Si,k, the slowdown factor calculated by (6),
which uses Si,k to bound the span of higher priority tasks,
cannot guarantee that œÑi will meet its deadline. To address
this problem, consider the following ratio:
Bi +
‚àë
q‚â§p<i Cp
‚åà
Si,k
Tp
‚åâ
Si,k ‚àí
‚àë
1‚â§r<q
Cr
Œ∑r
‚åà
Si,k
Tr
‚åâ (7)
Based on our definitions of Si and Inequality (5), Œ∑Àúi is larger
than the value in (7). We choose a value that is a little larger
than (7), but is smaller than Œ∑Àúi. We call this value Œ∑i,k. Clearly
Inequality (5) still holds when we replace Œ∑Àúi by Œ∑i,k in (5). We
let Œ∑‚Ä≤i,k = Max(Œ∑i,k, Œ∑i,k). Now we are sure that using Œ∑
‚Ä≤
i,k
as œÑi‚Äôs slowdown factor can not only meet œÑi‚Äôs deadline, but
also ensure œÑi‚Äôs kick-off before Si,k. In order to achieve better
energy savings we let œÑi‚Äôs new candidate slowdown factor Œ∑i =
MinSi,k‚ààSi(Œ∑
‚Ä≤
i,k). Finally, Lines 12-15 of Algorithm 1 further
choose the maximum candidate slowdown factor from among
Œ∑q, . . . , Œ∑n as the final slowdown factor for tasks œÑq, . . . , œÑm.
We present an example to illustrate our approach.
Example 2. We compute new slowdown factors for the task
set given in Example 1. The same as USFI, ISA involves
three iterations of computation, and the calculation details are
shown in Table I. Since the slowdown factor for œÑ1 computed
by ISA is the same as that of USFI, i.e., Œ∑1 = 0.6, we omit to
show the detail of computing Œ∑1. Now we come to the second
iteration, with Œ∑1 = 0.6, we continue to find Œ∑2 and Œ∑3. By
using Equations (3) and (4) with q = 2, we obtain Œ∑Àú2 =
0.45 and Œ∑Àú3 = 0.375. For task œÑ2, Inequality (5) holds at
S2,1 = 5 and S2,2 = 10, hence S2 = {S2,1(5), S2,2(10)}.
We go on to find Œ∑2,1 = 0.36, Œ∑2,1 = 0.3001, Œ∑2,2 = 0.45,
and Œ∑2,2 = 0.15001. Hence, Œ∑2 = Min(Œ∑
‚Ä≤
2,1, Œ∑
‚Ä≤
2,2) = 0.36.
Following the same steps, we get Œ∑3 = 0.3001. Since Œ∑2 > Œ∑3,
the second iteration is concluded with Œ∑2 = 0.36. Finally,
our third iteration determines Œ∑3 to be 0.09. In summary, the
new slowdown factors computed by our method are Œ∑1 = 0.6,
Œ∑2 = 0.36, and Œ∑3 = 0.09, respectively.
Now we need to be sure that all task deadlines can still be
met with our new slowdown factors. We first introduce two
important facts. From the computation process of ISA, it is
not difficult to derive the following fact.
TABLE I
CALCULATION OF OUR NEW SLOWDOWN FACTORS FOR THE TASK SET
GIVEN IN EXAMPLE 1 IN THREE ITERATIONS.
Iter. œÑ1 œÑ2 œÑ3
Œ∑Àú1 = 0.6 Œ∑Àú2 = 0.5 Œ∑Àú3 = 0.45
S1 = {S1,1}1st
Œ∑1 = 0.6
Œ∑1 = 0.6
Œ∑Àú2 = 0.45 Œ∑Àú3 = 0.375
S2 ={S2,1, S2,2} S3={S3,2, S3,4}2nd
Œ∑2 = 0.36 Œ∑3 = 0.3001
Œ∑1 = 0.6 Œ∑2 = 0.36
Œ∑Àú3 = 0.45
S3 ={S3,2, S3,4}3rd
Œ∑3 = 0.09
Œ∑1 = 0.6 Œ∑2 = 0.36 Œ∑3 = 0.09
Lemma 2. Given the tasks are ordered in a non-decreasing
order of their periods, the new slowdown factors computed by
ISA are in a non-increasing order.
Proof: This property is a direct result of how USFI
determines slowdown factors for unassigned tasks during each
iteration: find the task with the maximum candidate slowdown
factor among all unassigned tasks, and assign its slowdown
factor value, say Œ∑m, to the unassigned tasks up to index m.
This process is repeated until all tasks have their slowdown
factors assigned. Because this process of determining slow-
down factors is also employed in our approach, the lemma
immediately follows.
With our new slowdown factors computed, the run-time
scheduler will use them to control the processor speed at
different points in time so as to reduce energy consumption,
while still guaranteeing all task deadlines. In this paper, we
consider two policies to control the processor speed: (1)
Frequency inheritance (FI) [12]: When blocking occurs, if
the blocked job has a higher slowdown factor, this slowdown
factor is inherited by the blocking job. In all other cases, each
job will execute at its corresponding slowdown factor to save
energy. (2) Selective frequency inheritance (SFI). We consider
FI in this section, and will discuss SFI in Section IV-C. By
Lemma 2 and the FI policy, we have the following fact.
Lemma 3. During the time period when œÑi is executing, the
minimum slowdown of the processor is Œ∑i.
With the FI policy, we prove that the slowdown factors
computed by ISA can guarantee all task deadlines, as stated
below.
Theorem 1. Based on the DM scheduling policy, the slow-
down factors computed by ISA along with the FI policy of
controlling the processor speed guarantee all task deadlines.
Proof: Our approach assigns every task a slowdown factor
that is larger than or equal to the slowdown values given in
(6) and (7). Based on this, we know that for every task œÑi,
i = 1, . . . , n, there exists a scheduling point Si,k such that the
following two inequalities hold:
1
Œ∑i
Bi +
‚àë
1‚â§j<i
Cj
Œ∑j
‚åà
Sik
Tj
‚åâ
+
1
Œ∑i
Ci ‚â§ Di (8)
7 
 
 
SFI policy at time t2, we need to consider the follow-
ing two situations at time t1: 
ÔÇß œÑb inherits ?ÃÖ?ùëñ  at time t1: During [t1,t2], the 
processor speed is certainly no less than ?ÃÖ?ùëñ. 
No matter whether the SFI policy promotes 
the processor speed at time t2 or not, the pro-
cessor speed is still no less than ?ÃÖ?ùëñ  during 
[t2,t3]. Hence, t3 - t1 is upper bounded by 
ùêµùëñ
ùúÇÔøΩùëñ
. 
ÔÇß Frequency inheritance did not occur at time t1: 
This means that at time t1, Rb ‚â§  ùêµùëñùúÇÔøΩùëñ . During 
[t1,t2], the processor speed is certainly no less 
than the speed at time t1. Again, no matter 
whether the SFI policy promotes the proces-
sor speed at time t2 or not, the processor 
speed during [t2,t3] is no less than the speed at 
time t1. Hence,we know ùë°3 ‚àí ùë°1 ‚â§  ùêµùëñùúÇÔøΩùëñ. 
Based on the above facts and our arguments given 
in the proof of Theorem 1, œÑi can still finish its execu-
tion before t1 + Si,k, and hence its deadline. 
As for œÑa, i.e., the newly blocked task when the SFI 
policy is applied, it is not hard to see that its maxi-
mum blocking time is bounded by ùêµùëé ùúÇùëéÔøΩ . The work-
load on œÑa from higher priority tasks remains the same. 
By this fact, Lemma 4, and Theorem 1, we thus have 
the following theorem. 
Theorem 2. Based on the DM scheduling policy, the 
slowdown factors computed by ISA along with the 
SFI policy can guarantee all task deadlines. 
Fig. 2(c) shows the schedule produced by the 
SFI policy for the task set given in Example 1. At 
time 3, œÑ1,1 arrives while œÑ3,1 is running at speed ?ÃÖ?2 = 
0.36. At this time instant, œÑ3,1 needs at most 0.53 time 
units to complete at its current speed, whereas œÑ1,1 can 
tolerate a maximum blocking of ùêµ1 ?ÃÖ?1ÔøΩ = 2 0.6ÔøΩ . 
Based on Theorem 2, œÑ3,1 can keep running at speed 
0.36 without jeopardizing the deadline of œÑ1,1. Like-
wise, œÑ2,1 can continue execution at speed ?ÃÖ?2 at time 8. 
In this case, the energy consumed by our SFI policy is 
2.48 in the first 20 time units, which is 13.8% im-
provement over USFI. Our experimental results in 
Section VI show that the SFI policy gives us a large 
gain in energy savings under a variety of parameter 
settings. Note that the overhead involved with SFI is 
small ‚Äî the scheduler only needs to record the pro-
cessor time spent on executing each active job, and 
whenever blocking occurs it performs a simple com-
putation and test, as described in Lemma 4, to deter-
mine if the processor speed must be increased. 
D. Remarks for Practical Systems and DS 
Processors with discrete speeds: For commer-
cial processors, only discrete speeds might be availa-
ble. To address this problem, we can first derive a 
solution by assuming continuously available speeds as 
the approaches in Section IV-B, and then use some 
available speeds to approximate the speeds determined 
by ISA. For example, we first compute a speed for œÑ1 
by ISA. If this speed is unavailable, we then choose 
the next higher speed (also called inflated speed) 
which is available and assign it to œÑ1. After that, we 
utilize the inflated speed to compute slowdown factors 
for the remaining tasks œÑ2,‚Ä¶,œÑn and choose the next 
higher available speeds for them, in a similar way. 
Note here our method is similar to the one adopted by 
the PM-Clock algorithm proposed in [24], which fo-
cuses on fixed priority preemptive scheduing. 
Non-negligible frequency transition overhead: It 
is now a common knowledge that changing from one 
frequency level to another takes a fixed amount of 
time (denoted ‚àÜt, ranges from tens of microseconds to 
tens of milliseconds), referred to as the transition (or 
switch) overhead [16]. When frequency transition 
overhead is large enough that they cannot be ignored, 
we need to recompute the slowdown factors by consi-
dering such overhead. From the computation process 
of ISA, it can be observed that for task œÑi, the number 
of frequency switches, say Œª, is bounded by the num-
ber of jobs with higher priorities than œÑi, plus two ad-
ditional possible switches, one for œÑi itself (when the 
system state is changed from idle to executing œÑi, or 
conversely), and the other for a lower priority block-
ing job. Specifically, we have Œª =  ‚àë [Si,j TrÔøΩ ]1‚â§r<q +2 
for formulas (3) and (5), and Œª =  ‚àë [Si,k TrÔøΩ ]1‚â§r<q +2 for 
(6). Hence, if the transition workload is non-negligible, 
we should add Œª‚àÜùë° into the left-half-part of (3), (5) 
and (6) to compute ?ÃÖ?ùëñ . Note here our method for 
counting transition workload is a conservative one 
since it considers the worst case scenario, but it is also 
a safe one as it suffices to guarantee schedulability. 
Besides time overhead, transition also incurs energy 
overhead, hence, it is important to guarantee that the 
energy saved by slowdown policy must be no less 
than the increased energy consumption resulted from 
the transition overhead. Otherwise, the slowdown 
policy is meaningless. However, since the actual 
number of frequency transition may change during 
run-time and depends on the actual execution cycles 
of each job, it is hard to decide a situation where the 
energy saved by slowdown is less than the energy 
consumption by frequency transition. Because the 
focus of this paper is handling time overhead while 
meeting deadlines, we leave the problem of tackling 
the energy overhead for future work. 
Refinement on DS: Even though we only 
present algorithm ISA for deciding one individual 
speed for each task, it is also applicable to refine algo-
rithm DS [32]. A simple yet efficient refinement 
would be choosing the highest speed by ISA as the 
high speed in DS. This certainly will guarantee the 
schedulability, while energy reduction can also be 
achieved comparing to DS. Since the refinement is 
trivial and straightforward, we do not detail it in this 
work. 
9 
 
 
 
in a non-increasing order, since the highest-priority 
task in a task set in these two approaches has the larg-
est slowdown factor and executes and invokes speed 
promotion most frequently, it is natural to conjecture 
that energy savings would be more significant as the 
difference between the maximum slowdown factors 
computed by USFI and ISA-FI gets larger. This trend 
is indeed reflectedin Figs. 5 and 6. Both figures show 
that the performance gap between USFI and ISA-FI is 
wider at a larger value of SFr. On average, USFI 
consumes 8% to 15% more energy than ISA-FI. The 
energy reduction achieved by ISA-FI, relative to USFI, 
mainly comes from its lower slowdown factors, since 
both ISA-FI and USFI use the same speed-control 
policy. 
(3) ISA-SFI performs better than all other ap-
proaches in nearly all cases. Like ISA-FI, as the pro-
cessor utilization becomes larger, the energy reduction 
achieved by ISA-SFI becomes more. When the pro-
cessor utilization reaches 60%, ISA-SFI consumes, on 
average, 32% and 27% less energy than 
USFI when Œ∑ùëíùëí = 0.29 and 0.37, respectively. This 
is because at high processor utilization, tasks tend to 
experience more blocking, and the benefit of SFI be-
comes obvious. Unlike ISA-FI, the change of SFr 
does not affect the magnitude of energy reduction by 
ISA-SFI, as shown in both Figs. 5 and 6. This phe-
nomenon also explains ISA-SFI‚Äôs primary energy 
savings come from the SFI policy, rather than its 
smaller slowdown factors. 
(4) The effect of increasing the proportion of fre-
quency independent power in our energy consumption 
formula is clearly seen from Figure 5 to Figure 6. 
When the frequency independent power increases in 
the formula, our approach becomes less energy effi-
cient. However, we can still achieve 5% to 10% more 
energy savings, compared to USFI, when Œ∑ùëíùëí 
changes from 0.29 to 0.37. 
2) Task Granularity: In the second set of experi-
ment, we generate two types of task set, namely 
fine-grained task set and coarse-grained task set, to 
examine how different approaches fare in the energy 
consumption under different task granularity. In the 
fine-grained task set, one task is generated from the 
long period and one is selected from the medium pe- 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
riod, while the others are randomly generated from the 
short period. In the coarse-grained task set, one task is 
selected from the short period and one is from the 
medium period, while the others are randomly from 
the long period. In this simulation, all tasks are also 
assumed to execute up to their WCETs and Œ∑ùëíùëí  is 
fixed to be 0.29. The system utilization is varied from 
10% to 60%. It can be observed from Figure 7.(a) that 
for coarse-grained task set, ISA-SFI consistently out-
performs the other three schemes on energy consump-
tion performance, except when the utilization is below 
15%, all the four schemes have the same energy con-
sumption, due to that the slowdown factors computed 
by every algorithm are all less than the threshold fre-
quency Œ∑ùëíùëí  and they are set to be Œ∑ùëíùëí. Moreover, it 
can be seen that the gap between ISA-SFI and the 
other three approaches is increasing with the growth 
of the utilization. This is because at high utilization, 
tasks tend to experience more blocking, and the bene-
fit of SFI becomes obvious. For the fine-grained task 
set, ISA-SFI also outperforms the other three schemes, 
as shown in Figure 7.(b). However, the difference 
between ISA-SFI and ISA-FI is not that large as in the 
coarse-gained task set. The reason is that for 
fine-grained task set, the slowdown factors computed 
are more close to each other than in the coarse-grained 
case. In fact, in our experiment we observe that most 
of the task slowdown factors computed are the same 
for the fine-grained task set, hence the energy savings 
from the SFI policy is not that significant as in the 
coarse-grained task set. 
3) Slack Factor: In this set of simulations, we ex-
amine the additional energy gains derived through the 
dynamic slack reclamation policy. For simplicity, we 
only compare the performance of the following algo-
rithms: ISA-FI, ISA-SFI and ISA-DR. We define a 
notation namely slack factor (sf ) to specify the dif-
ference between the actual case execution time 
(ACET) and the task‚Äôs WCET. Specifically, the 
ACET of the tasks are uniformly distributed 
tween(1 ‚àí  ùë†ùëì) √ó WCET and the task‚Äôs WCET. Œ∑ùëíùëí 
is fixed to be 0.29. To facilitate comparison, the ener-
gy consumed by ISA-FI is used as the baseline, and 
the energy consumed by each other solution is norma-
lized against the baseline. Figure 8 shows the simula-
11 
 
ing for low-power embedded operating systems. In Proc. of 
SOSP, pages 89‚Äì102, 2001. 
[20] G. Quan and X. Hu. Minimum energy fixed-priority scheduling  
for variable voltageprocessors. In Proc. of DATE, pages 
782‚Äì787, 2002. 
[21] G. Quan, L. Niu, X. Hu, and B. Mochocki. Fixed priority  
scheduling for reducing overall energy on variable voltage 
processors. In Proc. Of IEEE Real-Time Systems Sympo-
sium, pages 309‚Äì318, 2004. 
[22] H. Ramaprasad and F. Mueller. Tightening the bounds on  
feasible preemption points. In Proc. of IEEE Real-Time 
Systems Symposium, pages 212‚Äì224, 2006. 
[23] X. Ruan, S. Yin, A. Manzanares, M. Alghamdi, and X. Qin. A  
messagescheduling scheme for energy conservation in mul-
timedia wireless systems. IEEE Transactions on Systems, 
Man and Cybernetics, Part A: Systems and Humans, 
(99):1‚Äì12, 2011. 
[24] S. Saewong and R. Rajkumar. Practical voltage-scaling for  
fixed-priority rt-systems. In Proc. of IEEE Real-Time and 
embedded Technology and Applications Symposium, pages 
106‚Äì115, 2003. 
[25] C. Schurgers, V. Raghunathan, and M. Srivastava. Power  
management for energy-aware communication systems. 
ACM Transactions on Embedded Computing Systems 
(TECS), 2(3):431‚Äì447, 2003. 
[26] L. Sha, R. Rajkumar, and J. Lehoczky. Priority inheritance  
protocols: an approach to real-time synchronization. IEEE 
Transactions on Computers, 39(9):1175‚Äì1185, 1990. 
[27] V. Swaminathan and K. Chakrabarty. Real-time task schedule 
ing for energy-aware embedded systems. Journal of the 
Franklin Institute, 338(6):729‚Äì750, 2001. 
[28] V. Swaminathan, C. Schweizer, K. Chakrabarty, and A. Patel.  
Experiences in implementing an energy-driven task schedu-
ler in RT-linux. In Proc. of IEEE RTAS, page 229, 2002. 
[29] F. Yao, A. Demers, and S. Shenker. A scheduling model for  
reduced CPU energy. In Proc. of the Annual Symposium on 
Foundations of Computer Science, pages 374‚Äì382, 1995. 
[30] G. Yao, G. Buttazzo, and M. Bertogna. Bounding the maxi 
mum length of non-preemptive regions under fixed priority 
scheduling. In Proc. of RTCSA, 2009. 
[31] F. Zhang and S. Chanson. Processor voltage scheduling for  
real-time tasks with non-preemptible sections. In Proc. of 
the 22nd IEEE Real- Time Systems Symposium, pages 
235‚Äì245, 2002. 
[32] F. Zhang and S. Chanson. Blocking-aware processor voltage  
scheduling 
for real-time tasks. ACM Transactions on Embedded Com-
putting Systems, 3(2):307‚Äì335, 2004. 
[33] Y. Zhang and K. Chakrabarty. Dynamic adaptation for fault  
Tolerance and power management in embedded real-time 
systems. ACM Transactions on Embedded Computing Sys-
tems, 3(2):336‚Äì360, 2004. 
[34] D. Zhu and H. Aydin. Reliability-aware energy management  
for periodic real-time tasks. In Proc. of IEEE RTAS, pages 
225‚Äì235, 2007. 
[35] D. Zhu, R. Melhem, and D. Moss√©. The effects of energy  
management 
on reliability in real-time embedded systems. In Proc. of 
ICCAD, pages 35‚Äì40, 2004. 
[36] J. Zhuo and C. Chakrabarti. System-level energy-efficient  
dynamic task scheduling. In Proc. of DAC, pages 628‚Äì631, 
2005. 
processing ‰πãÔ•ÅÊñáÔÇæ‚ÄúCkNN Query Processing over Moving Objects with Uncertain 
Speeds in Road NetworksÔºÇÔºåÂæóÂà∞‰∏Ä‰∫õÂêåÔ®à‰πãÔ®ùÈó¢Âª∫Ë≠∞„ÄÇÂè¶Â§ñÔºåÁï∂ÁÑ∂‰πüÔ¶∞ËÅΩÂÖ∂‰ªñ
‰ΩúËÄÖ‰πãÂ§ß‰ΩúÔºåËóâ‰ª•Ô¶∫Ëß£ÊúÄÊñ∞ÁöÑÁ†îÁ©∂Ë∂®Âã¢„ÄÇ 
Êú¨Ê¨°ÊúÉË≠∞Êúâ 3 Â†¥ keynotes Ô¶®ÊàëÁç≤Ô®óÔ•ºÂ§ö„ÄÇÁ¨¨‰∏ÄÂ†¥Áî±‰ºäÔßùÔ•ùÂ§ßÂ≠∏ÊïôÊéà Philip S. 
Yu ‰∏ªË¨õ ‚ÄúInformation Networks Mining and Analysis.‚Äù ‰∏ªË¶ÅÂÖßÂÆπÊòØ: With the 
ubiquity of information networks and their broad applications, there have been 
numerous studies on the construction, online analytical processing, and mining of 
information networks in multiple disciplines, including social network analysis, 
world-wide web, database systems, data mining, machine learning, and networked 
communication and information systems. Moreover, with a great demand of research 
in this direction, there is a need to understand methods for analysis of information 
networks from multiple disciplines. In this talk, the speaker presented various issues 
and solutions on scalable mining and analysis of information networks. These include 
data integration, data cleaning and data validation in information networks, 
summarization, OLAP and multi-dimensional analysis in information networks. 
Á¨¨‰∫åÂ†¥Áî±Ê≠¶Êº¢Â§ßÂ≠∏ÊïôÊéà Deyi Li ‰∏ªË¨õ ‚ÄúiMiner: From passive searching to active 
pushing.‚Äù ‰∏ªË¶ÅÂÖßÂÆπÊòØ : The success of a search engine in cloud computing 
environment relies on the numbers of users and their click-through. If we take the 
previous search key words as tags of users to study and differentiate the user 
interaction behaviors, the search engine is able to actively push related and useful 
information to users based on their previous actions instead of passively waiting for 
users‚Äô queries. The talk tries to answer the following questions: Is there any statistical 
property almost independent to search key words? How to push recommendation 
based on the queried key words? And how to extract user behavior models of 
searching actions in order to recommend the information to meet users‚Äô real needs 
more timely and precisely? The speaker takes the do-the-best strategy with uncertainty. 
Statistics play a great rule here. From the statistical point of view there are two very 
important distributions: the Gauss distribution and the power-law distribution. The 
speaker has studied an interesting cloud model to bridge the gap between the two by 
using 3 mathematical characteristics: expectation, entropy and hyper entropy. 
Á¨¨‰∏âÂ†¥Áî± Aarhus Â§ßÂ≠∏ÊïôÊéà Christian S. Jensen ‰∏ªË¨õ ‚ÄúOn the querying for 
places on the mobile web.‚Äù ‰∏ªË¶ÅÂÖßÂÆπÊòØ: The web is undergoing a fundamental 
transformation: it is becoming mobile and is acquiring a spatial dimension. Thus, the 
web is increasingly being used from mobile devices, notably smart phones, that can 
be geo-positioned using GPS or technologies that exploit wireless communication 
networks. In addition, web content is being geo-tagged. This information calls for new, 
spatio-textual query functionality. The talk touches on the following questions: How 
 Ë®àÁï´Á∑®Ëôü NSC 99-2221-E-006 -124 
Ë®àÁï´ÂêçÁ®± Âú®ÂèØËÆäÂãïÈÄüÔ®ÅËôïÔß§Âô®‰∏ä‰πãÈùûÊê∂‰ΩîÂºèÂç≥ÊôÇÊéíÁ®ã 
Âá∫Âúã‰∫∫Âì°ÂßìÂêç 
ÊúçÂãôÊ©üÈóúÂèäËÅ∑Á®±
ÂæêÔß∑Áæ§ 
ÂúãÔß∑ÊàêÂäüÂ§ßÂ≠∏ÊúÉË®àÂ≠∏Á≥ª ÊïôÊéà 
ÊúÉË≠∞ÊôÇÈñìÂú∞Èªû 
2011 Ô¶é 9 Êúà 14 Êó•Ëá≥ 2011 Ô¶é 9 Êúà 16 Êó• 
‰∏≠ÂúãË•øÂÆâ 
ÊúÉË≠∞ÂêçÁ®± International Conference on Signal Processing, Communications, and 
Computing, ICSPCC 2011 
ÁôºË°®Ô•ÅÊñáÈ°åÁõÆ ÂÉÖÂá∫Â∏≠ÊúÉË≠∞ 
 
‰∏Ä„ÄÅÁõÆÁöÑ 
ICSPCC 2011 ÊòØÁî±ÂúãÈöõËëóÂêçË≥áË®äÈõªÂ≠êÂçîÊúÉ IEEE ‰∏ªËæ¶ÔºåÂÖ∂ÁõÆÊ®ôÊòØÊèê‰æõÂÖ®‰∏ñÁïåÂæû‰∫ã
Ë≥áË®ä„ÄÅÈÄöË®äËàá‰ø°ËôüËôïÔß§Á†îÁôºÂ∑•‰Ωú‰πãÁî¢Â≠∏Á†îÁïå‰∏ÄÂÄãË®éÔ•ÅÂâµÊñ∞ÊÉ≥Ê≥ï(ÂåÖÊã¨Ôß§Ô•ÅËàáÂØ¶Âãô)
ÁöÑ‰∏ÄÂÄãÂπ≥Âè∞„ÄÇÊ≠§Ê¨°ÊúÉË≠∞Âú®Ë•øÂÆâËàâËæ¶ÔºåÊúâË®±Â§öÂñÆ‰ΩçÂçîËæ¶ÔºåÂåÖÊã¨ IEEE Xian section, 
IEEE Hong Kong Section, Northwestern Polytechnical University, Xidian 
University, Xian Jiaotong University, University of Hong Kong, 
Polytechnical University of Hong Kong, City University of Hong Kong, 
Chinese University of Hong Kong Á≠â„ÄÇ 
Êú¨Ê¨°ÂÖ±Êúâ 486 ÁØáÔ•ÅÊñáÊäïÁ®øËá≥ ICSPCC 2011 ÊúÉË≠∞ÔºåÔ§≠Ëá™Êñº 12 Âúã (ÂåÖÊã¨ÊàêÂäüÂ§ßÂ≠∏Èõª
Ê©üÁ≥ªÁöÑÁéãÈßøÁôºÊïôÊéàÁ≠â)ÔºåÊúÄÂæåÊúâ 275 ÁØáÔ•ÅÊñáÊî∂Ô§øËá≥ÊúÉË≠∞Ô•ÅÊñáÈõÜÔºåÊé•ÂèóÔ•°Á¥Ñ 56%; ÊØè
ÁØáÊúÉË≠∞Ô•ÅÊñáËá≥Â∞ëÊúâ 2 ‰Ωç technical program committee ‰πãÂßîÂì° reviewÔºå‰ª•Á¢∫‰øùÔ•ÅÊñáÂìÅ
Ë≥™„ÄÇÂõ†ÁÇ∫ÊúÉË≠∞ÈõÜÁî± IEEE Âá∫ÁâàÔºåÊïÖÂèØ‰ª•Áî® IEEE Explore ÊñπÔ••Èñ±Ô•ö„ÄÇÊúÉË≠∞ÂÖ±ÂàÜÁÇ∫ 40 
ÂÄã regular sessionsÔºå‰ª•ÂèäÔ•©ÂÄãspecial sessions„ÄÇSpecial sessions ÂåÖÊã¨ computational 
biology, underwater acoustic communication, speech signal processing, cognitive radio 
and networks, intelligent life, MIMO communication and signal processing, ÂÖßÂÆπÂèØÔ•Ø
ÈùûÂ∏∏Ë±êÂØå„ÄÇ 
   Êú¨Ê¨°ÊúÉË≠∞ÂÖ±Êúâ 4 ÂÄã keynote speech: (1) Subcarrier coding: a fast coordination 
Á†îÁ©∂Ë™≤È°åÔºåÂ¶ÇÂ§öÂ™íÈ´îË®äËôüÂëàÁèæ„ÄÅËôïÔß§„ÄÅÁ∑®Á¢ºË≥áÔ¶æÊé¢ÂãòËàáÁü•ÔßºÁôºÁèæÁ≠â„ÄÇÂ§öÂ™íÈ´îÁöÑÂÖß
ÂÆπ‰∏ÄËà¨ÁöÜÊòØ unstructured ÁöÑÔºåÂõ†Ê≠§Êé¢ÂãòÁöÑÂïèÈ°åÁâπÂà•Ë§áÈõú‰∏îÂØåÊåëÊà∞ÊÄß„ÄÇÈõñÁÑ∂ÂÄã‰∫∫Âú®Ë≥á
Ô¶æÊé¢ÂãòÊñπÈù¢ÁöÑÁ†îÁ©∂Ôºå‰∏ªË¶ÅÊòØÂ∞àÊ≥®Âú® structured data ‰∏äÔºåÁÑ∂ËÄåÈÄôÂ†¥ keynote speech
Â∏∂Áµ¶ÊàëË®±Â§öÁöÑÂïüÁôºÔºåË¨õËß£ËÄÖ‰ªãÁ¥πÂπæÈ†ÖÈáùÂ∞ç unstructured data mining ÁöÑÊäÄË°ì‰∏äÔºåÂàù
Ê≠•ÁúãËµ∑Ô§≠‰πüÂèØ‰ª•Áî®Âà∞ structured data mining ‰∏äÔºåÂÄã‰∫∫Â∞áÊåÅÁ∫åÊ≠§ÊñπÂêëÁöÑÊé¢Ô•™ÔºåÊúü
ÊúõËÉΩÂæóÂà∞Ô•ºÂ•ΩÁöÑÊàêÊûú„ÄÇ 
 Âè¶‰∏ÄÂ†¥Ô•ÅÊñáÂ†±Âëä„ÄåOptimizing the battery energy efficiency in wireless sensor„ÄçË®éÔ•Å
Â¶Ç‰ΩïÈáùÂ∞çÁî± nonrenewable ÈõªÊ±†Êé®ÂãïÁöÑÊÑüÊ∏¨Á∂≤Ô§∑ÁØÄÈªû‰πãÈõªÊ±†ÁØÄËÉΩÊïàÔ•°„ÄÇÊ≠§ÁØáÔ•ÅÊñáÂ∞ç
ÊñºÈõªÊ±†ËÉΩËÄóÊèê‰æõÂæàÂ•ΩÁöÑ overviewÔºåÂÆÉÂ∞çÊñºÊàëÂú®Âç≥ÊôÇÁ≥ªÁµ±ÁØÄËÉΩÊéíÁ®ãÁöÑÁ†îÁ©∂‰∏äÔºåÊáâËÉΩ
Êèê‰æõ‰∏Ä‰∫õÂâµÊñ∞ÊÉ≥Ê≥ï„ÄÇ 
 Âè¶‰∏ÄÈ†ÖÂ∞çÊñºÂÄã‰∫∫ÊúâÁõ∏Áï∂Á®ãÔ®ÅÁöÑÂïüÁôºÁöÑÊºîË¨õÊòØÁî±ÊàêÂäüÂ§ßÂ≠∏ÁéãÈßøÁôºÊïôÊéà‰∏ªË¨õ„ÄåSmart 
Living, Orange Computing & Cloud Computing„Äç„ÄÇÁî±Êñº‰∏ñÁïå‰∫∫Âè£Â¢ûÂä†ËàáÔ¶éÔ§¥ÂåñÔºåÂú∞
ÁêÉÂ§©ÁÑ∂Ë≥áÊ∫êÈÄêÊº∏ËÄóÁõ°ÔºåÁßëÊäÄÁ†îÁôºÂøÖÈ†àÊé¢Ô•™Â¶Ç‰ΩïËÆìÊàëÂÄëÁöÑÁîüÊ¥ªÁí∞Â¢ÉËàáÂú∞ÁêÉÁîüÊÖãÁ∂≠Áπ´
‰∏ãÂéª„ÄÇÁéãÊïôÊéàÊèêÂá∫ÁöÑ Orange Computing ËëóÁúºÊñº‰∫∫ÊñáÁÖßÈ°ßÔºåÂ∞àÊ≥®ÊñºÁÇ∫‰∫∫ÔßêË∫´ÂøÉÔ¶≥Âπ≥
Ë°°Ë®≠Ë®àÊºîÁÆóÊ≥ïËàáÁ≥ªÁµ±„ÄÇÈÄôÈ†ÖÊºîË¨õÁõ∏Áï∂ÊèêÂçáÊàëÁöÑÁ†îÁ©∂ÁúºÁïåÔºåÂ∞çÊú™Ô§≠Á†îÁ©∂ÊñπÂêë‰πãÂà∂ÂÆö
ÂæàÊúâÂä©Ô®ó„ÄÇÊ≠§Ê¨°Ô•´Âä†ÊúÉË≠∞ÈÅáÂà∞Âè∞ÁÅ£ÂÖâÂØ∂ÁßëÊäÄÁ†îÁôºÈÉ®Ë®±Á∂ìÔß§„ÄÇÂõ†‰ªñÂú®Ê•≠ÁïåÊúâÁõ∏Áï∂Ë±ê
ÂØåÁöÑÁ∂ìÔ¶åÔºåÊú¨Ë∫´‰∫¶ÂÖ∑ÊúâÂçöÂ£´Â≠∏‰ΩçÔºåÊïÖÁõ∏Áï∂ÁÜüÊÇâÁßëÊäÄÁî¢Ê•≠ÁôºÂ±ïË∂®Âã¢„ÄÇÂÄã‰∫∫Â∏åÊúõËÉΩË∑ü
‰ªñÊúâÔ§ÅÂ§öÁöÑÊ©üÊúÉÈÄ≤Ô®àÁ†îÁôºÊñπÂêë‰∏äÁöÑË®éÔ•ÅÔºåÊú™Ô§≠ËÉΩÈÄ≤Ô®à‰∏Ä‰∫õÂÖ∑ÊúâÂïÜÂìÅÂåñÊΩõÔ¶äÁöÑÁ†îÁ©∂„ÄÇ 
 „ÄåNetwork survivability against region failure„ÄçÁî± Xiaohong Jiang ÊºîË¨õÔºå‰ªñÊèêÂà∞ÈÄö
Ë®äÁ∂≤Ô§∑ÁõÆÂâçÈù¢Ôß∂ÊÑàÔ§≠ÊÑàÂ§öÂ§ßÁØÑÂúçÁöÑÂ®ÅËÑÖÔºåÂ¶ÇËá™ÁÑ∂ÁÅΩÈõ£ËàáÂØ¶È´îÊîªÊìä„ÄÇÁï∂ÈÄôÊ®£ÁöÑÂçÄÂüü
ÂïèÈ°åÁôºÁîüÊôÇÔºåË®±Â§öÂú∞Ôß§Áõ∏ÈóúÂçÄÂüüÁöÑÁ∂≤Ô§∑ÂÖÉÁ¥†ÂèØËÉΩÂêåÊôÇË¢´ÊØÄÔºåÈÄ†ÊàêÊï¥È´îÁ∂≤Ô§∑Ô¶öÊé•Ô®¶
ÂèØËÉΩÂèóÂΩ±ÈüøÁîöËá≥ÂÆåÂÖ®ÂÅúÊì∫„ÄÇProf. Jiang Êé¢Ë®é network survivability against region 
failureÔºåÂ∞áÂïèÈ°å model ÊàêÁ∂≤Ô§∑‰∏äÁöÑ circular cut. ‰ªñÊ™¢Ë¶ñÂú® region-disjoint Ëàá
node-disjoint ‰πã region failure ÊÉÖÂΩ¢‰∏ãÔºånetwork throughput performance degrade ÁöÑÊÉÖ
ÂΩ¢„ÄÇ 
Èô§Ô¶∫Ô¶∞ËÅΩË®±Â§öÔ®ùÂΩ©ÁöÑÊºîË¨õÔºåÊàë‰πüËàáÂêåÔ®à‰∫§ÔßäÂΩºÊ≠§ÁöÑÁ†îÁ©∂ÔºåÂåÖÊã¨Â§ßÊúÉ‰∏ªÂ∏≠ Prof. 
Dear LihChyun Shu, 
Thank you for your submission to APWeb 2011. On behalf of the Programme Committee, we are 
delighted to inform you that your paper 109 - CkNN Query Processing over Moving Objects with 
Uncertain Speeds in road networks has been accepted as a full research paper and for presentation at the 
conference. Congratulations!
You can view the reviews by logging in at http://cmt.research.microsoft.com/APWEB2011.
When preparing the final version of your paper, please address issues raised in the reviews. Instructions 
for preparing the camera-ready copy can be found at 
http://deke.ruc.edu.cn/apweb2011/page/papersubmission.htm. The deadline for camera-ready copy is Jan. 
22th, 2011.
This year we received 104 submissions. The Programme Committee has worked very hard to thoroughly 
review all the submitted papers, and has accepted 26 full papers and 11 short papers. The acceptance ratio 
for full papers is only 25%, which is very competitive for a regional conference. Congratulations for the 
acceptance of your paper!
You are also invited to give a demo of your results/system/tool at the conference. If you decide to accept 
the invitation, you may find details about preparing the demo from our website soon.
We look forward to seeing you in Beijing in April 2011.
Best regards,
Xiaoyong, Wenfei and Jianmin
È†Å 1 / 1
2012/1/30mhtml:file://C:\Users\Lin\Downloads\APWeb 2011_ Paper 109 Decision. .mht
of candidate moving objects.
Based on the two proposed distance functions, we present
a continuous UkNN (CUkNN ) query method, named
CUkNNMean, to continuously obtain the UkNN result of
query q during a given time period. In particular, the UkNN
result consists of k UkNN objects, each of which has a larger
probability of being one of the kNNs of query q than that of
any other objects not in UkNN result set. Our CUkNNMean
method consists of three stages, the pruning phase, the refining
phase, and the probability evaluation and ranking phase. In
the pruning phase, by calculating the pruning distance we
efficiently prune the objects that are impossible to be the kNN
result of query q and form the candidate set. Then in the
refining phase, by constructing the kNN-MaxD-PolyLine of
query q, we decide the possible kNN objects of query q at
time period [ts, te], depending on the candidate set determined
at the pruning phase. Then in the probability evaluation and
ranking phase, by introducing the MeanDq,o(t), the Mean
value of the distance between object o and query q at time t,
to represent the distance of each UkNN object, the probability
calculation is greatly simplified.
When updates of object speeds and directions occur, which
implicitly means that some objects change their moving
directions or their speeds go beyond the speed limits, a
straightforward way to derive the valid query result is to
re-execute the CUkNNMean algorithm which may greatly
degrade the performance of our method. Instead, we modify
the query result according to the speed and direction updates
of the candidate objects so as to keep the query result valid.
Moreover, by organizing the data structures in a novel and
neat way, we extend our method to handle CUkNN queries in
large networks with high efficiency. Especially, we can greatly
reduce disk access costs.
In summary, the main contributions of the paper are as
follows:
1) An uncertain network distance model is proposed to
represent the distance between a moving object and a
query, both of which move with variable speeds in a
road network.
2) A continuous UkNN query monitoring method, consist-
ing of three stages, i.e., the pruning phase, the refining
phase, and the probability evaluation and ranking phase,
is introduced to obtain the UkNN results at each
monitored time instant, where all moving objects and
queries move continuously in the road network with
uncertain speeds.
3) In the probability evaluation and ranking phase, by
introducing the mean distance of q and o within the
monitored time period, the calculation and ranking of
the probability of each candidate object being a kNN
of q are greatly simplified.
4) We address the issues of processing object updates,
instead of re-executing the CUkNNMean algorithm
when speed and direction updates of candidate objects
occur.
5) By organizing the data structures in a novel and neat
way, we extend our method to handle CUkNN queries
in large road networks with high efficiency and low disk
access costs.
6) We evaluate the performance of the CUkNNMean
method through a set of simulation experiments both
in a real road network and a synthetic 2-dimensional
grid network, and the performance result shows that our
method is efficient, precise and scalable.
The rest of the paper is organized as follows. Section II
reviews related work. Section III gives problem descriptions,
data structures used, and the network distance model proposed.
The algorithms of our CUkNNMean method are described in
Section IV and the extension of the proposed method to large
road networks is shown in Section V. Section VI gives the
experimental study. Finally, Section VII concludes the paper.
II. RELATED WORK
Jensen et al. [7] first addressed the problem of kNN
search in road networks in the year 2003. Since then, kNN
query processing in road networks has received widespread
attentions and a variety of algorithms are proposed. Section
II-A reviews the existing methods for processing kNN queries
in road networks, and Section II-B discusses the related work
that deals with CkNN queries in road networks. Finally,
Section II-C discusses the related works on kNN and other
similarity query methods in uncertain databases.
A. kNN Query Methods in Road Networks
Papadias et al. [8] presented an architecture that integrates
network and Euclidean information to process network-based
queries. They utilized the principle that, for any two objects
in the network, the network distance is the same as or larger
than the Euclidean distance between them. They proposed
two methods to process static kNN queries in network
spaces, which are incremental Euclidean restriction (IER) and
incremental network expansion (INE). We omit the discussion
about IER since INE algorithm outperforms IER. Starting
from the query point q, INE expands the network to search
for kNNs and examines the objects in the order they are
encountered. When the expansion radius is larger than the
distance between the kth-nearest neighbor object and query
point q, the expansion is terminated. The efficiency of INE
depends on the density of objects. If the objects are sparsely
distributed on a road network, the network expansion for
searching kNNs is expected to pass through many nodes in
the road network and the efficiency can be low.
To avoid on-line distance computation, Kolahdouzan et
al. proposed the Voronoi Network Nearest Neighbor (VN3)
method [9], which is based on network Voronoi diagrams.
In this method, the network Voronoi polygons (NVP) and
some network distances are pre-computed and the entire road
network is divided into a number of small regions which are
called cells. The first NN of any query q is the generator
of the NVP that contains q. If additional NNs are required,
the adjacent Voronoi polygons will be considered iteratively.
The VN3 can be efficiently used for networks that have a low
density. However, in high density networks, its efficiency can
be low since it includes constructing, storing and inquiring a
large number of Voronoi polygons to answer kNN queries.
2
n
14 9
n
13
n
1
n
2
n
3
n
4
n
7
n
4
8
10
12.5
19
12
n
11
n
10
n
8
n
9
5
8.5
15
6
18
4.5
10.5
18
4.5 10
6
6
n
6.55
(b) t=0.5
o6
o
3
o2
q
o
4
o5
o1
12
n
(a) t=0
13
n
6
q
1
n 2
n
3
n
4
n
7
n
6
4
9
12
11
20
11
n
10
n
8
n
8
5
8
16
6
18
4
12
18
4 10
8
e3
6
n
n14
o6
o1
o
2
o
5
o
3
o
4
9
n
5n5n
e1 e2
e4
e5 e3
e1 e2
e4
e5
(c) (d)
V.minobject V.max
O1 2         4
O2 -1  -2
O3 2         4
O5 -1  -2
O4 1         2
O6 1         3
q -1  -2
e
5
5
edge V.max
e
1
4
e
2
3
e3    5
e4 4
 Fig. 1. An example of a CUkNN query q and objects o1 thru o6 in a road network. (a) and (b) show the positions of the query and the objects at time = 0and 0.5 respectively. (c) shows the maximum and minimum speeds of each object. (d) shows the maximum speed of each relevant edge in the road network.
B. Data Structures
In this study, we use an undirected weighted graph to
represent a road network which includes an edge set and a
node set. The graph nodes and edges represent road nodes and
road segments respectively. A road node is the intersection
point of two or more road segments or the dead end of a
road segment. Each edge has a non-negative weight which
represents the cost of the road segment, e.g., the length of
a road segment or the time required to travel through the
road segment. We maintain a set of moving CUkNN queries
and moving objects in the road network. When we process
CUkNN queries in a road network, the distance we consider
is the distance in the road network, which we will describe
shortly below.
Four in-memory tables are constructed to store the infor-
mation about the edges and nodes of a road network, moving
objects and queries. The first is the edge table Tedge which
stores for each edge e: (1)its id (e.id), (2) its starting node
(e.snode), (3) its ending node (e.enode); (4) its weight (e.w);
(5) a set of identities for objects currently moving on edge
e (e.obj set); (6) the maximum speed limit of the edge
(e.vlimit). For the sake of ease in processing queries, we
assign an id for each node and assume that for the two nodes
of an edge e, the node with the smaller id is e.snode, and the
node with the larger id is e.enode. Henceforth, whenever we
refer to the position of a query or an object we mean the edge
e on which it travels and its distance from e.snode.
We use the node table Tnode to store for each node n: (1)
its id (n.id); (2) the set of edges connecting to n (n.eadj). The
third table we use is Tobj , which stores for each object obj:
(1) its id (obj.id); (2) the edge on which the object obj travels
(obj.e); (3) the distance from obj to the starting node of obj.e
(obj.dist); (4) the minimum speed of obj (obj.vmin); (5) the
maximum speed of obj (obj.vmax). We assume if obj moves
along the direction from the starting node to the ending node
of obj.e, its speed is positive; otherwise it is negative.
Lastly, we use the query table Tqry to store for
each query q: (1) its id (q.id); (2) its UkNN re-
sult within a certain time period (q.UkNN Final Set).
In particular, q.UkNN Final Set is a set of tuples <
[ti, tj ], {o1, o2, o3, ...} >, where o1, o2, and o3 are the objects
that are the probable kNNs of query q during the time period
[ti, tj], listed in descending order of probability values.
As object o and query q move with uncertain speeds in
the road network, their positions are not exactly determined
and this causes difficulties in calculating the distance between
q and o, especially when they move on two different edges.
However, based on the notion of distance determinate pro-
posed below, even if o and q move on two different edges, it
becomes much easier to calculate their distance in most cases.
Definition 1: Given two different edges ei and ej , if the
shortest distance between every pair of two points, one from
ei and another from ej , is always determined by the same
path, then we say ei and ej are distance determinate.
For example, e2 and e4 in Figure 1(a) are distance determi-
nate with a special path that starts and ends at the same node
n2, whereas e4 and e5 are distance indeterminate.
To determine whether a pair of edges ei and ej in a road
network is distance determinate, we use the following method.
Suppose that ei has two nodes ei.snode and ei.enode, and
ej also has two nodes ej .snode and ej .enode. The distance
between any pair of nodes ni and nj , d(ni,nj), can be pre-
computed and stored in the system. So d(ei.snode, ej .snode),
d(ei.snode, ej .enode), d(ei.enode, ej .snode) and d(ei.enode,
ej .enode) are available when determining whether ei and ej
are distance determinate. These four distance values can be
sorted in ascending order. Suppose these four distance values
after sorting are denoted as d1, d2, d3 and d4 respectively. It is
not hard to see that if the following three predicates all hold,
then ei and ej are distance determinate.
d2=d1 +min(ei.w, ej .w)
d3=d1 +max(ei.w, ej .w)
d4=d1 + ei.w + ej .w
To efficiently evaluate these predicates, we construct a
matrixDD, with each element having one of the following five
values: the value √ò means that ei and ej are distance indeter-
minate; otherwise, ei and ej are distance determinate and there
is a shortest path connecting ei and ej . Henceforth, we call
the two endpoints of the shortest path the connecting nodes.
There are four subcases: the value <0,0 > means these two
connecting nodes are ei.snode and ej .snode respectively; <-
1,-1> indicates that these two connecting nodes are ei.enode
and ej .enode respectively; <0,-1> indicates that these two
4
(b) The two edges q and o reside are
     distance indeterminate
nk nl
nm
nn
the shortest
path connecting
ei and ej
ei
ej
q
o Lo(t)lo(t)
lq(t) Lq(t)
(a) The two edges q and o reside are
     distance determinate
nk nl
nm nn
ei
ej
q
o
Lo(t)lo(t)
lq(t) Lq(t)
 
 Fig. 4. Illustration of MaxDq,o(t) and MinDq,o(t) when q and o move
on two different edges
Assume q is moving on the edge ei (nk, nl) and o is moving
on the edge ej (nm, nn). There are two subcases to consider:
(1) ei and ej are distance determinate. By Definition 1,
there is a shortest path connecting ei and ej . Without loss of
generality, we assume the two nodes connecting this shortest
path are nl and nn.
From Figure 4(a) we can see that no matter o and q
move toward or away from the shortest path connecting ei
and ej , MinDq,o(t) is equal to the sum of min(d(lq(t), nl),
d(Lq(t), nl)), DNl,n, and min(d(lo(t), nn), d(Lo(t), nn)),
where d(lq(t), nl) is the distance from lq(t) to nl,DNl,n is the
distance between nl and nn. Recall our use of the matrix DD
in which the entry DDij with < value0, value1 > is used to
represent different relations between ei and ej in terms of dis-
tance determinate. Note that when nl is ei.snode (the value0
of DDij is 0), d(lq(t), nl) = q.dist+q.vmin√ó(t‚àít0); when nl
is ei.enode (the value0 of DDij is -1), d(lq(t), nl) = ei.w‚àí
(q.dist+q.vmin√ó(t‚àít0)). Thus d(lq(t), nl) can be uniformly
represented by |q.dist+ q.vmin √ó (t‚àí t0) +DDij .value0 √ó
ei.w|, no matter whether nl is ei.snode or ei.enode.
The distance functions of d(Lq(t), nl), d(lo(t), nn) and
d(Lo(t), nn) can be defined similarly. Moreover, MaxDq,o(t)
is equal to the sum ofmax(d(lq(t), nl), d(Lq(t), nl)), DNl,n,
max(d(lo(t), nn), d(Lo(t), nn)).
(2) ei and ej are distance indeterminate. Note that
the possible locations of q and o lie in two segments, i.e.,
(lq(t), Lq(t)) and (lo(t), Lo(t)), respectively. Thus as shown
in Figure 4(b), the shortest path from any point in these
two segments to the outside network should pass through
their terminals. Therefore, MinDq,o(t) must be the distance
between one terminal of (lq(t), Lq(t)) and one terminal of
(lo(t), Lo(t)). As both (lq(t), Lq(t)) and (lo(t), Lo(t)) have
two terminals, we need to take into account four network
distances, i.e., d(lq(t), lo(t)), d(lq(t), Lo(t)), d(Lq(t), lo(t)),
and d(Lq(t), Lo(t)). MinDq,o(t) is equal to the minimum of
these four distances.
When ei and ej are distance indeterminate, it is difficult to
calculate the exact value of MaxDq,o(t) due to the following
reasons: (1) it is time-consuming to find the two points, one
on the position segment of q and the other on the position
segment of o, where the distance between these two points
equals MaxDq,o(t); (2) these two points are changing as
time goes by. Thus we use an approximate value, denoted
app MaxDq,o(t), as a substitute for MaxDq,o(t).
We consider two requirements in our calculation of
app MaxDq,o(t). Firstly, existing CkNN methods in the
pruning phase usually use MaxDq,o(t) of kNN objects
(evaluated at the starting time of each time period) to calculate
the pruning distance, which is in turn used to filter out objects
that can not be query q‚Äôs kNNs. Thus app MaxDq,o(t)
must be no smaller than the exact value of MaxDq,o(t)
at any time instant, so as not to leave out any potential
kNN objects. Second, app MaxDq,o(t) should be as close
as possible to the exact value of MaxDq,o(t). As a result,
we set app MaxDq,o(t) to be the sum of MinDq,o(t), the
length of the position segment of q, and the length of the
position segment of o. Our app MaxDq,o(t) meets these two
requirements aforesaid since it is no smaller than MaxDq,o(t)
at any time instant, and it is the smallest possible value we
can choose for app MaxDq,o(t), hence closest to the exact
value of MaxDq,o(t).
Now we use the objects in Figure 1 as an example to
illustrate the calculation of MaxDq,o(t) and MinDq,o(t).
Here we consider three different cases: object o1 and query
q that are on the same edge; object o2 and query q that are
on two different edges which are distance determinate; object
o5 and query q that are on two different edges which are
distance indeterminate. We obtain the following functions of
MaxDq,o(t) and MinDq,o(t) with respect to different pairs
of objects and query in different time intervals:
0‚â§t‚â§2
MinDq,o1(t) = 11 + 3√ó t MaxDq,o1(t) = 11 + 6√ó t
MinDq,o2(t) = 15‚àí 4√ó t MaxDq,o2(t) = 15‚àí 2√ó t
0‚â§t‚â§0.333
MinDq,o5(t) = 39 + 2√ó t MaxDq,o5(t) = 39 + 4√ó t
0.333‚â§t‚â§2
MinDq,o5(t) = 41‚àí 4√ó t MaxDq,o5(t) = 41‚àí 2√ó t
Observe that MaxDq,o(t) and MinDq,o(t) are linear func-
tions of time t, except that (1) when q and o move on the same
edge and they move toward each other, meet and then move
away from each other within the same time period being moni-
tored,MaxDq,o(t) is a polyline which consists of one segment
with a downward slope and another segment with an upward
slope, and MinDq,o(t) is a polyline which consists of one
segment with a downward slope, a horizontal segment, and one
segment with an upward slope; (2) when q and o move on two
edges which are distance indeterminate, bothMaxDq,o(t) and
MinDq,o(t) could be polylines which consist of one segment
with an upward slope and another segment with a downward
slope. The CUkNN algorithm presented below will divide
the time horizon being monitored into consecutive windows by
the turning points of MaxDq,o(t) and/or MinDq,o(t) of the
candidate objects of query q, hence we are sure that these two
functions are always linear in time within each time window.
IV. CUkNNMean ALGORITHM
In this section, we discuss how to continuously monitor a
UkNN query in a road network. To begin with, we use an
efficient snapshot kNN method, such as INE[7], NV3[8], to
calculate the kNN result at the starting time ts = 0. The
main issue is how to continuously monitor the kNN result
after ts. As the objects and the query in question may travel on
different edges in the road network and change their directions
6
Algorithm 2: Refining Phase
input : a set of moving objects O of previous kNNs at time
instant ts, the Cand Set reserved from the
pruning phase, and the time period [ts, te]
output: the UkNN Interm Set which is a set of tuples
< [ti, tj ], {o1, o2, ..., on} >, where
o1, o2, ..., on(n ‚â• k) are the UkNNs of q from ti to
tj , and ti, tj are time instants within [ts, te]
1 begin
2 /‚àóStage 1:determining the kNN-MaxD-Polyline ‚àó/ ;
3 let ok to be the kth-object at time instant ts;
4 list kNN-MaxD-Polyline=√ò; list L = √ò; ta = ts; tb = ts ;
5 while tb < te do
6 for each object o ‚àà Cand Set and o <> ok do
7 determine the time instant t by solving the
equation MaxDq,o(t) =MaxDq,ok (t) ;
8 select the earliest time instant t ‚àà [tb, te] ;
9 if t exists then
10 insert the tuple < t, o > into list L in
ascending order of time;
11 if L.head exists then
12 ta = tb; tb = L.head.t; ok = L.head.o;
13 insert < [ta, tb],MaxDq,ok(t) > into
kNN-MaxD-Polyline ;
14 L = √ò;
15 insert < [tb, te],MaxDq,ok (t) > into kNN-MaxD-Polyline
;
16 /‚àóStage 2: determining the UkNNs within [ts, te] ‚àó/ ;
17 let UkNN Set to be objects whose MinDq,o(t) are
below kNN-MaxD-Polyline at ts;
18 list L = √ò; set UkNN Interm Set = √ò; ta = ts;
tb = ts ;
19 for each object o ‚àà Cand Set do
20 if MinDq,o(t) intersects kNN-MaxD-Polyline at a time
instant t & t ‚àà [ta, tb] then
21 insert the tuple < t, o > into list L in ascending
order of time;
22 while L is not empty do
23 < t, o >=L.pop-front; ta = tb; tb = t;
24 insert the tuple < [ta, tb], {UkNN Set} > into
UkNN Interm Set ;
25 if o is not in UkNN Set then
26 insert o into UkNN Set;
27 else
28 delete o from UkNN Set;
29 insert the tuple < [tb, te], {UkNN Set} > into
UkNN Interm Set ;
30 return UkNN Interm Set ;
the list kNN-MaxD-Polyline, which is initialized to empty,
to organize the line segments of the polyline we want to
construct; 2) sets the list L, which is initialized to empty,
to organize the intersection points of the Maximum distance
functions of objects o ‚àà Cand Set and ok within [ts, te]; and
3) sets two variables ta and tb, which are both initialized to
ts, to record the beginning and the end of the sub-periods
being processed within [ts, te], respectively. Then, we repeat
the following steps when tb is smaller than te (lines 4-14):
1) for each object o ‚àà Cand Set, if the Maximum distance
function of o intersects with that of ok at time t ‚àà [tb, te],
insert the tuple < t, o > into list L in ascending order of
time t (line 10). If there is more than one such time instant,
the earliest one will be selected (line 8). After all the objects
in candidate set have been processed, list L has recorded
all the objects whose Maximum distance functions intersect
with that of ok within [tb, te] in ascending order of time.
Thus, the head element L.head includes the object which will
first replace ok; 2) if L.head exists (line 11), L.head.o will
work as new ok after the corresponding time point L.head.t.
Thus, we replace ta with tb and replace tb with L.head.t
which means a new sub-period has been formed (line 12).
Then, we insert the tuple < [ta, tb],MaxDq,ok(t) >, which
represents the newly formed line segment, into kNN-MaxD-
Polyline (line 13). Below we use query q in Figure 1 as an
example to illustrate how we construct kNN-MaxD-Polyline
within [ts, te].
As shown in Figure 5, the kth object at time ts is o2. Then
we should make sure that at which time instant o2 will not be
the kth object any more. Note that we don‚Äôt know in advance
which object in Cand Set will replace o2 as the kth object
first, thus we should consider each other object o in Cand Set
by solving the equation MaxDq,o2(t) = MaxDq,o(t) (line
7). In this way we determine that o1 replaces o2 as the
kth object at time t=0.5, and obtain the first tuple of kNN-
MaxD-Polyline, i.e., ([0, 0.5],MaxDq,o2(t)). Then we use o1
as the kth object at time t=0.5 and continue the construction
of kNN-MaxD-Polyline of q until time period [ts, te] is all pro-
cessed. Finally, kNN-MaxD-Polyline consists of three tuples:
< [0, 0.5],MaxDq,o2(t) >, < [0.5, 1.111],MaxDq,o1(t) >
and < [1.111, 2.0],MaxDq,o3(t) >.
Note for an object o, if MinDq,o is larger than
MaxDq,kth object at a time instant t, o can not be a kNN
of q at time t; otherwise, o can possibly be a kNN of
q at time t. Moreover, kNN-MaxD-Polyline characterizes
MaxDq,kth object within the time period [ts, te]. Hence, for an
object o whose Minimum distance line intersects kNN-MaxD-
Polyline at time t, 1) if o is not in current UkNN Set, which
means the Minimum distance function of o is above kNN-
MaxD-Polyline before t, intersects with it at t, and then comes
below it after t, thus o should be included into UkNN Set;
2) if o is already in current UkNN Set, which means the
Minimum distance function of o is below kNN-MaxD-Polyline
before t, intersects with it at t, and then comes above it after t,
thus o should be deleted from UkNN Set. That is, whenever
the Minimum distance line of an object intersects kNN-MaxD-
Polyline, UkNN Set will change. We call this time instant
Result Change Time instant (RCT).
In the second stage of Algorithm 2, lines 17-18 1) set
UkNN Set to include all the objects whose MinDq,o(t)
are below kNN-MaxD-Polyline at ts; 2) set two variables,
ta and tb, which are both initialized to ts, to record the
beginning and the end of the sub-periods being processed
within [tb, te], respectively; and 3) set the list L to organize
the time points t ‚àà [ts, te], when the Minimum distance
functions of candidate objects intersect with kNN-MaxD-
Polyline; and 4) use the set UkNN Interm Set to organize
the intermediate UkNN result which includes a set of tuples:
< sub ‚àí period, the corresopoing UkNN set >. Both L
8
02
4
6
8
10
12
14
16
18
20
t=0.833 1.667
MeanD(q,o1)
MeanD(q,o2)
MeanD(q,o3)
eanDq,o1(t
ea D ,o2(t)
eanDq,o3(t
t=0.111
 
Fig. 7. Divide the time period into sub-periods by the intersections of
MeanDq,o(t) of objects in UkNN Set.
within this period. This means that, for any two objects o1
and o2 in UkNN Set, if Po1(t) > Po2(t) at a time instant
t within a divided time period, Po1(t) is always larger than
Po2(t) at any time instant within this divided time period.
As shown in Figure 7, MeanDq,o1(t) intersects with
MeanDq,o3(t) at time t=1.111, thus the time period
[0.833, 1.667] is divided into 2 sub-periods: [0.833, 1.111]
and [1.111, 1.667]. Then, for each divided time period, we
calculate the mean distance of each object o in UkNN Set
from q at a time point within [ti, tj ]. Without loss of
generality, we choose the central time point of the time
period [ti, tj ], i.e., t=(ti + tj)/2 (line 17). Next, we in-
sert the objects in UkNN set into UkNN List in as-
cending order of mean distances at time (ti + tj)/2
(line 18). Thus the objects o in UkNN List are in
descending order of Po([ti, tj ]) and the first k objects
are the kNNs we want. All sub-periods together with
their ordered UkNN Lists form the UkNN Final Set
of query q (line 19), which consists of four tuples: <
[0, 0.833], {o1, o2} >, < [0.833, 1.111], {o2, o1, o3} >, <
[1.111, 1.667], {o2, o3, o1} >, and < [1.667, 2.0], {o2, o3} >
D. Handling the Moving Speed and Direction Update of
Candidate Objects
For an object o, if it changes its moving direction or
its moving speed goes beyond its speed limit due to vary-
ing traffic conditions, the object o may move out of its
position segment. Hereinafter, we call this situation, that an
monitored object moves out of its position segment at time
t ‚àà [ts, te], object update. For a query q, an object update
of its candidate objects can potentially invalidate the previous
query result so that the result needs to be updated accordingly.
Note the object updates of non-candidate objects will not affect
the query result of q. A straightforward approach to updating
the query result is re-executing the CUkNN algorithm when
object updates of candidate objects occur. However, this ap-
proach will greatly degrade the efficiency and flexibility of our
method. Instead, we propose the following strategy to handle
these object updates so as to maintain the UkNN query result.
Assume that when an object moves out of its position
segment at time tu ‚àà [ts, te], it sends its new location,
minimum speed, maximum speed, and moving direction to
the server. Once the server receives an update message for
moving object o, it first determines if object o belongs to
Algorithm 3: Probability evaluation and ranking phase
input : the UkNN Interm Set gotten in refining phase
output: the UkNN Final Set
1 begin
2 /‚àóStep1:further divide [ti, tj ] into shorter time periods ‚àó/;
3 set UkNN Final Set = √ò; list UkNN List=√ò ;
4 for each tuple < [ti, tj ], UkNN Set > of
UkNN Interm Set, whose size of UkNN Set > k do
5 if the MaxDq,o(t) or MinDq,o(t) function of any
o ‚àà UkNN Set has a turning point tu within [ti, tj ]
then
6 Divide it into two tuples: < [ti, tu], UkNN Set >
and < [tu, tj ], UkNN Set > ;
7 Construct the MeanDq,o(t) of each object in
UkNN Set ;
8 if the MeanDq,o(t) of any two objects intersect each
other at tu within [ti, tj ] then
9 Divide it into two tuples: < [ti, tu], UkNN Set >
and < [tu, tj ], UkNN Set > ;
10 /‚àóStep 2: arrange the objects in UkNN Set in descending
order of their probabilities being a kNN object of q ‚àó/ ;
11 for each tuple < [ti, tj ], UkNN Set > of
UkNN Interm Set do
12 if the size of UkNN Set ‚â§ k then
13 Insert the objects in UkNN Set into
UkNN List sequentially;
14 Insert the tuple < [ti, tj ], UkNN List > into
UkNN Final Set ;
15 else
16 for each object o in UkNN Set do
17 Calculate the mean distance of o at time point
(ti + tj)/2, i.e., MeanDq,o((ti + tj)/2) ;
18 Insert o into UkNN List in ascending order
of mean distances at time (ti + tj)/2;
19 Insert the tuple < [ti, tj ], UkNN List > into
UkNN Final Set ;
20 UkNN List=√ò ;
21 output UkNN Final Set;
the candidate set of query q. If not, no processing is needed.
Otherwise, we re-calculate the Minimum and the Maximum
distance functions of object o after tu, i.e., MinDq,o(t) and
MaxDq,o(t). Then, there are two cases regarding whether
object o is a UkNN object of query q within [tu, te].
Case 1: o is a UkNN object of query q within [tu, te].
Remember that in the pruning phase, we have calculated the
pruning distance, i.e,Dpruning , and determined the Cand Set
which consists of all the objects within Dpruning of query
q at time ts. Cand Set includes at least k objects. If
Cand Set includes more than k objects or the largest value
of MaxDq,o(t) within the time period [tu, te] is no more than
Dpruning , there are also at least k objects within Dpruning
of query q within [tu, te]. Otherwise, Dpruning should be
replaced by the largest value of MaxDq,o(t) within [tu, te],
and some new objects within this new Dpruning of query q
may be added into the new candidate set.
Next, we re-calculate kNN-MaxD-Polyline if MaxDq,o(t)
is part of original kNN-MaxD-Polyline within [tu, te] or
MaxDq,o(t) intersects with original kNN-MaxD-Polyline at
10
  Fig. 10. The subdivision rules of HC.
1
2
3
4
5 6
7
8
9 10
11
12
13 
 (a)                (b)                  (c) 
 
Fig. 11. Getting the filling curve and the number of each cell.
each point on the curve we got sequentially. Thus, we can get
the number of each cell. To continue the example in Figure 8,
we can get the space-filling curve and the number of each cell
which are shown in Figure 11. Figure 11(a) shows the first
level curve. Since the first level cells at the upper-left side
and at the upper-right side, which are corresponding to the
upper-left and the upper-right points of the first level filling
curve respectively, have sub-cells, thus these two points are
replaced by two small bottom cups and a right join (Figure
11(b); refer to the first line of Figure 10). The final filling
curve and the number of each cell are shown in Figure 11(c).
Similarly, we can get the second part number of a node,
i.e., the number of a node in the cell it resides. However, the
second part number is rather easy to get since the number of
nodes within a cell is small.
Up to now, we have discussed how to number the cells and
the nodes of the network. For example, the grey node in Figure
8 is within Cell 1, thus its first part number is 1. Assume
its number in the cell it resides is 3, we can get the serial
number of the grey node which is 1-3. Then we discuss how
to organize and store the matrix DD. In general, we consider
the nodes within a cell together and get the construction of
matrix DD as follows. Assume that we totally get m cells,
thus we have
DD =
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
DDCell1,Cell1 ¬∑ ¬∑ ¬∑ DDCell1,Cellm
DDCell2,Cell1 ¬∑ ¬∑ ¬∑ DDCell2,Cellm
...
. . .
...
DDCellm,Cell1 ¬∑ ¬∑ ¬∑ DDCellm,Cellm
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª (2)
DDCelli,Cellj =
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
dCell1
i
,Cell1
j
¬∑ ¬∑ ¬∑ dCell1
i
,CellŒ¥
j
dCell2
i
,Cell1
j
¬∑ ¬∑ ¬∑ dCell2
i
,CellŒ¥
j
...
. . .
...
dCellŒ¥
i
,Cell1
j
¬∑ ¬∑ ¬∑ dCellŒ¥
i
,CellŒ¥
j
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª (3)
In particular, dCellk
i
,Celll
j
is the distance between the node k
in celli and the node l in cellj and eachDDCelli,Cellj includes
Œ¥ √ó Œ¥ such distance values. Remember that we choose the
value Œ¥ especially to make sure that Œ¥ √ó Œ¥ distance values
just takes a whole disk block. Consider that the matrix DD
and each sub matrix DDCelli,Cellj are both symmetric with
respect to the main diagonals, thus we only need to keep the
upper (or lower) triangular matrix. Here the lower triangular
matrix is chosen. As a result, we can modify the value Œ¥ to
further save the space as follows:
Œ¥ =
‚àö
block size
the size of an element of matrix DN
‚àó 2 (4)
Thus DDCelli,Cellj , which is an element of matrix DD,
takes just a disk block. Then, we store the lower triangular
matrix of matrix DD in (m √ó m + m)/2 continuous disk
blocks in a row-major order: DDCell1,Cell1 , DDCell2,Cell1 ,
DDCell2,Cell2 , DDCell3,Cell1 , DDCell3,Cell2 , DDCell3,Cell3 ,
¬∑ ¬∑ ¬∑ , DDCellm,Cell1 , DDCellm,Cell2 , ¬∑ ¬∑ ¬∑ , DDCellm,Cellm .
Considering that the nodes within the same cell are very
close to each other and the distances between the nodes within
the same cell are frequently used in processing CUkNN
queries, we store m special elements DDCelli,Celli (i=1,2,¬∑ ¬∑ ¬∑ ,
m), which keeps the distances between the nodes within the
cell i, in main memory. In this way, we can greatly reduce
disk access for matrix DD.
Finally, we conclude this subsection with the techniques we
used to make the disk access as less as possible:
1. we partition the network space into cells according to the
nodes on the space. Moreover, we consider the adjacent nodes
in the same cell as a whole and store the distances between
nodes in disk blocks according to the cells they belong to.
2. we number the cells using a method similar to HC
order, thus the cells which are near to each other in space
are near in their numbers. Furthermore, our space-filling curve
is more efficient than HC, since we can get continuous cell
numbers even for the network space whose nodes are unevenly
distributed.
3. the number of a node consists of two part: the number of
the cell it resides and its number in the cell it resides. Thus,
given a set of nodes, we can easily get the cells they belong
to. Then, we can easily determine which blocks are needed
to access. Furthermore, these blocks are usually in sequential
order which will further short disk access time.
4. we store m special elements DDCelli,Celli (i=1,2,¬∑ ¬∑ ¬∑ , m)
in main memory to further reduce disk access.
VI. PERFORMANCE EVALUATION
In this section, we evaluate the performance of the pro-
posed CUkNNMean algorithm and the techniques we use to
12
01
2
3
4
10 30 50 70 100
Query interval length(time units)
C
P
U
 
t
i
m
e
(
s
)
CUkNN_Mean CUkNN_PLS
CUkNN_Mean_Imp CUkNN_PLS_Imp
75
80
85
90
95
100
10 30 50 70 100
Query interval length(time units)
P
r
e
c
i
s
i
o
n
(
%
)
CUkNN_Mean CUkNN_PLS
CUkNN_Mean_Imp CUkNN_PLS_Imp
0
10
20
30
40
50
60
70
10 30 50 70 100
Query interval length(time units)
 
D
i
s
k
 
b
l
o
c
k
s
(
K
)
CUkNN_Mean CUkNN_PLS
CUkNN_Mean_Imp CUkNN_PLS_Imp
 
                  (a)                                         (b)                                         (c) 
 
Fig. 12. Effect of query interval length
In the second set of experiments, we compare the perfor-
mance of the algorithms by varying the value of k from 1 to
20.
As shown in Figure 13(a), the CPU overhead for these four
algorithms grows when k increases. This is because that as the
value of k increases, the number of qualified objects increases,
thus more distance comparisons among these qualified objects
are needed. In general, CUkNNMean Imp and CUkNNMean
outperform their competitors in all cases.
Figure 13(b) shows that the precision of these four
algorithms increases slightly as k increases. The preci-
sion of CUkNNMean and CUkNNMean Imp is above
86% in all cases, and the precision of CUkNNPLS and
CUkNNPLS Imp is slightly less than that of CUkNNMean
and CUkNNMean Imp.
Figure 13(c) shows that the value k has great effect
on the number of accessed disk blocks for CUkNNMean
and CUkNNPLS . While, for CUkNNMean Imp, and
CUkNNPLS Imp, the varying value k has little effect on their
performance.
We then study the effect of varying the number of query
points on the performance of these four algorithms.
As shown in Figure 14(a), the running time of these four
methods increases almost linearly as the number of query
points increases. It is because the total running time is the
sum of the processing time of each query in the system.
Figure 14(b) shows that the query cardinality has little effect
on the precision of these four methods.
Figure 14(c) tells us that the query cardinality has very
different effect on accessed disk blocks of these four methods.
For CUkNNMean and CUkNNPLS , the number of accessed
disk blocks increases almost linearly as we increase the queries
on the system. While the increasing trend is rather gentle
for CUkNNMean Imp and CUkNNPLS Imp, contrast to
CUkNNMean and CUkNNPLS .
Next, we examine the effect of changing the number of
moving objects on the performance of the algorithms.
Figure 15(a) shows that the running time of these four meth-
ods decreases as the number of moving objects increases. This
is because that, as the number of objects increases, the object
density increases correspondingly, and the monitoring range of
a query decreases as well, thus the distance computation and
comparison needed decrease. As a result, the running time
decreases.
As shown in Figure 15(b), the precision of these methods
decreases slightly as the number of moving objects increases.
The reason lies in that as the object density increases, there
are more objects whose position segments overlap with each
other, thus some fault results may be caused.
Figure 15(c) shows that the object cardinality also has very
different effect on accessed disk blocks of these four methods.
For CUkNNMean and CUkNNPLS , the number of accessed
disk blocks decreases obviously as object cardinality increases.
On the contrary, the decreasing trend is rather gentle for
CUkNNMean Imp and CUkNNPLS Imp.
Finally, we compare the performance of these four algo-
rithms with respect to the number of vertices in the network.
We use a synthetic 2-dimensional grid network as the basic
network and 300*300 is the default grid size of the network,
which includes 5k queries and 500k objects following random
distributions. We consider the network with different grid sizes
that vary from 100*100 to 500*500, and the number of objects
(queries) included in the network changes correspondingly in
order to maintain a constant ratio of object (query) cardinality
and edge cardinality.
As shown in Figure 16(a), when the number of cells in-
creases, the CPU time of these algorithms increases obviously.
This is due to the fact that the number of queries in the system
increases as the network size increases.
Figure 16(b) shows that the precision of these four methods
remains almost unchanged as the network size varies.
As shown in Figure 16(c), the number of accessed disk
blocks of these algorithms increases greatly as the grid size
increases. The reason mainly lies in that the number of queries
need to be processed increases greatly when we increase the
the network size.
In summary, the CUkNNMean Imp algorithm has better
performance in CPU time, precision, and the number of
accessed disk blocks, and it is robust with respect to different
values of k, query cardinality, object cardinality, and network
size.
VII. CONCLUSION
This paper addresses the issue of processing CkNN queries
over moving objects with uncertain speeds (CUkNN ) in
road networks. We present an uncertain network distance
model to calculate the distances between moving objects and
a submitted query, both of which move with uncertain speeds
in the road network. Based on the distance model, we propose
a continuous UkNN query monitoring method, which is
14
[3] K. Mouratidis, M.L. Yiu, D. Papadias, and N.Mamoulis,‚ÄúContinuous
Nearest Neighbor Monitoring in Road Networks‚Äù, in VLDB, 2006, pp.
43-54.
[4] Y.K. Huang, Z.W. Chen, and C. Lee, ‚ÄúContinuous k-Nearest Neighbor
Query over Moving Objects in Road Network‚Äù, APWeb-WAIM 2009,
LNCS 5446, pp.27-38.
[5] W. Liao, X.P. Wu, C.H Yan ,and Z.N. Zhong, ‚ÄúProcessing of Continuous
k Nearest Neighbor Queries in Road Networks‚Äù, Software Engineering,
Artificial Intelligence, SCI 209, pp. 31-42.
[6] Y.K. Huang, S.J. Liao, and C. Lee, ‚ÄúEvaluating continuous K-nearest
neighbor query on moving objects with uncertainty‚Äù, Information Sys-
tems(June 2009), Volume 34 Issue 4-5, pp. 415‚Äì437.
[7] C.S. Jensen, J. Kolar, T.B. Pedersen, and I. Timko, ‚ÄúNearest neighbor
queries in road networks‚Äù, in ACM GIS(2003), pp. 1 - 8.
[8] D. Papadias, J. Zhang, N. Mamoulis, and Y. Tao, ‚ÄúQuery Processing in
Spatial Network Databases‚Äù, in VLDB, 2003, pp. 802-813.
[9] M.R. Kolahdouzan and C. Shahabi, ‚ÄúVoronoi-Based K Nearest Neighbor
Search for Spatial Network Databases‚Äù, in VLDB, 2004, pp. 840-851.
[10] X. Huang, C. S. Jensen, and S. Saltenis, ‚ÄúThe Islands Approach to
Nearest Neighbor Querying in Spatial Networks‚Äù, in SSTD(2005), pp.
73-90.
[11] H. Wang and R. Zimmermann, ‚ÄúLocation-based Query Processing on
Moving Objects in Road Networks‚Äù, in VLDB, 2007, pp. 321-332.
[12] A. Guttman, ‚ÄúR-Trees: A Dynamic Index Structure for Spatial Search-
ing‚Äù, in SIGMOD Conference, 1984, pp. 47-57.
[13] Maytham Safar, ‚ÄúEnhanced Continuous KNN Queries Using PINE on
Road Networks‚Äù, in ICDIM, 2006, pp. 248-256.
[14] C. R, N. Dalvi, and D. Suciu, ‚ÄúEfficient top-k query evaluation on
probabilistic data‚Äù, in ICDE, 2007, pp. 886-895.
[15] K. Yi, F. Li, G. Kollios, and D. Srivastava, ‚ÄúEfficient processing of top-k
queries in uncertain databases‚Äù, in ICDE, 2008, pp. 1669-1682.
[16] M.A. Soliman, I.F. Ilyas, and K.C.C. Chang, ‚ÄúTop-k query processing
in uncertain databases‚Äù, in ICDE, 2007, pp. 896‚Äì905.
[17] G. Beskales, M. Soliman, I.F. lyas, ‚ÄúEfficient search for the top-k
probable nearest neighbors in uncertain databases‚Äù, in VLDB, 2008, pp.
326-339.
[18] N. Augsten, D. Barbosa, M. Bo¬®hlen, and T. Palpanas, ‚ÄúTASM: Top-k
Approximate Subtree Matching‚Äù, in ICDE, 2010, pp. 353-364.
[19] L.F. Lin, Y.K. Huang, ‚ÄúScalable Processing of Continuous K-Nearest
Neighbor Queries with Uncertainty in Spatio-Temporal Databases‚Äù, in
ICRCCS, 2009, pp. 210-213.
[20] Goce Trajcevski, Roberto Tamassia, Hui Ding, Peter Scheuermann,
and F.Cruz Isabel, ‚ÄúContinuous probabilistic nearest-neighbor queries for
uncertain trajectories‚Äù, in EDBT, 2009, pp. 874-885 .
[21] J.C. Chen, R. Cheng, M. Mokbel,and C.Y. Chow, ‚ÄúScalable processing of
snapshot and continuous nearest-neighbor queries over one-dimensional
uncertain data‚Äù, The VLDB Journal(2009) 18, pp. 1219-1240.
[22] Y.K. Huang and C. Lee, ‚ÄúEfficient evaluation of continuous spatio-
temporal queries on moving objects with uncertain speed‚Äù, Geoinformat-
ica(April 2010), Volume 14 Issue 2, pp. 163-200.
[23] Gotsman, C., Lindenbaum, M.: On the metric properties of discrete
space-filling curves. IEEE Trans. Image Process. 5(5), 794C797 (1996)
[24] Guohui Li, Yanhong Li, and LihChyun Shu, ‚ÄúCkNN Query Processing
over Moving Objects with Uncertain Speeds in road networks‚Äù, in APweb
2011.
[25] http://www.fh-oow.de/institute/iapg/personen/brinkhoff/generator/
16
99 Âπ¥Â∫¶Â∞àÈ°åÁ†îÁ©∂Ë®àÁï´Á†îÁ©∂ÊàêÊûúÂΩôÊï¥Ë°® 
Ë®àÁï´‰∏ªÊåÅ‰∫∫ÔºöÂæêÁ´ãÁæ§ Ë®àÁï´Á∑®ËôüÔºö99-2221-E-006-124- 
Ë®àÁï´ÂêçÁ®±ÔºöÂú®ÂèØËÆäÂãïÈÄüÂ∫¶ËôïÁêÜÂô®‰∏ä‰πãÈùûÊê∂‰ΩîÂºèÂç≥ÊôÇÊéíÁ®ã 
ÈáèÂåñ 
ÊàêÊûúÈ†ÖÁõÆ ÂØ¶ÈöõÂ∑≤ÈÅîÊàê
Êï∏ÔºàË¢´Êé•Âèó
ÊàñÂ∑≤ÁôºË°®Ôºâ
È†êÊúüÁ∏ΩÈÅîÊàê
Êï∏(Âê´ÂØ¶ÈöõÂ∑≤
ÈÅîÊàêÊï∏) 
Êú¨Ë®àÁï´ÂØ¶
ÈöõË≤¢ÁçªÁôæ
ÂàÜÊØî 
ÂñÆ‰Ωç 
ÂÇô Ë®ª Ôºà Ë≥™ Âåñ Ë™™
ÊòéÔºöÂ¶ÇÊï∏ÂÄãË®àÁï´
ÂÖ±ÂêåÊàêÊûú„ÄÅÊàêÊûú
Âàó ÁÇ∫ Ë©≤ Êúü Âàä ‰πã
Â∞Å Èù¢ ÊïÖ ‰∫ã ...
Á≠âÔºâ 
ÊúüÂàäË´ñÊñá 0 0 100%  
Á†îÁ©∂Â†±Âëä/ÊäÄË°ìÂ†±Âëä 0 0 100%  
Á†îË®éÊúÉË´ñÊñá 0 0 100% 
ÁØá 
 
Ë´ñÊñáËëó‰Ωú 
Â∞àÊõ∏ 0 0 100%   
Áî≥Ë´ã‰∏≠‰ª∂Êï∏ 0 0 100%  Â∞àÂà© Â∑≤Áç≤Âæó‰ª∂Êï∏ 0 0 100% ‰ª∂  
‰ª∂Êï∏ 0 0 100% ‰ª∂  
ÊäÄË°ìÁßªËΩâ 
Ê¨äÂà©Èáë 0 0 100% ÂçÉÂÖÉ  
Á¢©Â£´Áîü 0 3 100%  
ÂçöÂ£´Áîü 0 0 100%  
ÂçöÂ£´ÂæåÁ†îÁ©∂Âì° 0 0 100%  
ÂúãÂÖß 
ÂèÉËàáË®àÁï´‰∫∫Âäõ 
ÔºàÊú¨ÂúãÁ±çÔºâ 
Â∞à‰ªªÂä©ÁêÜ 0 0 100% 
‰∫∫Ê¨° 
 
ÊúüÂàäË´ñÊñá 0 1 100%  
Á†îÁ©∂Â†±Âëä/ÊäÄË°ìÂ†±Âëä 0 0 100%  
Á†îË®éÊúÉË´ñÊñá 0 0 100% 
ÁØá 
 
Ë´ñÊñáËëó‰Ωú 
Â∞àÊõ∏ 0 0 100% Á´†/Êú¨  
Áî≥Ë´ã‰∏≠‰ª∂Êï∏ 0 0 100%  Â∞àÂà© Â∑≤Áç≤Âæó‰ª∂Êï∏ 0 0 100% ‰ª∂  
‰ª∂Êï∏ 0 0 100% ‰ª∂  
ÊäÄË°ìÁßªËΩâ 
Ê¨äÂà©Èáë 0 0 100% ÂçÉÂÖÉ  
Á¢©Â£´Áîü 0 0 100%  
ÂçöÂ£´Áîü 0 0 100%  
ÂçöÂ£´ÂæåÁ†îÁ©∂Âì° 0 0 100%  
ÂúãÂ§ñ 
ÂèÉËàáË®àÁï´‰∫∫Âäõ 
ÔºàÂ§ñÂúãÁ±çÔºâ 
Â∞à‰ªªÂä©ÁêÜ 0 0 100% 
‰∫∫Ê¨° 
 
ÂúãÁßëÊúÉË£úÂä©Â∞àÈ°åÁ†îÁ©∂Ë®àÁï´ÊàêÊûúÂ†±ÂëäËá™Ë©ïË°® 
Ë´ãÂ∞±Á†îÁ©∂ÂÖßÂÆπËàáÂéüË®àÁï´Áõ∏Á¨¶Á®ãÂ∫¶„ÄÅÈÅîÊàêÈ†êÊúüÁõÆÊ®ôÊÉÖÊ≥Å„ÄÅÁ†îÁ©∂ÊàêÊûú‰πãÂ≠∏Ë°ìÊàñÊáâÁî®ÂÉπ
ÂÄºÔºàÁ∞°Ë¶ÅÊïòËø∞ÊàêÊûúÊâÄ‰ª£Ë°®‰πãÊÑèÁæ©„ÄÅÂÉπÂÄº„ÄÅÂΩ±ÈüøÊàñÈÄ≤‰∏ÄÊ≠•ÁôºÂ±ï‰πãÂèØËÉΩÊÄßÔºâ„ÄÅÊòØÂê¶ÈÅ©
ÂêàÂú®Â≠∏Ë°ìÊúüÂàäÁôºË°®ÊàñÁî≥Ë´ãÂ∞àÂà©„ÄÅ‰∏ªË¶ÅÁôºÁèæÊàñÂÖ∂‰ªñÊúâÈóúÂÉπÂÄºÁ≠âÔºå‰Ωú‰∏ÄÁ∂úÂêàË©ï‰º∞„ÄÇ
1. Ë´ãÂ∞±Á†îÁ©∂ÂÖßÂÆπËàáÂéüË®àÁï´Áõ∏Á¨¶Á®ãÂ∫¶„ÄÅÈÅîÊàêÈ†êÊúüÁõÆÊ®ôÊÉÖÊ≥Å‰Ωú‰∏ÄÁ∂úÂêàË©ï‰º∞ 
‚ñ†ÈÅîÊàêÁõÆÊ®ô 
‚ñ°Êú™ÈÅîÊàêÁõÆÊ®ôÔºàË´ãË™™ÊòéÔºå‰ª• 100 Â≠óÁÇ∫ÈôêÔºâ 
‚ñ°ÂØ¶È©óÂ§±Êïó 
‚ñ°Âõ†ÊïÖÂØ¶È©ó‰∏≠Êñ∑ 
‚ñ°ÂÖ∂‰ªñÂéüÂõ† 
Ë™™ÊòéÔºö 
2. Á†îÁ©∂ÊàêÊûúÂú®Â≠∏Ë°ìÊúüÂàäÁôºË°®ÊàñÁî≥Ë´ãÂ∞àÂà©Á≠âÊÉÖÂΩ¢Ôºö 
Ë´ñÊñáÔºö‚ñ°Â∑≤ÁôºË°® ‚ñ°Êú™ÁôºË°®‰πãÊñáÁ®ø ‚ñ†Êí∞ÂØ´‰∏≠ ‚ñ°ÁÑ° 
Â∞àÂà©Ôºö‚ñ°Â∑≤Áç≤Âæó ‚ñ°Áî≥Ë´ã‰∏≠ ‚ñ†ÁÑ° 
ÊäÄËΩâÔºö‚ñ°Â∑≤ÊäÄËΩâ ‚ñ°Ê¥ΩË´á‰∏≠ ‚ñ†ÁÑ° 
ÂÖ∂‰ªñÔºöÔºà‰ª• 100 Â≠óÁÇ∫ÈôêÔºâ 
Â∑≤Á∂ìÊí∞Êñá‰∏¶ÊäïÁ®øËá≥ÂúãÈöõÊúüÂàä,ÁõÆÂâçÂØ©Êü•‰∏≠. 
3. Ë´ã‰æùÂ≠∏Ë°ìÊàêÂ∞±„ÄÅÊäÄË°ìÂâµÊñ∞„ÄÅÁ§æÊúÉÂΩ±ÈüøÁ≠âÊñπÈù¢ÔºåË©ï‰º∞Á†îÁ©∂ÊàêÊûú‰πãÂ≠∏Ë°ìÊàñÊáâÁî®ÂÉπ
ÂÄºÔºàÁ∞°Ë¶ÅÊïòËø∞ÊàêÊûúÊâÄ‰ª£Ë°®‰πãÊÑèÁæ©„ÄÅÂÉπÂÄº„ÄÅÂΩ±ÈüøÊàñÈÄ≤‰∏ÄÊ≠•ÁôºÂ±ï‰πãÂèØËÉΩÊÄßÔºâÔºà‰ª•
500 Â≠óÁÇ∫ÈôêÔºâ 
The past decade has seen signicant growth in the volume and sophistication of 
research on energy-efficient scheduling for real-time systems on machines with 
DVS capabilities. Although most of such research performed to date assumes 
preemptive scheduling, it is our belief that novel research is needed for 
non-preemptive workloads in a number of embedded applications, particularly those 
found in small embedded devices with limited memory capacity as well as those built 
on multiprocessor platforms. To the extent the proposed research is successful, 
it will provide a useful scheduling technique for designers of energy-constrained
applications in which 
task preemption is not an option to reduce task response times. Our technique in 
characterizing the workload on a non-preemptive task from higher-priority tasks 
may also be applicable in other non-preemptive scheduling research. It is our hope 
that the results obtained from this research may serve as enabling technologies 
for some of the recent research focuses promoted in the National Science Council's 
Computer Science & Information Engineering program that are supervised by the 
Department of Engineering and Applied Science. Some of these focused areas include 
life caring systems, smart living spaces, digital life technologies and 
