II 
 
Abstract 
Although many algorithms have been proposed over the past few decades, it is still very 
important for solving an NP-Complete problem. Due to the complexity of the problem, many 
algorithms including (Monte Carlo ï¼Œ MC) ã€ (Simulated Annealing ï¼Œ SA) ã€ (Particle Swarm 
Optimizationï¼ŒPSOï¼‰andï¼ˆGenetic Algorithmï¼ŒGAï¼‰have been proposed for searching good solutions 
in reasonable computation times.The aim of this research is to make a study and an analysis of the 
above optimal algorithms in order to find the advantage and processing of their approaches which 
were summarized and extracted the characteristics and essence of the various algorithms. Then to 
integrate and reconstruct a new algorithm framework which is the combination of these advantages 
of these algorithm characteristics. From the analysis of relevant literature and discussion, We found 
that the advantages of these algorithms can be expressed in two properties of the handling 
characteristics of these algorithms. The first is the neighborhood search processing, and second, 
moving the processing solution. Therefore, if we could make deconstruction analysis and extraction 
for the best of these algorithms in these two properties, and then be integrated into an advantage in 
these two properties make parametric algorithm modulation framework. With the adjustment of 
parameters that will enable our algorithm framework to transfer between the above-mentioned four 
kinds of algorithms smoothly, in order to solve problem with both intensification and 
diversification of the searching convergence. Therefore, the performance of our new algorithm 
framework could be able to greatly enhance. Most previous studies made a hybrid algorithm from 
these algorithms mentioned above, to make up the shortcomings of a single algorithm for each 
other. However, this improvement method only for a specific problem solving, it is to improve the 
solution process and the limitations of solution searching. 
The present study attempted to integrate and reconstruct a new algorithm framework which is 
with the advantages of the integration of reconstruction algorithms characteristics to break through 
this limitation and improve the solving efficiency. Believe that such a new algorithm framework 
not only can enhance the performance of algorithms but also help to cope with the diversity and 
complexity of problem solving. And to provide related researchers a different contemplation of the 
algorithm study. 
 
Keywordsï¼šNeighborhood, Hybrid, Monte Carlo, Simulated Annealing, Particle Swarm 
Optimization 
  
IV 
 
åœ–ç›®éŒ„ 
FIG. 1.è’™åœ°å¡ç¾…æ¨¡æ“¬æ³•æµç¨‹åœ–	 Â .................................................................................................	 Â 3	 Â 
FIG. 2. æ¨¡æ“¬é€€ç«æ³•æµç¨‹åœ–	 Â .......................................................................................................	 Â 3	 Â 
FIG. 3.é„°åŸŸé›†åˆ )(SN è¿­ä»£æ¼”é€²åœ–	 Â ...........................................................................................	 Â 4	 Â 
FIG. 4. PSO ç²’å­ä½ç½®ã€é€Ÿåº¦èˆ‡ PBEST, GBEST ä¹‹æœå°‹ç¤ºæ„åœ–	 Â ...................................................	 Â 4	 Â 
FIG. 5.ç•¶ dgbest ã€ idpbset åˆ†åˆ¥ä½æ–¼ ubx èˆ‡ lbx æ™‚ä¹‹é„°åŸŸé›†åˆç©ºé–“	 Â ...........................................	 Â 5	 Â 
FIG. 6.æ–°æž¶æ§‹æ··åˆå¼æœ€ä½³åŒ–æ¼”ç®—æ³•ç³»çµ±æž¶æ§‹	 Â .........................................................................	 Â 6	 Â 
FIG. 7. SPHERE å‡½æ•¸ä¹‹å„æœå°‹æ¬¡æ•¸æ‰€å¾—çµæžœå¹³å‡å€¼	 Â ................................................................	 Â 8	 Â 
FIG. 8. SPHERE å‡½æ•¸æ‰€å¾—çµæžœç´°éƒ¨	 Â ............................................................................................	 Â 8	 Â 
FIG. 9. ROSEN BROCK å‡½æ•¸ä¹‹å„æœå°‹æ¬¡æ•¸æ‰€å¾—çµæžœå¹³å‡å€¼	 Â ......................................................	 Â 8	 Â 
FIG. 10.  ROSEN BROCK å‡½æ•¸æ‰€å¾—çµæžœç´°éƒ¨	 Â ..............................................................................	 Â 8	 Â 
FIG. 11. GRIEWANK å‡½æ•¸ä¹‹å„æœå°‹æ¬¡æ•¸æ‰€å¾—çµæžœå¹³å‡å€¼	 Â .........................................................	 Â 9	 Â 
FIG. 12. GRIEWANK å‡½æ•¸æ‰€å¾—çµæžœç´°éƒ¨	 Â .....................................................................................	 Â 9	 Â 
FIG. 13. RASTRIGRIN å‡½æ•¸ä¹‹å„æœå°‹æ¬¡æ•¸æ‰€å¾—çµæžœå¹³å‡å€¼	 Â ........................................................	 Â 9	 Â 
 
  
1 
 
  ç ”ç©¶ç›®çš„èˆ‡æ–‡ç»æŽ¢è¨Ž 
æœ¬ç ”ç©¶æå‡ºä½¿ç”¨é„°åŸŸæ¦‚å¿µç‚ºæž¶æ§‹çš„æ··åˆå¼æ¼”ç®—æ³•ï¼Œçµåˆ MCã€SA åŠ PSO ä¸‰ç¨®è‡¨åŸŸæœ€ä½³
åŒ–æ¼”ç®—æ³•ä¹‹ç‰¹æ€§ï¼Œè—‰ç”±é„°åŸŸæž¶æ§‹åŠå„ç²’å­ä¸”ä¾ç…§ä¸åŒéœ€æ±‚é©æ™‚èª¿æ•´åƒæ•¸ï¼Œä½¿ç²’å­å…·æœ‰ä¸‰å€‹
æœ€ä½³åŒ–æ¼”ç®—æ³•ä¹‹ç‰¹æ€§ã€‚é€éŽé€™ä¸‰ç¨®æ¼”ç®—æ³•ä¹‹ç‰¹æ€§ç ”ç©¶å°‡å…¶å„é …ç‰¹è³ªäºˆä»¥åˆ†æžå’Œè§£æ§‹ï¼Œä¸¦åœ¨
é„°åŸŸæœå°‹æ³•çš„æž¶æ§‹ä¸‹åšå»ºæ§‹ä¸€æ··åˆå¼æ¼”ç®—æ³•ï¼Œå¯ç¶“ç”±èª¿æ•´åƒæ•¸æ”¹è®Šç‰¹æ€§ï¼Œå¦‚ç”± MC èª¿æ•´æˆ
SAï¼›SA èª¿æ•´æˆ PSOâ€¦ç­‰ï¼Œä»¥é”æˆæ¼”ç®—æ³•ä¹‹èžåˆèˆ‡æ¼¸é€²å¯é©ã€‚æœ¬ç ”ç©¶æŽ¢è¨Ž MCã€SAã€PSO ä¸‰
å€‹æœ€ä½³åŒ–æ¼”ç®—æ³•ä¹‹ç‰¹æ€§åˆ†æžï¼Œä»¥åŠæ··åˆå¼æ¼”ç®—æ³•ç›¸é—œç‰¹æ€§ä¹‹é—œé€£çš„ç ”ç©¶èˆ‡æž¶æ§‹è¨­è¨ˆï¼Œä¸¦å®Œ
æˆæ··åˆå¼æœ€ä½³åŒ–æ¼”ç®—æ³•ä¹‹è¨­è¨ˆã€é©—è­‰ï¼ŒæœŸæœ›çµåˆå„ªé»žæ”¹å–„ç¼ºé»žä»¥é¢å°å„ç¨®å•é¡Œï¼Œæå‡æ¼”ç®—
æ•ˆèƒ½ï¼Œä¸¦é©—è­‰æ­¤æž¶æ§‹ä¹‹å¯è¡Œæ€§ã€‚ 
è’™åœ°å¡ç¾…æ¨¡æ“¬æ³•æ˜¯ä¸€ç¨®æ•¸å€¼æ–¹æ³•ï¼Œåˆ©ç”¨äº‚æ•¸å–æ¨£ (Random Sampling) æ¨¡æ“¬ä¾†è§£æ±ºæ•¸å­¸
å•é¡Œ[1]ã€‚åœ¨æ•¸å­¸ä¸Šæ‰€è¬‚ç”¢ç”Ÿäº‚æ•¸ï¼Œå°±æ˜¯å¾žä¸€é–‹å§‹çµ¦å®šçš„æ•¸é›†åˆä¸­é¸å‡ºä¾†çš„æ•¸ï¼Œè‹¥å¾žé›†åˆä¸­
ä¸æŒ‰é †åºéš¨æ©Ÿé¸å–å…¶ä¸­æ•¸ï¼Œç¨±ç‚ºäº‚æ•¸ï¼Œè‹¥æ˜¯è¢«é¸åˆ°çš„æ©ŸçŽ‡ç›¸åŒæ™‚ï¼Œç¨±ç‚ºå‡å‹»äº‚æ•¸ã€‚è’™åœ°å¡
ç¾…æ¨¡æ“¬æ³•æ˜¯åŸºæ–¼å¤§æ•¸æ³•å‰‡çš„å¯¦è­‰æ–¹æ³•ï¼Œç•¶å¯¦é©—çš„æ¬¡æ•¸è¶Šå¤šï¼ŒæœŸå¹³å‡å€¼ä¹Ÿå°±æœƒè¶Šè¶¨è¿‘æ–¼ç†è«–
å€¼ã€‚æ¨¡æ“¬é€€ç«æ¼”ç®—æ³•ä¸»è¦çš„æ¦‚å¿µæ˜¯æŠŠç ”ç©¶çš„å•é¡Œçœ‹ä½œæˆä¸€å€‹çµ±è¨ˆç³»çµ±ï¼Œè€Œçµ±è¨ˆç³»çµ±ä¸­çš„æŸ
ä¸€å€‹æº«åº¦çš„ç‹€æ…‹åˆ†ä½ˆæ˜¯æ»¿è¶³ä¸€å€‹æ³¢èŒ²æ›¼(Boltzmann)åˆ†ä½ˆå‡½æ•¸ã€‚å› æ­¤åœ¨å•é¡Œä¸­å°‹æ‰¾æœ€ä½³åŒ–è§£
æ™‚ï¼Œå°±æ˜¯åˆ©ç”¨é€™å€‹åˆ†ä½ˆå‡½æ•¸ä¾†é¸å–ç­”æ¡ˆã€‚Kirkpatrick ç­‰äººåœ¨ 1983 å¹´æå‡ºæ¨¡æ“¬é€€ç«æ³•ï¼Œä¸¦æ‡‰
ç”¨åœ¨æœ€ä½³åŒ–å•é¡Œä¸­ï¼Œå±¬æ–¼è’™åœ°å¡ç¾…æ¼”ç®—æ³•çš„æŽ¨å»£ [2-9] ã€‚PSO æ˜¯ä¸€å€‹ä»¥æ—ç¾¤æ¦‚å¿µç‚ºåŸºç¤Žçš„
ç¾¤åŸŸæœ€ä½³è§£æœå°‹æ¼”ç®—æ³•ã€‚ä¸¦ä¸”æœ‰è‘—æ¼”åŒ–å¼è¨ˆç®—æ¦‚å¿µ ä¸­å¸¸è¦‹çš„ç‰¹æ€§ï¼Œå¦‚ã€Œé©æ‡‰åº¦ã€(fitness)
çš„è©•ä¼°ã€‚è€Œç²’å­ç§»å‹•æ™‚æœƒå¾€å€‹é«”æœ€ä½³ç¶“é©—(PBEST)æˆ–æ˜¯ç¾¤é«”æœ€ä½³ç¶“é©—(GBEST)çš„æ–¹å‘æ”¹è®Šã€‚
è€Œç²’å­åœ¨æ±ºå®šç§»å‹•æ–¹å‘æ™‚ä¹ŸåŠ å…¥äº†ã€Œéš¨æ©Ÿã€(randomness)çš„æ¦‚å¿µï¼Œè®“ç²’å­çš„ç§»å‹•æœ‰æ©Ÿæœƒè·³è„«
ç›®å‰çš„è¶¨å‹¢ï¼Œä»¥å…é™·å…¥ã€Œå€åŸŸæœ€ä½³è§£ã€(local optimum)ã€‚æ­¤å¤–ï¼ŒPSO ä¸­çš„ç²’å­ï¼Œä¹Ÿå°±æ˜¯æ¯å€‹
å¯èƒ½çš„æ½›åœ¨è§£ï¼Œæ˜¯æ“æœ‰é€Ÿåº¦ä¸¦åœ¨æ•´å€‹æœå°‹ç©ºé–“ä¸­ã€Œé£›è¡Œã€çš„ï¼Œ å¦å¤–åœ¨ PSO æž¶æ§‹ä¸‹ç²’å­è·Ÿ
ç²’å­ç¾¤éƒ½æœ‰è¨˜æ†¶èƒ½åŠ›ã€‚PSO çš„å¦ä¸€å€‹é‡è¦ç‰¹é»žæ˜¯ï¼Œåªéœ€è¦ç°¡å–®çš„æ•¸å­¸é‹ç®—ï¼Œä¸¦ä¸”å¯ä»¥ç”¨å°‘
é‡çš„ç¨‹å¼ç¢¼å°±èƒ½åœ¨é›»è…¦ä¸Šå¯¦ä½œã€‚ç”±æ–¼ PSO é‹ç®—æˆæœ¬çš„ä½Žå»‰ä¸¦ä¸”æŽ¡ç”¨äº†ç¤¾æœƒè¡Œç‚ºçš„æ¨¡åž‹ï¼Œè¨±
å¤šå¯¦éš›çš„æ‡‰ç”¨éƒ½å·²ç¶“ä½¿ç”¨é€™å€‹æŠ€è¡“ï¼Œå¤šæ•¸æ‡‰ç”¨éƒ½å·²è¢«æå‡ºä¸¦åœ¨æ–‡ç»ä¸­å±•ç¾å®ƒæ˜¯æˆåŠŸçš„
[10-12]ã€‚ 
å‚³çµ±é„°è¿‘æœå°‹æ³•ç‚ºå€åŸŸæœå°‹æ³•ï¼ˆlocal searchï¼‰ï¼Œå¸¸å®¹ï§ é¢ï§¶é™·å…¥ã€Œå±€éƒ¨æœ€ä½³è§£ï¼ˆLocal 
Optimumï¼‰ã€çš„å›°å¢ƒï¼Œä»¥è‡´ç„¡æ³•æœå°‹åˆ°ï¤å¥½çš„è¿‘ä¼¼è§£ã€‚ç‚ºå…‹æœæ­¤ä¸€ç¼ºé»žï¼Œ1980 ï¦Žä»£é–‹å§‹ï¼Œæ–°
çš„å•Ÿç™¼å¼è§£é¡Œæ¦‚ï¦£é€æ¼¸å½¢æˆï¼Œä¸¦é€ æˆä¸€è‚¡æ–°èˆˆçš„ç ”ç©¶é¢¨æ½®ã€‚æ­¤ç­‰æ–°è¿‘å•Ÿç™¼å¼æ–¹æ³•çš„ä¸»è¦æ±‚è§£
ç­–ï¥¶çš†ç‚ºåœ¨å¯ï¨ˆè§£å€åŸŸä¸­ä¸€æ­¥æ­¥ç§»å‹•ä»¥é€æ­¥æ”¹å–„ç›®æ¨™å‡½ï¥©å€¼ï¼Œä¸¦ä½¿æ¼”ç®—æ³•å…·æœ‰å¾žå±€éƒ¨æœ€ä½³è§£
è„«ï§ªçš„èƒ½ï¦Šï¼ŒåŒæ™‚åˆèƒ½å‘æœ€ä½³è§£é€¼è¿‘ã€‚ 
Hybrid æ··åˆæ¼”ç®—æ³•å³æ˜¯çµåˆï¥¸ç¨®ä»¥ä¸Šçš„æ¼”ç®—æ³•ï¼Œçµåˆå…¨åŸŸæœå°‹æ¼”ç®—æ³•å’Œé„°åŸŸæœå°‹(local 
search)æ¼”ç®—æ³•ï¼Œä¸€ç¨®æ–¹æ³•æ˜¯ç›´æŽ¥å°‡é„°åŸŸæœå°‹æ¼”ç®—æ³•ä¸­çš„ local improvement operator ç›´æŽ¥åŠ å…¥å…¨åŸŸ
æœå°‹æ¼”ç®—æ³•ä¸­ï¼Œä½¿å…¨åŸŸæœå°‹æ¼”ç®—æ³•å’Œ local search æ¼”ç®—æ³•çµåˆåœ¨ä¸€èµ·ï¼Œåœ¨æ··åˆå¼çš„æ¼”ç®—æ³•ï§¨ï¼Œ
é„°åŸŸæœå°‹æ¼”ç®—æ³•å¯ä»¥å¢žé€²å¾—åˆ°å…¨åŸŸ/å€åŸŸæœ€ä½³è§£çš„æ•ˆæžœã€‚ç›®å‰æ··åˆæ¼”ç®—æ³•ä»¥ç²’å­èšé›†æ³•(PSO)çµ
åˆæ¨¡æ“¬é€€ç«æ³•(SA)[13]èˆ‡éºå‚³æ¼”ç®—æ³•(GA)çµåˆæ¨¡æ“¬é€€ç«æ³•(SA)[14-17]ç‚ºè¼ƒå¸¸è¦‹ï¼ŒPSO-SA æ··åˆæœ
å°‹æ³•æ˜¯ä»¥æœå°‹é„°è¿‘è¼ƒä½³è§£çš„æ¦‚å¿µï¼Œå°‡ SA çš„è·³èºæ©Ÿåˆ¶ç‰¹æ€§å¼•å…¥ PSO æ¼”ç®—æ³•ä¸­ï¼Œæ‰€æå‡ºæ··åˆæœ
å°‹æ³•ã€‚è€Œ GA-SA æ¨¡æ“¬é€€ç«æ¼”ç®—æ³•åœ¨æœå°‹å…¨åŸŸæœ€ä½³è§£çš„éŽç¨‹ä¸­å¾€å¾€éœ€è¦èŠ±è²»å¾ˆé•·çš„æ™‚é–“ã€‚é™¤æ­¤
ä¹‹å¤–ï¼Œæ¨¡æ“¬é€€ç«æ¼”ç®—æ³•åªè‘—çœ¼æ–¼å–®ä¸€çš„å•é¡Œè§£ï¼Œå› æ­¤ä¸æ˜“å¹³è¡ŒåŒ–ã€‚æ–¼æ˜¯ï¼Œçµåˆéºå‚³æ¼”ç®—æ³•èˆ‡
æ¨¡æ“¬é€€ç«æ¼”ç®—æ³•çš„å„ªé»žï¼Œç™¼å±•å‡ºæ··åˆå¼æ¼”åŒ–æ¼”ç®—æ³•ä»¥è™•ç†æœ€ä½³åŒ–å•é¡Œçš„æ±‚è§£ä¾¿æˆç‚ºä¸€ç¨®æ¥µå…·
å¸å¼•åŠ›çš„ç­–ç•¥ã€‚ 
 
  
3 
 
è’™åœ°å¡ç¾…æ¨¡æ“¬æ³•ä¹‹é„°åŸŸè§£é›†åˆå¯è¡¨ç¤ºç‚ºï¼š  Â ð‘!" = {ð‘‹!!!|ð‘‹!!! = ð‘  âˆ™ ð‘Ÿð‘Žð‘›ð‘‘ 1 }  (2) 
é–‹å§‹
åˆå§‹åŒ– ç”Ÿæˆåˆå§‹è§£ ç”¢ç”Ÿæ–°è§£
æ˜¯æ˜¯å¦é”æˆ
çµæŸæ¢ä»¶
å¦
æ˜¯
æ˜¯å¦æŽ¥å—
æ–°è§£ç‚ºç›®å‰
æœ€ä½³è§£
æ›´æ–°ç›®å‰
æœ€ä½³è§£
çµæŸ
å¦
 
Fig. 1.è’™åœ°å¡ç¾…æ¨¡æ“¬æ³•æµç¨‹åœ– 
B.  æ¨¡æ“¬é€€ç«æ³•(Simulated Annealing) 
 Kirkpatrick ç­‰äººåœ¨ 1983 å¹´å‰‡å°‡è’™åœ°å¡ç¾…æ¼”ç®—æ³•å»¶ä¼¸æŽ¨å»£å‡ºæ¨¡æ“¬é€€ç«æ³•ï¼Œä¸¦æˆåŠŸæ‡‰ç”¨æ–¼æœ€
ä½³åŒ–å•é¡Œã€‚ä¸åƒ…å¯ä»¥æŽ¥å—è¼ƒä½³è§£ï¼Œä¹Ÿå…·æœ‰ä¸€å®šçš„æ©ŸçŽ‡åˆ¤æ–·æ˜¯å¦æŽ¥å—ä¸€å€‹è¼ƒå·®è§£ï¼Œå› è€Œå…·æœ‰è·³
è„«å‡ºå€åŸŸæœ€ä½³è§£çš„èƒ½åŠ›ã€‚ 
æ­¤å¤–æ¨¡æ“¬é€€ç«æŽ¡ç”¨ Metroplis æŽ¥å—æ³•å‰‡ï¼Œç”¨ä¾†æ±ºå®šæ˜¯å¦æŽ¥å—ä¸€å€‹èƒ½é‡è®Šå‹•çš„æ”¹è®Šã€‚è¨ˆç®—ç›®
å‰è§£çš„èƒ½é‡Eï¼Œä»¥ç›®å‰è§£ç‚ºä¸­å¿ƒéš¨æ©Ÿç”¢ç”Ÿæ–°çš„é„°è¿‘è§£ä¸¦è¨ˆç®—èƒ½é‡ E Ê¹â€² ï¼Œ EEE âˆ’Ê¹â€²=Î” ç‚ºå…©å€‹è§£
ä¹‹é–“çš„èƒ½é‡å·®ï¼Œæ ¹æ“šä¸‹åˆ—çš„æ©ŸçŽ‡å…¬å¼ä¾†æ±ºå®šæ˜¯å¦æŽ¥å—é„°è¿‘è§£å–ä»£ç›®å‰è§£ï¼š 
ð‘ƒ = 1 Â  Â  Â  Â  Â  Â  Â  Â , ð‘–ð‘“ Â âˆ†ð¸ â‰¤ 0ð‘’!âˆ†!!  Â , ð‘–ð‘“ Â âˆ†ð¸ > 0 (3) 
é€€ç«æ¨¡æ“¬æ³•åœ¨æœå°‹éŽç¨‹ä¸­ï¼Œç¶“ç”±é™æº«çš„ç¨‹åºï¼Œå…¶æœå°‹ç¯„åœæœƒé€æ¼¸æ”¶æ–‚è‡³å…¨åŸŸæœ€ä½³è§£æ‰€åœ¨çš„å€
åŸŸï¼Œé€²è€Œæ‰¾å‡ºå…¨åŸŸæœ€ä½³è§£ï¼›ä½†è‹¥åœ¨è¼ƒä½Žçš„æº«åº¦èµ·å§‹æŽ§åˆ¶ä¸‹ï¼Œå‰‡éœ€è¦è¼ƒå¤šæ¬¡çš„è¿­ä»£æ‰èƒ½é”åˆ°ç³»
çµ±å¹³è¡¡ï¼Œå› ç‚ºå…¶ç³»çµ±åœ¨æŽ¥å—çˆ¬æ˜‡çš„æ©ŸçŽ‡ç›¸å°è®Šä½Žï¼Œæ‰€ä»¥éœ€è¦è¼ƒå¤šæ¬¡çš„è·³åŸŸæ‰èƒ½è„«é›¢å±€éƒ¨æœ€ä½³
è§£çš„é™·é˜±ã€‚ç”±æ­¤å¯å¾—çŸ¥ï¼Œè¼ƒé«˜çš„èµ·å§‹æº«åº¦æœ‰è¼ƒé«˜çš„é·ç§»çŽ‡ä»¥è·³è„«å‡ºå±€éƒ¨æœ€ä½³è§£ï¼ŒåŒæ™‚åªè¦é™
æº«çš„ç¨‹åºå¤ æ…¢ä¸¦æœ‰è¶³å¤ çš„è¿­ä»£æ¬¡æ•¸ï¼Œå‰‡å¯é”åˆ°ç†±å‹•å¹³è¡¡ã€‚ 
å¦
é–‹å§‹
åˆå§‹åŒ– ç”Ÿæˆåˆå§‹è§£ ç”¢ç”Ÿé„°è¿‘è§£ æ˜¯å¦é”æˆçµæŸæ¢ä»¶æ˜¯å¦é™æº«
æ˜¯
æ˜¯å¦æŽ¥å—
é„°è¿‘è§£ç‚ºç›®å‰
æœ€ä½³è§£
æ›´æ–°ç›®å‰
æº«åº¦
æ›´æ–°ç›®å‰
æœ€ä½³è§£
å¦
æ˜¯
æ˜¯
çµæŸ
å¦
 
Fig. 2. æ¨¡æ“¬é€€ç«æ³•æµç¨‹åœ– 
æ¨¡æ“¬é€€ç«æ³•ä¹‹ä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š 
1. é‡å°å•é¡Œé¸å®šç›®æ¨™å‡½æ•¸ä½œç‚ºèƒ½é‡å‡½æ•¸ )(xEX = ï¼Œå…¶ä¸­çš„ xä¾¿æ˜¯å•é¡Œè§£ä¸­æ‰€éœ€åƒæ•¸ä¹‹é›†
åˆã€‚ 
2. æ±ºå®šåˆå§‹åƒæ•¸èµ·å§‹æº«åº¦T ã€çµ‚æ­¢æº«åº¦ã€å†·å»çŽ‡(æˆ–å†·å»å€¼)Î± ï¼Œå–®ä¸€æº«åº¦è¿­ä»£æ¬¡æ•¸ã€‚ 
3. è¨­å®šèµ·å§‹è¿­ä»£æ¬¡æ•¸ 0=t ï¼Œç”¢ç”Ÿåˆå§‹ç›®å‰è§£ X ã€‚ 
4. ä»¥ç›®å‰ tx ç‚ºä¸­å¿ƒèˆ‡æ“¾å‹•ç¯„åœ ts ,éš¨æ©Ÿç”¢ç”Ÿæ–°çš„é„°è¿‘è§£ X Ê¹â€²ã€‚ 
)( 1+=Ê¹â€² txEX  (4) 
å…¶ä¸­ )1()
2
(1 rands
sxx t
t
tt â‹…+âˆ’=+ ï¼Œ )1(rand ç‚ºä¸€[0,1]éš¨æ©Ÿæ‰€ç”¢ç”Ÿçš„äº‚æ•¸ï¼Œè€Œ sçš„åˆå§‹æ“¾å‹•ç¯„
åœç‚ºæ•´å€‹çµæ§‹ç©ºé–“ï¼Œä¸¦åŒæ™‚è¨ˆç®—å…¶èƒ½é‡å‡½æ•¸å€¼ )( 1+txE ã€‚ 
5. æŽ¡ç”¨ Metroplis æŽ¥å—æ³•å‰‡ï¼Œä½¿ç”¨å…¬å¼(3)ç”¨ä¾†æ±ºå®šæ˜¯å¦æŽ¥å—é„°è¿‘è§£ç‚ºç›®å‰è§£ã€‚è‹¥æŽ¥å—å‰‡
5 
 
ç²’å­ç¾¤èšæ¼”ç®—æ³•ä¹‹é„°åŸŸå€¼é›†åˆï¼š 
ç”±å‰æ–‡æ‰€è¿°ï¼Œç²’å­ç¾¤èšæœ€ä½³åŒ–æ¼”ç®—æ³•æ–°çš„é„°åŸŸå€¼ð‘¥!"!!!æ˜¯ä»¥å‰ä¸€é»žð‘¥!"! ç‚ºèµ·é»žèˆ‡é€Ÿåº¦v!"!!!ç›¸
åŠ æ‰€ç”¢ç”Ÿã€‚é€Ÿåº¦ð‘£!"!!!ä»£è¡¨æœå°‹çš„æ­¥å¹…å¤§å°ï¼Œé€™å€‹é€Ÿåº¦ç”±å„é«”æœ¬èº«ç¶“é©—èˆ‡åŒä¼´é£›è¡Œç¶“é©—ð‘”ð‘ð‘’ð‘ ð‘¡!"ï¼Œ
è—‰ç”±äº‚æ•¸ð‘Ÿ!!èˆ‡ð‘Ÿ!!åŠåŠ é€Ÿä¿‚æ•¸ð‘!èˆ‡ð‘!é€²è¡Œå‹•æ…‹èª¿æ•´ã€‚ 
å„ç²’å­ç§»å‹•æ­¥å¹…ç¯„åœ(é€Ÿåº¦çš„ä¸Šé™èˆ‡ä¸‹é™)å‰‡å¯ä»¥ä¸‹å¼è¡¨ç¤ºï¼š X!" = max v!"!!! = max w!v!"! + c!r!! pbest!" âˆ’ x!"! + c!r!! gbest!" âˆ’ x!"!  Â X!" = min v!"!!! = min w!v!"! + c!r!! pbest!" âˆ’ x!"! + c!r!! gbest!" âˆ’ x!"!  (8) (9) 
ç”±æ–¼ PSO ä¸­çš„ç²’å­æ˜¯è¿½éš¨ð‘ð‘ð‘’ð‘ ð‘¡!"åŠx!"! å…©å€‹æ¥µå€¼ä¾†æ›´æ–°ä¸‹ä¸€æ­¥æœå°‹ä¹‹é€Ÿåº¦åŠä½ç½®ï¼Œæ•…åœ¨è€ƒæ…®
å…©æ¥µå€¼å° PSO æ¼”ç®—æ³•ä½”æœ‰æ¥µå¤§çš„å½±éŸ¿åŠ›ä¸‹ï¼Œåˆ†åˆ¥æ ¹æ“šå…©æ¥µå€¼æå‡ºå…©ç¨®æ··åˆç­–ç•¥ï¼Œé„°åŸŸé›†åˆ
ç¯„åœç”±ð‘ð‘ð‘’ð‘ ð‘¡!"åŠx!"! ä¹‹å·®æ‰€æŽ§åˆ¶ï¼Œç•¶x!"! è¶ŠæŽ¥è¿‘ð‘ð‘ð‘’ð‘ ð‘¡!"åŠx!"! æ™‚ï¼Œå‰‡é„°åŸŸé›†åˆç¯„åœè¶Šå°ã€‚ 
ç ”ç©¶æ–¹æ³•åŠæœ€ä½³åŒ–æ¼”ç®—æ³•æž¶æ§‹ 
æœ¬ç ”ç©¶å°‡ä»¥é„°åŸŸæ¦‚å¿µæ•´åˆè’™åœ°å¡ç¾…æ¼”ç®—æ³•(MC)ã€æ¨¡æ“¬é€€ç«æ¼”ç®—æ³•(SA)èˆ‡ç²’å­ç¾¤èšæœ€ä½³åŒ–
æ¼”ç®—æ³•(PSO)ä¸‰å€‹æ¼”ç®—æ³•ï¼Œä¸¦èžåˆç‰¹æ€§ã€‚è§£æ§‹è’™åœ°å¡ç¾…æ¼”ç®—æ³•(MC)ã€æ¨¡æ“¬é€€ç«æ¼”ç®—æ³•(SA)èˆ‡ç²’
å­ç¾¤èšæœ€ä½³åŒ–æ¼”ç®—æ³•(PSO)å°é„°åŸŸé›†åˆç©ºé–“çš„ç¯„åœï¼Œå¯ä»¥æ­¸ç´å¦‚ä¸‹ï¼š ð‘!" = ð‘¥!!! ð‘¥!!! = ð‘  âˆ™ ð‘Ÿð‘Žð‘›ð‘‘ 1   ð‘!" = ð‘¥!!! ð‘¥!!! = ð‘¥! âˆ’ !!! + ð‘ ! âˆ™ ð‘Ÿð‘Žð‘›ð‘‘ 1 , ð‘ !!! = ð‘  âˆ™ ð›½! ,ð›½ âˆˆ 0,1   ð‘!"# = {ð‘¥!!!! |ð‘¥!"!!! = ð‘¥!"! + ð‘£!"!!!, ð‘£!"!!!ð‘¤!ð‘£! Â  ! + ð‘!ð‘Ÿ!! ð‘ð‘ð‘’ð‘ ð‘¡!" âˆ’ ð‘¥!"! + ð‘!ð‘Ÿ!! ð‘”ð‘ð‘’ð‘ ð‘¡!" âˆ’ ð‘¥!"!   
å…¶ä¸­ð‘!"ã€ð‘!"ã€ð‘!"!ä¾åºç‚ºè§£æ§‹è’™åœ°å¡ç¾…æ¼”ç®—æ³•(MC)ã€æ¨¡æ“¬é€€ç«æ¼”ç®—æ³•(SA)èˆ‡ç²’å­ç¾¤èš
æœ€ä½³åŒ–æ¼”ç®—æ³•(PSO)å°é„°åŸŸé›†åˆç©ºé–“çš„ç¯„åœã€‚ 
ç”±ð‘!" = ð‘¥!!! ð‘¥!!! = ð‘¥! âˆ’ !!! + ð‘ ! âˆ™ ð‘Ÿð‘Žð‘›ð‘‘ 1 , ð‘ !!! = ð‘  âˆ™ ð›½! ,ð›½ âˆˆ 0,1 ï¼Œæ¼”åŒ–å…¬å¼æˆç‚ºï¼š 
    ð‘¥!!! = ðœ“! ð‘¥! âˆ’ !!! + ð‘ ! âˆ™ ð‘Ÿð‘Žð‘›ð‘‘ 1 , ð‘ !!! = ð‘  âˆ™ ð›½! ,ð›½ âˆˆ 0,1   
ç•¶ðœ“! = 0,ð›½ = 1æ™‚ 
    ð‘ !!! = ð‘  âˆ™ 1! = ð‘   
    ð‘¥!!! = 0 âˆ™ ð‘¥! âˆ’ !!! + ð‘ ! âˆ™ ð‘Ÿð‘Žð‘›ð‘‘ 1   
        = ð‘  âˆ™ ð‘Ÿð‘Žð‘›ð‘‘ 1   
èˆ‡è’™åœ°å¡ç¾…æ¼”ç®—æ³•ä¹‹é„°åŸŸè§£ç›¸åŒï¼Œæ•…å¯æ•´åˆè’™åœ°å¡ç¾…æ¼”ç®—æ³•èˆ‡æ¨¡æ“¬é€€ç«æ¼”ç®—æ³•ä¹‹é„°åŸŸç‚ºï¼š ð‘!"#$% =  Â {ð‘¥!!!|ð‘¥!!! = ðœ“! ð‘¥! âˆ’ !!! + ð‘ ! âˆ™ ð‘Ÿð‘Žð‘›ð‘‘ 1 ,ðœ“! = 0,1 , ð‘ !!! = ð‘  âˆ™ ð›½! ,ð›½ âˆˆ 0,1 }  (10) 
 
Fig. 5.ç•¶ dgbest ã€ idpbset åˆ†åˆ¥ä½æ–¼ ubx èˆ‡ lbx æ™‚ä¹‹é„°åŸŸé›†åˆç©ºé–“  
 
ç”± Fig. 5 ä¸­å¯ä»¥ç™¼ç¾ï¼Œç²’å­ç¾¤æœ€ä½³åŒ–æ¼”ç®—æ³•ä¹‹é„°åŸŸé›†åˆç©ºé–“ï¼Œç•¶ dgbest ã€ idpbset åˆ†åˆ¥ä½æ–¼
ubx èˆ‡ lbx æ™‚ï¼Œå…¶é„°åŸŸé›†åˆç©ºé–“èˆ‡æ¨¡æ“¬é€€ç«æ¼”ç®—æ³•ä¹‹é„°åŸŸé›†åˆç©ºé–“ç›¸ä¼¼ï¼Œæ•…æŽ¨å°Žå…¶å…¬å¼å¦‚ä¸‹ï¼š ð‘¥!"!!! = ð‘¥!"! + ð‘£!"!!! ð‘£!"!!! = ð‘¤!ð‘£!"! + ð‘!ð‘Ÿ!! ð‘ð‘ð‘’ð‘ ð‘¡!" âˆ’ ð‘¥!"! + ð‘!ð‘Ÿ!! ð‘”ð‘ð‘’ð‘ ð‘¡!" âˆ’ ð‘¥!"!   Â  Â  Â  Â  Â  Â  Â  Â  Â  Â = ð‘¤!ð‘£!"! + ð‘!ð‘Ÿ!! ð‘¥!" âˆ’ ð‘¥!"! + ð‘!ð‘Ÿ!! ð‘¥!" âˆ’ ð‘¥!"!  
(11) 
 
(12) 
è¨­ð‘¤! = 0, ð‘! = ð‘! = 1 
7 
 
2. æ·±åŒ–ç­–ï¥¶ï¼šç•¶ 11 =Ïˆ æ™‚ï¼Œå¢žåŠ ï§¹å­å€åŸŸæœå°‹çš„èƒ½ï¦Šï¼Œå†è—‰ç”±å…¨é«”èˆ‡å€‹é«”æš«æ™‚æœ€ä½³è§£
( dub gbestx = , idlb pbsetx = )ï¼Œæ”¹è®Šå„ï§¹å­æœå°‹çš„æ–¹å‘ï¼Œä½¿ï§¹å­èƒ½è¿½éš¨æš«æ™‚æœ€ä½³ç¶“é©—çš„å°Žå¼•ï¼Œ
æå‡å€åŸŸæœå°‹çš„æ•ˆæžœã€‚ 
III.  é„°è¿‘è§£ï¤æ–°ç­–ç•¥ 
é„°è¿‘è§£ï¤æ–°ç­–ç•¥æ˜¯ä¾ç…§é¸å®šé„°åŸŸ(Neighborhood)å€åŸŸï¼Œå„ç²’å­å°‡è—‰ç”±äº‚æ•¸ç”¢ç”Ÿæ–°çš„ç²’å­ä½
ç½®ä¸¦ç§»å‹•(Move)è‡³æ–°è§£ä½ç½®æ±‚å¾—é©æ‡‰å€¼ï¼Œä¸¦ä¾æ“šæŽ¥å—æ³•å‰‡(Acceptance Rule)é‡å°é©æ‡‰å€¼ä¸ä½³è€…
æ–¼ä»¥æ‹’çµ•æˆ–é‡å°é©æ‡‰å€¼ä¸ä½³è€…æ ¹æ“šæ©ŸçŽ‡åˆ¤æ–·æ˜¯å¦æŽ¥å—ï¼Œç”¢ç”Ÿæ–°çš„é„°åŸŸè§£ï¼Œä¸€ç›´è¿­ä»£è‡³æ»¿è¶³çµ‚
æ­¢æ¢ä»¶ç‚ºæ­¢ã€‚ 
æˆæžœåŠé©—è­‰ 
æœ¬ç ”ç©¶åˆæœŸé¸æ“‡ç¶“å¸¸ä½¿ç”¨çš„å››å€‹ Benchmark å‡½æ•¸å•é¡Œé€²è¡Œæ•¸å€¼å¯¦é©—ï¼Œä¸¦æ ¹æ“šå‡½æ•¸æ€§è³ªåˆ†
ç‚ºå…·æœ‰å–®ä¸€æ¥µå°é»žï¼ˆå–®æ¨¡æ…‹ï¼‰å’Œå¤šå€‹å±€éƒ¨æ¥µå°é»žï¼ˆå¤šæ¨¡æ…‹ï¼‰å…©å¤§é¡žï¼Œæ¯é¡žå…©å€‹å‡½æ•¸ï¼Œå¦‚ Table 
1 æ‰€ç¤ºã€‚ 
Table 1. Benchmark Function 
Function Name Dim Search Space 
Min/Best 
posi t ion 
âˆ‘
=
=
n
i
ixf
2
2
1  Sphere 30 (-100,100)
N 0 / (0,â€¦0) 
âˆ‘
=
+ âˆ’+âˆ’=
n
i
iii xxxf
1
222
12 ))1()(100(  Rosen Brock 30 (-50,50)
N 0 / (1,â€¦1) 
1cos
4000
1
1 1
2
3 +âŽŸï£·âŽŸï£·
âŽ ï£¸
âŽžï£¶
âŽœï£¬âŽœï£¬
âŽï£­
âŽ›ï£«
âˆ’= âˆ‘ âˆ
= =
n
i
n
i
i
i
i
x
xf  Griewank 30 (-300,300)N 0 / (0,â€¦0) 
âˆ‘
=
=+âˆ’=
n
i
ii AAxAxf
1
2
4 10),)2cos(( Ï€  Rastrigrin 30 (-5.12,5.12)
N 0 / (0,â€¦0) 
Table 1 ä¸­ï¼Œå‡½æ•¸ 21 ~ ff æ˜¯å–®æ¥µå€¼å‡½æ•¸ï¼Œ 43 ~ ff å‰‡ç‚ºå¤šæ¥µå€¼å‡½æ•¸ã€‚å‡½æ•¸ 43 ~ ff æ˜¯å…¸åž‹çš„éž
ç·šæ€§å¤šæ¨¡æ…‹å‡½æ•¸ã€‚å®ƒå€‘å…·æœ‰å»£æ³›çš„æœç´¢ç©ºé–“ã€å¤§é‡çš„å€åŸŸæ¥µå°é»žå’Œé«˜å¤§çš„éšœç¤™ç‰©ï¼Œé€šå¸¸è¢«èª
ç‚ºå„ªåŒ–ç®—æ³•å¾ˆé›£è™•ç†çš„è¤‡é›œå¤šæ¨¡æ…‹å•é¡Œã€‚æœ¬ç ”ç©¶æŽ¡ç”¨ä¸Šé¢å››å€‹æ¸¬è©¦å‡½æ•¸ï¼Œåœ¨çµ±ä¸€çš„æ¨™æº–ä¸‹ï¼Œ
è—‰ç”±ç›¸åŒçš„ä»£æ•¸ï¼ˆæ™‚é–“ï¼‰ä¾†æ¸¬è©¦æ‰€æå‡ºçš„å¯†åº¦å°Žå‘ç²’å­ç¾¤æ¼”ç®—æ³•èˆ‡ä»¥å‰æ‰€æå‡ºçš„ç²’å­ç¾¤æ¼”ç®—
æ³•ç›¸æ¯”ï¼Œæ¸¬è©¦æ˜¯å¦æœ‰åœ¨ç›¸åŒä»£æ•¸ä¸‹ï¼Œæœ‰æ›´ä½³çš„è§£ï¼ˆæœ€å°å€¼ï¼‰ã€‚ 
I.  é©—è­‰çµæžœ 
 æˆ‘å€‘ä½¿ç”¨äº†ä¸Šè¿°ä¹‹å››å€‹é©—è­‰å‡½æ•¸ä¾†é©—è­‰æ–°å¼æ··åˆæ¼”ç®—æ³•ï¼Œä»¥æ¯æ¬¡æœå°‹ 10 ã€100 ã€500 ã€
1000 æ¬¡æœå°‹ç‚ºé™ï¼Œ å–å…¶å„æ¬¡æœ€ä½³å€¼ä¹‹å¹³å‡ï¼Œä»¥åœ–è¡¨è¡¨ç¤ºå…¶æœå°‹è§£æ”¶æ–‚èµ°å‘ï¼ŒTable 3ã€4ã€5ã€
6 åŠ Fig. 7ã€9ã€11ã€13 ä¾åºç‚ºä½¿ç”¨ Sphereã€Rosen Brockã€Griewankã€Rastrigrin é©—è­‰å‡½æ•¸æ‰€å¾—ä¹‹
æ•¸æ“šã€‚ 
å¦‚ Table 2 æ•¸æ“šæ‰€ç¤ºï¼Œå›  PSO åœ¨å„æœå°‹æ¬¡æ•¸ä¹‹æ”¶æ–‚é€Ÿåº¦åœ¨ä¸‰å€‹æ¼”ç®—æ³•ä¹‹ä¸­çš†æœ€å¿«ï¼Œè€Œæ··åˆ
æ¼”ç®—æ³•ä¹‹å„ªå‹¢ä¾†è‡ªä¸‰å€‹æ¼”ç®—æ³•ä¹‹ç‰¹æ€§ï¼Œæ‰€ä»¥ Hybrid ä¹‹æ•¸æ“šè¶¨è¿‘æ–¼ PSOã€‚  
  Table 2. Sphere å‡½æ•¸ä¹‹å„æœå°‹æ¬¡æ•¸æ‰€å¾—çµæžœå¹³å‡å€¼ 
 
MC SA PSO Hybrid 
10 æ¬¡ 6.1698 7.8873 0.5026 0.5003 
100 æ¬¡ 0.7289 0.0002 0 0 
500 æ¬¡ 0.0833 0 0 0 
1000 æ¬¡ 0.0117 0 0 0 
9 
 
ç”± Table 3 ä¹‹æ•¸æ“šåœ¨åˆå§‹æ™‚ PSO æ”¶æ–‚é€Ÿåº¦æœ€ä½³ï¼Œä½†ç”± 100 æ¬¡ä¹‹æ•¸æ“šç™¼ç¾ä¸­æ®µæ™‚æ”¶æ–‚é€Ÿåº¦ç‚º
SA è¼ƒä½³ï¼Œæ‰€ä»¥æˆ‘å€‘å°‡æ··åˆæ¼”ç®—æ³•ä¹‹åˆå§‹éšŽæ®µè¨­å®šç‚º PSOï¼Œä¸­æ®µæŽ¥ SA å¿«é€Ÿæ”¶æ–‚ï¼Œæœ€å¾Œå†è½‰å›ž
PSO åšæ·±åº¦æœå°‹ã€‚ 
Table 4. Griewank å‡½æ•¸ä¹‹å„æœå°‹æ¬¡æ•¸æ‰€å¾—çµæžœå¹³å‡å€¼ 
 
MC SA PSO Hybrid 
10 æ¬¡ 0.19979 0.38486 0.11044 0.11606 
100 æ¬¡ 0.02298 0.03177 0.00347 0.0034 
500 æ¬¡ 0.00195 0.00193 0.00094 0.00079 
1000 æ¬¡ 0.00016 0.00162 0.00055 0.00006 
 
Fig. 11. Griewank å‡½æ•¸ä¹‹å„æœå°‹æ¬¡æ•¸æ‰€å¾—çµæžœå¹³å‡å€¼ 
 
Fig. 12. Griewank å‡½æ•¸æ‰€å¾—çµæžœç´°éƒ¨ 
Table 5. Rastrigrin å‡½æ•¸ä¹‹å„æœå°‹æ¬¡æ•¸æ‰€å¾—çµæžœå¹³å‡å€¼ 
 
MC SA PSO Hybrid 
10 æ¬¡ 1.28469 1.94598 0.36256 0.36373 
100 æ¬¡ 0.35315 0.10758 0 0 
500 æ¬¡ 0.03692 0.002 0 0 
1000 æ¬¡ 0.00246 0.00099 0 0 
 
Fig. 13. Rastrigrin å‡½æ•¸ä¹‹å„æœå°‹æ¬¡æ•¸æ‰€å¾—çµæžœå¹³å‡å€¼ 
0	 Â 
0.05	 Â 
0.1	 Â 
0.15	 Â 
0.2	 Â 
0.25	 Â 
0.3	 Â 
0.35	 Â 
0.4	 Â 
0.45	 Â 
10æ¬¡	 Â  100æ¬¡	 Â  500æ¬¡	 Â  1000æ¬¡	 Â 
æœ€
ä½³
è§£
å¹³
å‡
å€¼
æœå°‹æ¬¡æ•¸
Griewankå‡½æ•¸å„æœå°‹æ¬¡æ•¸æ‰€å¾—æœ€ä½³è§£ä¹‹å¹³å‡å€¼
MC	 Â 
SA	 Â 
PSO	 Â 
Hybrid	 Â 
0	 Â 
0.0002	 Â 
0.0004	 Â 
0.0006	 Â 
0.0008	 Â 
0.001	 Â 
10	 Â  100	 Â  500	 Â  1000	 Â 
æœ€
ä½³
è§£
å¹³
å‡
å€¼
æœå°‹æ¬¡æ•¸
Griewankå‡½æ•¸å„æœå°‹æ¬¡æ•¸æ‰€å¾—æœ€ä½³è§£ä¹‹å¹³å‡å€¼
MC	 Â 
SA	 Â 
PSO	 Â 
Hybrid	 Â 
0	 Â 
0.5	 Â 
1	 Â 
1.5	 Â 
2	 Â 
10æ¬¡	 Â  100æ¬¡	 Â  500æ¬¡	 Â  1000æ¬¡	 Â 
æœ€
ä½³
è§£
å¹³
å‡
å€¼
æœå°‹æ¬¡æ•¸
Rastrigrinå‡½æ•¸å„æœå°‹æ¬¡æ•¸æ‰€å¾—æœ€ä½³è§£ä¹‹å¹³å‡å€¼
MC	 Â 
SA	 Â 
PSO	 Â 
Hybrid	 Â 
A Low-Complexity And High-Performance Hybrid 
Problem Solving Method  
Besed On Neighborhood Search Algorithms 
Chih-ming Kung, Guan-Zhou Chen 
Dept. of Information Technology & Communication,  
Shih Chien University, Taiwan. 
alex@mail.kh.usc.edu.tw, alex@cvig.org 
Shu-Tsung Chao, Wei-Sheng Yang, Li-Min Chuang  
Graduate School of Business and Operations Management, 
Chang Jung Christian University, Taiwan. 
{james, weiboy}@cvig.org, liming@mail.cjcu.edu.tw 
Abstractâ€” Despite many algorithms have been proposed over 
the past few decades, it still play a very important role for 
various types of improved hybrid algorithm. Due to the 
complexity of the problem, many algorithms including 
Simulated Annealing, Partical Swarm Optimization, and 
Monte Carlo have been proposed for searching better solution 
of NP-complete problem in reasonable computation time. The 
aim of this research is to make a study and an analysis of the 
above optimal algorithms in order to find out the advantage 
and processing features of these approaches which were 
summarized and extracted the characteristics and essence of 
the various algorithms. Then to integrate and reconstruct a 
new algorithm framework which is the combination of the 
processing solution of neighborhood search and moving. 
Therefore, the performance of our new algorithm framework 
could be able to greatly enhance. Believe that such a new 
algorithm framework not only can enhance the performance of 
algorithms but also help to cope with the diversity and 
complexity of problem solving.  
Keywords- Neighborhood, Hybrid, MC, SA, PSO  
I.  INTRODUCTION  
From the analysis of relevant literature, many algorithms 
have been proposed over the past few decades. It can be 
found that most previous studies made a hybrid algorithm 
from these algorithms mentioned above, to make up the 
shortcomings of a single algorithm for each other, such as 
Simulated Annealing (SA) with the Genetic Algorithm (GA) 
or Partical Swarm Optimization (PSO) with GA. The aim of 
this study is to improve the specially weakness of the 
algorithm by the advantage of another algorithm. 
This research is to make a study and analysis of these 
algorithms including PSO, SA, GA, and MC in order to find 
the advantage and processing of their approaches which were 
summarized and extracted the characteristics and essence of 
the various algorithms. Then to integrate and reconstruct an 
adaptability and robustness of new algorithm framework 
which is the combination of the processing solution of 
neighborhood search and moving. The present study 
integrates the neighborhood characteristics and reconstructs 
the neighborhood architecture of these algorithms to break 
through this limitation and improve the solving efficiency. 
According to the period of PSO, MC, SA, the parameters 
will be converted. By adjusting the parameters, the algorithm 
can be adaptive for searching better solution of NP-complete 
problem in reasonable computation time. 
II. REVIEW OF THE LITERATURE AND DISCUSSION 
A. Monte Carlo Algorithm 
The Monte Carlo method was invented by scientists 
working on the atomic bomb in the 1940s, who named it for 
the city in Monaco famed for its casinos and games of 
chance.  Its core idea is to use random samples of 
parameters or inputs to explore the behavior of a complex 
system or process.  The scientists faced physics problems, 
such as models of neutron diffusion that were too complex 
for an analytical solution, so they had to be evaluated 
numerically.  They had access to one of the earliest 
computers but their models involved so many dimensions 
that exhaustive numerical evaluation was prohibitively 
slow.  Monte Carlo simulation has been proved to be 
surprisingly effective at finding solutions to these 
problems.  Since that time, the Monte Carlo method has 
been applied to an incredibly diverse range of problems in 
science, engineering, finance, and business applications in 
virtually every industry. 
In 1949, the term of â€œMonte Carlo methodâ€ was 
proposed by J. Neyman and S. UlamÊ³[1]. In 1953, MetropolË¼s 
proposed Monte Carlo algorithm [2]. Monte Carlo method is 
used to solve the mathematical problem by random sampling. 
B. Simulated Annealing 
The Metropolis Monte Carlo integration algorithm 
(Metropolis et al, 1953) was generalized. Simulated 
Annealing (SA) [3-6] is extended from the Monte Carlo 
algorithm. In 1983, Simulated Annealing is proposed by 
Kirkpartrick [7]. Simulated annealing is an optimization 
technique that simulates the physical annealing process in the 
field of combinatorial optimization. Annealing is the 
physical process of heating up a solid until it melts, followed 
by slow cooling it down by decreasing the temperature of the 
environment in steps. At each step, the temperature is 
maintained constant for a period of time sufficient for the 
solid to reach thermal equilibrium. 
A standard SA procedure begins by generating an initial 
2011 International Conference on Fuzzy Systems and Neural Computing 
978-1-4244-9216-9/11/$26.00 Â©2011 IEEE                                                          FSNC 2011 
 
282
Boltzmann's probability distribution, to calculating the 
energy of the current solution E. The current solution 
randomly generated for the center â€œnearbyâ€ solution and 
calculates the energy E c , EEE c '  is the energy 
difference between the two solutions, according to the 
following formula to decide whether to accept the probability 
of nearby solution to replace the current solution:  
Â°Â¯
Â°Â®
Â­
!'
d'
 '
0 ,
0 ,1
Eife
Eif
P
T
E  (3)
If the center is tx with the range of disturbance randomly 
bring X c of  neighborhood  solution. 
 
)(' 1 txEX  (4) 
where Xt+1= (
2
t
t
sx  )+St Î£rand(1), rand (1) is the only 
[0,1] random number, while the s range of the initial 
disturbance is the structural space, and calculated the value 
of energy function values E( 1tx ). 
By (2), the new neighborhood xt+1 of SA is randomly 
generated for the center tx  and the range of disturbance St. 
)()1()
2
(1 SNrands
sxx tttt ÂÂ˜   (5) 
where ttt SSS EE u u 1 , ]0,1[ÂE , we may choose the 
neighborhood 1tx . 
The two basic equations which govern the working of 
PSO are that of velocity vector and position vector given by: 
)()( 2211
1 k
idd
dk
idid
dk
idi
k
id xgbestrcxpbsetrcvwv   (6)
k
id
k
id
k
id vxx  1 (7)
where k is the sub-algebra, d is the dimensional space, 
idpbset represent to the best location of the particle j at the 
time t, and dgbset represent to the best location of entire 
particle swarm in the time t. c1 and c2 represent the 
individuality and sociality coefficient, dr1 and
dr2  represent 
the random numbers, iw is the inertia weight. 
 
Figure 2.  PSO diagram 
According to Eq. (2), the new neighborhood value of 
PSO, kidv  represent the scope of search domain, it can be 
revision by pbestid, gbestd, c1, c2, dr1 and
dr2 .  
This study integrate the concept of MC, SA and PSO 
characteristics of algorithm, that spatial range of 
neighborhood set can be summarized as follows:  
)}1(|{ 11 randsxxN ttmc Â˜    Ê»8Ê¼ 
]}1,0[,
),1()
2
(|{ 111
Âu 
Â˜  
EE t
tt
t
tttsa
s
srandssxxxN
 
Ê»9Ê¼ 
)()(
,|{
2211
111
1
k
idd
dk
idid
dk
idi
k
id
k
id
k
id
k
id
d
tpso
xgbestrcxpbsetrcvw
vvxxxN
 
  
 
Ê»10Ê¼ 
where Nmc, Nsa, Npso are the neighborhood domain of Monte 
Carlo, Simulated Annealing, Particle Swarm Optimization. It 
can be integrated the neighborhood of PSO, SA and MC: 
1
1
1  u kidkidkid vxx \  Ê»11Ê¼
)()(                   
))(
2
1(
2211
11
k
idub
dk
idlb
d
lbub
k
idi
k
id
xxrcxxrc
xxvwv

  \
 
Ê»12Ê¼
From research results we have inferred as follows: 
1) MC is exceptional performance for SA, SA has 
convergence of neighborhood. If SA has no convergence 
and no change of searching scope, the neighborhood of 
MC will be the same. 
2) Using the concept of neighborhood re-deconstruct which 
MC, SA and PSO of algorithm, and combined with three 
kinds of algorithms to construct a new architecture such 
as the Eq. (11) and Eq.(12). 
3) Where the pbest, gbest of PSO extreme each other with 
the current solution( kidx - idpbset = dgbset -
k
idx ). The 
neighborhood set is the same as PSO and SA, it can be as 
the special case of  PSOâ€˜s neighborhood set. 
 
The new system architecture of algorithm is different 
from local search. Where the neighborhood optimum is 
worse than local optimum in searching, the neighborhood 
optimum will be the new solution. Therefore, it will have the 
chance to search the global optimum. The elements of search 
include three sections: 
1) Initial solution : Set the initial solution to theinitial value. 
2) Neighborhood solution : According to the definition of 
neighborhood solution, the neighborhood solution set 
will be generated by present solution. 
3) Move and update : Choose the best solution to move and 
update in all of neighborhood solution. 
 
Figure 3.  New system architecture of hybrid algorithm 
284
A Hybrid Algorithms of Low-Complexity And High-Performance  
Besed On Neighborhood Search Method 
Chih-ming Kung  
Dept. of Information Technology & Communication /Shih Chien University 
Tainan City 
alex@mail.kh.usc.edu.tw 
Wei-sheng Yang 
Dept. of Information Management/Chang Jung Christian University 
Tainan Country
 
Abstractâ€”Despite many algorithms have been proposed over 
the past few decades, it still play a very important role for 
various types of improved hybrid algorithm. Due to the 
complexity of the problem, many algorithms including 
Simulated Annealing, Partical Swarm Optimization, and 
Monte Carlo have been proposed for searching better solution 
of NP-complete problem in reasonable computation time. The 
aim of this research is to make a study and an analysis of the 
above optimal algorithms in order to find out the advantage 
and processing features of these approaches which were 
summarized and extracted the characteristics and essence of 
the various algorithms. Then to integrate and reconstruct a 
new algorithm framework which is the combination of the 
processing solution of neighborhood search and moving. 
Therefore, the performance of our new algorithm framework 
could be able to greatly enhance. Believe that such a new 
algorithm framework not only can enhance the performance of 
algorithms but also help to cope with the diversity and 
complexity of problem solving. 
Keywords- Neighborhood, Hybrid, MC, SA, PSO  
I.  INTRODUCTION  
From the analysis of relevant literature, many algorithms 
have been proposed over the past few decades. It can be 
found that most previous studies made a hybrid algorithm 
from these algorithms mentioned above, to make up the 
shortcomings of a single algorithm for each other, such as 
Simulated Annealing (SA) with the Genetic Algorithm (GA) 
or Partical Swarm Optimization (PSO) with GA. The aim of 
this study is to improve the specially weakness of the 
algorithm by the advantage of another algorithm. 
This research is to make a study and analysis of these 
algorithms including PSO, SA, GA, and MC in order to find 
the advantage and processing of their approaches which were 
summarized and extracted the characteristics and essence of 
the various algorithms. Then to integrate and reconstruct an 
adaptability and robustness of new algorithm framework 
which is the combination of the processing solution of 
neighborhood search and moving. The present study 
integrates the neighborhood characteristics and reconstructs 
the neighborhood architecture of these algorithms to break 
through this limitation and improve the solving efficiency. 
According to the period of PSO, MC, SA, the parameters 
will be converted. By adjusting the parameters, the algorithm 
can be adaptive for searching better solution of NP-complete 
problem in reasonable computation time. 
II. REVIEW OF THE LITERATURE AND DISCUSSION 
A. Metropolis Monte Carlo Algorithm 
The Monte Carlo method was invented by scientists 
working on the atomic bomb in the 1940s, who named it for 
the city in Monaco famed for its casinos and games of 
chance.  Its core idea is to use random samples of 
parameters or inputs to explore the behavior of a complex 
system or process.  The scientists faced physics problems, 
such as models of neutron diffusion that were too complex 
for an analytical solution, so they had to be evaluated 
numerically.  They had access to one of the earliest 
computers but their models involved so many dimensions 
that exhaustive numerical evaluation was prohibitively slow.  
Monte Carlo simulation has been proved to be surprisingly 
effective at finding solutions to these problems.  Since that 
time, the Monte Carlo method has been applied to an 
incredibly diverse range of problems in science, engineering, 
finance, and business applications in virtually every industry. 
In 1949, the term of â€œMonte Carlo methodâ€ was 
proposed by J. Neyman and S. Ulam [1]. In 1953, 
Metropolis proposed Monte Carlo algorithm [2]. Monte 
Carlo method is used to solve the mathematical problem by 
random sampling. 
B. Simulated Annealing 
The Metropolis Monte Carlo integration algorithm 
(Metropolis et al, 1953) was generalized. Simulated 
Annealing (SA) [3-6] is extended from the Monte Carlo 
algorithm. In 1983, Simulated Annealing is proposed by 
Kirkpartrick [7]. Simulated annealing is an optimization 
technique that simulates the physical annealing process in the 
field of combinatorial optimization. Annealing is the 
physical process of heating up a solid until it melts, followed 
by slow cooling it down by decreasing the temperature of the 
environment in steps. At each step, the temperature is 
using until now [12-14]. But the Meta-heuristics take the 
traditional heuristic method as the core architecture, 
combined with high search strategy, let algorithm can jump 
out local optimum [15]. Table I is the comparison of the 
optimal algorithm.  
III. RESEARCH METHODS 
This study is focus on two issues, first is how quickly and 
widely to search in any solution domain, and find out the 
approximate optimal solution in the son of the solution 
domain. Second, how quickly and widely to solve in solution 
domain and avoid getting a trouble in local optimum.  The 
new algorithm keeps on quality and speed.  
In this study, from neighborhood features to integrate 
ideas, developed into a Neighborhood Architecture of the 
hybrid optimization algorithm, and for the proposed 
algorithm to study and discuss, than compare with traditional 
algorithm, from comparing the test function to find the 
algorithmic parameter settings. Neighborhood definitions 
can be expressed as follows: 
EFmELet 2},,...2,1{ âŠ†= and â„œâ†’FfLet :   
Combinatorial optimization problem (COP) 
Minimize }:)({ FSSf âˆˆ   
EFN 2: â†’  ï€¨(1) 
Where E2 is the subset of all Eâ€™s sets, F is a feasible 
solution, f is the objective function, S is feasible set, N is a 
neighborhood function, for each FS âˆˆ , N(S) is a subset of  
E, N(S) become feasible solution S of neighborhood. 
MC often used in optimization of various fields. This 
method is mainly based on the Boltzmannâ€™s probability 
distribution function to help select the best solution. Itâ€™s the 
neighboring solutions can be expressed as: 
)}1(|{ 11 randsxxN ttmc â‹…== ++  ï€¨(2) 
 
 
Figure 2.   SA Flow diagram 
SA is different from other algorithms; it has probability 
to judge the solution which is poor or optimal, and leave out 
the local optimal to global optimal. SA only deal with non-
contain restrained conditions of optimization algorithms, so 
it need for an infinitely long time to stabilize find the global 
optimum, Aarts [16] in 1989 proposed parallel computation 
to speed up search of SA. 
 
Metropolis criterion is used to decide whether to accept a 
change in the energy changes. At a particular temperature T, 
each specifically atomic probability of existence that show 
Boltzmann's probability distribution, to calculating the 
energy of the current solution E. The current solution 
randomly generated for the center â€œnearbyâ€ solution and 
calculates the energy E â€² , EEE âˆ’â€²=Î”  is the energy 
difference between the two solutions, according to the 
following formula to decide whether to accept the probability 
of nearby solution to replace the current solution:  
âŽªâŽ©
âŽªâŽ¨
âŽ§
>Î”
â‰¤Î”= Î”âˆ’
0 ,
0 ,1
Eife
Eif
P
T
E  (3) 
SA method is simple and operation is as follows: 
1) Chose the objective function for the problem, X=E (x). 
2) Let the initial paramete: temperature T, end 
temperature, cooling rate Î± , single-temperature 
iteration. 
3) Set the initial iteration t = 0, generate an initial 
current solution X . 
4) as the center tx with the range of disturbance randomly 
bring X â€² of  neghborhood  solution. 
)( 1+=â€² txEX    (4) 
Where Xt+1= (
2
t
t
s
x âˆ’ )+St âˆ™rand(1), rand (1) is the 
only [0,1] random number, while the s range of the 
initial disturbance is the structural space, and calculated 
the value of energy function values E( 1+tx ). 
5) With Metropolis Acceptance Rule, and use (1) to 
determine whether accept the neghborhood solution to 
be the current solution. If it accept X=X', t = t +1. 
6) Determine whether reache to the iterations, if yes , 
reduce temperature t = 0. There two ways of reduce 
temperature methods, one is T = Ã—Î± T, ]0,1[âˆˆÎ± , 
another is T = T Î±âˆ’ , <Î± 0. 
7) Reduce the range of disturbanc St+1=St Î²Ã—  , ]0,1[âˆˆÎ² . 
8) Determine temperature whether be terminated, 
otherwise return to Step 4and re-run. 
9) The final solution X. 
By (2), the new neighborhood 1+tx of SA is randomly 
generated for the center tx  and the range of disturbance St. 
)()1()
2
(1 SNrands
sxx tttt âˆˆâ‹…+âˆ’=+  (5) 
Where ttt SSS Î²Î² Ã—=Ã—=+1 , ]0,1[âˆˆÎ² , we may choose 
the neighborhood 1+tx . Figure 3 is neighborhood 1+tx . 
4) Untill meet the rule of the end, ended PSO. 
5) Using equation (6) and (7) to update each particle's 
velocity and position. Back to step 2. 
According to Eq. (7), the new neighborhood value of 
PSO, kidv  represent the scope of search domain, it can be 
revision by  pbestid , gbestd, ,c1, c2, dr1 and
dr2 .  Fig. 6 is 
neighborhood set. Which kidx is closed to pbestid and gbestd, 
the range of neighborhood set is smaller, as Fig. 7(B). 
1
)96,112(),80,64(
),32,16(),0,0(
211 ===
==
==
ccw
gbestpbset
vx
did
k
id
k
id
 1
)96,112(),80,64(
),0,16-(),128,64(
211 ===
==
==
ccw
gbestpbset
vx
did
k
id
k
id
(A) (B) 
Figure 7.  Neighborhood set 
This study integrate the concept of MC, SA and PSO 
characteristics of algorithm, that spatial range of 
neighborhood set can be summarized as follows:  
)}1(|{ 11 randsxxN ttmc â‹…== ++  ï€¨(8) 
]}1,0[,
),1()
2
(|{ 111
âˆˆÃ—=
â‹…+âˆ’== +++
Î²Î² t
tt
t
tttsa
s
srands
s
xxxN
 
ï€¨(9) 
)()(
,|{
2211
111
1
k
idd
dk
idid
dk
idi
k
id
k
id
k
id
k
id
d
tpso
xgbestrcxpbsetrcvw
vvxxxN
âˆ’+âˆ’+=
+== ++++
 
(10) 
Where Nmc, Nsa, Npso are the neighborhood domain of 
Monte Carlo, is Simulated Annealing, is Particle Swarm 
Optimization. Eq.(9)  extend  as follows: 
)1()
2
(11 rands
s
xx t
t
tt â‹…+âˆ’=+ Ïˆ , ]1,0[,1 âˆˆÃ—=+ Î²Î² tt ss  
If 01 =Ïˆ , 1=Î²  
)1()1()
2
(0
1
1
1
randsrands
s
xx
sss
t
t
tt
t
t
â‹…=â‹…+âˆ’Ã—=
=Ã—=
+
+
 
As the same of MCâ€™s neighboring solutions, it can be 
integrated SA and MC neighborhood of algorithm. 
]}1,0[,},1,0{
),1()
2
(|{
11
111temp1
âˆˆÃ—==
â‹…+âˆ’==
+
++
Î²Î²Ïˆ
Ïˆ
t
t
t
t
ttt
ss
rands
s
xxxN
ï€¨(11)
 
 
Figure 8.  When dgbset , idpbset are located in ubx and lbx ,the scope 
of neighborhood set 
As Fig. 8, PSOâ€™s scope of the neighborhood set can be 
found, when the dgbset and idpbset  are located in ubx and lbx . 
The scope of neighborhood set is similar with SA, so 
derivation of the formula as follows:  
11 ++ += kidkidkid vxx  (12)
)()(
)()(
2211
2211
1
k
idub
dk
idlb
dk
idi
k
idd
dk
idid
dk
idi
k
id
xxrcxxrcvw
xgbestrcxpbsetrcvwv
âˆ’+âˆ’+=
âˆ’+âˆ’+=+
(13)
Set iw =0, 1c = 2c =1 
 
)()(
)()(0
)(1)(10
21
21
21
1
k
idub
dk
idlb
d
k
idub
dk
idlb
d
k
idub
dk
idlb
dk
id
k
id
xxrxxr
xxrxxr
xxrxxrvv
âˆ’+âˆ’Ã—=
âˆ’+âˆ’Ã—+=
âˆ’Ã—+âˆ’Ã—+Ã—=+
(14)
ub
d
lb
dk
id
dk
id
dk
id
k
idub
dk
idlb
dk
id
k
id
xrxrxrxrx
xxrxxrxx
Ã—+Ã—+Ã—âˆ’Ã—âˆ’=
âˆ’Ã—+âˆ’Ã—+=+
2121
21
1 )()(
 
(15)
For the scope of SAâ€™s neighborhood set, s= ubx lbxâˆ’ , 
ubx =
k
idx + 2
s , lbx =
k
idx - 2
s  
xub 
xlb 
xlb 
xub 
 
According to the chosen of neighborhood domain, the 
neighborhood solution will be update. The particles move 
position by the new particles of randomly generated, and get 
fitness value by the new position of solution. According to 
Acceptance Rule, the fitness value will reject or accept the 
poor fitness value by probability. Figure.9 is the new system 
architecture of hybrid algorithm. 
B. Generated strategies of neighborhood sets  
Search method can be improved performance by the 
strategy of intensification and diversification.  Intensification 
is enhanced the search method in previous results. Different 
from the intensification, diversification is widely search in 
the region of less searching. Therefore, two kinds of 
strategies may have search more good solutions in a range of 
a new region. Eq.(17) and Eq. (18) are the generated strategy 
of neighborhood sets. The main purpose is providing of the 
directional guidance in PSO searched, and solved by iterative 
process has become highlighted condition, in order to 
provide the particles to conduct the next time searching 
direction. r It mainly consists of global search and local 
search in updating parameter. There are two parts steageies. 
1) Diversification  strategy:  
Where 01 =Ïˆ , 1=Î² , 0=iw , 121 == cc , 
2
,
2
, sxxsxxxxs kidlb
k
idublbub âˆ’=+=âˆ’=  
Increasing the algorithm capability of full search to 
avoid to quickly convergence of the particle swarm 
search in the local optimum. 
2) Intensification Strategy : Where 11 =Ïˆ   , increasing 
the capability of particle region Search to  change the 
direction of particle search by the best solution of 
global and personal ( , ),the particles can follow the 
experience of being the best guide to improve local 
search results. 
 
This study selected four frequently Benchmark to run 
numerical experiments for algorithm validation and 
performance assessment. According to characteristic of 
function be divided into with a single minimum point (single 
mode) and a number of local minimum points (multi-modal) 
two categories, each category of two functions, as shown in 
Table II.  
TABLE II.  BENCHMARK FUNCTION 
Function Name Dim 
Search 
Space 
Min/Best 
position 
âˆ‘
=
=
n
i
ixf
2
2
1
 
Sphere 30 (-100,100) 0 /(0,â€¦0) 
âˆ‘
=
âˆ’+âˆ’+=
n
i
ixixixf
1
)2)1(2)21(100(2
 
Rosen Brock 30 (-50,50)N 0 /(1,â€¦1) 
1cos
4000
1
1 1
2
3 +âŽŸâŽŸâŽ 
âŽžâŽœâŽœâŽ
âŽ›âˆ’= âˆ‘ âˆ
= =
n
i
n
i
i
i i
x
xf
 
Griewank 30 (-300,300) 0 /(0,â€¦0) 
10,
))2cos((
1
2
4
=
+âˆ’=âˆ‘
=
A
AxAxf
n
i
ii Ï€  Rastrigrin 30 (-5.12,5.12) 0 /(0,â€¦0) 
In this study, this test functions in a unified standard, 
through the same algebra (time) to test the proposed hybrid 
optimization algorithm, compared with traditional types of 
algorithms, that to test whether is under the same of algebra 
has a better solution (minimum). 
 
IV. EXPERIMENTAL RESULTS 
In this study, we use PSO, SA, MC, Hybrid algorithms 
to search the best solution in Benchmark Function.  Shown 
as Figure 10, the purple line - All represent the experiment 
results of our Hybrid algorithm, we can find out that our 
hybrid algorithm obtain better results than the other 
algorithms on four benchmark functions. According to the 
experimental results shown in Figure 10, which shows that 
our Hybrid algorithm could improve the search result very 
well.We have the conclusion. If we donâ€™t consider the 
solution at present, MC is exceptional performance for SA. 
If SA has no convergence and no change of searching scope, 
the neighborhood of MC will be the same. By the concept of 
neighborhood, to re-deconstruct which MC, SA and PSO of 
algorithm, and combine with three kinds of algorithms to 
construct a new algorithm.                                                                            
In this study, we know that many algorithms have been 
proposed about PSO, MC, SA over the past few decades. 
And it can be found that most previous studies made up the 
shortcomings of a single algorithm for each other. The aim 
of this study is to improve the specially weakness of the 
algorithm by the advantage of another algorithm, and 
propose the new system architecture of algorithm for PSO, 
MC, SA. It integrates the algorithms about MC, SA, PSO 
and reconstructs a new algorithm framework which is the 
combination of the processing solution of neighborhood 
search and moving. The study integrates the neighborhood 
characteristics and reconstructs the neighborhood 
architecture of these algorithms to break through this 
limitation and improve the solving efficiency. The new 
algorithm framework not only can enhance the performance 
of algorithms but also help to cope with the diversity and 
complexity of problem solving. 
V. REFERENCES 
[1] N. Metropolis and S. Ulam,â€The Monte Carlo Method,â€Journal of the 
American Statistical Association,Vol.44,No.247,pp.335-341,Sep.1949. 
[2] N. Metropolis A. W. Rosenbluth, M. N. Rosenbluth,A. 
H. Teller, â€œEquations of state  calculations by fast 
compuing machines,â€Jourmal of Chamical 
Physics,Vol.21pp.1087-1092,1953 
[3] Yao, Xin, "New simulated annealing algorithm," 
International Journal of Computer Mathematics, Vol.56, 
No.3-4, pp.161-168, 1995. 
[4] V. Fabian, "Simulated annealing simulated," Computers 
& Mathematics with Applications, Vol. 33, No.1-2, pp. 
81-94, 1997. 
[5] J. Haddock, and J. Mittenthal, "Simulation optimization 
using simulated annealing", Computers & Industrial 
Engineering, Vol. 22, No.4, pp. 387-395, Oct, 1992. 
[6] F. Yang, Z. Zhuang, Y. Dai, "Using simulated 
annealing," Proceedings of the International Conference 
 
 
åœ‹ç§‘æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•«é …ä¸‹å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
                                    æ—¥æœŸï¼š100 å¹´ 7 æœˆ 1 æ—¥ 
                                 
ä¸€ã€åƒåŠ æœƒè­°ç¶“éŽ 
    2011 ç¥žç¶“ç¶²è·¯èˆ‡æ¨¡ç³Šç³»çµ±åœ‹éš›å­¸è¡“æœƒè­° (FSNC 2011)èˆ‡ 2011 ç¬¬äºŒå±†é«˜æ€§èƒ½ç¶²
è·¯ã€è¨ˆç®—èˆ‡é€šè¨Šç³»çµ±æš¨è¨ˆç®—æ©Ÿç§‘å­¸ç†è«–åŠæ•¸å­¸åŸºç¤Žåœ‹éš›ç ”è¨Žæœƒ(ICHCC-ICTMF 2011)
åˆ†åˆ¥æ–¼ 2 æœˆ 20-21 æ—¥åœ¨ä¸­åœ‹é¦™æ¸¯èˆ‡äº”æœˆ 5-6 æ—¥åœ¨æ–°åŠ å¡å¬é–‹ã€‚æœƒè­°åˆ†åˆ¥ç”±åœ‹éš›æ™ºèƒ½
ä¿¡æ¯æŠ€è¡“æ‡‰ç”¨å­¸æœƒï¼ˆIITAï¼‰ã€IEEE ã€ Springer åˆä½œä¸»è¾¦ã€‚æœƒè­°è«–æ–‡é›†å°‡åˆ†åˆ¥ç”± IEEE
å‡ºç‰ˆç¤¾èˆ‡ Springer å‡ºç‰ˆé›†åœ˜å‡ºç‰ˆï¼Œæ‰€æœ‰éŒ„å–çš„è«–æ–‡å°‡è¢« EI å’Œ ISTP æª¢ç´¢ã€‚å„ªç§€è«–æ–‡
è¨ˆç•«ç·¨è™Ÿ	 NSC99-2221-E-158-007	 
è¨ˆç•«åç¨±	 é„°åŸŸæœå°‹æž¶æ§‹æ•´åˆæœ€ä½³åŒ–æ¼”ç®—æ³•	 
å‡ºåœ‹äººå“¡å§“å	 é¾”å¿—éŠ˜	 
æœå‹™æ©Ÿæ§‹åŠ
è·ç¨±	 
å¯¦è¸å¤§å­¸	 è³‡è¨Šç§‘æŠ€èˆ‡é€šè¨Šå­¸ç³»	 
åŠ©ç†æ•™æŽˆ	 
æœƒè­°æ™‚é–“	 
100 å¹´ 2 æœˆ 20 æ—¥
è‡³	 
100 å¹´ 2 æœˆ 21 æ—¥	 
æœƒè­°åœ°é»ž	 é¦™æ¸¯	 
æœƒè­°åç¨±	 
2011	 International	 Conference	 on	 Fuzzy	 Systems	 and	 Neural	 Computing	 
(FSNC	 2011)	 
ç™¼è¡¨è«–æ–‡é¡Œç›®	 
A	 Low-Complexity	 And	 High-Performance	 Hybrid	 Problem	 Solving	 
Method	 Based	 On	 Neighborhood	 Search	 Algorithms	 
æœƒè­°æ™‚é–“	 
100 å¹´ 5 æœˆ 5 æ—¥	 
è‡³	 
100 å¹´ 5 æœˆ 6 æ—¥	 
æœƒè­°åœ°é»ž	 æ–°åŠ å¡	 
æœƒè­°åç¨±	 
2011	 International	 Conference	 on	 Theoretical	 and	 Mathematical	 
Foundations	 of	 Computer	 Science	 (ICHCC-ICTMF	 2011)	 
ç™¼è¡¨è«–æ–‡é¡Œç›®	 
Image	 Enhancement	 using	 the	 Multi-scale	 Filter	 :	 Application	 of	 	 the	 
bilateral	 filtering	 scheme	 and	 PSO	 algorithm	 
é™„ä»¶äº” 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
é¦–å…ˆæ„Ÿè¬åœ‹ç§‘æœƒè¨ˆç•«å°æ–¼é€™æ¬¡åœ‹éš›æœƒè­°ç ”ç©¶è¨ˆç•«çš„ç¶“è²»è£œåŠ©å’Œæ”¯æŒï¼Œé™¤ï¦ºå¸æ”¶
ä¸–ç•Œå„åœ°å„ªç§€å­¸è€…æ‰€æä¾›çš„ç ”ç©¶è³‡è¨Šä¹‹å¤–ï¼Œå°æ–¼é€™ç¨®ç›´æŽ¥é¢å°é¢äº¤ï§Šèˆ‡è§€æ‘©çš„æ©Ÿæœƒï¼Œ
èˆ‡æœƒè€…æå‡ºçš„æœ€æ–°æˆæžœå’Œäº¤æµæ€æƒ³å°æå‡æ–°æŠ€è¡“çš„ç ”ç©¶é–‹ç™¼å’Œæ‡‰ç”¨éƒ½èƒ½ä¿ƒé€²æ›´å¤šæŠ€
è¡“çš„æå‡ä¹Ÿæ›´èƒ½å¤ æå‡åœ‹å…§çš„ç ”ç©¶æ°´æº–ï¼Œä¸¦æé«˜å°ç£åœ¨åœ‹éš›å­¸è¡“ç ”ç©¶ä¸Šçš„èƒ½ï¨Šï¨ã€‚ 
   æ­¤æ¬¡å­¸è¡“æœƒè­°çš„èˆ‡æœƒäººå£«çœ¾å¤šï¼Œå¤šç‚ºäººå·¥æ™ºæ…§èˆ‡è³‡è¨Šé ˜åŸŸå°ˆé•·çš„å°ˆå®¶èˆ‡å­¸è€…ã€‚
è—‰ç”±æœƒè­°ç¬¬ä¸€å¤©çš„é¤æ•˜ä¸¦é‡å°æ–¼å­¸è¡“ä¸Šçš„ç ”ç©¶ä½œäº¤æµäº’å‹•ï¼Œçž­è§£è¨±å¤šå°ˆå®¶å­¸è€…æ–¼å­¸
è¡“ä¸Šçš„ç ”ç©¶é ˜åŸŸã€‚è€Œé€éŽç¬¬äºŒå¤©çš„å°ˆé¡Œè¬›åº§èˆ‡è«–æ–‡ç™¼è¡¨ï¼Œç²å–ç›®å‰å­¸è¡“çš„ç ”ç©¶å‹•å‘ã€‚ 
ä¸‰ã€è€ƒå¯Ÿåƒè§€æ´»å‹•(ç„¡æ˜¯é …æ´»å‹•è€…ç•¥) 
å››ã€å»ºè­° 
    åœ¨æœƒè­°ä¸­ç™¼è¡¨è‡ªå·±çš„ç ”ç©¶æˆæžœï¼Œä¸¦èˆ‡èˆ‡æœƒäººå£«ç›¸äº’è¨Žè«–æ˜¯éžå¸¸é›£å¾—çš„ç¶“é©—ï¼Œä¹Ÿ
æä¾›äº†ä¸€äº›ä¸åŒçš„æ€è€ƒæ¨¡å¼ï¼Œå°æ–¼æ—¥å¾Œçš„ç ”ç©¶æ–¹å‘æœ‰å¾ˆå¤§çš„å¹«åŠ©ï¼Œä¸”æœƒè­°å…§å®¹å¤§éƒ¨
åˆ†éƒ½æ˜¯å°šæœªç™¼è¡¨çš„ç ”ç©¶æˆæžœï¼Œå¯ä»¥å¾žä¸­äº†è§£ç›®å‰æœ€æ–°äººå·¥æ™ºæ…§èˆ‡è³‡è¨Šå·¥ç¨‹çš„ç™¼å±•è¶¨
å‹¢ï¼Œå•Ÿç™¼äº†æˆ‘æœªä¾†ç ”ç©¶çš„éˆæ„Ÿã€‚å¦å¤–ï¼Œå› ç‚ºï¦°è½ï¦ºï¤­è‡ªå„åœ‹ä¸€äº›å°ˆæ¥­å­¸è€…çš„ç ”ç©¶å ±
å‘Šï¼ŒåŒ…æ‹¬æ­äºžæ´²å„åœ‹ç­‰å­¸è€…ã€åšå£«ç”Ÿèˆ‡ç ”ç©¶å“¡ï¼Œç™¼ç¾åœ¨èˆ‡å€‹äººç ”ç©¶ï¦´åŸŸç›¸ï§ä¼¼çš„ä¸»
é¡ŒæŽ¢è¨Žä¸Šï¼Œé‚„æœ‰ï¤å¤šï¥§åŒçš„ç ”ç©¶æ–¹æ³•èˆ‡è¨Žï¥è§€é»žï¼Œå°‡å¯ç²è‡´ï¤å¤šçš„ï¨æ¹›çµæžœã€‚ 
äº”ã€æ”œå›žè³‡æ–™åç¨±åŠå…§å®¹ 
1. Program and Book of Abstracts: å…§å®¹ç‚ºæœƒè­°è­°ç¨‹èˆ‡æŠ•ç¨¿è€…ä¹‹è«–æ–‡æ‘˜è¦ã€‚ 
2. å¤§æœƒé™„è´ˆç’°ä¿è¢‹ä¸€åª 
3. å¤§æœƒç´€å¿µå“ (åŽŸå­ç­†ã€ CD)ä¸€ä»½ 
11/10/26 ä¸Šåˆ8:05å¯¦è¸å¤§å­¸é«˜é›„æ ¡å€æ•¸ä½é›»è¦–èˆ‡å¤šåª’é«”å¯¦é©—å®¤ éƒµä»¶ - FSNC 2011 Acceptance Letter
é é¢ 1âˆ•1https://mail.google.com/mail/u/0/?ui=2&ik=66f8830b49&view=pt&q=FSNC&qs=true&search=query&msg=12e029f63ffce866
Alex Kung <alex@itc.kh.usc.edu.tw>
FSNC 2011 Acceptance Letter
FSNC2010 <fsnc2011reg@163.com> 2011288:15
: Alex Kung <alex@itc.kh.usc.edu.tw>
Thank you for your submission to FSNC 2011. We are pleased to inform you that, according to the reports from anonymous reviewers, the
following distinguished work from you has been accepted for FSNC 2011, with the publisher of IEEE, which will be indexed by EI
Compendex and ISTP. 
You are kindly reminded with the following important notes:
1. Open the link and find a lot of information of paper submission and registration. 
http://www.iita-association.org/fsnc2011/ei/ieeeregistration.htm
2. In order to make high quality of Proceedings, the camera-ready version should follow format. Kindly download from here
http://www.iita-association.org/fsnc2011/ei/ieeeregistration.htm
3. After Finish the final Paper, you can prepare a Copyright Release Form. The copyright should download, print, write author names,
paper title, sign a name and date, and scanned it to PDF format 
http://www.iita-association.org/fsnc2011/ei/IEEE%20COPYRIGHT%20FORM.doc
4. Kindly download the registration form and pay for it,
http://www.iita-association.org/fsnc2011/ei/ieeeregistration.htm
 
and send both registration form and a scanned receipt from your bank to above Email  FSNC2011REG@163.com before Feb 8, 2011. If
you have not paid for your paper in that time, your paper will not be published. 
5. The e-copy official acceptance Letter could be download though
http://www.iita-association.org/fsnc2011/ei/Acceptance_and_invited_letter_ei.doc
 
Write you write author names, paper title and print it 
Kindly send Final paper (doc format), copyright, registration form and a scanned receipt to 
FSNC2011REG@163.com before Feb 8, 2011
Sincerely,
Program Committee of FSNC2011
E-mail: FSNC2011REG@163.com
URL: http://www.iita-association.org/fsnc2011/index.htm
 
 
paper ID 523
 
solution at random. At initial stages, a small random change is 
made in the current solution. Then the objective function value 
of new solution is calculated and compared with that of current 
solution. A move is made to the new solution if it has better 
value or if the probability function implemented in SA has a 
higher value than a randomly generated number. Otherwise a 
new solution generated and evaluated. 
 
Figure 1.   SA Flow diagram 
C. Particle Swarm Optimization 
Reynolds [8] observed flight behavior of the bird, he 
found that the individual behavior of each group can be 
modular by   individual, objective and group center of three 
vectors, and generate complex group behavior. The behavior 
model of bird flying is the basis of PSO. In 1995, PSO is 
proposed by the Eberhart and Kennedy [9-10], which is a 
kind of Evolutionary Computation. Every bird can be as a 
particle. Each particle keeps track of its coordinates in 
hyperspace which are associated with the best solution 
(fitness). The value of that fitness is also stored. This value 
is called pbest. Another â€œbestâ€ value is also tracked. The 
â€œglobalâ€ version of the particle swarm optimizer keeps track 
of the overall best value, and its location, obtained thus far 
by any particle in the population; this is called gbest. The 
particle swarm optimization concept consists of, at each 
time step, changing the velocity each particle toward its 
pbest and gbest (global version). Acceleration is weighted 
by a random term, with separate random numbers being 
generated for acceleration toward pbest and gbest. 
TABLE I.  COMPARISON OF THE OPTIMAL ALGORITHMS 
Items of 
comparison 
Algorithms 
MC SA PSO 
Selection method of 
neighborhood Random 
Temperature of 
probability function 
Speed and move 
Of itself with several 
groups 
Selection region of 
neighborhood Global 
Current point of 
neighborhood 
Integrate region of 
itself and group 
vector 
The number of 
particle 
Times a 
particle 
A particle or more 
particles at once 
More particles 
At once 
Convergence data  Annealing temperature Fitness 
The way of escape 
local  Probability Global optimum 
Exchange 
information of 
generation 
No No Yes 
Algorithm type  Evolutionary & Probabilistic 
Evolutionary & 
Global 
D. Neighborhood  search and the Hybrid algorithm 
Traditional Neighborhood search is also known as local 
search; usually gets a trouble in local optimum. In 1980s, the 
new concept of heuristics is evolved for solving problem. 
These new heuristics are all the main strategies for solving 
feasible solution in the region move step by step to gradually 
improve the objective function value, and make the best 
algorithm with partial relief from the ability to leave, while 
approximating the optimum. 
The hybrid algorithm is combining two or more over 
algorithms. Combining with global and local search, a 
method is doing the local improvement operator of local 
search into the global search. In the hybrid algorithm, local 
search can obtain a global / local optimum solution.  
Since Glover proposed "Meta-heuristics" in his article, 
that the "Meta-heuristics" get most scholars agreement and 
using until now [11-13]. But the Meta-heuristics take the 
traditional heuristic method as the core architecture, 
combined with high search strategy, let algorithm can jump 
out local optimum [14]. Table I is the comparison of the 
optimal algorithm.  
III. RESEARCH METHODS 
This study is focus on two issues, first is how quickly and 
widely to search in any solution domain, and find out the 
approximate optimal solution in the son of the solution 
domain. Second, is how quickly and widely to solve in 
solution domain and avoid getting a trouble in local optimum.  
The new algorithm keeps on quality and speed.  
In this study, we integrate with the hybrid algorithm to 
develop into a Neighborhood Architecture and proposed a 
new algorithm to compare with traditional algorithms and 
find the algorithmic parameter settings by the neighborhood 
features. The neighborhood definitions can be expressed as 
follows: 
EFmELet 2},,...2,1{ ÂŽ and ÂƒoFfLet :  
Combinatorial optimization problem (COP) 
Minimize }:)({ FSSf Â   
EFN 2: o  Òž(1) 
where E2 is the subset of all Eâ€™s sets, Ë™Ê³ is a feasible solution, 
f is the objective function, S is feasible set, N is a 
neighborhood function, for each FS Â , N(S) is a subset of  E, 
N(S) become feasible solution S of neighborhood. 
MC often used in optimization of various fields. This 
method is mainly based on the Boltzmannâ€™s probability 
distribution function to help to get the best solution. And the 
neighboring solutions can be expressed as: 
)}1(|{ 11 randsxxN ttmc Â˜    Òž(2) 
Metropolis criterion is used to decide whether to accept a 
change in the energy changes. At a particular temperature T, 
each specifically atomic probability of existence that show 
283
The new system architecture includes the strategies of 
initial solution generated, neighborhood set generated, and 
neighborhood solutions updated. Where the neighborhood 
set is generated by the parameter of neighborhood. 
According to the chosen of neighborhood domain, the 
neighborhood solution will be update. The particles move 
position by the new particles of randomly generated, and get 
fitness value by the new position of solution. According to 
Acceptance Rule, the fitness value will reject or accept the 
poor fitness value by probability. Figure. 3 is the new system 
architecture of hybrid algorithm. 
This study selected four frequently Benchmark to run 
numerical experiments for algorithm validation and 
performance assessment. According to characteristic of 
function be divided into with a single minimum point 
(single mode) and a number of local minimum points 
(multi-modal) two categories, each category of two 
functions, as shown in Table II. 
TABLE II.  BENCHMARK FUNCTION 
Function Name Dim Search Space 
Min/Best 
position 
Â¦
 
 
n
i
ixf
2
2
1
 
Sphere 30 (-100,100) 0 /(0,â€¦0) 
Â¦
 
 
n
i
ixixixf
1
)2)1(2)21(100(2
 
Rosen 
Brock 30 (-50,50)
N 0 /(1,â€¦1) 
1cos
4000
1
1 1
2
3 Â¸Â¸Â¹
Â·
Â¨Â¨Â©
Â§ Â¦ Â–
  
n
i
n
i
i
i i
x
xf
 
Griewank 30 (-300,300) 0 /(0,â€¦0) 
10,
)2cos((
1
2
4
 
 Â¦
 
A
AxAxf
n
i
ii S
 
Rastrigrin 30 (-5.12,5.12) 0 /(0,â€¦0) 
In this study, this test functions in a unified standard, 
through the same algebra (time) to test the proposed hybrid 
optimization algorithm, compared with traditional types of 
algorithms, that to test whether is under the same of algebra 
has a better solution (minimum). 
TABLE III.  PSO, SA, MC, HYBRID  SEARCH THE BEST VALUE IN 
BENCHMARK FUNCTION 
 
(a) Sphere Function (b) Rosen Brock Function 
 
(c) Girewank Function (d) Rastrigrin Function 
IV. EXPERIMENTAL RESULTS 
In this study, we use PSO, SA, MC, Hybrid algorithms to 
search the best solution in Benchmark Function.  Shown as 
table III, the purple line - All represent the experiment results 
of our Hybrid algorithm, we can find out that our hybrid 
algorithm obtain better results than the other algorithms on 
four benchmark functions. According to the experimental 
results shown in table III, which shows that our Hybrid 
algorithm could improve the search result very well. 
ACKNOWLEDGMENT 
This work is supported by National Science Council of 
Taiwan grants: NSC 99-2221-E-158-007 
V. REFERENCES 
[1] N. Metropolis and S. Ulam, â€The Monte Carlo 
Method,â€ Journal of the American Statistical 
Association, vol.44, No.247, pp.335-341,Sep.1949. 
[2] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, 
and A. H. Teller, â€œEquations of state  calculations by 
fast compuing machines,â€ Jourmal of Chamical 
Physics,vol.21, pp.1087-1092,1953 
[3] Yao, Xin, "New simulated annealing algorithm," 
International Journal of Computer Mathematics, Vol.56, 
No.3-4, pp.161-168, 1995. 
[4] V. Fabian, "Simulated annealing simulated," Computers 
& Mathematics with Applications, vol. 33, No.1-2, pp. 
81-94, 1997. 
[5] J. Haddock, and J. Mittenthal, "Simulation optimization 
using simulated annealing", Computers & Industrial 
Engineering, vol. 22, No.4, pp. 387-395, Oct, 1992. 
[6] F. Yang, Z. Zhuang, Y. Dai, "Using simulated 
annealing," Proceedings of the International 
Conference on Circuits and Systems, Nanjing, China, 
pp.175, July, 1989. 
[7] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi, 
"Optimization by simulated annealing," Science, Vol. 
220, pp. 671-680, 1983. 
[8] C. W. Reynolds, â€Flocks, herds and schools: a 
distributed behavioral model,â€ Computer Graphics,vol. 
21, No.4, pp.25-34, 1987. 
[9] J. Kennedy, R. C. Eberhart, â€œParticle swarm 
optimization,â€ in: Proc. IEEE Int. Conf, on Neural 
Networks, Perth, Australia, vol.4, pp.1942-1948,1995. 
[10] R. C. Eberhart, J. Kennedy, â€œA new optimizer using 
particle swarm theory,â€ in: Proc. IEEE Int. Symposium 
on Micro Machine and Human Science, Nagoya, 
Japan,pp.39-43,1995. 
[11] F. Glover, and M. Laguna, â€œTabu search,â€ Kluwer 
Academic Publishers," Massachusetts, 1997.  
[12] F. Glover, "Tabu Search, Part I," ORSA Journal on 
Computing, Vol. 1, No. 3, pp.190-206, 1989.  
[13] F. Glover, "Tabu Search- Part II," ORSA Journal on 
Computing, Vol. 2, No. 1, pp. 4-32, 1990. 
[14] I. H. Osman, and J. P. Kelly, "Meta-Heuristics: An 
overview," Meta-Heuristics: Theory & Applications, 
Kluwer Academic Publishers, Boston, London, 
Dordrecht, pp. 1-21, 1996 
285
11/10/26 ä¸Šåˆ8:01å¯¦è¸å¤§å­¸é«˜é›„æ ¡å€æ•¸ä½é›»è¦–èˆ‡å¤šåª’é«”å¯¦é©—å®¤ éƒµä»¶ - Acceptance Notification and Inviting Letter for ICHCC 2011 Paper 253 (è®ºæ–‡å½•ç”¨é€šçŸ¥ï¼‰
é é¢ 2âˆ•2https://mail.google.com/mail/u/0/?ui=2&ik=66f8830b49&view=pt&q=accept&qs=true&search=query&th=12ea0c9d72fcfc9f
Description, originality of the own contribution: 4
Presentation of the results: 5
Conclusions and future work: 5
Readability, quality of the English: 4
Quality of the figures: 4
Quality of format: 5
Overall Paper Recommendation (1-7, 1 strong reject, 7 strong accept) accepted as regular paper: 5
Please modify it according to ICHCC-ICTMF 2011 Format strictly. Otherwise, we will not publish your paper in the
proceedings. If you are not a native speaker (not familiar with in English environment), please check your
sentences and/or English one more time to improve the quality of the final camera-ready paper.
        
improving the visual identification, and provides the automatic image processing 
procedure in the future (e.g. analyze, detection, division, and identify). Most methods 
are proposed about image enhancement. [1-3], these methods mostly modify the value 
of histogram, the other methods is analyzing the edge and adjusting contrast or 
transforming the global entropy. 
2.1 Bilateral Filter 
Bilateral filter is a technology to smooth images. It is a non-linear filter. It is 
proposed for smoothing the noise and preserving edges in the image processing. It 
starts with standard Gaussian filtering in both spatial and intensity domains.  
It has been used in various contexts such as denoising, texture editing and 
relighting, tone management, demosaicking, stylization, and optical-flow estimation. 
The bilateral filter has several qualities that explain its success: 
1. Its formulation is simple: each pixel is replaced by a weighted average of its 
neighbors. This aspect is important because it makes it easy to acquire 
intuition about its behavior, to adapt it to application-specific requirements, 
and to implement it. 
2. It depends only on two parameters that indicate the size and contrast of the 
features to preserve. 
3. It can be used in a non-iterative manner. This makes the parameters easy to 
set since their effect is not cumulative over several iterations. 
Fig.1 is the bilateral filter deal with the High dynamic range image. The output 
image of the bilateral filter is become very well, the edge is obviously. 
 
 
(a) the high dynamic range image (b) output of  the bilateral filter 
Figure 1. Image processing use the bilateral filter 
 
The bilateral filtering is defined as follows: 
âˆ‘âˆ‘
âˆ‘âˆ‘
âˆ’= âˆ’=
âˆ’= âˆ’=
++
= a
ai
b
bj
a
ai
b
bj
jiW
iyixfjiW
yxg
),(
),(),(
),(  (1) 
Where f(x, y) is the original signal, g(x, y) is the smoothed signal by the bilateral 
filtering, )12()12( +Ã—+ ba  is the length of bilateral filter. W(i, j) is the kernel of  
bilateral filter: 
),(),(),( jiWjiWjiW Is Ã—=  (2) 
  Ws is the Gaussian filter on the spatial domains, is defined by : 
        
where *jx  (PBEST) denotes the best position of jth particle up to time t-1 and x
# 
(GBEST) denotes the best position of the whole swarm up to time t-1, Ï†1 and Ï†2 are 
random numbers, and c1 and c2 represent the individuality and sociality coefficients, 
respectively. 
The population size is first determined, and the position and velocity of each 
particle are initialized. Each particle moves according to (5) and (6), and the fitness is 
then calculated. Meanwhile, the best positions of each particle and the swarm are 
recorded. Finally, as the stopping criterion is satisfied, the best position of the swarm 
is the final solution. The block diagram of PSO is displayed in Fig. 3 and the main 
steps are given as follows: 
1. Set the swarm size. Initialize the position and the velocity of each particle 
randomly. 
2. For each j, evaluate the fitness value of jx and update the individual best 
position *jx  if better fitness is found. 
3. Find the new best position of the whole swarm. Update the swarm best 
position #x  if the fitness of the new best position is better than that of the 
previous swarm. 
4. If the stopping criterion is satisfied, then stop. 
5. For each particle, update the velocity and the position according (5) and (6). 
Go to step 2. 
3 Experimental methods  
The study focuses on the image enhancement by Bilateral filter with Particle 
Swarm Optimization Alogrithm. And we use the SSIM index [6-15]to assess the image 
quailty. According to the quailty, the image process will finish or again.  
In this study, we use this two parts to set a system to achive the image enhancement 
about adaptive video enhancement filters. The system diagram is given in Fig. 9. 
Bilateral filtei nput
Coarse 
image
residual 
image
SSI M
PSO
out putI mage 
Synt hesi s
 
Figure 3. The system diagram 
4.1 Bilateral filter with PSO 
As discussed in Section 2, we know the bilateral filter is a neighborhood filter by 
Gaussian filter. The bilateral filter has three parameters: |Nx| is the neighborhood size, 
Ïƒs  is the distance variance of Gaussian distribution function , Ïƒi is the gray value 
difference of Gaussian distribution function.  
We encode the particle as xj =(|Nx|, Ïƒs, Ïƒi), which is the position of the domain 
block. The steps of encoding a range block using PSO are summarized as follows: 
1. Initialize the parameters of PSO. 
        
6 Acknowledgment 
This work is supported by National Science Council of Taiwan grants: NSC 98-
2815-C-158-003-E, NSC 98-2221-E-158-005 
7 References 
1. Rosenfeld and A.C.Kak, Digital Picture Processing. New York: Academic, 
Vol.1(1982). 
2. Jain, Fundamentals of Digital Image Processing. Englewood Cliffs, NJ: Prentice-Hall 
(1989). 
3. T.L.Ji, M.K. Sundareshan, and H. Roefrig, â€œAdaptive image contrast enhancement 
based on human visual properties,â€ IEEE Trans. Med. Img., vol. 13, pp. 573-586, Dec. 
1994. 
4. J. Kennedy, R. C. Eberhart, â€œParticle swarm optimization,â€ in: Proc. IEEE Int. Conf, 
on Neural Networks, vol.4, pp.1942-1948, Perth,Australia (1995). 
5. R. C. Eberhart, J. Kennedy, â€œA new optimizer using particle swarm theory,â€in: 
Proc.IEEE Int.Symposium on Micro Machine and Human Science, pp.39-43, 
Nagoya,Japan (1995). 
6. W.S.McCulloch,and W.Pitts,â€A logical calculus of ideas immanent in nervo-Us 
activity,â€Bulletin of Mayhematical Biophysics, vol. 5,pp.115-133 (1943). 
7. D. O. Hebb, The Organization of Behavior:A Neuropsychological Theory, Wiley, New 
York (1949). 
8. F.Rosenblatt,â€The perceptron : A probabilistic model for information Storage and 
organization in the brain,â€Psychological Review,vol.65,pp.386-408 (1958). 
9. Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, "Image quality assessment: 
From error visibility to structural similarity," IEEE Transactions on Image Processing, 
vol. 13, no. 4, pp. 600-612, Apr. (2004). 
10. Z. Wang and Q. Li, "Video quality assessment using a statistical model of human 
visual speed perception," Journal of the Optical Society of America A, Dec. (2007). 
11. Z. Wang and X. Shang, â€œSpatial pooling strategies for perceptual image quality 
assessment,â€ IEEE International Conference on Image Processing, Atlanta, GA, Oct. 
8-11,(2006). 
12. Z. Wang and E. P. Simoncelli, â€œTranslation insensitive image similarity in complex 
wavelet domain,â€ IEEE International Conference on Acoustics, Speech and Signal 
Processing, vol. II, pp. 573-576, Philadelphia, PA, Mar.( 2005). 
13. Z. Wang, L. Lu, and A. C. Bovik, â€œVideo quality assessment based on structural 
distortion measurement,â€ Signal Processing: Image Communication, special issue on 
â€œObjective video quality metricsâ€, vol. 19, no. 2, pp. 121-132, Feb. (2004). 
14. Z. Wang, E. P. Simoncelli and A. C. Bovik, â€œMulti-scale structural similarity for 
image quality assessment,â€ Invited Paper, IEEE Asilomar Conference on Signals, 
Systems and Computers, Nov. (2003) 
15. Chih-hsien Kung, Wei-sheng Yang, Chun-yuan Huang, Chih-ming Kung, 
â€œInvestigation of the Image Quality Assessment using Neural Networks and Structure 
Similartyâ€, The Third International Symposium Computer Science and Computational 
Technology(ISCSCT 2010),pp. 219-222,  Jiaozuo, China ,Aug.14-15(2010) 
 
99 å¹´åº¦å°ˆé¡Œç ”ç©¶è¨ˆç•«ç ”ç©¶æˆæžœå½™æ•´è¡¨ 
è¨ˆç•«ä¸»æŒäººï¼šé¾”å¿—éŠ˜ è¨ˆç•«ç·¨è™Ÿï¼š99-2221-E-158-007- 
è¨ˆç•«åç¨±ï¼šé„°åŸŸæœå°‹æž¶æ§‹æ•´åˆæœ€ä½³åŒ–æ¼”ç®—æ³• 
é‡åŒ– 
æˆæžœé …ç›® å¯¦éš›å·²é”æˆ
æ•¸ï¼ˆè¢«æŽ¥å—
æˆ–å·²ç™¼è¡¨ï¼‰
é æœŸç¸½é”æˆ
æ•¸(å«å¯¦éš›å·²
é”æˆæ•¸) 
æœ¬è¨ˆç•«å¯¦
éš›è²¢ç»ç™¾
åˆ†æ¯” 
å–®ä½ 
å‚™ è¨» ï¼ˆ è³ª åŒ– èªª
æ˜Žï¼šå¦‚æ•¸å€‹è¨ˆç•«
å…±åŒæˆæžœã€æˆæžœ
åˆ— ç‚º è©² æœŸ åˆŠ ä¹‹
å° é¢ æ•… äº‹ ...
ç­‰ï¼‰ 
æœŸåˆŠè«–æ–‡ 0 0 0%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 0%  
ç ”è¨Žæœƒè«–æ–‡ 0 0 0% 
ç¯‡ 
 
è«–æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 0%   
ç”³è«‹ä¸­ä»¶æ•¸ 0 0 0%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 0% ä»¶  
ä»¶æ•¸ 0 0 0% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 0% åƒå…ƒ  
ç¢©å£«ç”Ÿ 1 1 100%  
åšå£«ç”Ÿ 1 1 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 0%  
åœ‹å…§ 
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆæœ¬åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 0 0 0% 
äººæ¬¡ 
 
æœŸåˆŠè«–æ–‡ 1 1 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 0%  
ç ”è¨Žæœƒè«–æ–‡ 1 1 100% 
ç¯‡ 
 
è«–æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 0% ç« /æœ¬  
ç”³è«‹ä¸­ä»¶æ•¸ 0 0 0%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 0% ä»¶  
ä»¶æ•¸ 0 0 0% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 0% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 0%  
åšå£«ç”Ÿ 0 0 0%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 0%  
åœ‹å¤– 
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆå¤–åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 0 0 0% 
äººæ¬¡ 
 
