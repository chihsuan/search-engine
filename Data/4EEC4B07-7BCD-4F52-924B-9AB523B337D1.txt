recognition is a recent re-emerging speech 
recognition research. The latest research shows that, 
by constructing a universal phone set based on the 
mapping between phones of different languages and the 
manners or places of articulation, we can establish a 
universal phone recognizer, which is trained by 
existing phone labeled speech corpora of various 
languages. The universal phone recognizer can be used 
to recognize speech of languages which lack labeled 
training data. In this way, we can develop Min-nan 
and Hakka speech recognition systems or Mandarin/Min-
nan/Hakka/English multilingual speech recognition 
systems even though the labeled training data for 
Min-nan and Hakka speech are not available. In view 
of this, the objective of this sub-project is to 
develop multilingual speech recognition systems and 
spoken language recognition systems based on our 
experience on developing detection-based English 
phone recognition on the TIMIT corpus. 
 
è‹±æ–‡é—œéµè©ï¼š speech attributes, speech recognition, spoken 
language recognition, multilingual speech recognition
 
include multilingual speech recognition and 
speech attribute detection-based speech 
recognition and spoken language recognition. 
Speech attribute detection-based speech 
recognition is a recent re-emerging speech 
recognition research. The latest research 
shows that, by constructing a universal phone 
set based on the mapping between phones of 
different languages and the manners or 
places of articulation, we can establish a 
universal phone recognizer, which is trained 
by existing phone labeled speech corpora of 
various languages. The universal phone 
recognizer can be used to recognize speech 
of languages which lack labeled training data. 
In this way, we can develop Min-nan and 
Hakka speech recognition systems or 
Mandarin/Min-nan/Hakka/English 
multilingual speech recognition systems even 
though the labeled training data for Min-nan 
and Hakka speech are not available. In view 
of this, the objective of this sub-project is to 
develop multilingual speech recognition 
systems and spoken language recognition 
systems based on our experience on 
developing detection-based English phone 
recognition on the TIMIT corpus. 
 
Keywords: speech attributes, speech 
recognition, spoken language recognition, 
multilingual speech recognition 
 
äºŒã€è¨ˆç•«ç·£ç”±èˆ‡ç›®çš„ 
 
æœ¬è¨ˆç•«çš„ç ”ç©¶èª²é¡Œæ©«è·¨è‡ªå‹•èªéŸ³è¾¨è­˜
(automatic speech recognition, ASR)ç ”ç©¶çš„
ä¸‰ å€‹ é‡ è¦ ä¸» é¡Œ ï¼š å¤š èª è¨€ èª éŸ³ è¾¨ è­˜
(multilingual ASR)ã€åŸºæ–¼èªéŸ³å±¬æ€§åµæ¸¬ä¹‹èª
éŸ³ è¾¨ è­˜ (speech attribute detection-based 
ASR)åŠå£èªèªè¨€è¾¨è­˜ (spoken language 
recognition)ã€‚ 
è‡ªå‹•èªéŸ³è¾¨è­˜æŠ€è¡“ç™¼å±•äº†åŠå€‹ä¸–ç´€ï¼Œ
ä¸­ã€è‹±ã€æ—¥ã€å¾·ã€æ³•ã€è¥¿ç­ç‰™åŠé˜¿æ‹‰ä¼¯(911
ææ€–æ”»æ“Šäº‹ä»¶ä¹‹å¾Œç¾åœ‹ç©æ¥µæŠ•å…¥)ç­‰ä¸»è¦
èªè¨€è‡ªå‹•èªéŸ³è¾¨è­˜æŠ€è¡“å·²ç›¸ç•¶æˆç†Ÿï¼Œåªè¦
æœ‰è¶³å¤ çš„è¨“ç·´èªæ–™å°±èƒ½å»ºç«‹é«˜è¾¨è­˜ç‡çš„èª
éŸ³è¾¨è­˜ç³»çµ±ã€‚éš¨è‘—å…¨çƒåŒ–çš„é¢¨æ½®ï¼Œä¸åŒèª
è¨€ä½¿ç”¨è€…é–“çš„å£èªæºé€šå ´åˆæ—¥ç›Šå¢åŠ ï¼Œå¼•
ç™¼å£èªèªè¨€è¾¨è­˜å’Œå¤šèªè¨€èªéŸ³è¾¨è­˜çš„éœ€æ±‚ï¼Œ
é€²è€Œè¡ç”Ÿå£èªèªè¨€ç¿»è­¯(spoken language 
translation)çš„ç™¼å±•ï¼Œæœ€è‘—åçš„ä¾‹å­æ˜¯ 90 å¹´
ä»£åˆæœŸé–‹å§‹çš„ C-STAR (The Consortium for 
Speech Translation Advanced Research)ï¼Œçµ
åˆç¾ã€æ—¥ã€å¾·ã€æ³•ã€éŸ“ã€ç¾©ã€ä¸­ç­‰åœ‹çš„ç ”
ç©¶åœ˜éšŠï¼Œå”åŠ›é–‹ç™¼å£èªèªè¨€ç¿»è­¯åœ¨è§€å…‰æ—…
éŠæ–¹é¢çš„æ‡‰ç”¨ï¼ŒåŒ…æ‹¬è¡Œç¨‹å®‰æ’ã€è³‡è¨ŠæŸ¥è©¢ã€
é è¨‚æœå‹™åŠæœƒè«‡å”å•†ç­‰ã€‚åœ¨å£èªèªè¨€è¾¨è­˜
æ–¹é¢ï¼Œç¾åœ‹ NIST (National Institute of 
Standards and Technology)è‡ª 1996å¹´èµ·ï¼Œç©
æ¥µ æ¨ å‹• èª è¨€ è¾¨ è­˜ è©• æ¯” (Language 
Recognition Evaluation, LRE)ï¼Œè—‰ç”±æä¾›æ¨™
æº–èªæ–™åº«å’Œç›¸é—œåŸºæº–(benchmarks)ï¼Œå¸¶å‹•äº†
é€™å€‹é ˜åŸŸçš„ç™¼å±•ã€‚å¦ä¸€æ–¹é¢ï¼Œå–®ä¸€åœ°å€å…§
å¤šç¨®èªè¨€ï¼Œä¾‹å¦‚ç•¶åœ°èªè¨€èˆ‡åœ‹éš›é€šç”¨èªè¨€
è‹±èªçš„äº¤éŒ¯ä½¿ç”¨å·²ç›¸ç•¶æ™®éï¼Œä½†ç›®å‰é›™èª
èªéŸ³è¾¨è­˜ç³»çµ±çš„ç™¼å±•ç¨‹åº¦å»é ä¸åŠå–®èªç³»
çµ±ï¼›è€Œæ–¹è¨€å› ç‚ºæœ¬èº«è³‡æºä¸è¶³ï¼Œå–®èªèªéŸ³
è¾¨è­˜çš„ç™¼å±•å·²ç¶“æ¥µç‚ºæœ‰é™ï¼Œæ›´é‘è«–æ··åˆä¸»
è¦èªè¨€èˆ‡æ–¹è¨€çš„é›™èªæˆ–å¤šèªèªéŸ³è¾¨è­˜ã€‚é‡
å°è³‡æºåŒ±ä¹èªè¨€çš„è™•ç†çš„ç›¸é—œè­°é¡Œåœ¨åœ‹éš›
å­¸è¡“ç•Œå·²é€æ¼¸å—åˆ°é‡è¦–ï¼Œç›¸é—œçš„ç ”è¨æœƒç¹¼
2006 å¹´çš„ Workshop on Resource-Scarce 
Language Engineeringï¼Œå¾ŒçºŒæœ‰Workshop on 
Spoken Languages Technologies for 
Under-Resourced Languages (SLTU2008, 
SLTU2010)ï¼Œå¦‚ä½•å°‡éå»åœ¨è³‡æºç›¸å°è±å¯Œ
èªè¨€è™•ç†ç´¯ç©çš„ç¶“é©—ç”¨åˆ°è³‡æºç›¸å°åŒ±ä¹èª
è¨€çš„è™•ç†ï¼Œé”åˆ°è³‡æºå…±ç”¨ï¼Œé™ä½é–‹ç™¼æˆæœ¬ï¼Œ
å·²æˆç‚ºé‡è¦çš„ç ”ç©¶èª²é¡Œã€‚ 
åŸºæ–¼èªéŸ³å±¬æ€§åµæ¸¬ä¹‹èªéŸ³è¾¨è­˜æ˜¯è¿‘å¹´
é‡æ–°èˆˆèµ·çš„èªéŸ³è¾¨è­˜ç ”ç©¶æ–¹å‘ã€‚è‡ªå‹•èªéŸ³
è¾¨è­˜æŠ€è¡“ç™¼å±•çš„æ­·å²ä¸­ï¼Œæ—©æœŸå³æ˜¯ä¾æ“šèª
è¨€å­¸å®¶çš„è²éŸ³åˆ†é¡çŸ¥è­˜ï¼Œåˆ©ç”¨è²å­¸èˆ‡èªè¨€
å­¸çš„è¦å‰‡ï¼Œå»ºç«‹èªéŸ³è¾¨è­˜çš„æ©Ÿåˆ¶ï¼Œæ¡ç”¨çŸ¥
è­˜é©…å‹•(knowledge-driven)çš„è§£æ±ºæ–¹æ³•ã€‚é€™
é¡ æ–¹ æ³• æ‰€ å»º ç«‹ ä¹‹ ä»¥ è¦ å‰‡ ç‚º åŸº ç¤ çš„
(rule-based)èªéŸ³è¾¨è­˜ç³»çµ±é›£ä»¥æ‡‰ä»˜è¤‡é›œçš„
è®ŠåŒ–ï¼Œæ¬ ç¼ºå¼·å¥æ€§(robustness)ã€‚åœ¨éš±è—å¼
é¦¬å¯å¤«æ¨¡å‹(hidden Markov model, HMM)
è¢«æå‡ºä¹‹å¾Œï¼Œä»¥çŸ¥è­˜é©…å‹•çš„èªéŸ³è¾¨è­˜æ–¹æ³•
å¹¾ ä¹ å®Œ å…¨ ç”± ä»¥ èª æ–™ åº« ç‚º åŸº ç¤ çš„
(corpus-based)èªéŸ³è¾¨è­˜æ–¹æ³•æ‰€å–ä»£ã€‚åˆ°äº†
90å¹´ä»£ï¼Œè¨±å¤šç”±è³‡æ–™é©…å‹•(data-driven)çš„æ¼”
ç®—æ–¹æ³•è¢«æå‡ºä¾†ï¼Œè‡ªå‹•èªéŸ³è¾¨è­˜æŠ€è¡“æœ‰äº†
å¿«é€Ÿçš„é€²æ­¥ï¼Œå¤§è©å½™é€£çºŒèªéŸ³è¾¨è­˜(large 
vocabulary continuous speech recognition, 
 2 
çš„ä½¿ç”¨ï¼Œä¹Ÿèƒ½åˆ©ç”¨å…¶èˆ‡ IPA ä¸€å°ä¸€ä¹‹å°æ‡‰
é—œä¿‚ï¼Œä»¥åŠç°¡å–®çš„ ASCII ç¬¦è™Ÿè¡¨ç¤ºæ³•ï¼Œä¾†
ç°¡åŒ–é›»è…¦ç³»çµ±çš„ç·¨ç¢¼æ–¹å¼ã€‚ 
  ç¾éšæ®µæˆ‘å€‘çš„ X-SAMPA èˆ‡å…¶èªéŸ³å±¬
æ€§å°æ‡‰è¡¨å·²å®Œæˆåœ‹èªå’Œè‹±èªéƒ¨åˆ†ï¼Œå…¶éŸ³ç´ 
å°æ‡‰åŸºæœ¬åŸå‰‡å¦‚ä¸‹ï¼š 
  ä¸€ã€åœ‹èªçš„éƒ¨åˆ†ï¼Œç”±æ–¼æ¼¢èªæ‹¼éŸ³å·²æ˜¯
è¯äººä¸–ç•Œè¼ƒç‚ºæ™®éçš„åœ‹èªæ‹¼éŸ³ç³»çµ±ï¼Œæˆ‘å€‘
ä¾¿åƒç…§åŒ—äº¬å¤§å­¸ä¸­æ–‡ç³»ç¾ä»£æ¼¢èªæ•™ç ”å®¤æ‰€
ç·¨è‘—ä¹‹ã€Šç¾ä»£æ¼¢èªã€‹ä½œç‚ºæ¼¢èªæ‹¼éŸ³ä¸­å„å€‹
éŸ³ç´ èˆ‡ IPA å°æ‡‰ä¹‹ä¸»è¦æ ¹æ“šã€‚å¦‚æ­¤ä¸€ä¾†ï¼Œ
åŸæœ¬åœ¨åœ‹èªæ³¨éŸ³ç¬¦è™Ÿä¸­å¸¸è¦‹çš„é›™å…ƒéŸ³ï¼Œå°±
ä¸éœ€å¦å»ºç«‹éŸ³ç´ ï¼Œè€Œæ˜¯å°‡å®ƒå€‘æ‹†æˆå…©å€‹å–®
ä¸€å…ƒéŸ³éŸ³ç´ ä¾†è¡¨ç¤ºã€‚é€™æ¨£ä¸åƒ…ä¹‹å¾Œçš„èªéŸ³
å±¬æ€§èƒ½å¤ è¡¨é”å¾—æ›´æ˜ç¢ºï¼Œä¹Ÿç°¡åŒ–äº†æ—¥å¾Œå¤š
é€£éŸ³(multi-phone)è¾¨è­˜ç³»çµ±å¯¦ä½œä¸Šçš„è¤‡é›œ
æ€§ã€‚ 
  äºŒã€è‹±èªçš„éƒ¨åˆ†ï¼Œæˆ‘å€‘ä¸»è¦æ˜¯åŸºæ–¼
DARPA å‡ºç‰ˆçš„ TIMIT èªæ–™åº«ä¸­æ‰€å®šç¾©çš„
61å€‹éŸ³ç´ [2]ï¼Œä¸¦åƒç…§[3, 4]ä¾†å°‹æ‰¾å…¶èˆ‡ IPA
çš„å°æ‡‰é—œä¿‚ã€‚ç”±æ–¼ TIMITä¸­å…·æœ‰ 5å€‹é›™å…ƒ
éŸ³ï¼ˆ/aw/ã€/ay/ã€/ey/ã€/ow/ã€/oy/ï¼‰ï¼Œç‚ºäº†
æ–¹ä¾¿ä½¿ç”¨å…·äººå·¥éŸ³ç´ é‚Šç•Œæ¨™è¨˜çš„ TIMITèª
æ–™åº«ä½œç‚ºè¾¨è­˜ç³»çµ±çš„è¨“ç·´èªæ–™ï¼Œå„˜ç®¡é€™äº”
å€‹é›™å…ƒéŸ³éƒ½æ˜¯ç”±å…©å€‹å–®å…ƒéŸ³æ‰€æ§‹æˆï¼Œæˆ‘å€‘
ä»ä¸å°‡å…¶æ‹†é–‹ï¼Œåè€Œè¦–å…¶ç‚ºå–®ä¸€å…ƒéŸ³éŸ³
ç´ ã€‚ 
  åœ¨æ‰¾å‡ºäº†åœ‹ã€è‹±èªä¸­æ‰€æœ‰éŸ³ç´ çš„
X-SAMPAè¡¨ç¤ºæ³•å¾Œï¼Œæˆ‘å€‘ä¾¿ä»¥[5, 6]ç‚ºä¸»ï¼Œ
[1]ç‚ºè¼”ï¼Œæ‰¾å‡ºé€™äº›éŸ³ç´ çš„ç™¼éŸ³æ–¹å¼èˆ‡éƒ¨ä½ï¼Œ
åŸå‰‡å¦‚ä¸‹ï¼š 
  ä¸€ã€è¼”éŸ³éŸ³ç´ çš„ç™¼éŸ³æ–¹å¼å¯åˆ†ç‚ºå¡éŸ³ã€
å¡æ“¦éŸ³ã€æ“¦éŸ³ã€é¼»éŸ³ã€è¿‘éŸ³ã€é‚Šè¿‘éŸ³å…±å…­
ç¨®ï¼Œç™¼éŸ³éƒ¨ä½å¯åˆ†ç‚ºé›™å”‡ã€å”‡é½’ã€é½’ã€é½’
é½¦ã€é½’é½¦å¾Œã€é½¦é¡ã€æ²èˆŒã€ç¡¬é¡ã€å”‡é¡ã€
è»Ÿé¡ã€è²é–€å…±åä¸€ç¨®ï¼›è€Œå…ƒéŸ³éŸ³ç´ çš„ç™¼éŸ³
å‰‡ä¸»è¦ä¾ç…§èˆŒä½èˆ‡å”‡å½¢ï¼Œå¯åˆ†ç‚ºèˆŒé¢å‰ã€
èˆŒé¢æ¬¡å‰ã€èˆŒé¢å¤®ã€èˆŒé¢æ¬¡å¾Œã€èˆŒé¢å¾Œã€
é–‰ã€åŠé–‰ã€ä¸­ã€åŠé–‹ã€é–‹ã€åœ“å”‡å…±åä¸€ç¨®ã€‚ 
  äºŒã€ç‚ºäº†å€åˆ¥è¨±å¤šå…·æœ‰ç›¸åŒç™¼éŸ³æ–¹å¼
èˆ‡éƒ¨ä½çš„éŸ³ç´ ï¼Œæˆ‘å€‘ä¹Ÿåƒç…§ IPA çš„ç‰¹æ®Šç¬¦
è™Ÿæ¨™è¨˜å¦åŠ ä¸Šæœ‰è²(voice)ã€é€æ°£(aspiration)
èˆ‡å¦çš„å…©é …å±¬æ€§ã€‚ 
  ä¸‰ã€åœ‹èªçš„éƒ¨åˆ†ï¼Œå€¼å¾—ä¸€æçš„æ˜¯å…©ç¨®
èˆŒå°–å…ƒéŸ³(Apicale)çš„è™•ç†ï¼Œä¹Ÿå°±æ˜¯æˆ‘å€‘åœ¨ç™¼
éŸ³ã€ä¹‹ã€ã€ã€åƒã€ã€ã€æ–½ã€ã€ã€æ—¥ã€ã€ã€è³‡ã€ã€
ã€ç–µã€ã€ã€æ–¯ã€æ™‚æ‰€åŠ ä¸Šçš„å…©ç¨®ç©ºéŸ»æ¯ï¼Œ
åœ¨X-SAMPAä¸­çš„è¡¨ç¤ºæ³•åˆ†åˆ¥ç‚º/z=/èˆ‡/z`=/ã€‚
å„˜ç®¡å®ƒå€‘å…·æœ‰è«¸å¦‚æ²èˆŒèˆ‡å¦çš„è¼”éŸ³ç™¼éŸ³å±¬
æ€§ï¼Œæˆ‘å€‘ä»åƒç…§ã€Šç¾ä»£æ¼¢èªã€‹ï¼Œå°‡å®ƒå€‘è¦–
ç‚ºå…ƒéŸ³ï¼Œä¸¦æ‰¾å‡ºæ‰€åœ¨çš„èˆŒä½èˆ‡å”‡å½¢ã€‚ 
  å››ã€è‹±èªçš„éƒ¨åˆ†ï¼Œå°æ–¼ TIMIT ä¸­çš„ 4
å€‹éŸ³ç¯€ä¸»éŸ³(syllabics)ï¼ˆ/em/ã€/en/ã€/eng/ã€
/el/ï¼‰ï¼Œç”±æ–¼ä¸»è¦åƒè€ƒè³‡æ–™[5, 6]ä¸¦æœªç‰¹åˆ¥é‡
å°å®ƒå€‘èªªæ˜ç™¼éŸ³æ–¹å¼èˆ‡éƒ¨ä½ï¼Œå› æ­¤æˆ‘å€‘æ ¹
æ“šè«–æ–‡[7]ä¸­é—œæ–¼TIMITèˆ‡SPEçš„å°æ‡‰è¡¨æ ¼ï¼Œ
å°‡å®ƒå€‘çš„èªéŸ³å±¬æ€§åˆ†åˆ¥è¨­å®šç‚ºèˆ‡/m/ã€/n/ã€
/ng/ã€/l/ç›¸åŒã€‚æ­¤å¤–ï¼Œå°æ–¼ TIMITä¸­çš„ 6å€‹
è²é–€é–‰é–éŸ³(glottal closures) ï¼ˆ/bcl/ã€/dcl/ã€
/gcl/ã€/kcl/ã€/pcl/ã€/tcl/ï¼‰ï¼Œæˆ‘å€‘çš„è™•ç†äº¦
åŒä¸Šï¼Œå³å°‡å…¶ç™¼éŸ³å±¬æ€§åˆ†åˆ¥é—œè¯æ–¼/b/ã€/d/ã€
/g/ã€/k/ã€/p/ã€/t/ã€‚æœ€å¾Œï¼Œåœ¨è¦–ç‚ºå–®ä¸€éŸ³ç´ 
çš„é›™å…ƒéŸ³éƒ¨åˆ†ï¼Œæˆ‘å€‘ä¹Ÿæ˜¯åƒç…§[7]ç‚ºå®ƒå€‘è¨‚
å‡ºèˆŒä½èˆ‡å”‡å½¢ã€‚ 
  å…ƒéŸ³èˆ‡è¼”éŸ³çš„å°æ‡‰è¡¨æ ¼å¦‚è¡¨ä¸€å’Œè¡¨äºŒï¼Œ
å…¶ä¸­æ¯ä¸€æ¬„ä½å‡ä»£è¡¨åŒä¸€å€‹éŸ³ç´ ï¼Œç”±ä¸Šè€Œ
ä¸‹çš„è¡¨ç¤ºæ³•ä¾åºç‚º X-SAMPAã€IPAã€æ³¨éŸ³
ç¬¦è™Ÿã€TIMITï¼Œè€Œã€*ã€å‰‡è¡¨ç¤ºæ²’æœ‰ç›¸å°æ‡‰
çš„è¡¨ç¤ºæ³•ã€‚è¡¨ä¸€ä¸­çš„ã€Vã€ä»£è¡¨æœ‰è²ï¼Œå‰‡ã€Vlã€
ä»£è¡¨ç„¡è²ï¼Œè€ŒX-SAMPAç¬¦è™Ÿä¸­è‹¥æœ‰ã€_hã€ï¼Œ
å‰‡ä»£è¡¨å…·é€æ°£å±¬æ€§ã€‚å„˜ç®¡æˆ‘å€‘ç™¼ç¾ï¼Œè¨±å¤š
éŸ³ç´ ï¼ˆç‰¹åˆ¥æ˜¯å…ƒéŸ³ï¼‰éƒ½å…·æœ‰ç›¸åŒçš„ç™¼éŸ³æ–¹
å¼èˆ‡éƒ¨ä½ï¼Œä¹‹å¾Œå°‡å¾…è¾¨è­˜å™¨å®Œæˆå¾Œï¼Œæ‰èƒ½
æ ¹æ“šè¾¨è­˜ç‡ä¿®æ­£é€™äº›ç™¼éŸ³å±¬æ€§çš„é‡è¦æ€§æˆ–
æ˜¯åŠ ä¸Šå…¶ä»–çš„ç™¼éŸ³å±¬æ€§ã€‚ 
  æœ€å¾Œï¼Œåœ¨é€šç”¨éŸ³ç´ é›†(universal phone 
set)çš„æ•´ä½µæ–¹é¢ï¼Œç”±æ–¼å…·æœ‰äººå·¥éŸ³ç´ åŠéŸ³ç´ 
é‚Šç•Œæ¨™è¨˜çš„èªæ–™éå¸¸æœ‰é™ï¼ˆæˆ‘å€‘ç›®å‰åªèƒ½
å–å¾— TIMITèˆ‡ OGIå¤šèªè¨€å£èªèªæ–™åº«ï¼‰ï¼Œ
é€šç”¨éŸ³ç´ é›†å¿…é ˆä»¥èªæ–™åº«ä¸­å‡ºç¾éçš„éŸ³ç´ 
ç‚ºé™ï¼Œè€Œå…¶ä»–æœªè¢«ä½¿ç”¨æˆ–ç½•ç”¨çš„éŸ³ç´ ï¼Œå‰‡
ä¾æ“šå…¶ç™¼éŸ³æ–¹å¼èˆ‡ç™¼éŸ³éƒ¨ä½ã€æˆ–æ˜¯çµ±è¨ˆæ¨¡
å‹çš„æ©Ÿç‡è·é›¢äºˆä»¥åˆä½µã€‚åˆä½µå¾Œçš„é€šç”¨éŸ³
ç´ é›†å¤§å°ç‚º 69 å€‹ï¼Œå…¶ä¸­åŒ…å«å…ƒéŸ³ 22 å€‹èˆ‡
è¼”éŸ³ 47å€‹ã€‚å°æ‡‰è¡¨æ ¼å¦‚è¡¨ä¸‰å’Œè¡¨å››ï¼Œå…¶ä¸­
åœ‹èªçš„éŸ³ç´ ç³»çµ±åƒè€ƒæ¼¢èªæ‹¼éŸ³ï¼Œå°èªå’Œå®¢
èªæ˜¯é€šç”¨æ‹¼éŸ³ï¼Œè€Œè‹±èªçš„éŸ³ç´ ç³»çµ±å‰‡æ¡ç”¨
TIMITèªæ–™åº«çš„æ¨™éŸ³ç³»çµ±(TIMITbet)ã€‚ 
 
(3) å»ºç½®åŸºæ–¼èªéŸ³å±¬æ€§åµæ¸¬ä¹‹é€šç”¨éŸ³ç´ è¾¨
è­˜ç³»çµ±åŠå–®èªéŸ³ç´ è¾¨è­˜ç³»çµ± 
 
 4 
ä¸‹å¹¾é»ï¼š 
  ä¸€ã€å¦‚ä½•æ›´ç²¾ç°¡åŒ–çš„è¡¨ç¤º Gï¼Ÿæˆ‘å€‘ç™¼
ç¾ï¼Œæ•´å€‹ WFST çš„å¤§å°ä¸»è¦æ±ºå®šåœ¨ Gï¼Œè€Œ
Gçš„è¡¨ç¤ºæ³•æ”¸é—œæ•´å€‹WFSTçš„é‹ä½œæ•ˆèƒ½ã€‚ 
  äºŒã€åœ¨ WFST çš„é‹ç®—æ–¹é¢ï¼Œç”±æ–¼æˆ‘å€‘
ç™¼ç¾å»ºç«‹é›¢ç·šçš„ WFST åœ–å½¢çš„é‹ç®—æ™‚é–“ä¸»
è¦åœ¨æ–¼ Det()èˆ‡ Push()ï¼Œå› æ­¤é€™å…©å€‹å‡½æ•¸çš„
æ¼”ç®—æ³•äº¦å€¼å¾—æ›´æœ‰æ•ˆç‡çš„æ”¹é€²ã€‚ 
  ä¸‰ã€æœªä¾†æˆ‘å€‘æœƒé€²ä¸€æ­¥å°‡æ•´å€‹å¤šé€£éŸ³
(multi-phone)çš„è³‡è¨Šå¯¦ä½œæ–¼WFSTä¸­ã€‚ 
  æ­¤å¤–ï¼Œæˆ‘å€‘ç™¼ç¾æœ€è¿‘å¾®è»Ÿå…¬å¸
(Microsoft)å»ºç½®äº†ä¸€å€‹åç‚º Kaldi [8]ï¼Œä»¥
WFST ç‚ºåŸºç¤çš„èªéŸ³è¾¨è­˜å·¥å…·ç®±ã€‚æ­¤å·¥å…·
ç®±æ•´åˆäº†ç›®å‰åœ¨å‰ç«¯èˆ‡å¾Œç«¯èªéŸ³è™•ç†ä¸Šæœ€
å…ˆé€²çš„æ–¹æ³•ï¼Œæ–¹ä¾¿èªéŸ³è™•ç†ç ”ç©¶è€…ä½¿ç”¨ã€‚
æˆ‘å€‘å·²é–‹å§‹è‘—æ‰‹è©•ä¼°å°‡ç›®å‰çš„å»ºç½®æˆæœèˆ‡
æ–°å·¥å…·ç®±æ•´åˆçš„å¯è¡Œæ€§ã€‚ 
 
(5) å»ºç½®åŸºæ–¼å­ç©ºé–“ç‰¹å¾µè¡¨ç¤ºèˆ‡å­¸ç¿’çš„å£
èªèªè¨€è¾¨è­˜ç³»çµ± 
 
åœ¨çœ¾å¤šè‡ªå‹•èªè¨€è¾¨è­˜çš„æ–¹æ³•ä¸­ï¼Œè¼ƒç›´è¦ºä¸”
å…·æœ‰èªè¨€æ„æ¶µçš„è¦ç®—æ˜¯èªéŸ³çµ„åˆæ³•
(phonotactics)ã€‚èªéŸ³çµ„åˆæ³•æ˜¯èªéŸ³å­¸çš„å…¶ä¸­
ä¸€å€‹åˆ†æ”¯ï¼Œç ”ç©¶åœ¨æŸä¸€ç¨®èªè¨€ä¸­çš„èªéŸ³çµ„
åˆæ³•å‰‡èˆ‡é™åˆ¶(phonotactic constraints)ã€‚èˆ‰
ä¾‹èªªï¼Œåœ¨åœ‹èªä¸­ï¼Œ/j/ã€/q/ã€åŠ/x/ä¹‹å¾Œåªèƒ½
è·Ÿ /i/å’Œ/Ã¼/ï¼Œè€Œä¸èƒ½è·Ÿ /a/ã€/o/æˆ–/e/ç­‰å…ƒéŸ³ï¼›
è€Œåœ¨è‹±èªè£¡ï¼Œè¼”éŸ³å¢é›†æœ€å¤šåªèƒ½ç”±å…©å€‹è¼”
éŸ³çµ„æˆï¼Œé™¤éç¬¬ä¸€å€‹è¼”éŸ³æ˜¯/s/éŸ³ï¼Œå‰‡å…¶å¾Œ
å¯å®¹è¨±ç¬¦åˆç‰¹æ®Šçµ„åˆçš„è¼”éŸ³ç¾¤ã€‚åœ¨ç›¸è¿‘çš„
èªè¨€ï¼Œç›¸åŒçš„èªéŸ³çµ„åˆæ³•å‰‡æœªå¿…ç›¸é€šï¼Œä¾‹
å¦‚ï¼šåœ¨ç¾ä»£è‹±èªä¸­ï¼Œ/kn/åŠ/É¡n/åœ¨å­—è©çš„é–‹
é¦–æ˜¯ä¸å®¹è¨±çš„ï¼Œä½†åœ¨åŒç³»çš„å¾·èªåŠè·èªä¸­
å»æ˜¯å…è¨±çš„ã€‚ 
  ç›®å‰åœ¨èªéŸ³è™•ç†ç¤¾ç¾¤ä¸­ï¼Œæœ€å¸¸è¦‹çš„åŸº
æ–¼èªéŸ³çµ„åˆæ³•çš„è‡ªå‹•èªè¨€è¾¨è­˜æ–¹æ³•æ˜¯åˆ©ç”¨
å‘é‡ç©ºé–“æ¨¡å‹(vector space model, VSM)[9]ï¼Œ
å°‡æ¯å€‹èªå¥çš„èªéŸ³çµ„åˆæƒ…å½¢è¡¨ç¤ºç‚ºæ ¹æ“šæ‰€
æœ‰å–®é€£è©èˆ‡äºŒé€£è©å‡ºç¾çš„é »ç‡æ‰€çµ„æˆçš„å‘
é‡ï¼Œå†åˆ©ç”¨æ”¯æŒå‘é‡æ©Ÿ (support vector 
machine, SVM)é€²è¡Œåˆ†é¡ã€‚ç”±æ–¼ Né€£è©çš„è¡¨
ç¤ºæ–¹æ³•éœ€è¦åˆ©ç”¨å¤§é‡çš„å„²å­˜ç©ºé–“ï¼Œå› æ­¤é€™
ç¨®æ–¹æ³•æœ€å¤§çš„ç¼ºé»åœ¨æ–¼åªèƒ½åœ¨æ¯å€‹èªå¥ä¸­
æ•æ‰æœ€å¤šäºŒé€£è©çš„èªéŸ³çµ„åˆæƒ…å½¢ã€‚ 
  å› æ­¤ï¼Œæˆ‘å€‘æå‡ºäº†åŸºæ–¼å­ç©ºé–“çš„èªå¥
è¡¨ç¤ºæ³•ï¼Œå…¶å„ªé»æœ‰äºŒï¼šä¸€ã€å®ƒå¯ä»¥å……åˆ†ä½¿
ç”¨éŸ³ç´ è¾¨è­˜å™¨æ‰€æä¾›çš„è³‡è¨Šï¼ŒåŒ…æ‹¬æ¯å€‹éŸ³
ç´ åœ¨æ¯å€‹éŸ³ç´ æ®µè½ä¸­çš„äº‹å¾Œæ©Ÿç‡ï¼Œè€Œéå–®
å–®çš„å‡ºç¾æ¬¡æ•¸èˆ‡é »ç‡ï¼›äºŒã€å®ƒåˆ©ç”¨å‘é‡ä¸²
æ¥çš„æ–¹å¼ï¼Œèƒ½æ•æ‰åˆ°æ›´é•·è·é›¢çš„èªéŸ³çµ„åˆ
æƒ…å½¢ä»¥åŠå‰å¾Œæ–‡è³‡è¨Šï¼Œè€Œéœ€è¦çš„ç©ºé–“æ¯”
VSMå°‘äº†è¨±å¤šã€‚ 
  åœ¨èªéŸ³çµ„åˆç‰¹å¾µçš„æŠ½å–èˆ‡ä¸²æ¥ä¸Šï¼Œå¦‚
åœ–äº”ï¼ŒéŸ³ç´ è¾¨è­˜å™¨æœƒå…ˆç”¢ç”Ÿä¸€æ¢æœ€ä½³çš„éŸ³
ç´ ä¸²åˆ—èˆ‡é‚Šç•Œï¼ˆå¦‚ğ‘¢1,ğ‘¢2,ğ‘¢1, â€¦ , ğ‘¢4ï¼‰ï¼Œæ“šæ­¤
é‚Šç•Œï¼Œæˆ‘å€‘å¯ä»¥æ±‚å¾—æ‰€æœ‰éŸ³ç´ åœ¨æ¯å€‹éŸ³ç´ 
æ®µè½(segment)çš„äº‹å¾Œæ©Ÿç‡ï¼Œå½¢æˆå‘é‡ä¸²åˆ—
ğ¬1, â€¦ , ğ¬ğ¾ã€‚å› æ­¤ï¼Œæ¯å€‹å‘é‡å‡å¯è¦–ç‚ºä¸€æ®µã€è²
éŸ³ã€çš„è¡¨ç¤ºæ³•ï¼Œè€Œå…¶ä¸­çš„æ•¸å€¼ä»£è¡¨äº†å„å€‹
éŸ³ç´ ç™¼ç”Ÿçš„å¯èƒ½æ€§ã€‚æ¥è‘—ï¼Œè—‰ç”±å‘é‡çš„å‰
å¾Œä¸²æ¥ï¼Œå½¢æˆäº†è¶…ç´šå‘é‡(supervector)ï¼Œæ­¤
å‘é‡æ¶µè“‹äº†æ­¤æ®µã€è²éŸ³ã€èˆ‡å…¶å‰å¾Œã€è²éŸ³ã€
çš„é—œä¿‚è³‡è¨Šï¼Œé€²è€Œå…·æœ‰æè¿°èªéŸ³çµ„åˆçš„ç‰¹
æ€§ã€‚ä»¥å‰å¾Œå„ä¸²æ¥ä¸€å€‹å‘é‡çš„è¶…ç´šå‘é‡ç‚º
ä¾‹ï¼Œå…¶æ¶µè“‹çš„å‰å¾Œæ–‡è³‡è¨Šç›¸ç•¶æ–¼ä¸‰é€£è©çš„
è¡¨ç¤ºæ³•ï¼Œè€Œæ‰€å æœ‰çš„ç©ºé–“å¤§å°å»åªè¦åŸä¾†
ç©ºé–“å¤§å°çš„ä¸‰å€ï¼Œè€Œ VSMçš„ä¸‰é€£è©å»éœ€è¦
åŸä¾†ç©ºé–“ï¼ˆå–®é€£è©ï¼‰å¤§å°çš„ä¸‰æ¬¡æ–¹ã€‚ 
  ç”±æ–¼æ¯æ®µèªå¥çš„éŸ³ç´ ä¸²åˆ—é•·åº¦ä¸¦ä¸ä¸€
è‡´ï¼Œä¸åŒèªå¥çš„çŸ©é™£ï¼ˆå…¶åˆ—æ•¸èˆ‡è¡Œæ•¸åˆ†åˆ¥
ç‚ºå‰è¿°è¶…ç´šå‘é‡çš„ç¶­åº¦(äº¦å³é€šç”¨éŸ³ç´ å€‹
æ•¸ä¹˜ä¸Šå‰å¾Œä¸²æ¥å‘é‡æ•¸)èˆ‡éŸ³ç´ ä¸²åˆ—é•·åº¦ï¼‰
è¡¨ç¤ºçš„è¡Œæ•¸å°‡ä¸åŒï¼Œéœ€è½‰æ›æˆå›ºå®šå¤§å°çš„
çŸ©é™£ã€‚æˆ‘å€‘ä½¿ç”¨ SVD (singular value 
decomposition)ä¾†æ±‚å¾—ä¸€çµ„å­ç©ºé–“ï¼Œæ­¤å­ç©º
é–“èƒ½å¤ è¿‘ä¼¼çš„ç”Ÿæˆ(span)å‡ºåŸä¾†çš„çŸ©é™£ã€‚å’Œ
è¨±å¤šä½¿ç”¨ SVDä¾†é™ç¶­çš„æ–¹æ³•æ‰€ä¸åŒçš„æ˜¯ï¼Œ
æˆ‘å€‘ä¸¦éå°‹æ‰¾ä¸€å€‹æŠ•å½±æˆ–é™ç¶­å¾Œçš„çŸ©é™£ï¼Œ
è€Œ æ˜¯ ç”¨ ä¸€ çµ„ æŠ• å½± ç©º é–“ æˆ– ç‰¹ å¾µ ç©º é–“
(eigen-space)ä¾†é‡æ–°è¡¨ç¤ºé€™æ®µèªå¥ã€‚å¦‚åœ–å…­
æ‰€ç¤ºï¼ŒåŸæœ¬çš„çŸ©é™£ğ—ï¼Œç¶“ç”± SVD å¾Œï¼Œæ‰€å¾—
å‡ºçš„çŸ©é™£ğ”æ‰æ˜¯é€™å€‹èªå¥çš„æ–°è¡¨ç¤ºæ³•ã€‚æˆ‘å€‘
ç¨±é€™ç¨®è¡¨ç¤ºæ³•ç‚º Grassmann Manifoldï¼Œæ˜¯ä¸€
çµ„æ•£ä½ˆåœ¨â„ğ‘ç©ºé–“çš„å­ç©ºé–“ã€‚é€™ç¨®è¡¨ç¤ºæ³•å…·
æœ‰ä»¥ä¸‹å¥½è™•ï¼šä¸€ã€è‹¥åŸæœ¬çš„éŸ³ç´ ä¸²åˆ—å«æœ‰
å°‘æ•¸ä¸æ­£ç¢ºçš„çµæœï¼Œå°æ–¼æ‰€æ±‚å¾—çš„å­ç©ºé–“
ä¸è‡³æ–¼é€ æˆå¤ªå¤§çš„å½±éŸ¿ï¼›äºŒã€æˆ‘å€‘å¯ä»¥ä½¿
ç”¨è¨±å¤šåœ¨æ•¸å­¸ä¸Šå·²ç¶“ç™¼å±•è¿‘ç™¾å¹´çš„å­ç©ºé–“
æ€§è³ªä¾†é€²è¡Œæœ‰æ„ç¾©çš„å­ç©ºé–“ä¹‹é–“çš„ç›¸ä¼¼åº¦
ä¼°é‡ã€‚ 
  æˆ‘å€‘åƒè€ƒè«–æ–‡[10]çš„åšæ³•ï¼Œä½¿ç”¨èˆ‡å­ç©º
 6 
ä¸€ ç¨® æ–¹ æ³• ï¼Œ ç¨± ä¹‹ ç‚º ç›¸ ç•° æ€§ å­¸ ç¿’ æ³•
(dissimilarity-based learning)ã€‚èˆ‡[11]ä¸­çš„æ ¸
æ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œé€™ç¨®å­¸ç¿’æ³•ä¸éœ€è¦å°‡æ¯å€‹
èªå¥ğ’æŠ•å½±è‡³é«˜ç¶­çš„æ ¸ç©ºé–“ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨
å­ç©ºé–“ä¹‹é–“çš„è·é›¢å‡½æ•¸ä¾†é€²ä¸€æ­¥å°‡æ¯å€‹èª
å¥è¡¨ç¤ºç‚ºå®ƒèˆ‡æ‰€æœ‰è¨“ç·´èªæ–™ä¸­çš„èªå¥çš„è·
é›¢å‘é‡ï¼Œæœ€å¾Œå†åˆ©ç”¨å‚³çµ±çš„ç·šæ€§é‘‘åˆ¥åˆ†æ
(linear discriminant analysis, LDA)ä¾†é€²è¡Œåˆ†
é¡ã€‚ 
åŒæ¨£åœ°ï¼Œæˆ‘å€‘ä»¥ OGIå¤šèªè¨€å£èªèªæ–™
åº«åšç‚ºè‡ªå‹•èªè¨€è¾¨è­˜çš„å¯¦é©—èªæ–™ã€‚å¯¦é©—çµ
æœå¦‚åœ–åï¼Œæˆ‘å€‘çš„æ–¹æ³•ç„¡è«–åœ¨åŒ…å« 1è‡³ 50
ç§’ä¸åŒé•·åº¦èªå¥çš„å®Œæ•´æ¸¬è©¦é›†æˆ–æ˜¯åªåŒ…å«
ç´„ 3 ç§’é•·åº¦çŸ­å¥çš„å­æ¸¬è©¦é›†ä¸Šå‡å„ªæ–¼ç¾æœ‰
çš„ VSMæ–¹æ³•ã€‚é€™éƒ¨åˆ†çš„ç ”ç©¶æˆæœèˆ‡å¯¦é©—è¨
è«–å·²ç¶“ç™¼è¡¨åœ¨åœ‹éš›å­¸è¡“æœƒè­° ICASSP 2013 
[12]ã€‚ 
 
å››ã€è¨ˆç•«æˆæœè‡ªè©• 
 
æœ¬è¨ˆç•«åœ¨åœ‹å°å®¢è‹±å››ç¨®èªè¨€çš„æ¸¬è©¦èªæ–™æ”¶
é›†æ¨™è¨˜ã€èªéŸ³å±¬æ€§ç©ºé–“å»ºç«‹ã€é€šç”¨éŸ³ç´ ç™¼
éŸ³è©å…¸å»ºç«‹ç­‰åŸºç¤å·¥ä½œå·²ç¶“å¤§è‡´å®Œæˆï¼Œå°‡
æœ‰åˆ©æ–¼å¯¦é©—å®¤ç¹¼çºŒç™¼å±•å¤šèªè¨€èªéŸ³è¾¨è­˜æŠ€
è¡“ï¼Œæœªä¾†ä¸¦å°‡é€²ä¸€æ­¥æ“´å±•åˆ°å¤šèªè¨€èªéŸ³åˆ
æˆæŠ€è¡“ã€‚åœ¨åŸºæ–¼åŠ æ¬Šå¼æœ‰é™ï§ºæ…‹è½‰æ›æ©Ÿçš„
å¤§è©å½™é€£çºŒèªéŸ³è¾¨è­˜ç³»çµ±çš„å»ºç½®æ–¹é¢ä¹Ÿå·²
ç¶“ç²å¾—ç›¸ç•¶ä¸éŒ¯çš„åˆæ­¥æˆæœï¼Œé›–ç„¶é€™å€‹ç³»
çµ±çš„è¤‡é›œåº¦å¾ˆé«˜ï¼Œä»æœ‰ä¸€äº›å•é¡Œå°šå¾…å…‹æœã€‚
åœ¨å£èªèªè¨€è¾¨è­˜æ–¹é¢çš„ç ”ç©¶æˆæœå·²ç¶“ç™¼è¡¨
åœ¨ InterSpeech2012 [11]èˆ‡ ICASSP2013 [12]
åœ‹éš›æœƒè­°ï¼Œç›®å‰æ­£åœ¨è‘—æ‰‹æ•´ç†èˆ‡æ’°å¯«æœŸåˆŠ
è«–æ–‡ï¼Œè¿‘æœŸå…§å°‡æŠ•ç¨¿è‡³ IEEE Transactions 
on Audio, Speech and Language 
Processingã€‚ 
 
äº”ã€åƒè€ƒæ–‡ç» 
 
[1]  N. Chomsky, and M. Halle, The Sound 
Pattern of English, MIT Press, 
Cambridge, MA, U.S.A, 1968. 
[2]  W. M. Fisher et al., "The DARPA speech 
recognition research database: 
specifications and status," in Proceedings 
of DARPA Workshop on Speech 
Recognition, pp. 93â€“99, 1986. 
[3]  J. L. Hieronymus, "ASCII phonetic 
symbols for the world's languages: 
Worldbet," 1994. 
[4]  T. Lander, "The CSLU labeling guide," 
1997. 
[5]  Handbook of the International Phonetic 
Association: A Guide to the Use of the 
International Phonetic Alphabet, 1999. 
[6]  P. Ladefoged and I. Maddieson, The 
Sounds of the World's Languages, 1996. 
[7]  S. King and P. Taylor, "Detection of 
phonological features in continuous 
speech using neural networks," Computer 
Speech and Language, vol. 14, pp. 
333-353, 2000. 
[8]  D. Povey, M. Hannemann et. al, 
â€œGenerating exact lattices in the WFST 
framework,â€ in Proc. ICASSP2012. 
[9]  H. Li et al., "A vector space modeling 
approach to spoken language 
identification," IEEE Trans. Audio, 
Speech, Lang. Process, vol. 15, no. 1, pp. 
271-284, 2007. 
[10] J. Hamm, and D. D. Lee, "Grassmann 
discriminant analysis: a unifying view on 
subspace-based learning," in Proc. 
ICML2008. 
[11] Y. C. Shih, H. S. Lee, H. M. Wang, and 
S. K. Jeng, "Subspace-based feature 
representation and learning for language 
recognition," in Proc. Interspeech2012. 
[12] H. S. Lee, Y. C. Shih, H. M. Wang, and 
S. K. Jeng, "Subspace-based phonotactic 
language recognition using multivariate 
dynamic linear models," in Proc. 
ICASSP2013. 
[13] K. D. Cock and B. D. Moor, "Subspace 
angles between ARMA models," Systems 
Control Lett., vol. 46, no. 4, 2002. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 8 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
è¡¨ä¸‰ã€åœ‹ã€å°ã€å®¢ã€è‹±èªè¼”éŸ³çš„éŸ³ç´ ç³»çµ±å°æ‡‰è¡¨ 
è¡¨å››ã€åœ‹ã€å°ã€å®¢ã€è‹±èªå…ƒéŸ³çš„éŸ³ç´ ç³»çµ±å°æ‡‰è¡¨ 
 10 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
åœ–äº”ã€èªéŸ³çµ„åˆç‰¹å¾µçš„æŠ½å–èˆ‡ä¸²æ¥ 
åœ–å…­ã€SVDæ‰€ç”Ÿæˆçš„å­ç©ºé–“ 
 12 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
åœ–ä¹ã€èªéŸ³çµ„åˆç‰¹å¾µçš„æŠ½å–èˆ‡éŸ³ç´ ç›¸ä¼¼åº¦å‘é‡çš„å½¢æˆ 
åœ–åã€å„ç¨®èªéŸ³çµ„åˆæ³•çš„å£èªèªè¨€è¾¨è­˜ DETçµæœï¼Œï¼ˆå·¦ï¼‰3ç§’èªæ–™ï¼›ï¼ˆå³ï¼‰
1-50ç§’èªæ–™ 
 14 
The rest of the paper is organized as follows. In Section 2, 
we introduce the new representation of a spoken utterance in the 
sense of subspaces. In Section 3, we present the learning mecha-
nism for subspaces with Grassmann kernels. Section 4 gives the 
evaluation results and some discussions. Finally, conclusions and 
future work are outlined in Section 5.  
2. Data representations and subspaces 
2.1. Phonotactic feature extraction 
Given the observed sequence of acoustic vectors         
derived from a spoken utterance, phonetic decoders single out 
the best phone sequence         based on Viterbi approxima-
tion, which ï¬nds the most likely time alignment path through a 
huge probabilistic network. The main task of phonotactic-based 
language classifiers is to take advantage of the phone sequence 
        as a basic unit for language identification or verifica-
tion. The basic idea behind our proposed phonotactic data 
representation is to take the single-best phone sequence produced 
by phonetic decoders as a kind of clusters toward the acoustic 
feature vectors in a phonotactic fashion. From the example in 
Figure 1, we can see that after phone decoding and time align-
ment, feature vectors {        } belong to top-1phone    while 
feature vectors {           } belong to top-1 phone   . Along 
this vein, given phone boundaries, each set of acoustic vectors in 
its corresponding phone segment is further used to derive a more 
meaningful phonotactic feature vector built up with the average 
posterior probability for each phone, which is characterized by a 
hidden Markov model (HMM), through the Viterbi search 
algorithm. Consequently, each phone segment (or phone frame) 
   is expressed by a vector, whose dimensionality is the size of 
the universal phone set {  }. Figure 1 also shows that the phone 
frame   , which is labeled as   , indeed has the highest posterior 
probability of 0.6 with respect to    due to the nature of dynamic 
programming contributed by the first three feature vectors 
{        }. As for   ,   , and    in this phone frame, although 
the posterior probabilities might be much smaller than the single-
best   , and even    never appears in the phone sequence, they 
are not cast off but included into the new phonotactic feature 
vector    to bring more uncertainty or information that might be 
helpful for classification. 
2.2. Frame concatenation 
In order to capture the contextual information as well as to learn 
patterns of phonotactic constraints, like the role that n-gram 
terms play in the VSM scheme, we concatenate the phone frames 
belonging to a given contextual window centered on a current 
phone frame   . Let        denotes the width of the 
contextual window, meaning that the larger the value n, the more 
phonotactic patterns can be modeled. In Figure 1, if we set n to 1, 
the 3 consecutive phone frames,   ,   , and   , are spliced 
together forming a context-dependent supervector. Moreover, it 
is worth noting that the size of the phonotactic representation no 
longer increases exponentially with respect to n; on the contrary, 
the size has a linear growth that guarantees to avoid memory 
deficiency and unnecessary reduction in practice. 
2.3. Subspace generation 
After frame concatenation, each utterance can be expressed by a 
matrix   consisting of       rows and   columns, where   
and   denote the size of the phone set and the number of phone 
frames in an utterance, respectively. Since   varies in utterances, 
we employ singular value decomposition (SVD) to map the 
varying-size phonotactic matrix to a fixed-size pattern as depict-
ed in Figure 2 [10]. The SVD of   can be written as a reduced 
form,       , where   is a     diagonal matrix containing 
the largest d singular values        ,   and   are matrices, 
whose orthonormal columns {  }  and {  } approximately span 
the column and row spaces of  , respectively. Our goal here is 
not to find a lower-dimensional projection of  , but to find a 
linear subspace that can approximately span the whole column 
spaces of  . In other words, what we need is the columns {  } of 
 , called eigenframes, that can characterize each phone frame in 
 . Therefore, each utterance j is represented as             , 
a collection of orthonormal bases belonging to a fixed-
dimensional linear subspace of a Euclidean space. The main 
difference between our representation and other subspace-based 
methods for an utterance lies in that, we represent an utterance as 
Figure 1: Phonotactic feature extraction and frame concatenation 
for a four-phone {ğ‘¢  ğ‘¢  ğ‘¢  ğ‘¢ } ASLR system. 
Figure 2: Subspace generation by SVD. 
used in the experiments. Each phone model models a phone in 
the universal phone inventory of size 69, which is a union of the 
phones appearing in the 619 transcription files, with the phones 
sharing the same manner and place merged into one. Each phone 
model is a 3-state left-to-right CD-HMM with 32 Gaussian 
mixture components per state. The acoustic feature vector has 39 
attributes comprised of 13 MFCCs including C0, along with their 
first and second order derivatives. All phone models were trained 
and refined from the 619 phone-transcribed utterances and the 
4650 training utterances according to the maximum likelihood 
criterion, respectively. 
We compared the proposed method with two well-known 
phonotactic language recognition methods, namely UPR-LM and 
UPR-VSM [7]. UPR-LM is an extension of PRLM by using a 
universal phone recognizer instead of a phone recognizer of only 
one specific language. We applied the bigram back-off language 
model with Good-Turing smoothing in the implementation. For 
UPR-VSM, a phone sequence is represented by a          
dimensional vector consisting of the TF-IDF of unigram and 
bigram phonotactic patterns. Latent semantic indexing (LSI) was 
further used for extracting key features needed for discriminating 
spoken utterances from the statistics of some salient units and 
their co-occurrence. In the back-end classification, UPR-VSM 
used the SVM system with an RBF kernel trained for each target 
language individually. The decision for each utterance was made 
based on the posterior probability given by applying the SVM 
output to a sigmoid function. Since the proposed method is also 
based on a universal phone recognizer front-end, it is denoted as 
UPR-Subspace.  
From Figure 4, we can see that UPR-subspace outperforms 
UPR-LM and UPR-VSM. UPR-subspace achieves relative 
reductions of 38.90% and 27.13% over UPR-VSM in equal error 
rate (ERR) on the 3-s and 1-to-50-s data sets, respectively. 
Figure 5 (a) shows EER with respect to the number of eigen-
frames (d). The results demonstrate that, with a moderate value 
of d, the EER is dramatically degraded. From Figure 5 (b), we 
observe that larger widths           of the contextual 
window not necessarily guarantee lower EERs. We can also see 
that the ERR is minimal when     (   ). The results seem 
to somewhat explain the reason why the maximum size of 
phonotactic constraints are set to 3 (trigram) in some phonotactic 
learning simulation systems [15] from another viewpoint. 
5. Conclusion 
This paper presents a new phonotactic feature representation 
based on subspace formulation for automatic spoken language 
recognition. On the basis of the representation, the combination 
of KLDA and the simple nearest neighbor classifier has been 
shown to perform excellently. It is expected that the integration 
of a more discriminative kernel-based classier, such as SVM, 
with a subspace-based kernel could perform even better. In our 
future work, we plan to evaluate the proposed framework on the 
more standard NIST LRE corpora, and other subspace-based 
methods will be implemented and compared in experiments.  
6. References 
[1] E. Ambikairajah et al., â€œLanguage identification: a tutorial,â€ IEEE Circuits 
and Systems Magazine, vol. 11, no. 2, pp. 82-108, 2011. 
[2] P. A. Torres-Carassquilo et al., â€œApproaches to language identification using 
Gaussian mixture models and shifted delta cepstral features,â€ in Proc. ICSLP, 
2002. 
[3] A. S. House and E. P. Neuburg, â€œToward automatic identification of the 
language of an utterance. I. Preliminary methodological considerations,â€ J. 
Acoust. Soc. Amer., vol. 62, no. 3, pp. 708-713, 1977. 
[4] Y. K. Muthusamy et al. â€œA comparison of approaches to automatic language 
identification using telephone speech,â€ in Proc. Eurospeech, 1993. 
[5] T. J. Hazen and V. W. Zue, â€œSegment-based automatic language identifica-
tion,â€ J. Acoust. Soc. Amer., vol. 101, no. 4, pp. 2323-2331, 1997. 
[6] M. A. Zissman and E. Singer, â€œAutomatic language identification of telephone 
speech messages using phoneme recognition and N-gram modeling,â€ in Proc. 
ICASSP, 1994. 
[7] H. Li et al., â€œA vector space modeling approach to spoken language identifica-
tion,â€ IEEE Trans. Audio, Speech, Lang. Process, vol. 15, no. 1, 2007. 
[8] M. Penagarikano et al., â€œImproved modeling of cross-decoder phone co-
occurrences in SVM-based phonotactic language recognition,â€ IEEE Trans. 
Audio, Speech, Lang. Process., vol. 19, no. 8, pp. 2348-2363, 2011. 
[9] J. L. Gauvain et al., â€œLanguage recognition using phone lattices,â€ in Proc. 
ICSLP, 2004. 
[10] J. Hamm, and D. D. Lee, â€œGrassmann discriminant analysis: a unifying view 
on subspace-based learning,â€ in Proc. ICML, 2008. 
[11] A. BjÃ¶rck and G. H. Golub, â€œNumerical methods for computing angles 
between linear subspaces,â€ Math. Computation, vol. 27, no. 123, 1973.  
[12] J. Hamm. Subspace-Based Learning with Grassmann Kernels, PhD Thesis, 
2008. 
[13] B. W. Silverman, Density Estimation for Statistics and Data Analysis, 
Chapman and Hall, New York, 1986. 
[14] A. Y. K. Muthusamy et al., â€œThe OGI multi-language telephone speech 
corpus,â€ in Proc. ICSLP, 1992. 
[15] B. Hayes and C. Wilson, â€œA maximum entropy model of phonotactics and 
phonotactic learning,â€ Linguistic Inquiry, vol. 39, no. 3, pp. 379-440, 2008. 
[16] M. Soufifar et al., â€œiVector approach to phonotactic language recognition,â€ 
Proc. Interspeech, pp. 2913-2916, 2011. 
[17] T. Mikolov et al., â€œPCA-based feature extraction for phonotactic language 
recognition,â€ Proc. Odyssey, pp. 251-255, 2010. 
Figure 4: DET plots for several phonotactic approaches on (a) the 3-s data 
set and (b) the 1-to-50-s data set.  
Figure 5: EER with respect to (a) d, the number of eigenframes, 
and (b) n, the width of the contextual window  ğ‘¤   ğ‘›   . 
superior performance in the speaker recognition field [12], some 
researchers have used probabilistic PCA (PPCA) to transform each 
high-dimensional vector filled with discrete features into a small-
size set of latent variables corresponding to a high variability 
subspace [13-15, 30]. For example, in [13] and [14], Soufifar et al. 
used the subspace multinomial model, along with the maximum 
likelihood criterion, to effectively represent the information 
contained in the n-grams.  
        In this paper, as an extension of our previous subspace-based 
work in [16], we propose a new approach for data representation, 
in which the phonetic information as well as the contextual 
relationship can be more abundantly retrieved by likelihood 
computation and dynamic linear models, given a universal phone 
recognizer. In the VSM framework, the count or frequency of a 
phonotactic term is the only attribute that is concerned. In contrast, 
our approach enables us to look much farther and deeper through 
the decoderâ€™s eyes without much more memory. That is, not only 
can more information, such as likelihood scores of any phone 
segments, be captured, but also all possible phones can be taken 
into account instead of the single most likely one. The spirit is 
somewhat similar to the employment of phone lattices [17] or 
posteriogram-based n-gram counts [15]. Moreover, our 
representation also fits for the back-end classification. Under the 
assumption that the utterance representation can be approximately 
described by a collection of lower dimensional linear subspaces, a 
suitable dissimilarity-based learning algorithm along with the well-
surveyed Projection metric are introduced for classification. 
        The remaining of this paper is organized as follows. In 
Section 2, we introduce the new representation of a speech 
utterance in the sense of dynamic linear models. In Section 3, we 
present the learning and scoring mechanisms for subspaces with 
the Projection metric. Section 4 gives the evaluation results and 
some discussions. Finally, conclusions and future work are 
outlined in Section 5. 
 
2. SUBSPACE-BASED REPRESENTATION 
 
According to the definition in [19, p. 3, 20], subspace-based 
learning is based on the extraction of the most conspicuous 
properties of each class separately, as represented or spanned by 
vector series expansions constructed from the feature vectors of 
each class. The learning mechanism focuses on how to measure the 
similarity between each subspace and a given data point. However, 
most widely-alleged subspace-based methods for the SLR tasks, 
such as i-vectors and PCA, do not fall into this definition. On the 
contrary, their goal is to derive a coordinate representation (or a 
vector-like point) for an utterance in a lower-dimensional linear 
space where all projected utterances are supposed to share the 
same set of bases. In contrast, we will introduce a new approach to 
represent each utterance as a subspace of the original feature space, 
where the salient structure of each utterance can be preserved. 
 
2.1. Phone-likelihood vectors 
 
Given the observed sequence of acoustic feature vectors         
derived from a speech utterance, a phone decoder singles out the 
best phone sequence         based on Viterbi decoding which 
ï¬nds the most likely time alignment path through a huge 
probabilistic network. The main task of phonotactic-based 
language classifiers is to take advantage of the phone sequence 
        as a basic unit for SLR. The basic idea behind our 
proposed phonotactic data representation is to take the single-best 
phone sequence given by the phone decoder as a kind of clusters 
toward the acoustic feature vectors in a phonotactic fashion. From 
the example in Figure 1, we can see that after phone decoding and 
time alignment, feature vectors {        } belong to top-1 phone 
   while feature vectors {           } belong to top-1 phone   . 
Along this vein, given phone boundaries, each set of acoustic 
vectors in its corresponding phone segment is further used to 
derive a more meaningful phonotactic feature vector built up with 
the average log-likelihood score for each phone, which is 
characterized by a hidden Markov model (HMM), through the 
Viterbi search algorithm. Consequently, each phone segment (or 
phone frame)    is expressed by a phone-likelihood vector   , 
whose dimensionality is the size of the universal phone set {  }. 
Figure 1 also shows that the first phone   , which is most likely 
labeled as   , indeed has the highest log-likelihood score of -14 
with respect to the first attribute    due to the nature of dynamic 
programming contributed by the first three acoustic feature vectors 
{        }. As for other attributes   ,   , and    in   , although 
their scores might be much smaller than the single-best   , and 
even    might never appear in the single best phone sequence, they 
are not cast off but included into the phone-likelihood vector    to 
bring more uncertainty or information that might be helpful for 
classification. 
 
2.2. Dynamic linear models 
 
After the aforementioned procedure, each utterance is expressed by 
a sequence of phone-likelihood vectors, which is more than just a 
set of vectors due to the temporal information, especially 
phonotactic constraints, contained in the sequence. To capture the 
temporal dynamics, we make a conjecture that each sequence was 
generated by a causal linear time-invariant (LTI) system, which 
might be a sub-system pertaining to some language production 
system. This conjecture is similar to the acoustic theory of speech 
production that assumes the speech production process to be a 
linear system, consisting of a source and ï¬lter [29]. A multivariate 
dynamic linear model (DLM), also known as a state space model, 
is one of causal LTI systems, which has been used to model 
moving human bodies or textures in computer vision [21] and 
signal analysis [22]. A simpler DLM to model the phone-
likelihood sequence is described as follows. 
Figure 1. Phone-likelihood vector extraction for an SLR system 
with four universal phones {           }. 
6871
 where  (     )  (     )  
  (     ), and    denotes the 
within-class covariance matrix derived in the projective space. 
 
4. EXPERIMENTS 
 
We conducted the language verification task on the Oregon 
Graduate Institute Multi-language Telephone Speech (OGI-TS) 
Corpus [27], which contains the speech of 10 languages: English, 
Farsi, French, German, Japanese, Korean, Mandarin, Spanish, 
Tamil, and Vietnamese. The corpus is divided into three parts: 
4650 utterances for training, 1899 utterances for development, and 
1848 utterances for test. Some test utterances with lengths ranging 
from 2 to 4 seconds are culled to form the 3-s set to evaluate the 
system performance for short utterances, while all test utterances 
form the 1-to-50-s set. Besides, the corpus also includes 619 
â€œstory-before-toneâ€ utterances, which have manually generated 
fine-phonetic transcriptions that can be used for supervised phone 
modeling for six languages.  
        A universal phone recognizer (UPR), which is composed of a 
set of language-independent context-independent phone models, is 
used in the experiments. Each phone model models a phone in the 
universal phone inventory of size 69, which is a union of the 
phones appearing in the 619 transcription files, with the phones 
sharing the same manner and place merged into one. Each phone 
model is a 3-state left-to-right CD-HMM with 32 Gaussian mixture 
components per state. The acoustic feature vector has 39 attributes 
comprised of 13 MFCCs including C0, along with their first and 
second order derivatives. According to the procedure shown in [8], 
all phone models were trained and refined from the 619 phone-
transcribed utterances and the 4650 training utterances according 
to the maximum likelihood criterion, respectively. 
        Given the same UPR, we compared the proposed method with 
two well-known VSM-based methods, namely UPR-VSM [8] and 
UPR-IVCT [13], and one subspace-based method, UPR-CONCAT 
[16]. For UPR-VSM, a phone sequence is represented by a 
(          )  dimensional vector consisting of the TF-IDF 
values of unigram, bigram, and trigram phonotactic patterns. 
Latent semantic indexing (LSI) was further used for extracting 
2000-dimensional key features needed for discriminating 
utterances from the statistics of some salient units and their co-
occurrences. However, in UPR-IVCT, only unigram and bigram 
phonotactic patterns were used in the multinomial subspace model 
to train the 700-dimensional i-vector of each utterance [28]. 
Instead of the DLM mentioned in Section 2.2, UPR-CONCAT 
models the phonotactic information within an utterance by simply 
concatenating the phone-likelihood vectors belonging to a fixed-
length sliding window centered on a current vector, whose size was 
set to be 3 in our experiment. All of the above parameter settings 
were determined by the experiments on the developing data set. 
        To make fair comparisons, in the back-end classification, we 
used LDA and its corresponding scoring technique mentioned in 
the end of Section 3 in all of the four methods. From Figure 2 and 
Table 1, we can see that when    , UPR-DLM outperforms two 
VSM-based methods, UPR-VSM and UPR-IVCT, in terms of the 
equal error rate (EER) nearly on both data sets, and achieves 
comparable performance to UPR-CONCAT on the 1-to-50-s data 
set. Some possible reasons why UPR-DLM performs worse than 
UPR-CONCAT lie in that, 1) the language production process we 
attempt to model may have some nonlinearity effects that linear 
systems cannot fully describe; 2) the compact solutions of model 
parameters shown in (3) and (6) may not be close enough to the 
true solutions although the computation is efficient; 3) in UPR-
CONCAT, SVD acts as a robust subspace generator that allows for 
high discrimination against noise contamination even when the 
phone decoder is not reliable, but UPR-DLM lacks this kind of 
operations. 
        Figure 3 shows the EER with respect to the state order ( ). 
We can see that the minimal ERR is achieved when    , which 
means that the phonotactic information contained in 4 consecutive 
phone-likelihood vectors are considered. Compared with the 
results of UPR-CONCAT, it seems to imply that the maximum size 
of phonotactic constraints can be set to 3 (trigram) or 4 (4-gram) in 
most of the phonotactic SLR tasks. 
 
5. CONCLUSIONS 
 
This paper presents a new phonotactic feature representation based 
on dynamic linear models and subspace formulation for automatic 
spoken language recognition. On the basis of the representation, 
the combination of the dissimilarity-based learning algorithm and 
LDA has been shown to perform well. In our future work, we plan 
to remedy the flaws found in the proposed framework and evaluate 
it on the NIST LRE corpora. Other nonlinear subspace-based 
methods will also be investigated, implemented, and compared in 
the experiments. 
Methods 1-to-50-s data set 3-s data set 
UPR-VSM 15.12 19.81 
UPR-IVCT 18.62 23.48 
UPR-CONCAT 10.68 16.78 
UPR-DLM 12.09 20.58 
Figure 3. EER with respect to d, the state order, in 
UPR-DLM on the 3-s and 1-to-50-s OGI-TS data 
sets. 
Figure 2. DET plots for two VSM-based and two subspace-based 
approaches on (a) the 3-s and (b) the 1-to-50-s OGI-TS data sets.  
Table 1. EER (%) for various phonotactic approaches on the 3-s 
and 1-to-50-s OGI-TS data sets. 
6873
åœ‹ç§‘æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•«é …ä¸‹å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
                                                        æ—¥æœŸï¼š101 å¹´ 9 æœˆ 17 æ—¥ 
è¨ˆç•«ç·¨è™Ÿ NSC99-2221-E-001-009-MY3 
è¨ˆç•«åç¨± 
æ‡‰ç”¨ç™¼éŸ³çŸ¥è­˜æºæ–¼å¤šæ¨£åŒ–èªè¨€ä¹‹èªéŸ³è™•ç†-æ‡‰ç”¨ç™¼éŸ³çŸ¥è­˜æºæ–¼
å¤šèªè¨€è‡ªå‹•èªéŸ³è¾¨è­˜ä¹‹ç ”ç©¶ 
å‡ºåœ‹äººå“¡
å§“å 
ç‹æ–°æ°‘ æœå‹™æ©Ÿæ§‹
åŠè·ç¨± 
ä¸­ç ”é™¢è³‡è¨Šæ‰€ 
ç ”ç©¶å“¡ 
æœƒè­°æ™‚é–“ 
101å¹´ 9æœˆ 9æ—¥è‡³ 
101å¹´ 9æœˆ 13æ—¥ æœƒè­°åœ°é» Portland, Oregon, USA 
æœƒè­°åç¨± 
 (ä¸­æ–‡)  
 (è‹±æ–‡) The 13th Annual Conference of the International Speech 
Communication Association (Interspeech2012) 
ç™¼è¡¨è«–æ–‡
é¡Œç›® 
1. Word Relevance Modeling for Speech Recognition 
2. A Study of Mutual Information for GMM-Based Spectral Conversion 
3. Subspace-Based Feature Representation and Learning for Language 
Recognition 
 
ä¸€. åƒåŠ æœƒè­°ç¶“é 
 
æ­¤è¡Œæ–¼ 9æœˆ 9æ—¥æ—©æ™¨æ­ä¹˜é”ç¾èˆªç©ºç­æ©Ÿï¼Œç¶“ç”±æ±äº¬è½‰æ©Ÿï¼Œæ–¼ç•¶åœ°æ™‚é–“ 9æ—¥æ—©ä¸ŠæŠµé”
æ³¢ç‰¹è˜­æ©Ÿå ´ï¼Œå†æ­ä¹˜è¼•è»Œé›»è»Šæ©Ÿå ´ç·šå‰å¾€ä¸‹æ¦»é£¯åº—ï¼Œè¡Œæå®‰ç½®å¦¥ç•¶ä¹‹å¾Œï¼Œä¾¿å‰å¾€æœƒå ´å ±
åˆ°ï¼Œå–å›æœƒè­°è³‡æ–™ã€‚åŒè¡Œçš„æœ‰æœ¬é™¢è³‡å‰µä¸­å¿ƒçš„æ›¹æ˜±åšå£«ã€æˆ‘çš„ä¸‰ä½å­¸ç”ŸåŠç ”ç©¶åŠ©ç†ã€åŠ
å¸«å¤§é™³æŸç³æ•™æˆçš„ä¸‰ä½å­¸ç”Ÿã€‚ 
Interspeechæ˜¯ç”±International Speech Communication Association(ISCA)ä¸»è¾¦ä¹‹åœ‹éš›ç ”
è¨æœƒï¼Œç‚ºå£èªè™•ç†ç›¸é—œç ”ç©¶é ˜åŸŸé™¤IEEEåœ‹éš›è²å­¸èªéŸ³ä¿¡è™Ÿè™•ç†ç ”è¨æœƒ(ICASSP)å¤–ï¼Œè¦æ¨¡
æœ€å¤§ã€æœ€é‡è¦ã€æœ€å°ˆæ¥­çš„ç ”è¨æœƒã€‚Interspeech2012å…±æ”¶åˆ°1åƒ3ç™¾å¤šç¯‡è«–æ–‡æŠ•ç¨¿ï¼Œç¶“éåš´
æ ¼çš„å¯©æŸ¥ï¼ŒéŒ„å–6ç™¾å¤šç¯‡è«–æ–‡ï¼Œè«–æ–‡æ¥å—ç‡ç´„50%ã€‚é€™äº›è«–æ–‡è¢«å®‰æ’åœ¨39å€‹å£é ­å ±å‘Šè­°ç¨‹ã€
37å€‹å£å ±å ±å‘Šè­°ç¨‹åŠ8å€‹ç‰¹åˆ¥è­°ç¨‹ç™¼è¡¨ï¼ŒåŒä¸€æ™‚é–“æœ‰6å€‹å£é ­å ±å‘Šè­°ç¨‹å’Œ8å€‹å£å ±å ±å‘Šè­°
ç¨‹å¹³è¡Œé€²è¡Œï¼Œå¦å¤–ï¼Œæ­¤æ¬¡æœƒè­°å®‰æ’4å€‹å°ˆé¡Œæ¼”è¬›ã€8å€‹tutorialsã€5å€‹workshopsã€‚ 
æ­¤æ¬¡æœƒè­°çš„4å€‹å°ˆé¡Œæ¼”è¬›éƒ½å¾ˆæœ‰è¶£ï¼Œç¬¬ä¸€ä½è¬›å“¡æ˜¯ç¾åœ‹Georgia Institute of Technologyçš„æ
éŒ¦è¼æ•™æˆï¼Œä¹Ÿæ˜¯ä»Šå¹´çš„ISCA Medalistå¾—ä¸»ï¼Œæ¼”è¬›å…§å®¹æ˜¯æœ‰é—œéŸ³è¨Šå’Œè²å­¸ä¿¡è™Ÿè™•ç†çš„æœªä¾†
ç™¼å±•ï¼›ç¬¬äºŒä½è¬›å“¡æ˜¯ç¾åœ‹Carnegie Mellon Universityçš„Roger B. Dannenbergæ•™æˆï¼Œæ¼”è¬›å…§
å®¹æ˜¯è‡ªå‹•èªéŸ³è¾¨è­˜çš„æ–°è§€é»ï¼Œå¾èªéŸ³ä¿¡è™Ÿè™•ç†åˆ°èªéŸ³è³‡è¨Šæ“·å–ï¼Œç¬¬ä¸‰ä½è¬›å“¡æ˜¯Googleçš„
Michael Rileyåšå£«ï¼Œæ¼”è¬›å…§å®¹æ˜¯Weighted Transduser and Push Transduserï¼Œç¬¬å››ä½è¬›å“¡æ˜¯
ç¾åœ‹Oregon Health & Science Universityçš„Garet Lahvisæ•™æˆï¼Œæ¼”è¬›å…§å®¹æ˜¯æœ‰é—œè¨ˆç®—ç¥ç¶“å­¸
èˆ‡è…¦æ³¢è§£ç¢¼çš„ç™¼å±•èˆ‡æ‡‰ç”¨ã€‚ 
æˆ‘å€‘çš„ç¬¬ä¸€ç¯‡è«–æ–‡è¢«å®‰æ’åœ¨Music: Classification and Recognitionè­°ç¨‹ï¼Œæ¡å£é ­å ±å‘Š
æ–¹å¼ï¼Œå ±å‘Šæ™‚é–“æ˜¯ 13:30 - 13:50ï¼›ç¬¬äºŒç¯‡è«–æ–‡è¢«å®‰æ’åœ¨ Classification and Clusteringè­°ç¨‹ï¼Œ
æ¡å£å ±è§£èªªæ–¹å¼ï¼›ç¬¬ä¸‰ç¯‡è«–æ–‡ã€‚ 
æ­¤è¡Œæ–¼ 11æ—¥ä¸­åˆå—é‚€ä»£è¡¨ ISCA SIG-CSLP (Special Interest Group on Chinese Spoken 
Language Processing) åƒåŠ  The Students Meet Experts/Student Lunch æ´»å‹•ï¼Œèˆ‡æ–°åŠ å¡
åœ‹ç§‘æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•«é …ä¸‹å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
                                                        æ—¥æœŸï¼š101 ï¦ 11 æœˆ 6 æ—¥ 
è¨ˆç•«ç·¨è™Ÿ NSC99-2221-E-001-009-MY3 
è¨ˆç•«åç¨± æ‡‰ç”¨ç™¼éŸ³çŸ¥ï§¼æºæ–¼å¤šæ¨£åŒ–èªè¨€ä¹‹èªéŸ³è™•ï§¤-æ‡‰ç”¨ç™¼éŸ³çŸ¥ï§¼æºæ–¼å¤šèªè¨€è‡ªå‹•èªéŸ³è¾¨ï§¼ä¹‹ç ”ç©¶ 
å‡ºåœ‹äººå“¡
å§“å ç‹æ–°æ°‘ 
æœå‹™æ©Ÿæ§‹
åŠè·ç¨± 
ä¸­ç ”é™¢è³‡è¨Šæ‰€ 
ç ”ç©¶å“¡ 
æœƒè­°æ™‚é–“ 
101 ï¦ 10 æœˆ 29 æ—¥è‡³ 
101 ï¦ 11 æœˆ 2 æ—¥ æœƒè­°åœ°é» Nara, Japan 
æœƒè­°åç¨± 
 (ä¸­æ–‡)  
 (è‹±æ–‡) ACM Multimedia 2012 
ç™¼è¡¨ï¥æ–‡
é¡Œç›® 
1. The Acoustic Emotion Gaussians Model for Emotion-based Music 
Annotation and Retrieval 
2. The Acousticvisual Emotion Gaussians Model for Automatic Generation of 
Music Video 
3. Exploring the Relationship between Categorical and Dimensional Emotion 
Semantics of Music 
 
ä¸€. ï¥«åŠ æœƒè­°ç¶“é 
 
æ­¤ï¨ˆæ–¼ 10 æœˆ 28 æ—¥æ—©æ™¨æ­ä¹˜è¯èˆªç­æ©Ÿï¼Œæ–¼ä¸­åˆæ™‚é–“æŠµé”æ—¥æœ¬é—œè¥¿åœ‹éš›æ©Ÿå ´ï¼Œå…¥é—œä¹‹
å¾Œå…ˆæ­ä¹˜å—æµ·é›»éµé›»ï¤‚è‡³å¤§é˜ªé›£æ³¢ï¼Œå†è½‰ä¹˜è¿‘éµé›»ï¤‚å‰å¾€ï¤Œï¥¼ï¼Œç´„æ–¼ä¸‹åˆä¸‰é»æŠµé”ä¸‹æ¦»
ï¨ªåº—ï¼Œï¨ˆï§¡å®‰ç½®å¦¥ç•¶ä¹‹å¾Œï¼Œï¥¥å…ˆå‰å¾€æœƒå ´ç†Ÿæ‚‰ç’°å¢ƒã€‚ 
ACM Multimediaæœƒè­°æ˜¯å¤šåª’é«”ç ”ç©¶ï¦´åŸŸæœ€é ‚å°–çš„ç ”è¨æœƒã€‚æœ¬æ¬¡æœƒè­°å…±æ”¶åˆ°331ç¯‡é•·
ç¯‡ï¥æ–‡(long paper)åŠ407ç¯‡çŸ­ç¯‡ï¥æ–‡(short paper)æŠ•ç¨¿ï¼Œç¶“éåš´æ ¼çš„å¯©æŸ¥ï¼Œåªæ¥å—67ç¯‡é•·
ç¯‡ï¥æ–‡(æ¥å—ï¥¡ç‚º20.2%)ï¼ŒçŸ­ç¯‡ï¥æ–‡çš„æ¥å—ï¥¡å‰‡æ˜¯31.2%ã€‚ 
10 æœˆ 29 æ—¥æœƒè­°ç¬¬ä¸€å¤©çš„è­°ç¨‹ç‚º Workshop åŠ Tutorialï¼Œä¸Šåˆæˆ‘ï¥«åŠ ï¦º International 
Workshop on Crowdsourcing for Multimediaï¼Œæ—¥æœ¬ AIST çš„ Masataka Goto æ•™æˆä»¥
ã€ŒPodCastle and Songle: Crowdsourcing-Based Web Services for Spoken Content Retrieval 
and Active Music Listeningã€ç‚ºé¡Œï¼Œåšï¦ºä¸€å€‹ï¨é‡‡çš„å°ˆé¡Œæ¼”è¬›ã€‚ä¸‹åˆå‰‡ï¥«åŠ  Hatice Gunes
å’Œ BjÃ¶rn Schuller ï¥¸ä½æ•™æˆåˆè¬›çš„ Tutorial ã€ŒContinuous Analysis of Emotions for 
Multimedia Applicationsã€ã€‚ 
10 æœˆ 30 æ—¥æ˜¯ Main Conference çš„ç¬¬ä¸€å¤©ï¼Œé–‹å¹•å¼ä¹‹å¾Œç·Šæ¥è€…æ˜¯ Best Paper Sessionï¼Œ
å…±æœ‰å››ç¯‡ï¥æ–‡ç«¶é€ï¼Œå…¶ä¸­ä¸€ç¯‡æ˜¯ï¤­è‡ªå°ç£å¤§å­¸æ´ªä¸€å¹³æ•™æˆåœ˜éšŠçš„ç ”ç©¶æˆæœï¼Œé€™ç¯‡ï¥æ–‡é›–
ç„¶æ²’æœ‰å¥ªå¾—æœ€ä½³ï¥æ–‡çï¼Œä½†ä¹Ÿè´å¾—æœ€ä½³å­¸ç”Ÿï¥æ–‡çï¼Œè®“æˆ‘å€‘è¦ºå¾—èˆ‡æœ‰æ¦®ç„‰ã€‚æˆ‘å€‘çš„é•·ç¯‡
ï¥æ–‡ã€ŒThe Acoustic Emotion Gaussians Model for Emotion-based Music Annotation and 
Retrievalã€è¢«å®‰æ’åœ¨ä¸‹åˆçš„ã€ŒAudio and Musicã€è­°ç¨‹ï¼Œå¦å¤–ï¼Œä¹Ÿåœ¨ä¸­åˆçš„æ™‚é–“å…ˆä»¥æµ·å ±
è§£ï¥¯çš„æ–¹å¼ç™¼è¡¨ã€‚æ™šä¸Šå‰‡å‡ºå¸­ The 20th Anniversary Keynote Talk ã€ŒFuture Direction of 
Digital Contentã€ã€‚ 
10 æœˆ 31 æ—¥ä¸Šåˆçš„è­°ç¨‹åŒ…æ‹¬ã€ŒTechnical Achievement Awardsã€åŠ The 20th Anniversary 
Panelã€ŒCoulda, Woulda, Shoulda: 20 Years of Multimedia Opportunitiesã€ï¼Œå…¨é«”èˆ‡æœƒäººå“¡é½Š
èšä¸€å ‚äº¤æ›æ„ï¨Šã€‚ä¸‹åˆå‰‡æ˜¯ä¸€èˆ¬çš„ oral åŠ poster è­°ç¨‹ã€‚ 
åœ‹ç§‘æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•«é …ä¸‹å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
                                                        æ—¥æœŸï¼š102 å¹´ 6 æœˆ 6 æ—¥ 
è¨ˆç•«ç·¨è™Ÿ NSC99-2221-E-001-009-MY3 
è¨ˆç•«åç¨± 
æ‡‰ç”¨ç™¼éŸ³çŸ¥è­˜æºæ–¼å¤šæ¨£åŒ–èªè¨€ä¹‹èªéŸ³è™•ç†-æ‡‰ç”¨ç™¼éŸ³çŸ¥è­˜æºæ–¼
å¤šèªè¨€è‡ªå‹•èªéŸ³è¾¨è­˜ä¹‹ç ”ç©¶ 
å‡ºåœ‹äººå“¡
å§“å 
ç‹æ–°æ°‘ æœå‹™æ©Ÿæ§‹
åŠè·ç¨± 
ä¸­ç ”é™¢è³‡è¨Šæ‰€ 
ç ”ç©¶å“¡ 
æœƒè­°æ™‚é–“ 
102å¹´ 5æœˆ 26æ—¥è‡³ 
102å¹´ 5æœˆ 31æ—¥ æœƒè­°åœ°é» åŠ æ‹¿å¤§æº«å“¥è¯ 
æœƒè­°åç¨± 
 (ä¸­æ–‡)  
 (è‹±æ–‡) The 38th International Conference on Acoustics, Speech and Signal 
Processing (ICASSP2013) 
ç™¼è¡¨è«–æ–‡
é¡Œç›® 
1. Subspace-based Phonotactic Language Recognition Using Multivariate 
Dynamic Linear Models 
2. Weighted Matrix Factorization for Spoken Document Retrieval 
3. Effective Pseudo-Relevance Feedback for Spoken Document Retrieval 
 
ä¸€. åƒåŠ æœƒè­°ç¶“é 
 
æ­¤è¡Œæ–¼ 5æœˆ 26æ—¥æ·±å¤œæ­ä¹˜è¯èˆªç­æ©Ÿï¼Œæ–¼ç•¶åœ°æ™‚é–“ 5æœˆ 26æ—¥æ™šé–“ 7é»æŠµé”æº«å“¥è¯æ©Ÿ
å ´ï¼Œéš¨å³è¶•å¾€ä¸‹æ¦»é£¯åº—ä¼‘æ¯ï¼Œç¬¬äºŒå¤©å…ˆåˆ°æœƒå ´è¾¦ç†å ±åˆ°æ‰‹çºŒï¼Œä¸¦åƒèˆ‡æ™šé–“çš„ receptionã€‚ 
ç”± IEEE Signal Processing Societyæ‰€èˆ‰è¾¦çš„ ICASSPæ˜¯åœ‹éš›ä¸Šé—œæ–¼ä¿¡è™Ÿè™•ç†åŠå…¶ç›¸é—œ
æ‡‰ç”¨ç ”ç©¶è¦æ¨¡æœ€å¤§ï¼Œæœ€è² ç››åçš„å­¸è¡“ç ”è¨æœƒã€‚èªéŸ³èˆ‡éŸ³è¨Šè™•ç†ç ”ç©¶é ˜åŸŸçš„å­¸è€…äº¦è¦–å…¶ç‚º
å¹´åº¦æœ€é‡è¦çš„ç ”è¨æœƒï¼Œæ‰€æœ‰çŸ¥åå­¸è€…å°ˆå®¶å¹¾ä¹éƒ½æœƒèˆ‡æœƒç™¼è¡¨æœ€æ–°ç ”ç©¶æˆæœåŠäº’ç›¸è¨è«–ã€
äº¤æ›ç ”ç©¶å¿ƒå¾—ã€‚æ­·å±† ICASSP é‚€è«‹çš„æ¼”è¬›å­¸è€…éƒ½æ˜¯ç›¸é—œé ˜åŸŸä¸–ç•Œç´šçš„ä¸€æ™‚ä¹‹é¸ï¼Œoral åŠ
poster è­°ç¨‹ç¸½æ•¸åœ¨ 50 ä»¥ä¸Šï¼Œå¤§æœƒä¹Ÿæä¾›è¨±å¤šå„ªè³ªçš„ tutorial èª²ç¨‹ä¾›èˆ‡æœƒè€…é¸æ“‡ï¼Œå±•è¦½æœƒ
ä¹Ÿéå¸¸ç››å¤§ã€‚æœ¬å±†ä¹Ÿä¸ä¾‹å¤–ï¼Œä¸åŒ…æ‹¬ç‰¹åˆ¥è­°ç¨‹å³æœ‰ä¸‰åƒä¸‰ç™¾å¤šç¯‡è«–æ–‡æŠ•ç¨¿ï¼Œç¶“éåš´æ ¼å¯©
æŸ¥ï¼Œç´„ä¸€åƒä¸ƒç™¾ç¯‡è«–æ–‡ç²æ¥å—åœ¨å¤§æœƒç™¼è¡¨ã€‚ä¸»é¡Œæ¼”è¬›æœ‰äº”å ´ï¼Œè¬›é¡Œå’Œè¬›å“¡åˆ†åˆ¥æ˜¯ 1. Recent 
Developments in Deep Neural Networks (Geoffrey Hinton); 2. Inverse Problems Regularized 
by Sparsity (Martin Vetterli); 3. Information measures and estimation theory (Sergio VerdÃº); 
4. The Splendor of Nature: Lessons in Adaptation and Learning over Networks (Ali H. 
Sayed); 5. The Online Revolution: Education for Everyone (Daphne Koller)ã€‚èª²é¡Œæ¶µè“‹æ©Ÿå™¨
å­¸ç¿’ã€compressed sensingã€è³‡è¨Šç†è«–ã€ä¿¡è™Ÿè™•ç†èˆ‡ç·šä¸Šå­¸ç¿’ã€‚ 
    æ­¤æ¬¡æœƒè­°æˆ‘å€‘å…±ç™¼è¡¨ä¸‰ç¯‡è«–æ–‡ï¼Œåˆ†åˆ¥æ˜¯ 1. Subspace-based Phonotactic Language 
Recognition Using Multivariate Dynamic Linear Modelsï¼›2. Weighted Matrix Factorization 
for Spoken Document Retrievalï¼›3. Effective Pseudo-Relevance Feedback for Spoken 
Document Retrievalã€‚ç¬¬ä¸€ç¯‡è«–æ–‡è¢«å®‰æ’åœ¨ 5æœˆ 30æ—¥ä¸‹åˆ 17:00-17:20åš oralå ±å‘Šï¼Œå¦å¤–
å…©ç¯‡å‰‡è¢«å®‰æ’åœ¨ 5æœˆ 31æ—¥ä¸Šåˆ 8:00-10:00é–“åš posterè¬›è§£ã€‚ä¸‰ç¯‡è«–æ–‡éƒ½ç²å¾—èˆ‡æœƒè€…ä¸å°‘
çš„è¨è«–èˆ‡å»ºè­°ã€‚ 
    å››å¤©çš„ä¸»æœƒè­°éå¸¸ç·Šæ¹Šï¼Œæœƒè­°åœ¨ 5æœˆ 31æ—¥çµæŸï¼Œæ—‹å³æ­ä¹˜ 6æœˆ 1æ—¥å‡Œæ™¨çš„ç­æ©Ÿè¿”
å°ã€‚ 
 
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«è¡ç”Ÿç ”ç™¼æˆæœæ¨å»£è³‡æ–™è¡¨
æ—¥æœŸ:2013/10/21
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«
è¨ˆç•«åç¨±: æ‡‰ç”¨ç™¼éŸ³çŸ¥è­˜æºæ–¼å¤šèªè¨€è‡ªå‹•èªéŸ³è¾¨è­˜ä¹‹ç ”ç©¶
è¨ˆç•«ä¸»æŒäºº: ç‹æ–°æ°‘
è¨ˆç•«ç·¨è™Ÿ: 99-2221-E-001-009-MY3 å­¸é–€é ˜åŸŸ: è‡ªç„¶èªè¨€èˆ‡èªéŸ³è™•ç†
ç„¡ç ”ç™¼æˆæœæ¨å»£è³‡æ–™
å…¶ä»–æˆæœ 
(ç„¡æ³•ä»¥ï¥¾åŒ–è¡¨é”ä¹‹æˆ
æœå¦‚è¾¦ï§¤å­¸è¡“æ´»å‹•ã€ç²
å¾—çé …ã€é‡è¦åœ‹éš›åˆ
ä½œã€ç ”ç©¶æˆæœåœ‹éš›å½±éŸ¿
ï¦ŠåŠå…¶ä»–å”åŠ©ç”¢æ¥­æŠ€
è¡“ç™¼å±•ä¹‹å…·é«”æ•ˆï¨—äº‹
é …ç­‰ï¼Œè«‹ä»¥æ–‡å­—æ•˜è¿°å¡«
ï¦œã€‚) 
ç„¡ 
 æˆæœé …ç›® ï¥¾åŒ– åç¨±æˆ–å…§å®¹æ€§è³ªç°¡è¿° 
æ¸¬é©—å·¥å…·(å«è³ªæ€§èˆ‡ï¥¾æ€§) 0  
èª²ç¨‹/æ¨¡çµ„ 0  
é›»è…¦åŠç¶²ï¤·ç³»çµ±æˆ–å·¥å…· 0  
æ•™æ 0  
èˆ‰è¾¦ä¹‹æ´»å‹•/ç«¶è³½ 0  
ç ”è¨æœƒ/å·¥ä½œåŠ 0  
é›»å­å ±ã€ç¶²ç«™ 0  
ç§‘ 
æ•™ 
è™• 
è¨ˆ 
ç•« 
åŠ  
å¡« 
é … 
ç›® è¨ˆç•«æˆæœæ¨å»£ä¹‹ï¥«èˆ‡ï¼ˆé–±è½ï¼‰äººï¥© 0  
 
