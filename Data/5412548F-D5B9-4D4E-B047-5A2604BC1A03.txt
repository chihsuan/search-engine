In this project, we plan to use theoretical and 
empirical study to have an in-depth of the 
construction of Multiple Classifier System, and we 
will focus on the Multiple Classifier Systems built 
on heterogeneous classification algorithms. This 
project could make practical contributions to 
research and applications of Multiple Classifier 
System, and part of the results have been published 
on professional international conferences. 
è‹±æ–‡é—œéµè©ï¼š Multiple Classifier System, Multi-Classifier System, 
Ensemble, Diversity, Classifier Combination 
 
I 
 
ç›®éŒ„ 
ä¸­æ–‡æ‘˜è¦ ................................................................................................................................... II 
Abstract .................................................................................................................................... III 
å‰è¨€ ........................................................................................................................................... 1 
ç ”ç©¶ç›®çš„ ................................................................................................................................... 1 
æ–‡ç»æ¢è¨ ................................................................................................................................... 2 
ç ”ç©¶æ–¹æ³• ................................................................................................................................... 3 
çµæœèˆ‡è¨è«– ............................................................................................................................... 4 
åƒè€ƒæ–‡ç» ................................................................................................................................. 14 
è¨ˆç•«æˆæœè‡ªè©• ......................................................................................................................... 18 
é™„éŒ„ï¼šå‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Šèˆ‡å·²ç™¼è¡¨ä¹‹è«–æ–‡å…¨æ–‡ ................................................. 20 
 
 
  
III 
 
Abstract 
The purpose of this project is to develop theory and practice of the 
construction of Multiple Classifier System. Recently, in the data mining 
field, Multiple Classifier System has been an increasingly attractive 
research topic and been applied to various domains. However, we still lack 
a comprehensive understanding of its foundation. We may know which 
Multiple Classifier System is effective but we are not clear why it is 
effective, and we have limited knowledge of how to effectively construct 
an effective Multiple System. Although there are many success cases, there 
are even more cases where Multiple Classifier System is not considered 
success. Therefore, being able to explain why a Multiple Classifier System 
is unsuccessful is equally important to being able to explain why a 
Multiple Classifier System is successful. Furthermore, most systems are 
ad-hoc designs, i.e. they are designed specifically to some problems or 
data sets, while such designs severely restrict their possible extensions 
and applications. For that reason, a critical research topic is how to 
design a general-purpose Multiple Classifier System. In this project, we 
plan to use theoretical and empirical study to have an in-depth of the 
construction of Multiple Classifier System, and we will focus on the 
Multiple Classifier Systems built on heterogeneous classification 
algorithms. This project could make practical contributions to research 
and applications of Multiple Classifier System, and part of the results 
have been published on professional international conferences. 
 
Keywords: Multiple Classifier System, Multi-Classifier System, Ensemble, 
Diversity, Classifier Combination 
 
 
2 
 
å’Œé‹ä½œæœ‰æ›´å»£ã€æ›´æ·±çš„ç†è§£ï¼Œè€Œé€™æ­£æ˜¯æœ¬è¨ˆç•«çš„ç›®çš„ã€‚ 
åœ¨è¨±å¤šåˆ©ç”¨è³‡æ–™æ¢å‹˜è§£æ±ºå¯¦å‹™å•é¡Œçš„ç«¶è³½ä¸­ï¼Œå¤§éƒ¨åˆ†çš„å¾—çä½œå“ä½¿ç”¨å¤šåˆ†é¡
å™¨ç³»çµ±ï¼Œç”šè‡³åœ¨æŸäº›ç«¶è³½ä¸­ï¼Œååˆ—å‰èŒ…çš„ä½œå“éƒ½æ˜¯ä½¿ç”¨å¤šåˆ†é¡å™¨ç³»çµ±ï¼Œè€Œä¸”é€™å€‹
è¶¨å‹¢åœ¨è¿‘å¹´ä¾†è¶Šç™¼æ˜é¡¯ã€‚æˆ‘å€‘å¯ä»¥å¾é€™è¶¨å‹¢çœ‹åˆ°å¤šåˆ†é¡å™¨ç³»çµ±ç†±é–€ç¨‹åº¦ä»¥åŠç ”ç©¶
æ½›åŠ›å·²ç¶“è¶…è¶Šå–®ä¸€åˆ†é¡æ¼”ç®—æ³•ã€‚ç„¶è€Œï¼Œåœ¨é€™äº›è³‡æ–™æ¢å‹˜ç«¶è³½ç•¶ä¸­ï¼Œå¤§å¤šæ•¸çš„å¤šåˆ†é¡
å™¨ç³»çµ±æ¡å–ç‰¹åˆ¥è¨­è¨ˆï¼Œä¹Ÿå°±æ˜¯èªªé€™äº›ç³»çµ±æ˜¯é‡å°ç‰¹å®šåˆ†é¡å•é¡Œæˆ–è³‡æ–™é›†æ‰€åšçš„è¨­è¨ˆã€‚
æ­¤ç¨®ç‰¹åˆ¥è¨­è¨ˆçš„ä¸¦ä¸å…·æœ‰å¯æ”œæ€§ï¼Œé‡å°æŸåˆ†é¡å•é¡Œæˆ–è³‡æ–™é›†æ‰€è¨­è¨ˆå¤šåˆ†é¡å™¨ç³»çµ±ä¸¦
ä¸é©åˆç”¨ä¾†è§£æ±ºå…¶ä»–åˆ†é¡å•é¡Œæˆ–æ˜¯åˆ†æå…¶ä»–è³‡æ–™é›†ï¼Œæ‰€ä»¥å¯¦å‹™ä¸Šæˆ‘å€‘å¸¸å¸¸éœ€è¦é‡å°
ä¸åŒçš„å•é¡Œå»è¨­è¨ˆä¸åŒçš„å¤šåˆ†é¡å™¨ï¼Œè€Œä¸”å› ç‚ºç›®å‰ä¸¦æ²’æœ‰ä¸€å¥—æŒ‡å¼•ï¼Œæ‰€ä»¥æˆ‘å€‘å¸¸å¸¸
èŠ±è²»å¤§é‡æ™‚é–“åœ¨è¨­è¨ˆèˆ‡å˜—è©¦éŒ¯èª¤çš„è¿´åœˆè£¡é¢ã€‚éƒ¨åˆ†çš„å¤šåˆ†é¡å™¨ç³»çµ±æ˜¯ç”±æ³›ç”¨å‹çš„æ¼”
ç®—æ³•å»ºåˆ¶è€Œä¾†ï¼Œå…·æœ‰å¯æ”œæ€§è€Œéé‡å°ç‰¹å®šåˆ†é¡å•é¡Œæˆ–è³‡æ–™é›†ã€‚ä½†æ˜¯å°±åƒç›®å‰æ²’æœ‰æ‰€
è¬‚æœ€å¥½çš„æˆ–æœ€é©ç”¨çš„å–®ä¸€åˆ†é¡æ¼”ç®—æ³•ï¼Œç›®å‰ä¹Ÿæ²’æœ‰æœ€å¥½çš„æˆ–æœ€é©ç”¨çš„å¤šåˆ†é¡å™¨ç³»çµ±
æ¼”ç®—æ³•ã€‚è«¸å¤šæˆåŠŸå¯¦ä¾‹é¡¯ç¤ºäº†å¤šåˆ†é¡å™¨ç³»çµ±çš„æ½›åŠ›ï¼Œä½†ä¸Šè¿°é—œæ–¼è¨­è¨ˆå¤šåˆ†é¡å™¨ç³»çµ±
çš„ç¾æ³é»å‡ºäº†é—œæ–¼å¤šåˆ†é¡å™¨ç³»çµ±çš„ç ”ç©¶ç©ºé–“ã€‚ 
ç¸½çµä¾†èªªï¼Œåœ¨ç›®å‰å­¸è¡“ç ”ç©¶ä»¥åŠå¯¦å‹™æ‡‰ç”¨ä¸Šï¼Œæˆ‘å€‘ç›®å‰å°å¤šåˆ†é¡å™¨ç³»çµ±æ˜¯ã€çŸ¥
å…¶ç„¶è€Œä¸çŸ¥å…¶æ‰€ä»¥ç„¶ã€ï¼Œæœ¬è¨ˆç•«çš„æœ€çµ‚ç›®çš„å°±æ˜¯è¦ã€çŸ¥å…¶æ‰€ä»¥ç„¶ã€ã€‚å¾å¦ä¸€æ–¹é¢ä¾†
èªªï¼Œé›–ç„¶ç‚ºæ•¸çœ¾å¤šçš„å¤šåˆ†é¡å™¨ç³»çµ±å±•ç¤ºäº†è«¸å¤šæˆåŠŸæ¡ˆä¾‹ï¼Œä½†æ˜¯å¦æœ‰æ›´å¤šçš„å¤šåˆ†é¡
å™¨ç³»çµ±ä¸¦æ²’æœ‰ç‰¹åˆ¥å„ªç•°çš„è¡¨ç¾ã€‚æœ¬è¨ˆç•«çš„ä¸€éƒ¨åˆ†ç ”ç©¶ä¸åƒ…æ˜¯è¦èƒ½è§£é‡‹ç‚ºä»€éº¼æŸäº›
å¤šåˆ†é¡å™¨ç³»çµ±æœ‰å¥½çš„è¡¨ç¾ï¼Œé‚„è¦èƒ½è§£é‡‹ç‚ºä»€éº¼å¦å¤–æŸäº›å¤šåˆ†é¡å™¨ç³»çµ±æ²’æœ‰å¥½çš„è¡¨
ç¾ã€‚ 
æ–‡ç»æ¢è¨ 
èˆ‡æœ¬è¨ˆç•«æœ‰é—œä¹‹ç†è«–åˆ†æçš„ç ”ç©¶æƒ…æ³èˆ‡é‡è¦åƒè€ƒæ–‡ç»å¯åˆ†ç‚ºå…©å¤§é¡ï¼Œç¬¬ä¸€å¤§é¡
è‘—é‡æ–¼å¦‚ä½•ç”¢ç”Ÿä¸€çµ„æœ‰æ•ˆçš„æˆå“¡åˆ†é¡å™¨ (æˆå“¡å‡½æ•¸) ï¼Œç¬¬äºŒå¤§é¡è‘—é‡æ–¼å¦‚ä½•æ•´åˆæˆ
å“¡åˆ†é¡å™¨ (æˆå“¡å‡½æ•¸) çš„åˆ†é¡çµæœã€‚ 
ç¬¬ä¸€å¤§é¡ç ”ç©¶çš„æ¨™çš„æ˜¯å…·æœ‰å·®ç•°æ€§çš„æˆå“¡åˆ†é¡å™¨ï¼Œä¾‹å¦‚ Melvilleåˆ©ç”¨äººå·¥æ·»åŠ 
çš„è¨“ç·´è³‡æ–™æ¨£æœ¬ä¾†å¢åŠ å·®ç•°æ€§[9]ï¼Œè€Œ Tsymbalé‡å°ç”¨éš¨æ©Ÿå­ç©ºé–“æ³•æ‰€å»ºç«‹çš„å¤šåˆ†é¡
å™¨ç³»çµ±è£¡é¢æˆå“¡åˆ†é¡å™¨ä¹‹é–“çš„å·®ç•°æ€§åšæ¢è¨[10]ã€‚å·®ç•°æ€§åº¦é‡çš„æå‡ºæœ‰å…©å€‹ç›®çš„ï¼Œ
ä¸€æ˜¯å”åŠ©æˆå“¡åˆ†é¡å™¨çš„é¸æ“‡ (ä¾‹å¦‚ï¼Œç”¨é€™äº›åº¦é‡å»ç¯©é¸æ‰ç›¸ä¼¼çš„æˆå“¡åˆ†é¡å™¨) ï¼Œå¦
ä¸€æ˜¯è©•é‡åˆ†é¡å™¨æ•´åˆæ–¹æ³•æˆ–æ˜¯ç­–ç•¥ (ä¾‹å¦‚ï¼Œç”¨é€™äº›åº¦é‡å»æª¢è¦–åˆ†æˆå“¡åˆ†é¡å™¨æ•´åˆå‰
å¾Œçš„å·®ç•°æ€§è®ŠåŒ–) [11, 12]ã€‚Tangæå‡ºå·®ç•°æ€§åº¦é‡çš„åˆ†æ[13]ï¼Œè€Œ Chungç ”ç©¶å¤šå€‹
å·®ç•°æ€§åº¦é‡ä¹‹é–“çš„é—œä¿‚[14]ã€‚Tsangé‡å°å¤§çš„è³‡æ–™é›†ç ”ç©¶å¦‚ä½•å°‡å·®ç•°æ€§å¼•å…¥ä»¥æ”¯æŒå‘
é‡æ©Ÿç‚ºåŸºç¤çš„å¤šåˆ†é¡å™¨ç³»çµ±[15, 16]ã€‚Shiåˆ©ç”¨ç‰¹å¾µæˆ–å±¬æ€§é¸å–ä»¥åŠå·®ç•°æ€§åº¦é‡ä¾†
å»ºç½®å¤šåˆ†é¡å™¨ç³»çµ±[17]ã€‚Torres-Sospedraå°‡è¨“ç·´è³‡æ–™æ¨£æœ¬æˆ–è¨˜éŒ„åšé‡æ–°æ’åºä¾†å¢åŠ 
å·®ç•°æ€§[18]ã€‚Brownæ¢è¨ç”¨å¤šæ•¸æŠ•ç¥¨æ³•æ‰€å»ºç«‹çš„å¤šåˆ†é¡å™¨ç³»çµ±è£¡é¢å¥½çš„èˆ‡ä¸å¥½çš„å·®ç•°
æ€§[19]ã€‚Wangé‡å°å¤šåˆ†é¡å™¨ç³»çµ±çš„å»ºç½®å»æ¢è¨é¡ç¥ç¶“ç¶²è·¯å’Œæ±ºç­–æ¨¹ä¹‹é–“çš„å·®ç•°æ€§
4 
 
å››ã€ å»¶ä¼¸å¤šåˆ†é¡å™¨ç³»çµ±çš„æ¦‚å¿µï¼Œæå‡ºä¸€å€‹å¯ä»¥ç”¢ç”Ÿä¸€çµ„äººå€‘èƒ½è¼•æ˜“çœ‹å¾—æ‡‚çš„åˆ†é¡è¦
å‰‡çš„åˆ†é¡å™¨çš„å»ºç½®æ¼”ç®—æ³•ï¼Œä¸¦æ¢è¨ä¸€å€‹ç”¨æ–¼ä¸²æµè³‡æ–™åˆ†æçš„æ¦‚å¿µé£„ç§»åµæ¸¬æ¨¡
çµ„ã€‚ 
çµæœèˆ‡è¨è«– 
åœ– 1å‘ˆç¾å¤šåˆ†é¡å™¨ç³»çµ±çš„è¨“ç·´èˆ‡ä½¿ç”¨ (æˆ–æ¸¬è©¦) ã€‚ç›®å‰å­¸è€…å·²é«”èªåˆ°æˆå“¡åˆ†é¡
å™¨ (æˆå“¡å‡½æ•¸) ä¹‹é–“çš„å·®ç•°æ€§å°å¤šåˆ†é¡å™¨ç³»çµ± (æˆ–åˆ†é¡å™¨é›†æˆ) æ•´é«”æ•ˆèƒ½çš„å½±éŸ¿ï¼Œ
å› æ­¤æœ¬è¨ˆç•«ä»¥å¤šåˆ†é¡å™¨ç³»çµ±ä¸­çš„æˆå“¡åˆ†é¡å™¨ä¹‹é–“çš„å·®ç•°æ€§ç‚ºåŸºç¤å»æ¢è¨å¤šåˆ†é¡å™¨ç³»
çµ±çš„å»ºç½®ï¼Œæ›´é€²è€Œè—‰æ­¤ç™¼å±•ä¸€å¥—æŒ‡å¼•ã€‚é¦–å…ˆï¼Œæˆ‘å€‘è¦å…ˆç­è§£æˆå“¡åˆ†é¡å™¨ä¹‹é–“å·®ç•°æ€§
çš„ä¾†æºã€‚ 
ä¸€å€‹åˆ†é¡æ¼”ç®—æ³•å¯ä»¥è¦–ç‚ºä¸€å€‹è—åœ–ï¼Œå› ç‚ºå®ƒä¸¦ä¸æ˜¯ä¸€å€‹å‡½æ•¸ï¼Œè€Œæ˜¯ä¸€å€‹æè¿°å¦‚
ä½•åˆ©ç”¨ä¸€çµ„è¨ˆç®—å’Œé‚è¼¯è¦å‰‡ä¾†å°‹æ‰¾æˆ–å»ºç«‹å‡½æ•¸çš„æ–¹æ³•æµç¨‹ã€‚åˆ†é¡å™¨æ˜¯ä¸€å€‹åˆ†é¡æ¼”ç®—
æ³•çš„å¯¦é«”ï¼Œå®ƒä»£è¡¨ä¸€å€‹å¯ä»¥ç›´æ¥æ‹¿ä¾†ä½¿ç”¨çš„å‡½æ•¸ï¼Œå®ƒæ˜¯ä¸€å€‹åˆ†é¡æ¼”ç®—æ³•å…·é«”åŒ–ä¹‹å¾Œ
çš„çµæœã€‚å¦‚æœç”¨ç‰©ä»¶å°å‘è¨­è¨ˆçš„æ¦‚å¿µä¾†èªªï¼Œåˆ†é¡æ¼”ç®—æ³•æ˜¯ã€é¡åˆ¥ã€è€Œåˆ†é¡å™¨å‰‡æ˜¯ã€ç‰©
ä»¶ã€ã€‚æ‰€ä»¥ï¼Œä¸€å€‹åˆ†é¡å™¨çš„åŸºæœ¬æ§‹æˆè¦ç´ æ˜¯å®ƒæ‰€ä¾æ“šçš„åˆ†é¡æ¼”ç®—æ³•ã€‚ 
å°å¤šåˆ†é¡å™¨ç³»çµ±ä¾†èªªï¼Œæˆå“¡åˆ†é¡å™¨ä¹Ÿå¯ç¨±ç‚ºåŸºåº•åˆ†é¡å™¨ (å› ç‚ºå®ƒå€‘æ˜¯æ§‹æˆå¤šåˆ†
é¡å™¨ç³»çµ±çš„åŸºæœ¬æ§‹æˆè¦ç´ ) ï¼Œè€Œæˆå“¡åˆ†é¡å™¨æ‰€ä¾æ“šçš„åˆ†é¡æ¼”ç®—æ³•ç¨±ç‚ºåŸºåº•æ¼”ç®—æ³•ã€‚
å¦ä¸€å€‹åˆ†é¡å™¨çš„åŸºæœ¬æ§‹æˆè¦ç´ æ˜¯è³‡æ–™ï¼Œæ›´ç²¾ç¢ºåœ°èªªæ˜¯è³‡æ–™é›†ï¼šå°å¾ˆå¤§ä¸€éƒ¨åˆ†çš„åˆ†é¡
æ¼”ç®—æ³•è€Œè¨€ï¼Œåœ¨è¨“ç·´éšæ®µ (å°‹æ‰¾é©ç•¶çš„å‡½æ•¸çš„éšæ®µ) è³‡æ–™é›†æœƒè¢«åˆ†æä¸¦è—‰ä»¥ç”¨ä¾†å»º
ç«‹åˆ†é¡å™¨ï¼›å°å¦ä¸€éƒ¨ä»½çš„åˆ†é¡æ¼”ç®—æ³•è€Œè¨€ï¼Œæ›´å…·é«”åœ°èªªæ˜¯å°è¨˜æ†¶å‹çš„åˆ†é¡æ¼”ç®—æ³• (è­¬
å¦‚æœ€è¿‘é„°å±…æ¼”ç®—æ³•) è€Œè¨€ï¼Œåœ¨è¨“ç·´éšæ®µè³‡æ–™é›†æœƒè¢«ç›´æ¥å„²å­˜è€Œå¾Œåœ¨åˆ†é¡è³‡æ–™æ¨£æœ¬æˆ–
è¨˜éŒ„æ™‚è³‡æ–™é›†æœƒè¢«åƒè€ƒã€åˆ©ç”¨ã€‚è³‡æ–™é›†è£¡é¢çš„æ¯ä¸€å€‹è³‡æ–™æ¨£æœ¬æˆ–è¨˜éŒ„æœƒæ˜¯å­˜åœ¨æ–¼ç‰¹
å¾µç©ºé–“è£¡é¢çš„ä¸€å€‹é»ã€‚ 
 
6 
 
ä»£è¡¨ä¸€å€‹è³‡æ–™æ¨£æœ¬æˆ–è¨˜éŒ„ï¼Œè€Œä¸€å€‹è¡Œä»£è¡¨ä¸€å€‹å±¬æ€§æˆ–ç‰¹å¾µï¼Œé‚£éº¼ç¬¬äºŒå€‹æ–¹é¢å°±æ˜¯
æŒ‡å·®ç•°æ€§çš„æœ‰ç„¡å¯ä»¥å–æ±ºæ–¼æ˜¯å¦å°è³‡æ–™çŸ©é™£çš„åˆ—åšå–æ¨£ï¼Œè€Œç¬¬ä¸‰å€‹æ–¹é¢å°±æ˜¯æŒ‡å·®
ç•°æ€§çš„æœ‰ç„¡å¯ä»¥å–æ±ºæ–¼æ˜¯å¦å°è³‡æ–™çŸ©é™£çš„è¡Œåšå–æ¨£ã€‚ 
 
 
 
æˆ‘å€‘ç”¨ A ä»£è¡¨åˆ†é¡æ¼”ç®—æ³•ï¼Œç”¨ S ä»£è¡¨è³‡æ–™æ¨£æœ¬æˆ–è¨˜éŒ„ (äº¦å³ï¼Œè³‡æ–™çŸ©é™£çš„
åˆ—) ï¼Œç”¨ F ä»£è¡¨è³‡æ–™æ¨£æœ¬æˆ–è¨˜éŒ„çš„ç‰¹å¾µç©ºé–“ (äº¦å³ï¼Œè³‡æ–™çŸ©é™£çš„è¡Œ) ã€‚å¾é€™ä¸‰æ–¹
é¢ä¾†è€ƒæ…®ï¼Œæˆ‘å€‘æ­¸ç´æ•´ç†å‡ºä¸ƒå¤§é¡å°‡å·®ç•°æ€§å¼•å…¥æˆå“¡åˆ†é¡å™¨çš„æ–¹æ³•ï¼Œæˆ–æ˜¯å»ºç«‹ä¸
åŒçš„æˆå“¡åˆ†é¡å™¨çš„æ–¹æ³•ã€‚é€™æ˜¯ä¸€ç¨®æ–°çš„åˆ†é¡æ–¹å¼ã€‚æˆ‘å€‘è—‰ç”± Aã€Sã€F çš„æ”¹è®Šä¾†
å°‡å¤šåˆ†é¡å™¨ç³»çµ±æ­¸ç´æ•´ç†ç‚ºä¸ƒå¤§é¡ã€‚ 
ä¸€ã€ æ”¹è®Š Aï¼Œå…¶é¤˜ä¸è®Šã€‚ä½¿ç”¨ä¸åŒçš„åŸºåº•æ¼”ç®—æ³•å»å»ºç«‹ä¸åŒçš„æˆå“¡åˆ†é¡å™¨ï¼Œä½†æ˜¯ä¸å°
è³‡æ–™çŸ©é™£åšä»»ä½•çš„å–æ¨£ã€‚å°æ¯å€‹æˆå“¡åˆ†é¡å™¨è€Œè¨€ï¼Œä½œç‚ºè¨“ç·´æˆ–æ˜¯åƒè€ƒç”¨çš„è³‡æ–™
æ¨£æœ¬æˆ–è¨˜éŒ„æ˜¯ä¸€æ¨£çš„ï¼Œè€Œä¸”ä½œç‚ºè¨“ç·´æˆ–æ˜¯åƒè€ƒç”¨çš„è³‡æ–™æ¨£æœ¬æˆ–è¨˜éŒ„æ‰€åœ¨çš„ç‰¹å¾µ
ç©ºé–“ä¹Ÿæ˜¯ä¸€æ¨£çš„ã€‚éƒ¨åˆ†ç‰¹åˆ¥è¨­è¨ˆçš„æ¼”ç®—æ³•å¯æ­¸ç‚ºæ­¤é¡ã€‚ 
äºŒã€ æ”¹è®Š Sï¼Œå…¶é¤˜ä¸è®Šã€‚å°æ¯å€‹æˆå“¡åˆ†é¡å™¨è€Œè¨€ï¼Œä½œç‚ºè¨“ç·´æˆ–æ˜¯åƒè€ƒç”¨çš„è³‡æ–™æ¨£æœ¬æˆ–
è¨˜éŒ„æ˜¯ä¸ä¸€æ¨£çš„ï¼Œä½†æ˜¯åŸºåº•æ¼”ç®—æ³•æ˜¯ä¸€æ¨£çš„ï¼Œè€Œä¸”ä½œç‚ºè¨“ç·´æˆ–æ˜¯åƒè€ƒç”¨çš„è³‡æ–™
æ¨£æœ¬æˆ–è¨˜éŒ„æ‰€åœ¨çš„ç‰¹å¾µç©ºé–“æ˜¯ä¸€æ¨£çš„ã€‚Boosting[48]ã€Bagging (bootstrap 
aggregating) [49]ï¼Œä»¥åŠ DECORATE (Diverse Ensemble Creation by 
Oppositional Relabeling of Artificial Training Examples) [9, 36]ï¼Œå¯
æ­¸ç‚ºæ­¤é¡ã€‚DECORATEåˆ©ç”¨äººé€ è³‡æ–™ï¼Œæˆ–ç¨±ç‚ºäººå·¥æ·»åŠ è³‡æ–™ï¼Œå»ç”¢ç”Ÿä¸åŒçš„è¨“ç·´
è³‡æ–™é›†ï¼Œä¸¦è—‰æ­¤å°‡å·®ç•°æ€§å¼•é€²å¤šåˆ†é¡å™¨ç³»çµ±ç•¶ä¸­[9, 36]ã€‚ 
ä¸‰ã€ æ”¹è®Š Fï¼Œå…¶é¤˜ä¸è®Šã€‚å°æ¯å€‹æˆå“¡åˆ†é¡å™¨è€Œè¨€ï¼Œä½œç‚ºè¨“ç·´æˆ–æ˜¯åƒè€ƒç”¨çš„è³‡æ–™æ¨£æœ¬æˆ–
RSM
ä½¿ç”¨ç›¸åŒ(0)æˆ–
ä¸åŒ(1)çš„è¨“ç·´
è³‡æ–™æ¨£æœ¬(è¨˜éŒ„)
ä½¿ç”¨ç›¸åŒ(0)æˆ–ä¸åŒ(1)
çš„åˆ†é¡æ¼”ç®—æ³•
ä½¿ç”¨ç›¸åŒ(0)æˆ–ä¸
åŒ(1)çš„ç‰¹å¾µ(å±¬æ€§)
Baggingã€ 
Boostingã€
DECORATE
Random Forest
Stackingï¼Œé‚„æœ‰æœ¬è¨ˆç•«ç”¢
å‡ºä¹‹æœƒè­°è«–æ–‡ â€œHybrid 
Ensembles of Decision 
Trees and Artificial 
Neural Networksâ€
0 1
1
1
åœ– 2ï¼šä¸‰å€‹è·Ÿåˆ†é¡å™¨å·®ç•°æ€§çš„ç”¢ç”Ÿæˆ–å¼•å…¥æœ‰é—œçš„é¢å‘ã€‚ 
8 
 
 
 
é—œæ–¼å¤šåˆ†é¡å™¨ç³»çµ±çš„ç¬¬äºŒå€‹ç ”ç©¶ä¸»é¡Œï¼Œäº¦å³ï¼Œå¦‚ä½•æ•´åˆæˆå“¡åˆ†é¡å™¨çš„åˆ†é¡çµæœï¼Œ
å¸¸è¦‹æ–¹æ³•å¦‚ä¸‹ï¼š 
ä¸€ã€ ä»£æ•¸æ³•ã€‚åªè¦åˆ†é¡å™¨èƒ½å¤ è¼¸å‡ºé€£çºŒæ•¸å€¼ï¼Œè­¬å¦‚æ˜¯æœƒé‡å°æŸä¸€è³‡æ–™æ¨£æœ¬çµ¦å‡ºå…¶å±¬
æ–¼æŸé¡åˆ¥çš„æ©Ÿç‡ï¼Œæˆ–æ˜¯æœƒçµ¦å‡ºè©²åˆ†é¡çš„ä¿¡å¿ƒç¨‹åº¦ï¼Œå‰‡åˆ†é¡å™¨çš„çµ„åˆå¯ä»¥é€éé€™
äº›æ•¸å€¼çš„ä»£æ•¸é‹ç®—ä¾†é”æˆã€‚ 
äºŒã€ æŠ•ç¥¨æ³•æˆ–è¡¨æ±ºæ³•ã€‚æ¡ç”¨æ™®é€š (ä¸åŠ æ¬Š) å¤šæ•¸è¡¨æ±ºæ³•çš„å¤šé¡å™¨ç³»çµ±æ¼”ç®—æ³•åŒ…æ‹¬
Baggingå’Œéš¨æ©Ÿæ£®æ—ï¼Œè€Œæ¡ç”¨åŠ æ¬Šå¤šæ•¸è¡¨æ±ºæ³•çš„å¤šåˆ†é¡å™¨ç³»çµ±æ¼”ç®—æ³•åŒ…æ‹¬
Boostingã€‚æœ¬è¨ˆç•«ç”¢å‡ºä¹‹æœƒè­°è«–æ–‡â€œHybrid Ensembles of Decision Trees and 
Artificial Neural Networksâ€[52]æ¡ç”¨æ™®é€š (ä¸åŠ æ¬Š) å¤šæ•¸è¡¨æ±ºæ³•ã€‚ 
ä¸‰ã€ é¸æ“‡æ³•æˆ–ç¯©é¸æ³•ã€‚å„æˆå“¡å‡½æ•¸æœƒç›¸äº’æ¯”è¼ƒï¼Œæœ€å¥½çš„é‚£ä¸€å€‹ï¼Œæœƒè¢«æ‹¿ä¾†å°æ–°è³‡æ–™
é€²è¡Œåˆ†é¡æˆ–é æ¸¬ã€‚æœ¬è¨ˆç•«ç”¢å‡ºä¹‹æœƒè­°è«–æ–‡â€œA Rule-Based Classification 
Algorithm: A Rough Set Approachâ€[55]ï¼Œèˆ‡åŸºæ–¼è©²è«–æ–‡çš„ç¢©å£«è«–æ–‡[56]ï¼Œ
æ¡ç”¨é¸æ“‡æ³•ï¼šé€™å…©ç¯‡è«–æ–‡æ‰€æå‡ºçš„æ–¹æ³•æœƒç”¢ç”Ÿæ•¸æ¢åˆ†é¡è¦å‰‡ï¼Œè€Œä¸€æ¢åˆ†é¡è¦å‰‡
è¨“ç·´è³‡æ–™
é›† 1
è¨“ç·´è³‡æ–™
é›† j
è¨“ç·´è³‡æ–™
é›† T
... ...
æˆå“¡åˆ†é¡
å™¨ 1&1
åˆ†é¡æ¼”ç®—
æ³• 1
æˆå“¡åˆ†é¡
å™¨ i&j
æˆå“¡åˆ†é¡
å™¨ m&T
... ...
è³‡æ–™é›†
æ•´åˆ
... ...
åˆ†é¡æ¼”ç®—
æ³• i
åˆ†é¡æ¼”ç®—
æ³• m
...
...
è¨“ç·´è³‡æ–™
é›† 2
è¨“ç·´è³‡æ–™
é›† T-1
æˆå“¡åˆ†é¡
å™¨ 1&T-1
æˆå“¡åˆ†é¡
å™¨ m&2
 
åœ– 3ï¼šæœ¬è¨ˆç•«ç”¢å‡ºä¹‹æœƒè­°è«–æ–‡â€œHybrid Ensembles of Decision Trees and 
Artificial Neural Networksâ€[52]æ‰€æ¡ç”¨çš„å¤šåˆ†é¡å™¨ç³»çµ±æ¶æ§‹ã€‚ 
10 
 
4 ï¬ è¨­å®š prunning çš„ä¿¡
å¿ƒé–€æª»å€¼ç‚º 0.2 
ï¬ æ¯å€‹æ¨¹è‘‰çš„æœ€å°æ¨£
æœ¬æ•¸é‡ç‚º 5 
ï¬ ä½¿ç”¨ supervised 
discretization å»è™•
ç†æ•¸å€¼ç‰¹å¾µ 
ï¬ è¨­å®šæœ€è¿‘é„°å±…æ•¸ç‚º
5 
ï¬ ä»¥ 1-é„°å±…è·é›¢ä½œç‚º
åŠ é‡æ¬Šå€¼ 
5 ï¬ ä½¿ç”¨ prunning 
ï¬ åƒ…ä½¿ç”¨äºŒå…ƒåˆ†è£‚ 
ï¬ æ¯å€‹æ¨¹è‘‰çš„æœ€å°æ¨£
æœ¬æ•¸é‡ç‚º 5 
ï¬ ä½¿ç”¨ supervised 
discretization å»è™•
ç†æ•¸å€¼ç‰¹å¾µ 
ï¬ è¨­å®š k ç‚º 5 
ï¬ åœ¨è¨“ç·´é›†ä¸Šä½¿ç”¨
hold-one-out è©•ä¼°æ³•
å»é¸æ“‡1åˆ°kçš„æ•¸ç•¶
ä½œæœ€è¿‘é„°å±…æ•¸ 
ï¬ ç•¶è€ƒæ…®æ•¸å€¼é æ¸¬æ™‚
ä½¿ç”¨æœ€å° mean 
squared error è€Œä¸æ˜¯ 
mean absolute error 
 
è¡¨ 2ï¼šåœ¨äººé€ è³‡æ–™é›†ä¸Šï¼Œé‡å°ä¸åŒåˆ†é¡æ¼”ç®—æ³•åŠåƒæ•¸è¨­å®šï¼ŒåŒè³ªæ¼”ç®—æ³•å»ºç«‹
çš„åˆ†é¡å™¨ä¹‹é–“çš„å·®ç•°æ€§é¡¯ç¤ºæ–¼æ·ºè‰²åˆ—ï¼Œç•°è³ªæ¼”ç®—æ³•å»ºç«‹çš„åˆ†é¡å™¨ä¹‹é–“çš„å·®ç•°
æ€§é¡¯ç¤ºæ–¼æ·±è‰²åˆ—ï¼›ç®­è™Ÿè¡¨ç¤ºå·®ç•°æ€§è©•é‡æŒ‡æ¨™çš„å€¼æ˜¯é«˜æˆ–ä½ä»£è¡¨è¼ƒä½³çš„å·®ç•°
æ€§ã€‚ 
è³‡æ–™é›† å¯¦é©— æ¼”ç®—æ³• Qâ†“ Ïâ†“ DISâ†‘ DFâ†“ Eâ†‘ KWâ†‘ Îºâ†“ Î¸â†“ GDâ†‘ CFDâ†‘ 
A 
Set 1 
J48+J48 1.00  1.00  0.00  0.05  0.00  0.00  1.00  0.19  0.00  0.00  
J48+NB 0.72  0.21  0.15  0.02  0.15  0.04  0.69  0.12  0.76  0.87  
Set 2 
J48+J48 1.00  1.00  0.00  0.05  0.00  0.00  1.00  0.19  0.00  0.00  
J48+IBk 0.77  0.17  0.06  0.01  0.06  0.02  0.87  0.18  0.80  0.89  
Set 3 
NB+NB 1.00  1.00  0.00  0.15  0.00  0.00  1.00  0.14  0.00  0.00  
NB+J48 0.72  0.21  0.15  0.02  0.15  0.04  0.69  0.12  0.76  0.87  
Set 4 
NB+NB 1.00  1.00  0.00  0.15  0.00  0.00  1.00  0.14  0.00  0.00  
NB+IBk 0.70  0.16  0.15  0.02  0.15  0.04  0.69  0.13  0.84  0.91  
Set 5 
IBk+IBk 1.00  1.00  0.00  0.03  0.00  0.00  1.00  0.20  0.00  0.00  
IBk+J48 0.77  0.17  0.06  0.01  0.06  0.02  0.87  0.18  0.80  0.89  
Set 6 
IBk+IBk 1.00  1.00  0.00  0.03  0.00  0.00  1.00  0.20  0.00  0.00  
IBk+NB 0.70  0.16  0.15  0.02  0.15  0.04  0.69  0.13  0.84  0.91  
B 
Set 1 
J48+J48 1.00  1.00  0.00  0.32  0.00  0.00  1.00  0.08  0.00  0.00  
J48+NB 0.14  0.06  0.47  0.17  0.47  0.12  0.06  0.02  0.58  0.73  
Set 2 
J48+J48 1.00  1.00  0.00  0.32  0.00  0.00  1.00  0.08  0.00  0.00  
J48+IBk 0.76  0.37  0.25  0.12  0.25  0.06  0.50  0.05  0.51  0.67  
Set 3 
NB+NB 1.00  1.00  0.00  0.49  0.00  0.00  1.00  0.06  0.00  0.00  
NB+J48 0.14  0.06  0.47  0.17  0.47  0.12  0.06  0.02  0.58  0.73  
Set 4 
NB+NB 1.00  1.00  0.00  0.49  0.00  0.00  1.00  0.06  0.00  0.00  
NB+IBk 0.12  0.05  0.48  0.09  0.48  0.12  0.04  0.03  0.72  0.84  
Set 5 
IBk+IBk 1.00  1.00  0.00  0.17  0.00  0.00  1.00  0.13  0.00  0.00  
IBk+J48 0.76  0.37  0.25  0.12  0.25  0.06  0.50  0.05  0.51  0.67  
Set 6 
IBk+IBk 1.00  1.00  0.00  0.17  0.00  0.00  1.00  0.13  0.00  0.00  
IBk+NB 0.12  0.05  0.48  0.09  0.48  0.12  0.04  0.03  0.72  0.84  
C 
Set 1 
J48+J48 1.00  1.00  0.00  0.14  0.00  0.00  1.00  0.14  0.00  0.00  
J48+NB 0.35  0.13  0.32  0.06  0.32  0.08  0.36  0.05  0.71  0.83  
Set 2 
J48+J48 1.00  1.00  0.00  0.14  0.00  0.00  1.00  0.14  0.00  0.00  
J48+IBk 0.86  0.32  0.13  0.03  0.13  0.03  0.74  0.13  0.68  0.81  
Set 3 
NB+NB 1.00  1.00  0.00  0.31  0.00  0.00  1.00  0.08  0.00  0.00  
NB+J48 0.35  0.13  0.32  0.06  0.32  0.08  0.36  0.05  0.71  0.83  
Set 4 
NB+NB 1.00  1.00  0.00  0.31  0.00  0.00  1.00  0.08  0.00  0.00  
NB+IBk 0.22  0.05  0.31  0.02  0.31  0.08  0.37  0.07  0.89  0.94  
Set 5 
IBk+IBk 1.00  1.00  0.00  0.05  0.00  0.00  1.00  0.19  0.00  0.00  
IBk+J48 0.86  0.32  0.13  0.03  0.13  0.03  0.74  0.13  0.68  0.81  
Set 6 IBk+IBk 1.00  1.00  0.00  0.05  0.00  0.00  1.00  0.19  0.00  0.00  
12 
 
IBk+NB 0.74  0.29  0.18  0.05  0.18  0.05  0.64  0.10  0.64  0.78  
diabetes 
Set 1 
J48+J48 1.00  1.00  0.00  0.22  0.00  0.00  1.00  0.11  0.00  0.00  
J48+NB 0.85  0.51  0.18  0.15  0.18  0.04  0.65  0.06  0.38  0.55  
Set 2 
J48+J48 1.00  1.00  0.00  0.22  0.00  0.00  1.00  0.11  0.00  0.00  
J48+IBk 0.68  0.30  0.22  0.08  0.22  0.05  0.57  0.07  0.59  0.74  
Set 3 
NB+NB 1.00  1.00  0.00  0.24  0.00  0.00  1.00  0.10  0.00  0.00  
NB+J48 0.85  0.51  0.18  0.15  0.18  0.04  0.65  0.06  0.38  0.55  
Set 4 
NB+NB 1.00  1.00  0.00  0.24  0.00  0.00  1.00  0.10  0.00  0.00  
NB+IBk 0.73  0.34  0.22  0.09  0.22  0.05  0.57  0.07  0.56  0.72  
Set 5 
IBk+IBk 1.00  1.00  0.00  0.14  0.00  0.00  1.00  0.14  0.00  0.00  
IBk+J48 0.68  0.30  0.22  0.08  0.22  0.05  0.57  0.07  0.59  0.74  
Set 6 
IBk+IBk 1.00  1.00  0.00  0.14  0.00  0.00  1.00  0.14  0.00  0.00  
IBk+NB 0.73  0.34  0.22  0.09  0.22  0.05  0.57  0.07  0.56  0.72  
kr-vs-kp 
Set 1 
J48+J48 1.00  1.00  0.00  0.01  0.00  0.00  1.00  0.22  0.00  0.00  
J48+NB 0.81  0.12  0.13  0.00  0.13  0.03  0.74  0.15  0.94  0.97  
Set 2 
J48+J48 1.00  1.00  0.00  0.01  0.00  0.00  1.00  0.22  0.00  0.00  
J48+IBk 0.91  0.18  0.04  0.00  0.04  0.01  0.93  0.20  0.85  0.92  
Set 3 
NB+NB 1.00  1.00  0.00  0.13  0.00  0.00  1.00  0.15  0.00  0.00  
NB+J48 0.81  0.12  0.13  0.00  0.13  0.03  0.74  0.15  0.94  0.97  
Set 4 
NB+NB 1.00  1.00  0.00  0.13  0.00  0.00  1.00  0.15  0.00  0.00  
NB+IBk 0.71  0.18  0.13  0.02  0.13  0.03  0.73  0.14  0.82  0.90  
Set 5 
IBk+IBk 1.00  1.00  0.00  0.03  0.00  0.00  1.00  0.20  0.00  0.00  
IBk+J48 0.91  0.18  0.04  0.00  0.04  0.01  0.93  0.20  0.85  0.92  
Set 6 
IBk+IBk 1.00  1.00  0.00  0.03  0.00  0.00  1.00  0.20  0.00  0.00  
IBk+NB 0.71  0.18  0.13  0.02  0.13  0.03  0.73  0.14  0.82  0.90  
sick 
Set 1 
J48+J48 1.00  1.00  0.00  0.01  0.00  0.00  1.00  0.22  0.00  0.00  
J48+NB 0.89  0.17  0.07  0.00  0.07  0.02  0.87  0.18  0.89  0.94  
Set 2 
J48+J48 1.00  1.00  0.00  0.01  0.00  0.00  1.00  0.22  0.00  0.00  
J48+IBk 0.94  0.23  0.02  0.00  0.02  0.01  0.95  0.20  0.79  0.88  
Set 3 
NB+NB 1.00  1.00  0.00  0.07  0.00  0.00  1.00  0.18  0.00  0.00  
NB+J48 0.89  0.17  0.07  0.00  0.07  0.02  0.87  0.18  0.89  0.94  
Set 4 
NB+NB 1.00  1.00  0.00  0.07  0.00  0.00  1.00  0.18  0.00  0.00  
NB+IBk 0.82  0.20  0.07  0.01  0.07  0.02  0.86  0.17  0.80  0.89  
Set 5 
IBk+IBk 1.00  1.00  0.00  0.02  0.00  0.00  1.00  0.21  0.00  0.00  
IBk+J48 0.94  0.23  0.02  0.00  0.02  0.01  0.95  0.20  0.79  0.88  
Set 6 
IBk+IBk 1.00  1.00  0.00  0.02  0.00  0.00  1.00  0.21  0.00  0.00  
IBk+NB 0.82  0.20  0.07  0.01  0.07  0.02  0.86  0.17  0.80  0.89  
 
å¾é¡¯ç¤ºæ–¼è¡¨ 2å’Œè¡¨ 3çš„å¯¦é©—çµæœä¾†çœ‹ï¼Œä½¿ç”¨ç•°è³ªæ¼”ç®—æ³•çš„ç¢ºèƒ½å¢åŠ æˆå“¡åˆ†é¡å™¨
ä¹‹é–“çš„å·®ç•°æ€§ã€‚æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘æ¯”è¼ƒç•°è³ªå¤šåˆ†é¡å™¨ç³»çµ±å»ºç½®æ¼”ç®—æ³• (Bagging DT+NB) 
å’Œæ•¸å€‹å¸¸ç”¨çš„å¤šåˆ†é¡å™¨ç³»çµ±å»ºç½®æ¼”ç®—æ³•åœ¨å¯¦éš›è³‡æ–™é›†ä¸Šçš„åˆ†é¡éŒ¯èª¤ç‡ï¼Œå¦‚è¡¨ 4æ‰€
ç¤ºã€‚ 
 
è¡¨ 4ï¼šå¯¦éš›è³‡æ–™é›†çš„ç‰¹æ€§ï¼Œä»¥åŠç•°è³ªå¤šåˆ†é¡å™¨ç³»çµ±å»ºç½®æ¼”ç®—æ³• (Bagging DT+NB) 
å’Œæ•¸å€‹å¸¸ç”¨çš„å¤šåˆ†é¡å™¨ç³»çµ±å»ºç½®æ¼”ç®—æ³•åœ¨é€™äº›è³‡æ–™é›†ä¸Šçš„åˆ†é¡éŒ¯èª¤ç‡ï¼›ç²—é«”æ•¸å­—
è¡¨ç¤ºåœ¨ä¸€å€‹å¯¦éš›è³‡æ–™é›†ä¸Šçš„æœ€ä½åˆ†é¡éŒ¯èª¤ç‡ã€‚ 
è³‡æ–™é›† 
æ¨£æœ¬æ•¸ ç‰¹å¾µæ•¸ Bagging AdaBoost Random 
Forests 
DECORATE StackingC 
  DT+NB DT NB DT NB DT NB DT+NB 
Anneal_U 898 38 0.0212 0.0089 0.0913 0.0067 0.0223 0.0045 0.0089 0.0835 0.0089 
Biomed 209 8 0.0813 0.0861 0.0957 0.0861 0.1148 0.0909 0.1100 0.1053 0.0957 
14 
 
çµè«–èˆ‡å»ºè­°ï¼šä¸»æŒäººèªç‚ºï¼Œå”¯æœ‰å°å¤šåˆ†é¡å™¨ç³»çµ±çš„çµ„æˆå’Œé‹ä½œæœ‰é€šç›¤çš„ç†è§£ï¼Œ
åœ¨å¯¦å‹™ä¸Šæˆ‘å€‘æ‰èƒ½æ›´æœ‰æ•ˆåœ°è¨­è¨ˆå’Œæ‡‰ç”¨å¤šåˆ†é¡å™¨ç³»çµ±ï¼Œæ•…å»ºè­°åœ‹ç§‘æœƒèƒ½ç¹¼çºŒè£œåŠ©
é—œæ–¼å¤šåˆ†é¡å™¨ç³»çµ±çš„ç ”ç©¶ã€‚ 
åƒè€ƒæ–‡ç» 
 
1. Opitz, D. and Maclin, R. 1999. Popular ensemble methods: An empirical 
study. Journal of Artificial Intelligence Research, 11:169â€“198. 
2. Dietterich, T. 2000. Ensemble methods in machine learning. In Proc. 
of the 1st International Workshop on Multiple Classifier Systems 
(MCS) ,1-15. 
3. Polikar, R. 2006. Ensemble based systems in decision making. IEEE 
Circuits and Systems Magazine, 6 (3) :21-45. 
4. Ranawana, R. and Palade, V. 2006. Multi-classifier systems: Review 
and a roadmap for developers. International Journal of Hybrid 
Intelligent Systems, 3 (1) :35-61. 
5. Brown, G. 2010. Ensemble learning, Encyclopedia of Machine Learning. 
Springer. 
6. Rokach, L. 2010. Ensemble-based classifiers. Artificial Intelligence 
Review, 33 (1-2) :1â€“39. 
7. Kuncheva, L. 2004. Combining Pattern Classifiers: Methods and Algorithms. 
John Wiley and Sons. 
8. Melville, P., Shah, N., Mihalkova, L., and Mooney, R. 2004. Experiments 
on ensembles with missing and noisy data. In Proc. of the 5th 
International Workshop on Multiple Classifier Systems (MCS) , 293-302. 
9. Melville, P. and Mooney, R. 2003. Constructing diverse classifier 
ensembles using artificial training examples. In Proc. of the 
International Joint Conference on Artificial Intelligence (IJCAI) , 
505-510. 
10. Tsymbal, A., Pechenizkiy, M., and Cunningham, P. 2004. Diversity in 
random subspacing ensembles. In Proc. of the International Conference 
Data Warehousing and Knowledge Discovery (DaWaK) , 309-319. 
11. Kuncheva, L. and Whitaker, C. 2001. Ten measures of diversity in 
classifier ensembles: limits for two classifiers. In Proc. of the 
Workshop on Intelligent Sensor Processing, 1-10. 
12. Kuncheva, L. 2003. That elusive diversity in classifier ensembles. 
16 
 
24. Diplaris, S., Tsoumakas, G., Mitkas, P., and Vlahavas,I. 2005. 
Protein classification with multiple algorithms. In Proc. of the 10th 
Panhellenic Conference on Informatics (PCI) , 448-456. 
25. Bian, S. and Wang, W. 2007. On diversity and accuracy of homogeneous 
and heterogeneous ensembles. International Journal of Hybrid 
Intelligent Systems, 4 (2) :103-128. 
26. Jacobs, R., Jordan, M., Nowlan, S., and Hinton, G. 1991. Adaptive 
mixtures of local experts. Neural Computation, 3:79-87. 
27. Nowlan, S. and Hinton, G. 1991. Evaluation of adaptive mixtures of 
competing experts. In Proc. of the 1990 Conference on Advances in 
Neural Information Processing Systems 3 (NIPS-3) , 774â€“780. 
28. Jordan, M. and Xu, L. 1995. Convergence results for the EM approach 
to mixtures of experts architectures. Neural Networks, 8:1409â€“1431. 
29. Kuncheva, L., Whitaker, C., Shipp, C., and Duin, R. 2003. Limits on 
the majority vote accuracy in classifier fusion. Pattern Analysis and 
Applications, 6 (1) :22-31. 
30. Ruta, D. and Gabrys, B. 2005. Classifier selection for majority voting. 
Information Fusion, 6 (1) :63-81. 
31. He, H., Cao, Y., Wen, J., and Cheng, S. 2008. A boost voting strategy 
for knowledge integration and decision making. In Proc. of the 5th 
International Symposium on Neural Networks: Advances in Neural 
Networks (ISNN) , 472-481. 
32. Orrite, C., Rodriguez, M., Martinez, F., and Fairhurst, M. 2008. 
Classifier ensemble generation for the majority vote rule. In Proc. 
of the 13
th
 Iberoamerican congress on Pattern Recognition: Progress 
in Pattern Recognition, Image Analysis and Applications (CIARP) , 
340-347. 
33. Abreu, M. and Canuto, A. 2007. An experimental study on the importance 
of the choice of the ensemble members in fuzzy combination methods. 
In Proc. of the 7th International Conference on Intelligent Systems 
Design and Applications (ISDA) , 723-728. 
34. Zanda, M., Brown, G., Fumera, G., and Roli, F. 2007. Ensemble learning 
in linearly combined classifiers via negative correlation. In Proc. 
of the 7th International Workshop on Multiple Classifier Systems (MCS) , 
440-449. 
35. Brown, G., Yao, X., Wyatt, J., Wersing, H., Sendhoff, B. 2002. 
Exploiting ensemble diversity for automatic feature extraction. In 
Proc. of the 9th International Conference on Neural Information 
18 
 
48. Schapire, R. 1990. The strength of weak learnability. Machine 
Learning, 5:197-227. 
49. Breiman, L. 1996. Bagging predictors. Machine learning, 24 
(2) :123-140. 
50. Wolpert, D. 1992. Stacked generalization. Neural Networks, 5 
(2) :241-259. 
51. Seewald, A. 2002. How to make stacking better and faster while also 
taking care of an unknown weakness. In Proc. of the 19th International 
Conference on Machine Learning (ICML) , 554-561. 
52. Hsu, Kuo-Wei. 2012. Hybrid Ensembles of Decision Trees and Artificial 
Neural Networks. In IEEE International Conference on Computational 
Intelligence and Cybernetics (CYBERNETICSCOM) . 
53. Lu, Z., Wu, X., and Bongard, J. 2009. Active learning with adaptive 
heterogeneous ensembles. In Proc. of the 19th International Conference 
on Data Mining (ICDM) , 327-336. 
54. Breiman, L. 2001. Random forests. Machine learning, 45 (1) :5-32. 
55. Liao, Chia-Chi Liao, and Hsu, Kuo-Wei. A Rule-Based Classification 
Algorithm: A Rough Set Approach. In IEEE International Conference on 
Computational Intelligence and Cybernetics (CYBERNETICSCOM) . 
56. å»–å®¶å¥‡. 2012. ä»¥è¦å‰‡ç‚ºåŸºç¤çš„åˆ†é¡æ¼”ç®—æ³•ï¼šæ‡‰ç”¨ç²—ç³™é›†. ç¢©å£«è«–æ–‡,åœ‹ç«‹
æ”¿æ²»å¤§å­¸, è³‡è¨Šç§‘å­¸å­¸ç³». (å­¸ç”Ÿç•¢æ¥­è«–æ–‡)  
57. Lin, Hong-Che and Hsu, Kuo-Wei. 2012. An Empirical Study of Applying 
Data Mining Techniques to the Prediction of TAIEX Futures, in Proc. 
of IEEE International Conference on Granular Computing (GrC) -- 
International Symposium on Foundations and Frontiers of Data Mining 
(FFDM) , 334-339. 
58. Ting, K. and Witten, I. 1999. Issues in stacked generalization. 
Journal of Artificial Intelligence Research, 10:271â€“289. 
59. Kuncheva, L. and Whitaker, J. 2003. Measures of Diversity in 
Classifier Ensembles and Their Relationship with the Ensemble 
Accuracy. Machine Learning, 51 (2) , pp. 181-207. 
è¨ˆç•«æˆæœè‡ªè©• 
ç ”ç©¶å…§å®¹èˆ‡åŸè¨ˆç•«å¤§è‡´ç›¸ç¬¦ã€‚æœ¬è¨ˆç•«å…±æœ‰ä¸‰ç¯‡è«–æ–‡ç™¼è¡¨æ–¼å…©å€‹ IEEEåœ‹éš›æœƒ
è­°ã€‚ä¸»æŒäººä»¥è¨ˆç•«ç¶“è²»å‡ºå¸­å…¶ä¸­ä¸€å€‹åœ‹éš›æœƒè­°ä¸¦å£é ­å ±å‘Šå…¶ä¸­å…©ç¯‡è«–æ–‡ï¼Œå¦å¤–ï¼Œ
ä¸»æŒäººè‡ªè²»å‡ºå¸­å¦ä¸€åœ‹éš›æœƒè­°ä¸¦å£é ­å ±å‘Šå¦ä¸€ç¯‡è«–æ–‡ã€‚æœ¬è¨ˆç•«ä¹Ÿå¹«åŠ©ä¸€ä½ç¢©å£«ç­
20 
 
é™„éŒ„ï¼šå‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Šèˆ‡
å·²ç™¼è¡¨ä¹‹è«–æ–‡å…¨æ–‡ 
for Assistive Technologyï¼Œå…¶å…§å®¹è‘—é‡åœ¨å¦‚ä½•å°‡æ©Ÿå™¨å­¸ç¿’çš„æŠ€è¡“æ‡‰ç”¨æ–¼è¼”åŠ©å™¨å…·
çš„é–‹ç™¼ï¼ŒåŒ…æ‹¬æ™ºæ…§å±…å®¶ã€å°èˆªç³»çµ±ã€æç¤ºç³»çµ±ï¼Œé‚„æœ‰é€é BCI (brain-computer 
interface) æ§åˆ¶çš„è¼ªæ¤…ã€‚Michael Lightneræ•™æˆæ›¾ä»» IEEEä¸»å¸­ã€‚æ¥ä¸‹ä¾†æ˜¯éŸ“åœ‹ Seoul 
National University çš„ Byeong Gi Lee æ•™æˆçš„æ¼”è¬›ï¼Œè¬›é¡Œæ˜¯ Convergence of 
Communications towards Smart Eraï¼Œå…¶å…§å®¹åœ¨å›é¡§ä¸¦å±•æœ›é€šè¨Šç³»çµ±çš„ç™¼å±•ï¼Œä¸¦æ
å‡ºæœªä¾†çš„ç§‘æŠ€ç™¼å±•æ‡‰è¦å”åŠ©äººå€‘å»ºç«‹æ™ºæ…§ä¸”ç’°ä¿çš„ç¤¾æœƒï¼Œä½¿å¾—äººå€‘å¯ä»¥éè‘—æ™ºæ…§ä¸”
ç’°ä¿çš„ç”Ÿæ´»ã€‚Byeong Gi Leeæ•™æˆç‚º IEEE Fellowã€‚æœ€å¾Œæ˜¯åœŸè€³å…¶ Erciyes University
çš„ Mehmet Emin Yuksel æ•™æˆçš„æ¼”è¬›ï¼Œè¬›é¡Œæ˜¯ Digital Image Restoration Operators 
Based on Neuro-Fuzzy Techniquesï¼Œå…¶ä¸»è¦å…§å®¹æ˜¯åœ¨ä»‹ç´¹ç¥ç¶“æ¨¡ç³ŠæŠ€è¡“æ–¼å½±åƒè™•ç†
çš„æ‡‰ç”¨ï¼ŒåŒ…æ‹¬é›œè¨Šéæ¿¾ã€é›œè¨Šåµæ¸¬ï¼Œé‚„æœ‰é‚Šç·£æŠ½å–ã€‚Mehmet Emin Yukselç‚º IEEE Senior 
Memberã€‚ä¸‹åˆæ˜¯è«–æ–‡å ±å‘Šçš„ sessionã€‚æœ¬äººæ‡‰é‚€æ“”ä»»ä¸€å€‹ä»¥äººå·¥æ™ºæ…§ç‚ºä¸»é¡Œçš„ session
çš„ä¸»æŒäººï¼Œä¸¦å£é ­ç™¼è¡¨è«–æ–‡ä¸€ã€‚7æœˆ 13æ—¥ï¼Œæœ¬äººå†æ¬¡æ‡‰é‚€æ“”ä»»ä¸€å€‹ä»¥äººå·¥æ™ºæ…§ç‚ºä¸»
é¡Œçš„ session çš„ä¸»æŒäººï¼Œä¸¦å£é ­ç™¼è¡¨è«–æ–‡äºŒã€‚æœ¬æœƒè­°èˆ‡ 2012 IEEE International 
Conference on Communications, Network and Satellite (COMNETSTAT 2012) åˆ
è¾¦ï¼Œè€Œ 7/14 çš„ä¸»é¡Œç‚ºé€šè¨Šã€ç¶²è·¯ï¼Œä»¥åŠè¡›æ˜ŸæŠ€è¡“ã€‚æœƒè­°æ–¼ 7æœˆ 14æ—¥çµæŸã€‚ 
 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
 
é€™é›–ç„¶æ˜¯ä¸€å€‹æ–°çš„åœ‹éš›æœƒè­°ï¼Œä¸”åœ¨æŠ•ç¨¿ç³»çµ±ã€è¨»å†Šç³»çµ±ï¼Œä»¥åŠè­°ç¨‹å®‰æ’æ–¹é¢éƒ½é‚„æœ‰
å¾…æ”¹å–„ï¼Œä½†ä¸»è¾¦å–®ä½ä»æ˜¯å®‰æ’æ•¸å€‹è³‡æ·±ä¸”æœ‰åæœ›çš„å­¸è€…æ“”ä»»ä¸»é¡Œæ¼”è¬›çš„è¬›è€…ï¼Œè®“è³‡
æ·ºè€…å¦‚æœ¬äººå¾ä¸­ç²ç›Šè‰¯å¤šã€‚é€™å€‹æœƒè­°çš„åƒèˆ‡è€…å¤§å¤šä¾†è‡ªäºæ´²ï¼Œä½¿å¾—æœƒè­°çš„åœ‹éš›åŒ–ç¨‹
åº¦ä¸å¦‚é æœŸï¼Œä½†è‹¥è€ƒæ…®å¤§å¤šæ•¸äºæ´²åœ‹å®¶æ­£åœ¨å¿«é€Ÿç™¼å±•ï¼Œä¸”å°æ‡‰ç”¨ç§‘æŠ€æœ‰å¼·çƒˆçš„éœ€æ±‚ï¼Œ
 è«–æ–‡äºŒæ‘˜è¦ï¼šEnsemble learning is inspired by the human group decision making 
process, and it has been found beneficial in various application domains. 
Decision tree and artificial neural network are two popular types of 
classification algorithms often used to construct classic ensembles. 
Recently, researchers proposed to use the mixture of both types to construct 
hybrid ensembles. However, researchers use decision trees and artificial 
neural networks together in an ensemble without further discussion. The focus 
of this paper is on the hybrid ensemble constructed by using decision trees 
and artificial neural networks simultaneously. The goal of this paper is not 
only to show that the hybrid ensemble can achieve comparable or even better 
classification performance, but also to provide an explanation of why it 
works. 
 
å››ã€å»ºè­° 
 
æœ¬äººèªç‚ºæ­¤é¡æ–°èˆˆåœ‹éš›æœƒè­°æˆ–å¯å¹«åŠ©æ¨å»£è‡ªå·±çš„ç ”ç©¶çš„æ‡‰ç”¨ï¼Œä¾‹å¦‚æ›¿è‡ªå·±çš„ç ”ç©¶æ‰¾
åˆ°ä½¿ç”¨è€…ï¼Œä¸”èƒ½æå‡å°ç£çš„åœ‹éš›è²æœ›ï¼Œæ•…å¸Œæœ›æ—¥å¾Œä»æœ‰æ©Ÿæœƒç²å¾—åœ‹ç§‘æœƒè£œåŠ©åƒèˆ‡æ­¤
é¡æœƒè­°ã€‚ 
 
äº”ã€æ”œå›è³‡æ–™åç¨±åŠå…§å®¹ 
1. æœƒè­°è­°ç¨‹æ‰‹å†Šï¼Œå…§å«å„ä¸»é¡Œæ¼”è¬›çš„æ‘˜è¦èˆ‡è¬›è€…ç°¡æ­·ï¼Œé‚„æœ‰å„è«–æ–‡çš„é¡Œç›®ã€ä½œè€…ï¼Œ
  
Class = CHOOSE(ClassSet)  
 SEPERATE&CONQUER(Class,TrainData):  
RuleSet =âˆ…  
while POSITIVE(TrainData)â‰ âˆ…  
Rule=[nullâ†’Class]  
Covered=COVER(TrainData,Rule)  
while NEGATIVE(Covered)â‰ âˆ…  
GROW(Rule,Covered) 
Covered= COVER(Covered ,Rule)  
RuleSet=RuleSet âˆª{Rule}  
TrainData=TrainData \ Covered 
return RuleSet 
The rest of this paper is organized is the following way: 
Section 2 will give the preliminaries, and the proposed 
classification algorithm will be introduced in Section 3. The 
experimental results are presented in Section 4. The paper will 
be concluded in Section 5 with potential directions for future 
work. 
II. PRELIMINARY 
A. Rule-based classification algorithm 
Rule learning is to learn rules from the given training data, 
and a rule-based classification algorithm uses the learned rules 
to classify unseen data records. For classification, a decision 
rule is a logic statement with the following form: 
condition1 âˆ§ condition2 âˆ§â€¦â†’class, 
where a condition is usually an attribute-value pair indicating a 
certain value of a certain attribute that is required to trigger the 
condition. 
If a training data record matches all conditions of the rule, 
we say that the rule covers the data record; if the rule covers a 
data record and classify the data record to the right class, we 
say that the rule explains the data record. 
RIPPER [3] is a popular rule-based classification algorithm. 
It has two stages: The generation stage and the optimization 
stage. The classification algorithm proposed in this paper 
competes with it in the generation stage. 
B. The Rough Set Theory 
The rough set theory is first introduced by ZdzisÅ‚aw I. 
Pawlak in 1982 as a mathematical tool to characterize 
imprecise knowledge [11][12]. As shown in Figure 1 (a) and 
(b), the main difference between a rough set and a classic set is 
the appearance of a boundary â€œregionâ€ (not just a boundary), 
where the uncertain elements exist, in a rough set. The fuzzy 
set theory [17] is another tool to characterize imprecise 
knowledge. The main difference between a fuzzy set and a 
rough set is that a fuzzy set needs a membership function to the 
degree of an elementâ€™s participation in it. Practically speaking, 
such a membership function is defined under some 
assumptions and on a case-by-case basis. Nevertheless, a rough 
set needs no membership function, since the uncertain elements 
are located in the boundary region in a rough set. 
Given a decision table A=(U,CâˆªD), as shown in Figure 1 
(c), where U={x1,x2,x3,x4,x5,x6,x7,x8} is the universe or the 
training data, C={a1,a2} is the condition or the attribute set of 
the training data, and D={d} is the decision or the set of class 
labels of the training data. A rough set of d=y is shown in 
Figure 1 (d). Since there is no difference between the condition 
of x4 and that of x5, they are in the boundary region. 
For data records x1 and x2, if the values of all their attributes 
are the same, we say that x1 and x2 are indiscernible. The set of 
all data records including x that are indiscernible by the 
attribute set C is denoted by C(x). Below are definitions for Y 
corresponding to the set of d=y: 
The positive region of Y: 
POSA (Y) = {xâªC(x) âŠ† Y}. 
The negative region of Y:  
NEGA (Y) = {x|C(x) âˆ© Y=âˆ…}. 
The boundary region of Y: 
BOUNDA (Y) = Y âˆ’ POSA (Y) âˆ’ NEGA (Y). 
Figure 1.  (a) Classic set. (b) Rough set. (c) Decision table A=(U,CâˆªD). (d) 
Rough set of d=y. 
III. THE PROPOSED METHOD 
A.  Separate-and-Conquer 
The separate and conquer strategy first builds a rule that 
explains a part of the training data, separates them, and 
conquers the rest recursively until no data remain. It ensures 
that every data record is at least covered by one rule. Figure 2 
gives the separate and conquer algorithm, the core of the 
proposed classification algorithm in this paper. Before the 
algorithm begins, one of the classes is chosen. POSITIVE 
chooses the data that should be classify to the chosen class, and 
NEGATIVE chooses the others. Every Rule is empty in the 
beginning, and continues to grow until no negative data is 
covered by it. 
 
Figure 2.  The SEPERATE&CONQUER algorithm. 
 
 
 
 
 
 
 
 
          (a)    (b) 
 U a1 a2 d 
 x1 1 2 y 
x2 2 1 y 
x3 2 2 y 
x4 3 2 y 
x5 3 2 n 
x6 3 3 n 
x7 3 4 n 
x8 4 3 n 
           (c)    (d) 
negative 
positive 
negative 
boundary 
positive 
x6, x7, x8 
 x4, x5 
 x1, x2, x3 
2
 BUILD_CLASSIFIER( ):  
build a ClassList by ascending frequency or
for each Class in ClassList: 
RuleSet=SEPERATE&CONQUER(Clas
 concatenate the RuleSet to the bottom
return RuleList 
ROUSER generates set of rules for each cl
rule set is generated, it is concatenated to the b
list. The BUILD_CLASSIFIER algorithm 
shown in Figure 7. The class list is sorte
frequency order in order to prevent the 
distribution problem. For an unseen case, RO
down the rule list and uses the first rule that c
classify it. 
Figure 7.  The BUILD_CLASSIFIER alg
ROUSER ignores missing value when eva
The reason is simple: missing value cannot be 
rule. In some situations, the PotBound of an a
with all missing value, which means that this
valid value in the PotBound. This attribute sho
since missing value gives no power to discern 
IV. EXPERIMENT AND RESUL
In this section, we discuss experiment
ROUSER with ID3, J48 (an implementation o
JRip (an implementation of RIPPER [3]),
provided by WEKA [7]. 
TABLE I.  THE DATA SETS USED FOR EXP
UCI 
data 
set 
Data description 
Data name #instances #attributes Class dis
1 
 
audiology 
 
226 69 
2 
 
car evaluation 
 
1728 6 
3 
 
chess 
 
3196 36 
4 
 
mushroom 
 
8124 22 
5 
molecular biology: 
spice gene 
sequences 
3190 61 
6 
molecular biology: 
promotor gene 
sequences 
106 58 
7 
 
nersery 
 
12960 8 
8 
 
tic-tac-toe 
 
958 9 
9 
 
voting-records 
 
435 16 
der 
s,TrainData)  
 of the RuleList
ass. As soon as a 
ottom of the rule 
of ROUSER is 
d by ascending 
imbalance class 
USER searches 
overs the case to 
orithm. 
luating DiscPow. 
chosen to grow a 
ttribute are filled 
 attribute has no 
uld be removed, 
data. 
TS 
s that compare 
f C4.5 [14]) and 
 which are all 
ERIMENTS. 
tribution missing value 
yes 
no 
no 
yes 
no 
no 
no 
no 
yes 
The data sets used for experim
UCI Machine Learning Repository 
They are collected from different ap
biology, gaming, politics, and mark
attributes ranges from 6 to 69; th
ranges from 2 to 24; for some of th
are imbalanced; and some data sets 
some attributes. 
We use 10-fold cross-valid
classification performance. The 
summarized in Table II. The numb
accuracy rates in percentage. The 
ROUSER provides comparable 
performance. Some results for ID3 
limitations. On two data sets, JRip
but it is outperformed by ROUSER
that ROUSER performs well on da
attributes and data sets where t
imbalanced. However, ROUSER do
data sets 2 (car evaluation) and 5
think that there is no optimization st
is in RIPPER) to reduce errors a
deeper investigation of this will b
Furthermore, ROUSER does not 
expected on the data sets 1 (audiolo
We think that ROUSER needs a mo
handle missing values. An improvem
the future work. 
TABLE II.  EXPERIMENT RESULT
UCI 
data set 
Classific
ROUSER 
1 71.0 
2 84.1 
3 99.3 
4 100.0 
5 82.0 
6 85.8 
7 98.8 
8 96.9 
9 94.5 
V. CONCLUSIONS AND
A rule-based classification algo
proposed. It is designed to process 
human understandable decision rul
set approach as its search heuristi
method of ROUSER is based on
strategy. 
As a prototype without the 
pruning stage to reduce errors, 
classification performance compa
accurate on the data sets 3, 4, 7, a
least 4% more accurate on the data s
rule-based or tree-based classificatio
experiments. Since the search heuri
different from the search heuristics
Gain) used by the other three algori
the proposed PotBound and DiscP
ents are all available from 
[18], as shown in Table I. 
plication domains, such as 
eting; the number of their 
e number of their classes 
em, the class distributions 
are with missing values on 
ation to evaluate the 
experimental results are 
ers reported in Table II are 
numbers are highlighted if 
or better classification 
are not available due to its 
 outperforms J48 and ID3, 
. The results also indicate 
ta sets that are with more 
he class distributions are 
es not perform well on the 
 (molecular biology). We 
age in ROUSER (but there 
nd over fitting occurs. A 
e part of the future work. 
perform as well as we 
gy) and 9 (voting-records). 
re sophisticated strategy to 
ent on this will be part of 
S IN ACCURACY RATES (%). 
ation algorithms
J48 ID3 JRip 
77.0 x 74.0 
92.4 89.4 86.7 
99.4 99.7 99.2 
100.0 x 100.0 
94.4 89.5 94.1 
81.1 76.4 78.3 
97.1 98.2 97.0 
85.1 83.2 97.8 
96.3 x 96.1 
 FUTURE WORK 
rithm named ROUSER is 
nominal data and generate 
es. ROUSER uses a rough 
c, and the rule generation 
 the separate-and-conquer 
optimization stage or the 
ROUSER still provides 
rable (2% more or less 
nd 9) to or even better (at 
et 6) than that given by the 
n algorithms considered in 
stics of ROUSER is totally 
 (Entropy and Information 
thms, the results imply that 
ow are useful. This also 
4
Hybrid Ensembles of Decision Trees and Artificial
Neural Networks
Kuo-Wei Hsu
Department of Computer Science
National Chengchi University
Taipei 11605, Taiwan (R.O.C.)
Email: hsu@cs.nccu.edu.tw
Abstractâ€”Ensemble learning is inspired by the human group
decision making process, and it has been found beneficial in
various application domains. Decision tree and artificial neural
network are two popular types of classification algorithms often
used to construct classic ensembles. Recently, researchers pro-
posed to use the mixture of both types to construct hybrid ensem-
bles. However, researchers use decision trees and artificial neural
networks together in an ensemble without further discussion. The
focus of this paper is on the hybrid ensemble constructed by using
decision trees and artificial neural networks simultaneously. The
goal of this paper is not only to show that the hybrid ensemble
can achieve comparable or even better classification performance,
but also to provide an explanation of why it works.
Index Termsâ€”Machine learning, classification, neural nets
I. INTRODUCTION
For classification tasks, an ensemble trains and uses a group
of member classifiers. Given a data record whose class label
is unknown, an ensemble will use its member classifiers to
generate individual classifications and then aggregates these
classifications to generate an overall classification. Ensemble
learning is inspired by the human group decision making
procedure.
Diversity among member classifiers plays a significant role
in the success of most ensembles. Some ensemble construction
approaches use sampling techniques to generate different data
sets for training and further diverse member classifiers; some
use different classification algorithms to create diverse member
classifiers, and we can view these ensemble construction
approaches as the ones that transform heterogeneity between
classification algorithms into diversity between member clas-
sifiers.
We focus on ensembles that are variants of bagging (boot-
strap aggregating) [1], a popular ensemble construction ap-
proach that uses the bootstrap procedure to manipulate the data
set for training and further to create diverse member classifiers
[2]. Decision tree and artificial neural network are the favorite
classification algorithms in the machine learning community,
and they are often used to construct ensembles [3].
Ensembles of decision trees have been applied to various
problems in many application domains. Sun uses ensembles
of decision trees in pitch accent prediction [4]. Tong et al.
apply a combination of multiple decision trees to the problem
of classifying a new chemical [5]. MareÂ´e et al. use ensembles
of decision trees along with another technique in biomed-
ical image classification [6]. Ensembles of artificial neural
networks have been applied to various problems in many
application domains, too. Giacinto and Roli apply them to the
image classification problem [7]. Yao et al. apply them to the
telecommunication traffic flow prediction problem [8]. Zhou
et al. apply them to the lung cancer cell identification problem
[9]. Shu and Burn study ensembles of artificial neural networks
in pooled flood frequency analysis [10]. Maqsood study them
in weather forecasting [11]. West et al. apply them to the
financial decision problem [12]. GuÂ¨lera and Â¨UbeylÄ±b apply
them to the electrocardiogram beats classification problem
[13].
Recently, researchers proposed the use of the mixture of
these two types of classification algorithms to construct hy-
brid ensembles (and the those mentioned earlier are called
classic or non-hybrid ensembles). Lu and Bongard start from
an observation that â€œsome classifiers are more suitable for
different data sets than othersâ€ and propose an algorithm to
train an ensemble composed of decision trees and artificial
neural networks [14]. Lu et al. discuss sampling techniques
along with ensembles composed of decision trees and artificial
neural networks for active learning [15]. Hsu and Srivastava
demonstrate that ensembles composed of decision trees and
artificial neural networks together could outperform ensembles
composed of either of them alone [16]. However, researchers
use decision trees and artificial neural networks together in
an ensemble without further discussion. If we additionally
consider hybrid ensembles composed of other types of classi-
fication algorithms, we can find various applications of hybrid
ensembles, such as credit scoring [17], risk assessment [18],
medical databases [19], activity recognition [20], and energy
consumption forecasting [21]. The insights gained from this
paper can have a potential impact on these applications.
The rest of this paper is organized as follows: Section II
gives a the analysis based on bias-variance decomposition.
Section III describes experimental settings and reports exper-
imental results. Section IV presents the discussion. Section V
concludes this paper and suggests possible extensions of this
paper.
978-1-4673-0890-8/12/$31.00 Â©2012 IEEE 25 COMNETSAT 2012
are accurate or more member classifiers that are less accurate
by using a classification algorithm with high bias, we can
have an ensemble achieving low error rate or high accuracy
(as the member classifiers work together) if the classification
algorithm used by us is also with high variance or if the
member classifiers built using the classification algorithm are
diverse. In such a case, when some member classifiers make an
error, others diverse or different from these member classifiers
would have a chance to correct the error so that the ensemble
composed of them would generate a correct classification. This
holds when noise appears in the data set, since noise may
â€œaccidentallyâ€ correct an error made by some or even most
member classifiers.
III. EXPERIMENTS
Algorithm 1 presents the construction of a hybrid ensemble
of decision trees and artificial neural networks, and it is
implemented by modifying the implementation of bagging
[1] provided by WEKA [23]. In experiments, we compare it
with classic ensembles of decision trees or artificial neural
networks. These classic ensembles are constructed by using
the implementation of bagging [1] provided by WEKA [23]
without modifications. We use the implementation of the
C4.5 algorithm [24] (with default parameters) provided by
WEKA [23] to build a decision tree. Similarly, we use the
implementation of the multilayer perceptron algorithm (with
default parameters) provided by WEKA [23] to build an
artificial neural network.
Input: ğ· is the given data set for training, and ğ‘ is the
number of member classifiers
Output: ğ¸ is the resulting ensemble, i.e. a set of
member classifiers
initialize ğ¸ = âˆ…;
for ğ‘– = 1; ğ‘– â‰¤ ğ‘ do
ğ‘†ğ‘– â† Bootstrap(D);
if ğ‘– is even then
specify ğ´ğ‘– as a decision tree algorithm;
else
specify ğ´ğ‘– as an artificial neural network
algorithm;
end
ğ¶ğ‘– â† BuildClassifier(ğ‘†ğ‘–, ğ´ğ‘–);
ğ¸ â† ğ¸ âˆª ğ¶ğ‘–;
end
return ğ¸;
Algorithm 1: The construction of a hybrid ensemble of
decision trees and artificial neural networks.
In Algorithm 1, Bootstrap() performs sampling with re-
placement. Bootstrap is a procedure that replaces an unknown
distribution with a known distribution to compute statistics
in which we are interested. When performing classification
tasks in real-world applications, we hardly know the under-
lying distribution but have the distribution of observed or
collected samples. We can also use the bootstrap procedure
when training samples are limited or when we use ensem-
ble approaches that require diverse training samples drawn
from different distributions. Furthermore, in Algorithm 1,
BuildClassiï¬er() constructs a classifier ğ¶ğ‘– using the given
training set ğ‘†ğ‘– and the specified classification algorithm ğ´ğ‘–,
and ğ¶ğ‘– will be a member classifier in the resulting ensemble.
What makes Algorithm 1 different from bagging is the if-then-
else statement. Algorithm 1 alternately selects one between
the decision tree algorithm and the artificial neural network
algorithm, but does not bagging. Algorithm 1 is simple but
we find that it yields good results. Similar findings are also
reported by others, as mentioned earlier.
All the data sets considered in experiments are available on
the web 1, and most are derived from the data sets available
on the UC Irvine Machine Learning Repository [25]. Every
data set is either originally for a binary classification task or
transformed from a multi-class classification task into a binary
one. Table I summarizes the characteristics of the data sets
considered in experiments. In Table I, each row is for a data
set, while the first column is its name, the second column is
its number of data records, the third column is its number
of attributes, and the fourth column is the percentage of the
majority class of the data set.
TABLE I
THE SUMMARY OF DATA SETS.
data set size dim. maj. (%)
sonar 208 60 53
ionosphere 351 34 64
colic 368 22 63
vote 435 16 61
credit 490 15 56
boston 506 13 74
hprice 546 11 50
credit-a 690 15 56
credit-g 1000 20 70
halloffame 1340 16 91
Table II summarizes experimental results 2. The values
of bias and variance (var) are given by the bias-variance
decomposition method [26], provided by WEKA [23], with
the number of iterations set to 100 and the percentage of
data for training set to 50. In Table II, each row reports the
result of applying a classification model (a single classifier
or an ensemble of classifiers) to a data set, the first column
is the name of the data set, the second column is the name
of the classification model, the third column is the value of
bias, the fourth column is the value of variance, and the fifth
column is the error rate. Notations for the classification models
considered in experiments are as follows: DT is for a single
decision tree, MLP is for a single artificial neural network,
B-DT is for a classic ensemble of ten decision trees, B-MLP
is for a classic ensemble of ten artificial neural networks, and
B-DT+MLP is a hybrid ensemble of five decision trees and
1http://tunedit.org/
2The details are omitted due to the page limit.
27
Group 1 in which we replace MLP with DT, then we introduce
DT with lower variance into the ensemble, and then the error
rate is lower when variance is lower according to Equation 1.
If there is noise, ğ‘ > 0, and if we think this group closer to
Group 2 in which we replace DT with MLP, then we introduce
MLP with higher variance into the ensemble of DT, and then
the error rate is lower when there is noise and variance is
high according to Equation 2. This is what credit-g in Table
III shows us.
Group 4. B-DT, B-MLP, and B-DT+MLP perform equally
well. If we think that this group closer to Group 1 where DT
is introduced into an ensemble of MLP, and if the introduced
DT is with high bias but the same variance, as shown to us by
hprice in Table III, then we speculate that noise appears but
has surprisingly positive effect according to Equation 2. If we
think this group closer to Group 2 where MLP is introduced
into an ensemble of DT, and if the introduced MLP is with
low bias but the same variance, as shown to us by halloffame
in Table III, then more test data records are made correctly
classified by MLP but their positive effect is shifted by noise
according to Equation 1. No matter we think that this group
closer to Group 1 or Group 2, if the introduced DT or MLP
is with the same bias and the same variance, as what happens
on halloffame, then we speculate that noise appears but has
surprisingly positive effect according to Equation 2.
V. CONCLUSIONS
In this paper, we study the hybrid ensemble constructed
by using decision trees and artificial neural networks si-
multaneously. Ensemble learning is to construct a classifier
composed of several classifiers, and it has been found bene-
ficial by researchers and practitioners. Both decision tree and
artificial neural network are popular types of classification
algorithms often used to construct classic ensembles. Re-
cently, researchers proposed to use the combination or mixture
of these two types of classification algorithms to construct
hybrid ensembles. However, researchers use decision trees
and artificial neural networks together in ensembles without
further discussion. The goal of this paper is to have a better
understanding of the hybrid ensemble. We not only show that
the hybrid ensemble can achieve comparable or even better
classification performance but we also provide an explanation
of why it works. As part of the future work, we plan to
investigate the hybrid ensemble composed of classifiers other
than decision trees and artificial neural networks. We also plan
to apply the analysis to other types of ensembles.
ACKNOWLEDGMENT
The National Science Council of Taiwan (R.O.C.) supported
this work under Grant NSC 100-2218-E-004-002. The support
is gratefully acknowledged. The author would also like to
thank anonymous reviewers for their valuable time.
REFERENCES
[1] L. Breiman, â€œBagging predictors,â€ Machine learning, vol. 24, no. 2, pp.
123â€“140, 1996.
[2] P. Tan, M. Steinbach, V. Kumar et al., Introduction to data mining.
Pearson Addison Wesley Boston, 2006.
[3] J. Ghosh, â€œMulticlassifier systems: Back to the future,â€ in Proc. of
International Workshop on Multiple Classifier Systems, vol. 3, 2002,
p. 1.
[4] X. Sun, â€œPitch accent prediction using ensemble machine learning,â€
in Seventh International Conference on Spoken Language Processing,
2002.
[5] W. Tong, H. Hong, H. Fang, Q. Xie, and R. Perkins, â€œDecision
forest: Combining the predictions of multiple independent decision tree
models,â€ J. Chem. Inf. Comput. Sci, vol. 43, pp. 525â€“531, 2003.
[6] R. MareÂ´e, P. Geurts, J. Piater, and L. Wehenkel, â€œBiomedical image
classification with random subwindows and decision trees,â€ Computer
Vision for Biomedical Image Applications, pp. 220â€“229, 2005.
[7] G. Giacinto and F. Roli, â€œDesign of effective neural network ensembles
for image classification purposes,â€ Image and Vision Computing, vol. 19,
no. 9-10, pp. 699â€“707, 2001.
[8] X. Yao, M. Fischer, and G. Brown, â€œNeural network ensembles and their
application to traffic flow prediction in telecommunications networks,â€
in Proc. of International Joint Conference on Neural Networks, vol. 1,
2001, pp. 693â€“698.
[9] Z. Zhou, Y. Jiang, Y. Yang, and S. Chen, â€œLung cancer cell identification
based on artificial neural network ensembles,â€ Artificial Intelligence in
Medicine, vol. 24, no. 1, pp. 25â€“36, 2002.
[10] C. Shu and D. Burn, â€œArtificial neural network ensembles and their ap-
plication in pooled flood frequency analysis,â€ Water Resources Research,
vol. 40, no. 9, p. W09, 2004.
[11] I. Maqsood, M. Khan, and A. Abraham, â€œAn ensemble of neural
networks for weather forecasting,â€ Neural Computing & Applications,
vol. 13, no. 2, pp. 112â€“122, 2004.
[12] D. West, S. Dellana, and J. Qian, â€œNeural network ensemble strategies
for financial decision applications,â€ Computers & operations research,
vol. 32, no. 10, pp. 2543â€“2559, 2005.
[13] I. GuÂ¨lera and E. Â¨UbeylÄ±b, â€œEcg beat classifier designed by combined
neural network model,â€ Pattern Recognition, vol. 38, pp. 199â€“208, 2005.
[14] Z. Lu and J. Bongard, â€œExploiting multiple classifier types with active
learning,â€ in Proc. of Annual Conference on Genetic and Evolutionary
Computation, 2009, pp. 1905â€“1906.
[15] Z. Lu, X. Wu, and J. Bongard, â€œAdaptive informative sampling for active
learning,â€ in Proc. of SIAM International Conference on Data Mining,
2010, pp. 894â€“905.
[16] K. Hsu and J. Srivastava, â€œAn empirical study of applying ensembles of
heterogeneous classifiers on imperfect data,â€ New Frontiers in Applied
Data Mining, pp. 28â€“39, 2010.
[17] N. Hsieh and L. Hung, â€œA data driven ensemble classifier for credit
scoring analysis,â€ Expert Systems with Applications, vol. 37, no. 1, pp.
534â€“545, 2010.
[18] G. Wang and J. Ma, â€œA hybrid ensemble approach for enterprise credit
risk assessment based on support vector machine,â€ Expert Systems with
Applications, 2011.
[19] B. Verma and S. Hassan, â€œHybrid ensemble approach for classification,â€
Applied Intelligence, vol. 34, no. 2, pp. 258â€“278, 2011.
[20] J. Min and S. Cho, â€œActivity recognition based on wearable sensors
using selection/fusion hybrid ensemble,â€ in Proc. of IEEE International
Conference on Systems, Man, and Cybernetics, 2011, pp. 1319â€“1324.
[21] L. Tang, L. Yu, S. Wang, J. Li, and S. Wang, â€œA novel hybrid ensemble
learning paradigm for nuclear energy consumption forecasting,â€ Applied
Energy, 2012.
[22] P. Domingos, â€œA unified bias-variance decomposition for zero-one and
squared loss,â€ in Proc. of National Conference on Artificial Intelligence,
2000, pp. 564â€“569.
[23] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. Witten, â€œThe weka data mining software: an update,â€ ACM SIGKDD
Explorations Newsletter, vol. 11, no. 1, pp. 10â€“18, 2009.
[24] R. Quinlan, C4.5: Programs for Machine Learning. San Mateo, CA:
Morgan Kaufmann Publishers, 1993.
[25] A. Frank and A. Asuncion, â€œUCI machine learning repository,â€ 2010.
[Online]. Available: http://archive.ics.uci.edu/ml
[26] R. Kohavi and D. Wolpert, â€œBias plus variance decomposition for zero-
one loss functions,â€ in Proc. of International Workshop on Machine
Learning, 1996, pp. 275â€“283.
29
â€¢ Being ready to generate results at any point of time. 
We consider the TAIEX Futures data as streaming data 
because of the following reasons. First, every one of its 
transaction records should be referenced only once, because a 
transaction would not be traded under the same condition at 
two time points. Second, we are not able to save all trading 
information in the memory. In other words, the memory space 
is not unbounded. Third, we want to predict the rising or falling 
of price at any time point. Consequently, we have sufficient 
reasons to support that one should treat the TAIEX Futures data 
as streaming data (and further that one should use data stream 
mining techniques on the TAIEX Futures data). 
Moreover, an important issue that a data stream mining 
technique is expected to handle is concept drift. Concept means 
the underlying data distribution, and in most cases in the real 
world it is not always fixed but usually changes (or â€œdriftsâ€) as 
time passes. As a result, we have sufficient reasons to assume 
that there probably exists concept drift in a stream of TAIEX 
Futures data (and accordingly that we should use a method that 
can detect concept drift). Concept drift will cause the 
decreasing of the accuracy of a mining model, or a 
classification model in this paper, built using techniques 
developed under the assumption of the fixed data distribution. 
In order to deal with this issue, we employ a data stream 
mining toolkit, Massive Online Analysis, abbreviated as MOA 
[2], designed for data streaming mining and extended from a 
widely employed data mining toolkit, Waikato Environment 
for Knowledge Analysis, abbreviated as WEKA [3]. Our 
findings could be concluded as follows:  
â€¢ The performance of data stream mining techniques or 
algorithms with the concept drift detection method 
(DDM) is significantly better than those without DDM. 
â€¢ Complex estimators do not necessarily correspond to 
better performance. 
In the following sections, we first discuss studies related to 
applying data mining techniques on stocks or futures, and we 
give an overview of methods proposed by others. Next, we 
introduce the steps and methods used in experiments. Then, we 
discuss our experimental results. Finally, we give conclusions 
and discuss potential directions for future work. 
II. RELATED WORK 
In this section, we briefly review related work and give a 
comparison of related work, as shown in Table I. In Table I, a 
column represents a paper related to the topic of this paper and 
a row represents a data mining technique. A cell of the table 
indicates that if the paper corresponding to a column uses the 
method corresponding to a row. If yes then it is marked by â€œVâ€. 
For example, the cell in the upper right corner indicates that the 
paper [4] uses non-streaming decision tree (and so do the 
papers [5] and [6]) and the second right cell in the second row 
indicates that the paper [9] uses sequential pattern mining. For 
decision tree, we listed various versions used in related work. 
According to Table I, we can see that the popular methods 
include decision tree, Support Vector Machine (SVM), and 
neural network. 
 
TABLE I.  A COMPARISON OF RELATED WORK. 
Methods\papers 
Non-streaming streaming 
[4] [5] [6] [7] [8] [9] This paper 
Decision tree V1 V2 V1    V3 
Sequential 
pattern mining      V
4  
Adaboost     V   
SVM  V      
Neural network  V      
Fuzzy-based rule    V    
k-means   V V    
Association rule   V     
Note: 1. C5.0 Tree; 2. Unknown (not specified by the authors); 3. Hoeffding Tree; 4. ICspan, IAspam. 
A. Non-streaming methods 
Data of stocks or futures is naturally streaming data. When 
we use data mining techniques designed for non-streaming data, 
it is important to transform a data stream into a set of data 
samples by using a time frame. 
In [4], 5-minute time frame is selected for TAIEX Futures 
forecasting. The paper uses C5.0 decision tree with two models: 
One is trained for rising or waiting, and it is named rising 
model; the other is trained for falling or waiting, and it is 
named falling model. These two models can be used with 
majority vote. To use majority vote, it would need a decision 
matrix, as the one shown in Table II. The row of the table is the 
decision of rising model, and the column is the decision of 
falling model. The cell of table is the joint decision given by 
majority vote.  
TABLE II.  DECISION MATRIX USED IN PAPER [4]. 
Rising\Falling Waiting Falling 
Waiting Waiting Falling 
Rising Rising Waiting 
 
The paper [5] compares methods for forecasting Taiwan IC 
Stocks. It compares SVM, SVM with Genetic Algorithm (GA-
SVM), decision tree, and Back-Propagation Network (BPN). 
Differing from other papers, it trains a model by financial 
indexes of accounting rather than the records of transactions.  
The paper [6] uses data mining techniques such as 
classification, clustering, and association rule to forecast the 
trend of stocks in the future. It analyzes the financial account of 
a company, and in order to examine whether the stock of a 
company is worth to hold, it uses C5.0 decision tree to train 
models. 
The paper [7] uses TSK fuzzy model for forecasting rather 
than classifying the TAIEX. Combining clustering method, 
TSK fuzzy model is better than BPN and multiple regression 
analysis for forecasting TAIEX in the paper. The data types in 
the paper are technical indexes which are statistical or 
calculated information of transactions. Because there are many 
technical indexes, factor selection is a problem discussed in the 
paper. Using the groups of data clustered by K-means, the 
paper sets up simplified fuzzy rules and trains the parameters.  
335
A. Prepairing Data  
TAIEX Futures data used in experiments, downloaded from 
Taiwan Stock Exchange Corporation web, includes transaction 
between January 1st in 2000 and December 31th in 2011, and 
the size of the time frame is set to 1 day. In order to train a 
model that can be used in the future, we remove all absolute 
timestamps and instead use relative timestamps.  
Labeling also becomes a problem since we do not have real 
settlement values for futures. So, we label each record with a 
closest value. Furthermore, to avoid confusion, we remove 
instances having 0 on the volume attribute. 
The attributes of data are listed below: 
â€¢ Relative Timestamp: The number of months from the 
trading date to the due date. 
â€¢ Open Value: The open price of a trading day. 
â€¢ Close Value: The close price of a trading day. 
â€¢ Highest and Lowest Value: The highest and lowest 
price traded in a trading day. 
â€¢ Volume: How many transactions of a trading day. 
â€¢ Settlement Value: The settlement price of a trading day. 
â€¢ Remains: How many remaining contracts in a trading 
day. 
â€¢ Last Buy: The last price to buy at close time in a 
trading day. 
â€¢ Last Sell: The last price to sell at close time in a trading 
day. 
â€¢ Trading Time: In the format of Year/Month/Day. 
â€¢ Class: Higher or lower, compared to the close price of 
the final settlement day with the close price of a trading 
day. 
Currently we select attributes by heuristics, and using some 
feature selection algorithms or statistical approaches will be 
part of our future work. Furthermore, we remove attributes that 
are directly related to the class label. For example, we use 
Close Value in the settlement day to tag the class label, so we 
remove it in the experiments. We also select attributes through 
experiments. Consequently, after testing some subsets of these 
attributes, we decide to use only Relative Timestamps, Open 
Value, Highest Value, Lowest Value, Volume, Remains, and 
Class.  
The distributions of attributes are shown in Figure 2. Each 
subfigure in Figure 2 corresponds to an attribute. In the bottom 
is the attribute name. The x-axis is the set of values of the 
attribute, and the number in the top in each subfigure is the 
number of instances of the corresponding attribute value  
The number of instances of each class is similar. We 
observe that the attributes Volume and Remains have 
imbalanced distributions with respect to Class (or the target 
class label).   
 
 
Figure 2.  Distribution of each atrributes: 
V1 = 1-6409, V2 = 134589-140998, V3 = 275587-281996, 
 R1 = 4-3677, R2 = 36743-40417, and R3 = 77156-80830. 
B. Massive Online Analysis 
Massive Online Analysis3, MOA, is proposed by A. Bifet et 
al [2]. It is based on Waikato Environment for Knowledge 
Analysis4, WEKA [3], and written in Java5 (an object-oriented 
and cross-platform programming language), and it supports a 
Graphical User Interface (GUI) and a command line interface. 
MOA offers a large set of functions to control the data mining 
process for data streams. For evaluation, MOA offers 
Prequential Evaluation, which tests each instances first and 
then trains a model using them; and it also offers Periodic Held 
Out Test, which tests instances by a (static) trained model and 
outputs results for the evaluation period. For controlling data, 
MOA includes artificial data generation functions and, of 
course, it supports the use of data provided by users. 
C. Result of experiment 
In the experiment, we classify instances with the default 
settings. For Hoeffding Tree, the settings include 95% 
confidence, information gain and 200 instances for grace 
period. Additionally, 30 instances ignored for DDM.  
We mainly use Hoeffding tree for evaluation and use NaÃ¯ve 
Bayes as the control group. One of the reasons that we consider 
NaÃ¯ve Bayes is that it is one of the most popular algorithms in 
the field of data mining [14] and it could be used in data stream 
mining. Furthermore, another major reason of using NaÃ¯ve 
Bayes as the control group is that there are many references, 
such as [15][16][17][18], where NaÃ¯ve Bayes is used in finance 
data mining. Nevertheless, using other algorithms as the control 
group will be part of our future work. 
We present experimental results in Figures 3-6, each of 
which uses accuracy in percentage as the y-axis and 
timestamps as the x-axis. It is shown in Figure 3 that the three 
algorithms achieve the average accuracy higher than 50%. We 
also could find that Hoeffding Tree and Hoeffding Adaptive 
                                                           
3 http://moa.cs.waikato.ac.nz/ 
4 http://www.cs.waikato.ac.nz/ml/weka/ 
5 http://www.java.com 
337
There still is an interesting observation that there is a 
significant decrease and increase between the 3rd season of 
2006 and the 3rd season of 2009 for NaÃ¯ve Bayes. It was the 
time during financial tsunami. However, this phenomenon does 
not occur if the algorithm with DDM is used. Furthermore, 
there might be a relationship between accuracy and the trend of 
the stock. The bottom of accuracy of NaÃ¯ve Bayes without 
DDM is in the 3rd season of 2007, in which the stock began to 
decrease heavily, and the stock began to increase when 
accuracy had increased to the same level which prior to the 
decrease. 
V. CONCLUSIONS 
The importance of extracting useful knowledge from 
massive data is undeniable. Data miming has been one of 
popular research fields, and recently data stream mining has 
become a popular subfield of data mining since in the real 
world data usually comes in a form of a stream. In this paper, 
we present our study of applying data stream mining 
techniques to the prediction of the rising or falling of the short-
term futures of Taiwan Stock Exchange Capitalization 
Weighted Stock Index Futures (TAIEX Futures). First, we give 
an overview of related studies. Next, we model the problem as 
a binary classification problem and then describe the toolkit, 
data pre-processing steps, and methods for experiments. Finally, 
we observe from the experimental results that using the drift 
detection method has significant impact on accuracy. Therefore, 
we assume that there exists concept drift in the Futures market. 
In this paper, we also see that applying data stream mining 
techniques on data from the futures market is useful and worth 
further investigation. 
There are many possible directions to extend the work 
presented in this paper. First, we use only transaction records 
for classification. However, more data sources may help us find 
better results. Hence, we can combine data from multiple 
sources to extend the base of data samples. Furthermore, 
feature selection will become more important when multiple 
data sources are considered. Additionally, how to retrieve 
association of different attributes of various data sources is a 
practical problem. Of course we can alternatively model the 
problem as a multi-class classification problem or a regression 
problem. Nevertheless, there still is an issue on which we can 
concentrate -- how to explain the associations between the time 
points of concept drift observed from experimental results and 
the trend of the stock in the real world. 
ACKNOWLEDGMENT 
The National Science Council of Taiwan (R.O.C.) 
supported the work presented in this paper under Grant NSC 
100-2218-E-004-002. The support is gratefully acknowledged. 
The authors would also like to thank anonymous reviewers for 
their precious time and valuable comments. 
REFERENCES 
[1] Handling Concept Drift: Importance, Challenges & Solutions, PAKDD
Ì2011 Tutorial. http://www.cs.waikato.ac.nz/~abifet/PAKDD2011/ 
[2] Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA 
- Massive Online Analysis. Journal of Machine Learning Research 11. 
[3] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. 
Witten, "The WEKA data mining software: an update," SIGKDD 
Explor. Newsl., vol. 11, pp. 10-18, 2009. 
[4] Data mining techniques to identify the direction of Taiwan Stock Index 
Futures day trading. Shih Hsiao Cheng. PhD Thesis, Department of 
Financial Engineering and Actuarial Mathematics of Soochow 
University. 2011. (in Chinese) 
[5] Application of Data Mining Technologies for IC Stock Category. Chia-
Hsien Chiu, Zne-Jung Lee. Digital Technology Information 
Management. 2009. (in Chinese 
[6] Data Mining for Analysis of Choosing Stocks from Taiwan Stock 
Market. 2009 International Conference on Advanced Information 
Technologies (AIT). (in Chinese 
[7] A TSK type fuzzy rule based system for stock price prediction. Pei-
Chann Chang , Chen-Hao Liu. Department of Information Management 
& Department of Industrial Engineering and Management, Yuan-Ze 
University. 2008, Expert Systems with Applications. 
[8] Using Adaboost for Taiwan Stock Index Future Intraday Trading System. 
Tien-Nan Lin. Master Thesis, Graduae Institute of Network and 
Multimedia college of Electrical Engineering and computer Science, 
National Taiwan University. 2008. (in Chinese) 
[9] Application of Multiple Data Streams Sequential Pattern Mining on 
Taiwan Stock Market. Ching-Ming Chao, Huei-Wen Yang. Journal of 
Information Management, Vol. 12, No.2, June. 2010. (in Chinse 
[10] Pedro Domingos and Geoff Hulten. Mining high-speed data streams. In 
Knowledge Discovery and Data Mining, pages 71â€“80, 2000. 
[11] Albert Bifet and Ricard Gavald`a. Adaptive parameter-free learning 
from evolving data streams. In 8th International Symposium on 
Intelligent Data Analysis, 2009. 
[12] Holmes, G., Kirkby, R., Pfahringer, B.: Stress-testing hoeffding trees. In: 
Jorge, A.M., Torgo, L., Brazdil, P.B., Camacho, R., Gama, J. (eds.) 
PKDD 2005. LNCS (LNAI), vol. 3721, pp. 495â€“502. Springer, 
Heidelberg (2005) 
[13] J. Gama, P. Medas, G. Castillo, and P. Rodrigues. Learning with drift 
detection. In SBIA Brazilian Symposium on Artificial Intelligence, 
pages 286-295, 2004. 
[14] X. Wu, V. Kumar, J.R. Quinlan, J. Ghosh, Q. Yang, H. Motoda, G.J. 
McLachlan, A.F.M. Ng, B. Liu, P.S. Yu, Z.-H. Zhou, M. Steinbach, D.J. 
Hand, and D. Steinberg, â€œTop 10 Algorithms in Data Mining,â€ 
Knowledge and Information Systems, vol. 14, no. 1, pp. 1-37, 2008. 
[15] Kirkos S., Spathis C., Manolopoulos Y., Data Mining techniques for the 
detection of fraudulent financial statements, Expert Systems with 
Applications., 2006. 
[16] Ou, P., Wang, H. Prediction of Stock Market Index Movement by Ten 
Data MiningTechniques. Modern Applied Science 3, 2009. 
[17] B. Rosenberg and W. McKibben. "The Prediction of Systematic and 
Specific Risk in CommonStocks." Journal of Financial and Quantitative 
Analysis, 1973. 
[18] Gidofalvi, G., Using News Articles to Predict Stock Price Movements. 
2001, University of California, San Diego: Department of Computer 
Science and Engineering. 
 
339
for Assistive Technologyï¼Œå…¶å…§å®¹è‘—é‡åœ¨å¦‚ä½•å°‡æ©Ÿå™¨å­¸ç¿’çš„æŠ€è¡“æ‡‰ç”¨æ–¼è¼”åŠ©å™¨å…·
çš„é–‹ç™¼ï¼ŒåŒ…æ‹¬æ™ºæ…§å±…å®¶ã€å°èˆªç³»çµ±ã€æç¤ºç³»çµ±ï¼Œé‚„æœ‰é€é BCI (brain-computer 
interface) æ§åˆ¶çš„è¼ªæ¤…ã€‚Michael Lightneræ•™æˆæ›¾ä»» IEEEä¸»å¸­ã€‚æ¥ä¸‹ä¾†æ˜¯éŸ“åœ‹ Seoul 
National University çš„ Byeong Gi Lee æ•™æˆçš„æ¼”è¬›ï¼Œè¬›é¡Œæ˜¯ Convergence of 
Communications towards Smart Eraï¼Œå…¶å…§å®¹åœ¨å›é¡§ä¸¦å±•æœ›é€šè¨Šç³»çµ±çš„ç™¼å±•ï¼Œä¸¦æ
å‡ºæœªä¾†çš„ç§‘æŠ€ç™¼å±•æ‡‰è¦å”åŠ©äººå€‘å»ºç«‹æ™ºæ…§ä¸”ç’°ä¿çš„ç¤¾æœƒï¼Œä½¿å¾—äººå€‘å¯ä»¥éè‘—æ™ºæ…§ä¸”
ç’°ä¿çš„ç”Ÿæ´»ã€‚Byeong Gi Leeæ•™æˆç‚º IEEE Fellowã€‚æœ€å¾Œæ˜¯åœŸè€³å…¶ Erciyes University
çš„ Mehmet Emin Yuksel æ•™æˆçš„æ¼”è¬›ï¼Œè¬›é¡Œæ˜¯ Digital Image Restoration Operators 
Based on Neuro-Fuzzy Techniquesï¼Œå…¶ä¸»è¦å…§å®¹æ˜¯åœ¨ä»‹ç´¹ç¥ç¶“æ¨¡ç³ŠæŠ€è¡“æ–¼å½±åƒè™•ç†
çš„æ‡‰ç”¨ï¼ŒåŒ…æ‹¬é›œè¨Šéæ¿¾ã€é›œè¨Šåµæ¸¬ï¼Œé‚„æœ‰é‚Šç·£æŠ½å–ã€‚Mehmet Emin Yukselç‚º IEEE Senior 
Memberã€‚ä¸‹åˆæ˜¯è«–æ–‡å ±å‘Šçš„ sessionã€‚æœ¬äººæ‡‰é‚€æ“”ä»»ä¸€å€‹ä»¥äººå·¥æ™ºæ…§ç‚ºä¸»é¡Œçš„ session
çš„ä¸»æŒäººï¼Œä¸¦å£é ­ç™¼è¡¨è«–æ–‡ä¸€ã€‚7æœˆ 13æ—¥ï¼Œæœ¬äººå†æ¬¡æ‡‰é‚€æ“”ä»»ä¸€å€‹ä»¥äººå·¥æ™ºæ…§ç‚ºä¸»
é¡Œçš„ session çš„ä¸»æŒäººï¼Œä¸¦å£é ­ç™¼è¡¨è«–æ–‡äºŒã€‚æœ¬æœƒè­°èˆ‡ 2012 IEEE International 
Conference on Communications, Network and Satellite (COMNETSTAT 2012) åˆ
è¾¦ï¼Œè€Œ 7/14 çš„ä¸»é¡Œç‚ºé€šè¨Šã€ç¶²è·¯ï¼Œä»¥åŠè¡›æ˜ŸæŠ€è¡“ã€‚æœƒè­°æ–¼ 7æœˆ 14æ—¥çµæŸã€‚ 
 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
 
é€™é›–ç„¶æ˜¯ä¸€å€‹æ–°çš„åœ‹éš›æœƒè­°ï¼Œä¸”åœ¨æŠ•ç¨¿ç³»çµ±ã€è¨»å†Šç³»çµ±ï¼Œä»¥åŠè­°ç¨‹å®‰æ’æ–¹é¢éƒ½é‚„æœ‰
å¾…æ”¹å–„ï¼Œä½†ä¸»è¾¦å–®ä½ä»æ˜¯å®‰æ’æ•¸å€‹è³‡æ·±ä¸”æœ‰åæœ›çš„å­¸è€…æ“”ä»»ä¸»é¡Œæ¼”è¬›çš„è¬›è€…ï¼Œè®“è³‡
æ·ºè€…å¦‚æœ¬äººå¾ä¸­ç²ç›Šè‰¯å¤šã€‚é€™å€‹æœƒè­°çš„åƒèˆ‡è€…å¤§å¤šä¾†è‡ªäºæ´²ï¼Œä½¿å¾—æœƒè­°çš„åœ‹éš›åŒ–ç¨‹
åº¦ä¸å¦‚é æœŸï¼Œä½†è‹¥è€ƒæ…®å¤§å¤šæ•¸äºæ´²åœ‹å®¶æ­£åœ¨å¿«é€Ÿç™¼å±•ï¼Œä¸”å°æ‡‰ç”¨ç§‘æŠ€æœ‰å¼·çƒˆçš„éœ€æ±‚ï¼Œ
 è«–æ–‡äºŒæ‘˜è¦ï¼šEnsemble learning is inspired by the human group decision making 
process, and it has been found beneficial in various application domains. 
Decision tree and artificial neural network are two popular types of 
classification algorithms often used to construct classic ensembles. 
Recently, researchers proposed to use the mixture of both types to construct 
hybrid ensembles. However, researchers use decision trees and artificial 
neural networks together in an ensemble without further discussion. The focus 
of this paper is on the hybrid ensemble constructed by using decision trees 
and artificial neural networks simultaneously. The goal of this paper is not 
only to show that the hybrid ensemble can achieve comparable or even better 
classification performance, but also to provide an explanation of why it 
works. 
 
å››ã€å»ºè­° 
 
æœ¬äººèªç‚ºæ­¤é¡æ–°èˆˆåœ‹éš›æœƒè­°æˆ–å¯å¹«åŠ©æ¨å»£è‡ªå·±çš„ç ”ç©¶çš„æ‡‰ç”¨ï¼Œä¾‹å¦‚æ›¿è‡ªå·±çš„ç ”ç©¶æ‰¾
åˆ°ä½¿ç”¨è€…ï¼Œä¸”èƒ½æå‡å°ç£çš„åœ‹éš›è²æœ›ï¼Œæ•…å¸Œæœ›æ—¥å¾Œä»æœ‰æ©Ÿæœƒç²å¾—åœ‹ç§‘æœƒè£œåŠ©åƒèˆ‡æ­¤
é¡æœƒè­°ã€‚ 
 
äº”ã€æ”œå›è³‡æ–™åç¨±åŠå…§å®¹ 
1. æœƒè­°è­°ç¨‹æ‰‹å†Šï¼Œå…§å«å„ä¸»é¡Œæ¼”è¬›çš„æ‘˜è¦èˆ‡è¬›è€…ç°¡æ­·ï¼Œé‚„æœ‰å„è«–æ–‡çš„é¡Œç›®ã€ä½œè€…ï¼Œ
  
100ï¦ï¨å°ˆé¡Œç ”ç©¶è¨ˆç•«ç ”ç©¶æˆæœå½™æ•´è¡¨ 
è¨ˆç•«ä¸»æŒäººï¼šå¾åœ‹å‰ è¨ˆç•«ç·¨è™Ÿï¼š100-2218-E-004-002- 
è¨ˆç•«åç¨±ï¼šå¤šåˆ†ï§å™¨ç³»çµ±å»ºæ§‹ä¹‹ï§¤ï¥èˆ‡å¯¦å‹™ 
ï¥¾åŒ– 
æˆæœé …ç›® å¯¦éš›å·²é”æˆ
ï¥©ï¼ˆè¢«æ¥å—
æˆ–å·²ç™¼è¡¨ï¼‰
é æœŸç¸½é”æˆ
ï¥©(å«å¯¦éš›å·²
é”æˆï¥©) 
æœ¬è¨ˆç•«å¯¦
éš›è²¢ç»ç™¾
åˆ†æ¯” 
å–®ä½ 
å‚™ è¨» ï¼ˆ è³ª åŒ– ï¥¯
æ˜ï¼šå¦‚ï¥©å€‹è¨ˆç•«
å…±åŒæˆæœã€æˆæœ
ï¦œ ç‚º è©² æœŸ åˆŠ ä¹‹
å° é¢ æ•… äº‹ ...
ç­‰ï¼‰ 
æœŸåˆŠï¥æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒï¥æ–‡ 0 0 100% 
ç¯‡ 
 
ï¥æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100%   
ç”³è«‹ä¸­ä»¶ï¥© 0 0 100%  å°ˆï§ å·²ç²å¾—ä»¶ï¥© 0 0 100% ä»¶  
ä»¶ï¥© 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šï§ï¤Š 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 2 2 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å…§ 
ï¥«èˆ‡è¨ˆç•«äººï¦Š 
ï¼ˆæœ¬åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ï§¤ 0 0 100% 
äººæ¬¡ 
 
æœŸåˆŠï¥æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒï¥æ–‡ 3 3 100% 
ç¯‡ 
 
ï¥æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100% ç« /æœ¬  
ç”³è«‹ä¸­ä»¶ï¥© 0 0 100%  å°ˆï§ å·²ç²å¾—ä»¶ï¥© 0 0 100% ä»¶  
ä»¶ï¥© 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šï§ï¤Š 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å¤– 
ï¥«èˆ‡è¨ˆç•«äººï¦Š 
ï¼ˆå¤–åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ï§¤ 0 0 100% 
äººæ¬¡ 
 
