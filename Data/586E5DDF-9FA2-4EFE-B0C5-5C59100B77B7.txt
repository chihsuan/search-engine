users can accept collaborating work with a robot in an office 
environment as well as general usersâ€˜ expectation of the robotâ€™s 
ability. The second sub-project developed an office delivery robot, 
mainly focusing on its visual servoing capabilities to carry out three 
kinds of services, namely, â€™serve teaâ€™, â€™people followingâ€™ 
and â€™document deliveryâ€™. The third sub-project proposed a learning 
collision-free navigation in static environments could be 
accomplished using only instance-based examples without using 
global information such as the map and the goal location. The fifth 
sub-project investigated pitch patterning and durational cues of 
emotional speech in Taiwan Mandarin. Acted connected speech in 
anger, joy, sorrow, fear and neutral emotions were recorded for 
analyses. An emotion model based on Hidden Markov Model is 
proposed in the sixth sub-project to simulate the dynamic processes 
of emotional self-regulation and emotional transference under the 
influence of the same stimuli arouse by the result of emotional 
speech recognition. 
Carrying out this integrated project could effectively combine the 
development, manufacture, design, and application of robots for 
technical manpower, and will reach the goals as technological 
communicating and resource sharing. 
 
ä¸€ã€å‰è¨€ï¼š 
éš¨è‘—å·²é–‹ç™¼åœ‹å®¶å°‘å­åŒ–åŠé«˜é½¡åŒ–çš„ç¾è±¡æ—¥è¶¨åš´é‡ï¼Œå„åœ‹çš†æœŸæœ›è—‰ç”±ç ”ç™¼æ©Ÿå™¨äººæŠ€è¡“ä»¥è§£
æ±ºé€™ç¤¾æœƒç¾è±¡æ‰€å¸¶ï¤­çš„è¡ç”Ÿå•é¡Œï¼Œä¹Ÿæ¼¸æ¼¸åœ°å°‡æœå‹™å‹æ©Ÿå™¨äººå®šç‚ºç ”ç™¼é‡å¿ƒï¼Œå¸‚å ´é ä¼°åœ¨ï¥§ä¹…
çš„å°‡ï¤­æœå‹™å‹æ©Ÿå™¨äººå¸‚å ´çš„ç¸½ç”¢å€¼å°‡è¶…éå·¥æ¥­ç”¨æ©Ÿå™¨äººï¼Œè€Œæ™ºæ…§å‹æ©Ÿå™¨äººå¸‚å ´ä¹Ÿå°‡å¤§å¹…æˆé•·ã€‚
æ ¹æ“šå„åœ‹æ©Ÿå™¨äººç”¢æ¥­èª¿æŸ¥è³‡ï¦¾é¡¯ç¤ºï¼Œé ä¼°åˆ°ï¦º 2015 å¹´æ™ºæ…§å‹æ©Ÿå™¨äººå¸‚å ´ç”¢å€¼å°‡çªç ´åƒå„„ç¾
å…ƒã€‚ 
ä¸€èˆ¬è€Œè¨€ï¼Œæ™ºæ…§å‹æ©Ÿå™¨äººå¯åˆ†ç‚ºå·¥æ¥­å‹åŠæœå‹™å‹æ©Ÿå™¨äººå…©ï§ï¼Œå·¥æ¥­å‹æ©Ÿå™¨äººè‡ª 1970 å¹´ä»£
ä¾¿é–‹å§‹ç™¼å±•ï¼Œå¤§å¤šé‹ç”¨æ–¼çµ„è£ã€ç„Šæ¥å™´æ¼†ã€åŠ å·¥åŠæ¬é‹ç­‰ï¼Œä»¥å–ä»£äººï§åœ¨å±éšªçš„ç’°å¢ƒä¸‹å’Œé‡
è¦†æ€§é«˜çš„å·¥ä½œã€‚ç›®å‰å·¥æ¥­å‹æ©Ÿå™¨äººæŠ€è¡“å·²æ¼¸è¶¨æˆç†Ÿï¼Œæœªï¤­å·¥æ¥­å‹ç”¨æ©Ÿå™¨äººå°‡ç°¡åŒ–ç”¢å“åŠŸèƒ½ï¼Œ
æœå‘å–®ä¸€åŒ–åŠŸèƒ½ç”¢å“ç™¼å±•ï¼Œé™ä½æ©Ÿå™¨äººè£½é€ æˆæœ¬ï¼Œæé«˜å» å•†ä½¿ç”¨æ„é¡˜ã€‚ç›®å‰å·¥æ¥­å‹æ©Ÿå™¨äººä»¥
é›»å­ã€å¹³é¢é¡¯ç¤ºå™¨ã€åŠå°é«”ç­‰ç„¡å¡µå®¤ç”¨æ©Ÿå™¨äººæˆé•·æœ€å¿«ã€‚ 
æ™ºæ…§å‹æ©Ÿå™¨äººæ¶‰åŠçš„æŠ€è¡“ååˆ†å¤šå…ƒï¼ŒåŒ…æ‹¬ç§»å‹•å¹³å°ã€æ©Ÿé›»æ§åˆ¶ã€é›»æºæŠ€è¡“ã€ç³»çµ±æ•´åˆæŠ€
è¡“ã€é€ å‹è¨­è¨ˆã€é›»è…¦å·¥ç¨‹ã€äººå·¥æ™ºæ…§ã€æ„Ÿæ¸¬æŠ€è¡“ã€ç²¾å¯†æ©Ÿæ¢°ç­‰ï¼Œå…¶ç›¸é—œçš„ç”¢æ¥­äº¦æ¶µè“‹é›»æ©Ÿã€
æ©Ÿæ¢°ã€è³‡è¨Šã€é€šè¨Šã€é›»å­ã€èƒ½æºåŠæï¦¾ï¼Œç‚ºä¸€é«˜åº¦æ•´åˆå‹ç”¢æ¥­ã€‚éå»æ™ºæ…§å‹æ©Ÿå™¨äººçš„ç™¼å±•å—
é™æ–¼æŠ€è¡“ï¼Œå› æ­¤æ²’æœ‰çªç ´æ€§çš„ç™¼å±•ï¼Œæ‡‰ç”¨ä¹Ÿåªå±€é™åœ¨å·¥æ¥­å‹æ©Ÿå™¨äººï¼Œä½†å¾ 1990å¹´ä»£èµ·ï¼Œå¾®é›»
å­ç§‘æŠ€ã€é›»è…¦ç§‘æŠ€ã€ç”Ÿç‰©ç§‘æŠ€ã€æï¦¾æŠ€è¡“ã€é€šè¨Šå’Œè‡ªå‹•åŒ–æ©Ÿæ¢°æŠ€è¡“ç­‰ç›¸é—œç§‘æŠ€å¿«é€Ÿç™¼å±•ï¼Œæ
ä¾›ï¦ºæ©Ÿå™¨äººæ‰€éœ€çš„åŸºç¤æŠ€è¡“ã€‚ä½¿æ™ºæ…§å‹æ©Ÿå™¨äººçš„ç™¼å±•è¶Šï¤­è¶Šè¿…é€Ÿï¼Œé€²è€Œå¸¶å‹•ï¦ºç›¸é—œçš„ç ”ç©¶é ˜
åŸŸï¼Œæ©Ÿå™¨äººçš„æ‡‰ç”¨å ´åˆï¥§å†å±€é™æ–¼å·¥å» ã€è»äº‹ã€å¯¦é©—å®¤ç­‰ï¼Œè€Œæ˜¯æ›´å¤šå…ƒçš„æ‡‰ç”¨å ´åˆï¼Œå¦‚å®¶åº­ã€
é†«é™¢ã€å…¬å…±å ´æ‰€ç­‰ï¼Œå› æ­¤æ©Ÿå™¨äººç³»çµ±ä¹‹ç ”ç©¶å·²é€æ¼¸æ¼”é€²è‡³ä¸€å€‹å¶„æ–°çš„ä¸–ä»£ï¼Œå¦‚ä½•è®“æ©Ÿå™¨äººæˆ
åŠŸçš„èå…¥åœ¨äººï§çš„ç”Ÿæ´»ç’°å¢ƒï¼Œä¸¦æœå‹™èˆ‡æ»¿è¶³äººï§å„å¼å„æ¨£çš„éœ€æ±‚ï¼Œå·²æ˜¯å„å€‹å…ˆé€²åœ‹å®¶å’Œåœ‹éš›
å¤§å» ç©æ¥µæŠ•å…¥çš„ç ”ç™¼é …ç›®ä¹‹ä¸€ï¼Œä¾‹å¦‚æ—¥æœ¬çš„æ©Ÿå™¨äººé€ é®è¨ˆç•«ã€éŸ“åœ‹çš„å®¶ç”¨æ©Ÿå™¨äººæ™®åŠè¨ˆç•«ä»¥
åŠåœ‹éš›å¤§å»  Googleã€Intelå’Œ Microsoftå…±åŒæŠ•è³‡æ©Ÿå™¨äººç›¸é—œç ”ç™¼è¨ˆç•«ç­‰ã€‚ 
äºŒã€ç ”ç©¶ç›®çš„ 
æœ¬è¨ˆç•«çš„ç ”ç©¶ç›®çš„ç‚ºç™¼å±•ä¸€å¥—æ™ºæ…§è¾¦å…¬ç’°å¢ƒä¹‹æœå‹™å‹æ©Ÿå™¨äººæ—ç³»çµ±(å¦‚åœ– 1 æ‰€ç¤º)ï¼Œå…¶ä¸­
å·®é£éç‰©æ©Ÿå™¨äººå‰‡æä¾›ä¸€èˆ¬ä¸Šç­å“¡å·¥æ‰€éœ€çš„å·®é£æœå‹™ï¼Œå¦‚ç‰©å“éé€ã€èŒ¶é»æœå‹™ã€å…¬æ–‡éé€ç­‰
æœå‹™ï¼Œä¸¦é€²ä¸€æ­¥èˆ‡ä¸Šç­å“¡å·¥å’Œè«§å…±è™•ï¼›å¤šåŠŸèƒ½ç§˜æ›¸æ©Ÿå™¨äººå‰‡æä¾›ä¸»ç®¡æ‰€éœ€çš„ç§˜æ›¸æœå‹™ï¼Œå¦‚è¡Œ
ç¨‹è¦åŠƒã€é–‹æœƒæé†’ã€ç§»å‹•å¼æ–‡æ›¸åˆ—å°ç­‰ï¼›ï¥§åŒåŠŸèƒ½å±¬æ€§ä¹‹æ©Ÿå™¨äººæœå‹™ä»»å‹™è¨­è¨ˆçš†åƒè€ƒä¸€èˆ¬è·
  
åœ– 2 æ—¥æœ¬ Sumihisa Iwashita å’Œ Tsutomu Asada æ‰€ç ”ç™¼çš„æœå‹™å‹æ©Ÿå™¨äººæ¦‚å¿µèˆ‡é››å‹ 
  
                        (a)                                (b) 
åœ– 3 è¾¦å…¬å®¤æ©Ÿå™¨äºº: (a)Jijo-2 (b)Robovie-IV 
HÃ¼ttenrauch ç­‰äºº[4]é–‹ç™¼å‡ºé©ç”¨æ–¼è¾¦å…¬ç’°å¢ƒä¹‹éé€æ©Ÿå™¨äºº Ceroï¼Œå…¶è¼‰å…·ç”± Nomadic 
Technologies å…¬å¸å‡ºç”¢ï¼ŒCero èƒ½å”åŠ©åœ¨è¾¦å…¬å®¤è¡Œå‹•ï¥§ä¾¿çš„äººå‚³éæ‰€éœ€çš„ç‰©å“ï¼ŒCero çš„é ­é ‚é…
æœ‰è½‰å‹•å°äººæ¨¡å‹å¯å‘ˆç¾ï¥§åŒå‹•ä½œï¼Œï¤­ä»£è¡¨ Cero çš„ç§»å‹•æ–¹å‘ï¼Œå¦‚åœ– 4 æ‰€ç¤ºã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒCero
äº¦å…¼å…·ä¸€èˆ¬è¾¦å…¬å®¤æ©Ÿå™¨äººæ‰€éœ€ä¹‹è½èˆ‡æ’­æ”¾ä¹‹åŠŸèƒ½ï¼Œä¸¦å¯ç”± PDA é ç«¯æ–æ§ã€‚ 
æ´»èªç¾©ï¼ŒHitachi å…¬å¸å¸Œæœ›åœ¨ï¥§ä¹…çš„å°‡ï¤­èƒ½è®“ Emiew 2 åœ¨è¾¦å…¬å®¤ç­‰å ´æ‰€ä¸­è² è²¬å¼•å°ï¤­å®¢ã€å·¡é‚
ç›£è¦–æˆ–éé€æ–‡ä»¶ç­‰å·¥ä½œã€‚ 
  
      (a)                              (b) 
åœ– 6 æ—¥æœ¬ Hitachi æ‰€é–‹ç™¼çš„ç¬¬äºŒä»£æ©Ÿå™¨äºº Emiew 2ï¼š(a)ç«™ç«‹èˆ‡è·ªå§¿ (b)è¡Œé€²ä¸­ 
ç”±æ–¼ç›®å‰è¾¦å…¬å®¤æ©Ÿå™¨äººé€ åƒ¹ä»ç›¸ç•¶æ˜‚è²´ï¼Œåœ– 5 ä¸­çš„ ASIMO å’Œ Wakamaru åŠåœ– 6 çš„ Emiew 2
å¯èªªæ˜¯ç›®å‰æ—¥æœ¬ç ”ç™¼ä¸­å¯ç”¨æ–¼è¾¦å…¬å®¤ä¹‹æ©Ÿå™¨äººçš„ä»£è¡¨ï¼Œé™¤æœ‰å’Œè—¹å¯è¦ªã€å€¼å¾—ä¿¡è³´çš„å¤–è§€å¤–ï¼Œ
ç³»çµ±çš„ç©©å®šæ€§ã€å°æŒ‡ä»¤çš„é”æˆæ•ˆï¥¡ç­‰ï¼Œæ˜¯å•†æ¥­ç”¨é€”ï¥§å¯å¿½è¦–çš„ã€‚ 
 
åœ– 7 Fujitsu æ‰€é–‹ç™¼çš„è¾¦å…¬å®¤å‚³éç‰©å“æ©Ÿå™¨äºº 
 
æœ€è¿‘ï¼Œæ—¥æœ¬ Fujitsu å…¬å¸æ‰€é–‹ç™¼çš„è¾¨å…¬å®¤å‚³éæ©Ÿå™¨äºº[8]å¦‚åœ– 7 æ‰€ç¤ºï¼Œé…å‚™æœ‰ç«‹é«”è¦–è¦ºã€é›·
å°„æ¸¬è·å„€ã€å¢å¼·å‹åŒæ­¥å®šä½èˆ‡åœ°åœ–å»ºç½®ï¼Œå¯ç²¾ç¢ºåœ°å®šä½åœ¨ 10 å…¬åˆ†ä»¥å…§ï¼Œä¸­é–“ç‚ºç½®ç‰©å±¤å…±å¯ä¹˜
  ä½æ–¼ç¾åœ‹åŠ å·çŸ½è°·çš„ä¸€å®¶ç§‘æŠ€å…¬å¸ Anybotsï¼Œæœ€è¿‘é–‹ç™¼å‡ºä¸€å°é ç¨‹å‡ºå¸­æ©Ÿå™¨äºº QA[10]å¦‚
åœ– 9 æ‰€ç¤ºï¼Œå…¶é…æœ‰ 802.11g ç„¡ç·šç¶²ï¤·ã€500 è¬ç•«ç´ æ”å½±æ©Ÿã€å¤œè¦–ç‡ˆã€7 å‹ LCD è¢å¹•ç­‰ï¼Œåœ¨å®¶
æˆ–å…¬å¸é€é QA ä¾¿å¯ä»¥è¼•æ˜“åœ°åœ¨è¾¦å…¬å®¤è¡Œèµ°ç§»å‹•ï¼Œä¸¦èˆ‡ä»–äººé€²è¡Œè¦–è¨Šæœƒè­°ï¼Œç‚ºå‚³çµ±è¦–è¨Šæœƒè­°
å®¤æå‡ºï¦ºä¸€å€‹æ–°çš„è§£æ±ºæ–¹æ¡ˆã€‚ 
å››ã€ç ”ç©¶æ–¹æ³• 
ç¸½è¨ˆç•«ç¬¬ä¸€å¹´ä¹‹è¨ˆç•«ç›®æ¨™åŒ…æ‹¬ï¼šæ¢è¨å·¥ä½œå ´åŸŸä¸­å°å…¥æ™ºæ…§å‹æ©Ÿå™¨äººä¹‹ä½¿ç”¨è€…éœ€æ±‚ã€æ¢è¨
æ©Ÿå™¨äººæ–¼å·¥ä½œè·å ´è§’è‰²å®šä½ã€å®šç¾©æ©Ÿå™¨äººèˆ‡äººä¹‹è§’è‰²é—œä¿‚åŠæå‡ºï¥§åŒåŠŸèƒ½ä¹‹æ©Ÿå™¨äººè§’è‰²èˆ‡éœ€
æ±‚åˆ†æã€‚ç¬¬ä¸€å¹´ä»¥èª¿æŸ¥ç ”ç©¶(survey)ç­‰æ–¹æ³•é‡å°å·¥ä½œç’°å¢ƒçš„éœ€æ±‚èˆ‡æ©Ÿå™¨äººè§’è‰²å®šä½é€²è¡Œç ”ç©¶ï¼Œ
å”åŠ©å„å­è¨ˆç•«å®šç¾©å„è‡ªçš„æ©Ÿå™¨äººè§’è‰²åŠæ©Ÿå™¨äººç¾¤å…§äº’å‹•å½¢å¼èˆ‡äººèˆ‡æ©Ÿå™¨äººç¾¤äº’å‹•å½¢å¼ï¼Œåšç‚º
å„å­è¨ˆç•«æ©Ÿå™¨äººç™¼å±•çš„åŸºç¤ã€‚ç‚ºï¦ºè§£è¾¦å…¬å®¤ç’°å¢ƒä¸­ï¼Œå·¥ä½œè·å“¡å°æ™ºæ…§å‹æ©Ÿå™¨äººçš„çœ‹æ³•ã€‚æœ¬ç ”
ç©¶ä»¥å•å·èª¿æŸ¥ç ”ç©¶çš„æ–¹å¼ï¼Œæ¢ç´¢è·å ´å“¡å·¥å°è¾¦å…¬å®¤ç’°å¢ƒæœå‹™å‹æ™ºæ…§æ©Ÿå™¨äººåœ¨æœå‹™ã€å¤–å‹ã€èˆ‡
äººäº’å‹•ä¸‰å€‹æ–¹å‘çš„éœ€æ±‚ã€åå¥½åŠåŒæ„ç¨‹åº¦ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè©¦åœ–åœ¨çœ¾å¤šï¥§åŒçš„å·¥ä½œå±¬æ€§ä¸­ï¼Œå€åˆ†
å…¬é—œã€ç¤¾äº¤åŠå·®é£éç‰©å‹æ©Ÿå™¨äººçš„éœ€æ±‚å·®ç•°æ¯”è¼ƒã€‚å•å·åˆ†ç‚ºå…©éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†æ¢è¨æ•´é«”ä¸Šå°
è¾¦å…¬å®¤æ©Ÿå™¨äººçš„çœ‹æ³•ï¼›ç¬¬äºŒéƒ¨åˆ†é‡å°ä¸‰ç¨®ï§å‹æ©Ÿå™¨äººï¼Œæ‰¾å‡ºå…¶å·®ç•°çš„å„é …éœ€æ±‚ã€‚çµæœä»¥æè¿°
æ€§çµ±è¨ˆæ–¹å¼é€²è¡Œåˆ†æåŠæ¯”è¼ƒã€‚å—è¨ªè€…èƒŒæ™¯ï¼š29ä½ï¤­è‡ªå°å¤§æ™ºæ…§ç”Ÿæ´»ç§‘æŠ€æ•´åˆèˆ‡å‰µæ–°ç ”ç©¶ä¸­å¿ƒ
çš„å·¥ä½œæˆå“¡ã€‚å¹´é½¡åˆ†å¸ƒ 19~45 æ­²ï¼Œ28~35 æ­²å å¤§å¤šæ•¸(45%)ã€‚ç”·å¥³ç”Ÿå¹¾ä¹å„å ä¸€åŠ(52%ã€
48%)ã€‚å·¥ä½œä¸­çš„ä½éšï¼Œå¤§éƒ¨åˆ†ç‚ºåŸºå±¤å“¡å·¥(82%)ï¼Œå³ç ”ç©¶åŠ©ç†ã€‚æœå‹™åŠŸèƒ½ï¼šç”±è¡¨ä¸€å¯çŸ¥ï¼Œç§˜
æ›¸å‹æ©Ÿå™¨äººéœ€è¦äº‹å‹™æ€§æœå‹™ã€æ–‡æ›¸æœå‹™ã€è³‡è¨Šæœå‹™ï¼›å…¬é—œå‹æ©Ÿå™¨äººï¼Œèªç‚ºéœ€è¦å¤šåª’é«”æœå‹™ã€
ç¤¾äº¤æœå‹™ ï¼›å·®é£éç‰©æ©Ÿå™¨äººæœ€éœ€è¦æ–‡æ›¸æœå‹™ã€å‹å‹™æœå‹™ã€é™æ§æœå‹™ ã€‚å¤–å‹ï¼šç”±è¡¨äºŒå¯çŸ¥ï¼Œ
ç§˜æ›¸åŠå…¬é—œå‹å¤šå¼·èª¿åƒå¥³æ€§ã€é«”å‹å¬Œå°ã€åƒäººä¸”ä»¥è‚¢é«”è®ŠåŒ–ã€æœ‰äººè²ã€‚ç§˜æ›¸èˆ‡å…¬é—œå”¯ä¸€å·®åˆ¥
åœ¨æ–¼è¡¨æƒ…è®ŠåŒ–çš„éœ€æ±‚ï¼Œå…¬é—œæ˜¯è¢«è¦æ±‚è¦æœ‰è¡¨æƒ…è®ŠåŒ–çš„ï¼›å·®é£éç‰©å‹å¼·èª¿åƒç”·æ€§ã€é«”å‹é«˜å¤§å£¯
ç¢©ã€åƒæ©Ÿå™¨ä¸”æŠ½åƒã€æ©Ÿæ¢°è²ã€‚èˆ‡äººäº’å‹•ï¼šç”±è¡¨ä¸‰å¯çŸ¥ï¼Œç§˜æ›¸åŠå·®é£éç‰©å‹å¼·èª¿æƒ…ç·’ç©©å®šã€åˆ
ä½œã€è¬¹æ…ï¼›å…¬é—œæ©Ÿå™¨äººå¼·èª¿æƒ…ç·’ç©©å®šã€é–‹æ”¾ã€å¤–å‘ã€‚å°æ–¼ç§˜æ›¸å’Œå…¬é—œï¼Œäº¦å¼·èª¿æœ‰åˆ¤æ–·èƒ½åŠ›åŠ
æƒ…ç·’è¡¨é”èƒ½åŠ›ã€‚æœ¬ç ”ç©¶çµæœç™¼ç¾ï¼Œé‡å°ï¥§åŒï§å‹çš„æ©Ÿå™¨äººï¼Œåœ¨åŠŸèƒ½ã€å¤–å‹ã€äº’å‹•æ¨¡å¼ä¸­ï¼Œå­˜
åœ¨å„è‡ªï¥§åŒçš„éœ€æ±‚åŠåå¥½å·®ç•°ã€‚ä½†åœ¨æŸäº›å‘åº¦åŠç¨‹åº¦ä¸Šï¼Œäº¦æœ‰é‡ç–Šä¹‹è™•ã€‚åœ¨æ­¤åˆæ¢èª¿æŸ¥ä¸­ï¼Œ
å¯ä»¥ï¦ºè§£å°æœªï¤­è¾¦å…¬å®¤ä¸­æ™ºæ…§æ©Ÿå™¨äººå¯èƒ½ç™¼å±•çš„æ–¹å‘ï¼Œä¸¦ç”±æ­¤çµæœï¼Œæ·±å…¥è·å ´ç’°å¢ƒä¸­ï¼Œé€²è¡Œ
é€²ä¸€æ­¥çš„æ¢ç´¢åˆ†æåŠéœ€æ±‚èª¿æŸ¥ã€‚ 
 
 
äº”ã€å„åˆ†é …è¨ˆç•«æˆæœç°¡è¿° 
æ–°ä¸–ä»£è¾¦å…¬å¤§æ¨“ä¹‹ã€Œæ™ºæ…§å‹æœå‹™æ©Ÿå™¨äººæ—ã€(1/3) 
å­è¨ˆç•«ä¸€ï¼šè¾¦å…¬å®¤å¤šåŠŸèƒ½æ™ºæ…§å‹ç§˜æ›¸æ©Ÿå™¨äºº (1/3) 
ä¸»  æŒ  äººï¼šå‚…ç«‹æˆ æ•™æˆ    å°å¤§é›»æ©Ÿç³»ã€è³‡å·¥ç³» 
å…±åŒä¸»æŒäººï¼šå²³ä¿®å¹³ æ•™æˆ    å°å¤§ç”Ÿå‚³ç³» 
Abstract: 
This paper presents our new intelligent interactive robot, which is constructed to eagerly provide 
multi-functional services in an office environment. We investigate the conditions under which users 
can accept collaborating work with a robot in an office environment as well as general users' 
expectation of the robotâ€™s ability. Besides, in order to endow a full interactive capability of our 
robots for realizing so-called human-robot interaction (HRI), there are two kinds of cameras to 
accomplish human detection/tracking and object detection/recognition. In addition to cameras, there 
are two kinds of lasers mounted in the front and at the back of the robot to detect and track human 
legs. Not only by these sensors, human interact with the robot also by some natural way, such as 
touching the interface screen and talking with the robot through microphone. Last but not the least, 
since the office environment is dynamic, this paper proposes a context-aware navigation method to 
deal with some situations which may take place in the office environment. For overall situation, we 
design a finite state machine (FSM) approach to enable the robot to solve the emergent tasks and to 
accommodate human users in real office environment. Finally, the effectiveness of the proposed 
work is tested and validated by some of experiments. 
 
Results of Projects: 
ï¬ Intelligent Interactive Robot 
The appearance, as shown as in Fig. 10 , and specification of our office robot is briefly described 
in this section. The design philosophy is to build an autonomous mobile robot with various 
functions so that it can exhibit the capability for interacting with human. There are two computers 
employed in charge all of the computational tasks, and they communicate with each other through a 
local area network. The computational tasks of the office robot are divided into low and high levels. 
PC-1 is employed for low level computations, taking care of such as motor motion and intelligent 
navigation, whereas PC-2 is employed for high level computations, handling e.g. human-robot 
interaction, speech recognition, vision, etc. Table 4 lists various specifications of our office robot. 
 Fig. 11. Finite state machine of the office robot 
ï¬ Human Guiding and Following 
Fig. 12 shows the guiding mode, the robot will stop to wait when it detects no human behind it.  
On the other hand, Fig. 13 shows the situation where the human is in front of the robot, and 
therefore it can follow human to anywhere and keep a comfortable distance between human and 
robot when human stop in front of the robot. 
   
t=1 t=4 t=7 
   
t=10 t=13 t=15 
Fig. 12. The snapshots of human guiding. 
   
t=1 t=3 t=6 
   
t=6 t=8 t=10 
Fig. 13. The snapshots of human following. 
 
 
 
use of the Visual C++ programming tool and the OpenCV image processing library. There is also a 
wireless Ethernet module on the SBC reserved for multi-robot communication. 
For the subsequent experiments, the visual servo system is required to grabs digital color images 
of 320x240 pixels in a 30Hz frame rate, performs the visual tracking task in real-time (i.e. within 
every 33ms period), and sends the servo command to the motion control module via the parallel I/O 
interface. 
 
 
Fig. 14 System setup of the office delivery robot Fig. 15 SPCE061A EMU board developed by 
SUNPLUS Technology Co., Ltd. 
 
ï¬ Hand and Head Gesture System 
Usually  interactions  from  the  vocal  modality  alone can not fully express oneâ€™s 
intention; therefore, we try to implement additional modalities of human-robot interactions  
that  help resolve the ambiguity  and  express friendly. For example, when people interact 
with each other, it is natural to indicate their agreement, acknowledgment, or disinterest simply 
by hand and head gestures. On the other hand, people use the gestures together with verbal 
expressions to properly convey their intentions and emotional states. Body signs from gestures 
reveal rich information to indicate oneâ€™s emotional state and intentions. 
To realize the hand and head gestures, we first mount an animated head system on the top of 
the robot body and two seven-degree-of-freedom manipulators on the shoulder. The animated 
head is a fully assembled mechanical construction with three movable parts, including eyes, 
mouth, and neck, as shown in Fig. 16. These movable parts are controlled by  mini-servo 
motors to produce several head gestures such as â€œhead  shakeâ€, â€œhead  nodâ€,  and â€eyes  
glanceâ€, which can supplement additional information accompanying with a vocal 
communication. We utilized the Parallax servo controller, as shown in Fig. 17, which can 
æ–°ä¸–ä»£è¾¦å…¬å¤§æ¨“ä¹‹ã€Œæ™ºæ…§å‹æœå‹™æ©Ÿå™¨äººæ—ã€(1/3) 
å­è¨ˆç•«ä¸‰ï¼šå­¸ç¿’å‹è¾¦å…¬å®¤ç¤¾äº¤æ‹›å¾…æ©Ÿå™¨äºº (1/3) 
ä¸»  æŒ  äººï¼šç‹å‚‘æ™º å‰¯æ•™æˆ  å°å¤§è³‡å·¥ç³» 
Abstractï¼š 
While collision-free navigation could be done using traditional rule-based approaches, it 
becomes more attractive to use learning from demonstration (LfD) approaches to ease the burden of 
tedious coding and parameter tuning procedures recently. Given the world model, the starting pose, 
and the goal pose, a path could be learned using trajectory-based LfD approaches. In this paper, we 
argue that learning collision-free navigation in static environments could be accomplished using only 
instance-based examples without using global information such as the map and the goal location. The 
experimental results using a real robot with a laser scanner demonstrated that the low level control 
commands for collision-free navigation are learnable from demonstration using the proposed 
approach. 
 
Results of Projectsï¼š 
ï¬ The Learning to Search (LEARCH) Algorithm 
Recall that the goal is to identify a mapping function from features of any given state to costs 
such that the resulting minimum cost plans capture well the demonstrated behavior. To get a 
minimum cost path, it is essential to get the cost value on each state in the map as ï€¨ ï€©iFs  which 
could be generated by querying the feature function F to get the features in a given state i and 
applied it with the learned costmap s. The costmap is then further modified to the exponential form 
as ï€¨ ï€©iFse  to create a hypothesis space of cost functions with substantially higher dynamic ranges for a 
given set of features and fulfill the requirement of many planning algorithms with strictly positive 
costs in order to ensure the existence of an admissible heuristic. 
The procedure of the LEARCH algorithm is presented in Algorithm 1. The log-costmap is 
initialized as zero. Then the loss function and feature function are used to compute the 
loss-augmented costmap. Here the loss function is used to perform margin-maximization like the 
similar interpretation in support vector machine [11] and could be easily defined by the users. The 
feature function is used to provide the observed features at a given state. A planner in the learner 
then plans a path using the loss-augmented costmap. According to the current planned and example 
paths, the learner generates positive and negative examples and then further trains a regressor or 
 
Fig. 21. One of the demonstrations in the navigation environment. The ambiguous regions within the green rectangles 
would cause the poor performance when testing. 
 
  
(a) The laser scans in the bottom ambiguous region in Fig. 
21. The left and right passages with respect to the robot may 
cause the ambiguity in control policy. The control command 
is turning left in the training data. 
(b) The laser scans in the top ambiguous region in Fig. 21. 
The center and right passages with respect to the robot may 
cause the ambiguity in control policy. The control command 
is turning right in the training data. 
Fig. 22. The blue dots are the corresponding laser scans in the ambiguous regions. The cross in red color indicates the 
robot position. 
  
(a) The overall generated path. The robot exchanged the 
control commands between trying to pass through the 
center passage and the right passage whereas resulted in 
hitting the obstacles. 
(b) A closer look at the end of the generated path. The 
direction of the robot is changed obviously at the final 
few steps before hitting the obstacle. 
Fig. 23. The result when testing with the ambiguous environment. 
 
 
ï¬ Speech Rate 
Fig. 25 shows the results of speech rate. As can be seen, sorrow had the slowest speech rate 
among all the emotions, and the speech rate was negatively correlated with strength. On the other 
hand, speech rates were in a positive correlation with anger and joy. An emotion (4) Ã—strength (2) 
two-way ANOVA was conducted to test the above observations. Results showed that only the main 
effect of emotion was significant  [F(3,175) = 4.81, p< .01, Î·2=.08]. Post-hoc tests with LSD 
adjustments showed that speech rates were significantly slower in sorrow than in anger, joy, and 
neutral (p < .01). No significant differences were found between fear and the other emotion 
categories, and no significant effects involving strength were found. 
 
Fig. 25. Results of speech rate for each emotion 
ï¬ Pitch Range 
Figure 3 shows the results of pitch range. As can be seen, the overall pitch range and register 
of females were significantly larger and higher than those of males. Comparing across different 
emotions, one can see that the pitch register was the highest in anger, followed by joy, and it was 
the lowest in sorrow, fear, and neutral. Comparing across different strengths, one can see that there 
was a positive correlation in anger, which is mainly resulting from higher maximum ğ¹0 .For joy, 
there was no obvious correlation between pitch range and emotion strength. For sorrow, male  
speakers showed a raised pitch register in the strong version, while female speakers expanded pitch 
range by raising maximum F 0and lowering minimum ğ¹0 . Three emotion (4) Ã— strength (2) Ã— 
gender (2) three-way ANOVAs were conducted on pitch range, maximum ğ¹0 , and minimum ğ¹0 
to verify the above observations. For pitch range, results showed that there was a significant main 
effect of gender [F(1, 168) = 88.65, p < .01, Î·2= .35]. The interaction effect of emotion Ã— gender 
æ–°ä¸–ä»£è¾¦å…¬å¤§æ¨“ä¹‹ã€Œæ™ºæ…§å‹æœå‹™æ©Ÿå™¨äººæ—ã€(1/3) 
å­è¨ˆç•«äº”ï¼šæ©Ÿå™¨äººèˆ‡äººï§å·¥ä½œç’°å¢ƒå…±ç”Ÿæ¨¡å¼ä¹‹åˆ†æèˆ‡è¨­è¨ˆ (1/3) 
ä¸»  æŒ  äººï¼šé€£è±ŠåŠ› å‰¯æ•™æˆ  å°å¤§é›»æ©Ÿç³» 
Abstractï¼š 
Artificial emotion model is considered as a key factor to achieve a more effective and 
believable human-robot interaction. As the speech communication plays an essential part of our 
daily life, the utilization of the emotional speech is expected to make human-robot communication 
smooth. In this paper, an emotion model based on Hidden Markov Model is proposed to simulate 
the dynamic processes of emotional self-regulation and emotional transference under the influence 
of the same stimuli arouse by the result of emotional speech recognition. In addition, by selecting 
the parameters of the model, the models of optimistic and pessimistic personality traits are also built. 
In order to maintain the consistency of the personality traits, the personality models under the 
influence of all the combinations of recognition error are considered and analyzed to figure out the 
relationship with model parameters through a series of simulation. 
 
Results of Projectsï¼š 
ï¬ Emotional Speech Recognition 
Emotional speech recognition has been developed for decades with more and more interest. 
The classification of emotional speech is the first important task of emotion recognition, and it 
can be widely used in variety of fields for application. The typical process of emotional speech 
classification is depicted in Fig. 27. The speech samples used in this paper are obtained from 
Berlin database. Several typical acoustic features based on previous researches are selected to 
proceed with feature extraction. And then, the best suitable features are selected by WEKA 
(Waikato Environment for Knowledge Analysis) in [13] which is a machine learning software 
written in Java as description of emotional speech to analyze the characteristics of emotions in 
speech. Finally, the selected features are also classified by WEKA to present certain kinds of 
emotion.  
near 1 in correspondence with the change of T , while ( )Tp  of other emotion states decreases from 
corresponding emotion state initial probability distribution to near 0. The rule of change in emotion 
state probability distribution satisfies the hypothesis that one kind of stimulus arouses only one 
corresponding kind of emotion state. 
 
Figure 29: The change curve of emotion state probability distribution in correspondence with the change of Î¸ 
It can be seen that the maximum change rate of 
ï€¨ ï€©T
ipï„  is increasing when parameter ï±  is 
increasing. In the meanwhile, the linear change region will become narrower and the maximum 
value of T  will become smaller. That is to say, with greater value of ï± , the reaction rate of 
emotion to stimulus is faster. Therefore, parameter ï±  can be used to adjust the response rate of 
emotion to stimulus. 
Figure 30 shows the change curve of emotion intensity of emotion state 1 ( )
1
TPï„  in 
correspondence with different initial probability distribution ï° . The relevant parameters are as 
follows: 1m ï€½ , 1.06k ï€½ , 12ï± ï€½ , ï› ï* 1 3 1 3 1 3ï° ï€½ . 
 
Fig. 30: The change curve of emotion intensity corresponds to the change of T, while the limit probability is the same, 
but the initial probability distribution of personality transfer matrix is different. 
It can be seen that emotion intensity ( )
1
TPï„  is increasing in correspondence with the changes of 
T  under the influence of stimulation. When ( )10 0.45
TPï„ï‚£ ï‚£ , the change of emotion intensity 
seems to grow linearly; while when ( )
1 0.45
TPï„ ï€¾ , the change of emotion intensity becomes very 
slow. As a whole, the change rate of ( )
1
TPï„  is similar in different initial probability distribution ï° . 
That is to say, initial probability distribution only provides different value of emotion intensity in 
transfer process of emotion state. 
0 10 20 30 40 50 60
-0.1
-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
T
ï„
p
i(T
)
 
 
ï±=7
ï±=12
ï±=18
0 5 10 15 20 25 30 35 40 45 50 55 60
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
T
P
1
ï„
(T
)
 
 
ï°=[4/12 4/12 4/12]
ï°=[6/12 3/12 3/12]
ï°=[8/12 2/12 2/12]
ï°=[9.4/12 1.3/12 1.3/12]
ï°=[10/12 1/12 1/12]
ï°=[3/12 4.5/12 4.5/12]
ï°=[2/12 5/12 5/12]
ï°=[1/12 5.5/12 5.5/12]
ï°=[0.4/12 5.8/12 5.8/12]
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«è¡ç”Ÿç ”ç™¼æˆæœæ¨å»£è³‡æ–™è¡¨
æ—¥æœŸ:2011/10/31
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«
è¨ˆç•«åç¨±: ç¸½è¨ˆç•«ï¼šæ–°ä¸–ä»£è¾¦å…¬å¤§æ¨“ä¹‹ã€Œæ™ºæ…§å‹æœå‹™æ©Ÿå™¨äººæ—ã€
è¨ˆç•«ä¸»æŒäºº: å‚…ç«‹æˆ
è¨ˆç•«ç·¨è™Ÿ: 99-2221-E-002-190- å­¸é–€é ˜åŸŸ: æ™ºæ…§å‹æ©Ÿå™¨äºº
ç„¡ç ”ç™¼æˆæœæ¨å»£è³‡æ–™
å…¶ä»–æˆæœ 
(ç„¡æ³•ä»¥é‡åŒ–è¡¨é”ä¹‹æˆ
æœå¦‚è¾¦ç†å­¸è¡“æ´»å‹•ã€ç²
å¾—çé …ã€é‡è¦åœ‹éš›åˆ
ä½œã€ç ”ç©¶æˆæœåœ‹éš›å½±éŸ¿
åŠ›åŠå…¶ä»–å”åŠ©ç”¢æ¥­æŠ€
è¡“ç™¼å±•ä¹‹å…·é«”æ•ˆç›Šäº‹
é …ç­‰ï¼Œè«‹ä»¥æ–‡å­—æ•˜è¿°å¡«
åˆ—ã€‚) 
ç„¡ 
 æˆæœé …ç›® é‡åŒ– åç¨±æˆ–å…§å®¹æ€§è³ªç°¡è¿° 
æ¸¬é©—å·¥å…·(å«è³ªæ€§èˆ‡é‡æ€§) 0  
èª²ç¨‹/æ¨¡çµ„ 0  
é›»è…¦åŠç¶²è·¯ç³»çµ±æˆ–å·¥å…· 0  
æ•™æ 0  
èˆ‰è¾¦ä¹‹æ´»å‹•/ç«¶è³½ 0  
ç ”è¨æœƒ/å·¥ä½œåŠ 0  
é›»å­å ±ã€ç¶²ç«™ 0  
ç§‘ 
æ•™ 
è™• 
è¨ˆ 
ç•« 
åŠ  
å¡« 
é … 
ç›® è¨ˆç•«æˆæœæ¨å»£ä¹‹åƒèˆ‡ï¼ˆé–±è½ï¼‰äººæ•¸ 0  
 
