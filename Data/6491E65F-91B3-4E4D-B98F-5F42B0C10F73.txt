1 
 
ï¨ˆæ”¿é™¢åœ‹å®¶ç§‘å­¸å§”å“¡æœƒå°ˆé¡Œç ”ç©¶è¨ˆç•«æˆæœå ±å‘Š 
æ¨¡ç³Šèª¿æ§æ”¯æŒå‘ï¥¾æ©Ÿ-ï§åˆ¥æ¨™ç±¤æ¨¡ç³ŠåŒ– 
(Fuzzy Regularized Support Vector Machines - Class Label Fuzzification) 
 
è¨ˆç•«ç·¨è™Ÿï¼šNSC 95-2221-E-149-016 
åŸ·ï¨ˆæœŸé™ï¼š95 ï¦ 8 æœˆ 1 æ—¥è‡³ 96 ï¦ 7 æœˆ 31 æ—¥ 
ä¸» æŒ äººï¼šæ¥Š æ£§ é›²      ï¥£å°ç£ç§‘å­¸æŠ€è¡“å­¸é™¢ æ©Ÿæ¢°å·¥ç¨‹å­¸ç³» 
å…±åŒä¸»æŒäººï¼šæ¥Š æ™º æ—­      æ·¡æ±Ÿå¤§å­¸ æ©Ÿæ¢°å·¥ç¨‹å­¸ç³» 
è¨ˆç•«ï¥«èˆ‡äººå“¡ï¼šæˆ´ä¿¡ï¥¼ã€å‘¨å³°æ¯…ã€ï§´ï§·å¤« 
 
 
æ‘˜è¦ 
æœ¬ç ”ç©¶å¾å­¸ç¿’æ¨£æœ¬çš„ï§åˆ¥æ¨™ç±¤è»ŸåŒ–çš„è§’ï¨ï¼Œå°å…¥ä¸€å€‹åŸºæœ¬çš„æ¨¡ç³Šæ¦‚ï¦£ï¼Œç”¨ä»¥æ”¹è®Šå­¸
ç¿’æ¨£æœ¬åœ¨çµ„æ…‹æ”¯æŒå‘ï¥¾æ©Ÿçš„æ±ºç­–å‡½ï¥©æ™‚çš„äºŒå…ƒåŒ–å°å³™æ…‹ï¨ã€‚æ¨¡ç³ŠåŒ–çš„ï§åˆ¥æ¨™ç±¤å¯å…¼å®¹ä¸€
å€‹è¶³ä»¥è¡¨é”å­¸ç¿’æ¨£æœ¬åœ¨ç¾¤é«”ä¸­å› å„ç¨®åŸå› è€Œå°è‡´çš„å­¸ç¿’æ…‹ï¨ï¥§ä¸€è‡´ã€‚æœ¬ç ”ç©¶åŒæ™‚æå‡ºä¸€
å€‹äºŒéšæ®µçš„æ¨£æœ¬æ…‹ï¨è©•ä¼°æ–¹æ³•ï¼Œç”¨ä»¥æŒ‡å®šï§åˆ¥æ¨™ç±¤æ¨¡ç³Šå€¼ï¼Œå…¶å¤§å°æ­£ç›¸é—œæ–¼å…¶å°æ–¼æ”¯æŒ
å‘ï¥¾æ©Ÿçš„æ±ºç­–å‡½ï¥©çš„å½±éŸ¿ï¼Œæ˜¯ç‚ºä¸€ï§åˆ¥æ¨™ç±¤æ¨¡ç³ŠåŒ–æ”¯æŒå‘ï¥¾æ©Ÿã€‚ç¶“ç¬¬ä¸€éšæ®µæª¢è¦–çš„å­¸ç¿’
æ¨£æœ¬ï¼Œä¾è¨ˆç®—æ‰€å¾—çš„æ¨£æœ¬é–“éš”å¤§å°ï¼Œåœ¨ç¬¬äºŒéšæ®µé‡æ–°è³¦äºˆå¤§å°ï¥§ä¸€çš„æ¨£æœ¬æ…‹ï¨ï¼Œä»¥åŠ©æ–¼
æ”¯æŒå‘ï¥¾æ©Ÿåœ¨è¨“ï¦–æ™‚å› è€Œç²å–ä¸€è¼ƒå¤§é–“éš”çš„æ±ºç­–é‚Šç•Œã€‚ä¹Ÿå› é€™å€‹å¤§é–“éš”çš„æ±ºç­–é‚Šç•Œï¼Œæœ¬
ç ”ç©¶é€²è€Œç²å¾—æœ€å€¼å¾—æ³¨æ„çš„çµæœï¼Œä¸€å€‹è¼ƒä½³çš„æ³›åŒ–æ€§èƒ½çš„æ±ºç­–å‡½ï¥©ã€‚ 
é—œéµè©ï¼šæ”¯æŒå‘ï¥¾æ©Ÿ, æ¨¡ç³Š, æ¨¡å¼è¾¨ï§¼ 
 
Abstract 
The paper attempts to introduce a fundamental fuzzy concept to break the equivalent at-
titude of the input training set of SVM, and tries to give individual example in the set a dif-
ferent attitude. The attitude can stand for the influence that the example takes into account in 
the classification. In the paper, we present a method to refresh the attitude by assigning proper 
fuzzy value to the class label of each example. Based on the benefit of individualized fuzzifi-
cation, we re-examine the formulation of SVM, and try to find out the corresponding effects. 
Although it costs a little more computation, the introduction of fuzzy class label is eventually 
worth for the better generalization performance with a large margin. The effect is especially 
both crucial and subtle in worse confused training data set with many hard examples. 
Keywordsï¼šSupport vector machine; Fuzzy; Pattern Recognition 
3 
 
å¦‚å‰è¿°ï¼Œåœ¨æœ€ä½³åŒ–çš„éç¨‹ä¸­ï¼Œæå¤±å‡½ï¥©è‘—é‡æ–¼æ‡²ç½°è² å€¼é–“éš”çš„æ¨£æœ¬ï¼Œä¸¦ä»¥æ‰€å¾—åˆ°çš„
åˆ†ï§æ­£ç¢ºç¨‹ï¨ï¤­æ±ºå®šæ‡²ç½°çš„å¤§å°ï¼Œä»¥ä¸€å€‹è¦æ±‚è¨“ï¦–æ­£ç¢ºæ€§é«˜çš„åˆ†ï§å™¨ï¤­ï¥¯ï¼Œå¯åŠ é‡æå¤±
å‡½ï¥©å°è² å€¼é–“éš”æ¨£æœ¬çš„æ‡²ç½°ï¼Œæ‡²ç½°ä»£åƒ¹é«˜è¿«ä½¿åˆ†ï§å™¨æ¸›å°‘èª¤å·®åˆ†ï§æ­£ç¢ºæ€§è‡ªç„¶æé«˜ã€‚ç›¸
å°è€Œè¨€ï¼Œåœ¨ä¸€èˆ¬çš„æƒ…å½¢ä¸‹ï¼Œåªæœƒè¦æ±‚ä¸€å€‹è¶³å¤ çš„æ‡²ç½°ç¨‹ï¨ä»¥è¿½æ±‚èƒ½ç”¢ç”Ÿæœ€å¤§çš„åˆ†ï§é–“
éš”ã€‚æ‰€ä»¥åœ¨ä¸Šè¿°çš„æ¢ä»¶ä¸‹ï¼Œä»¥ä¸€å€‹ï¥«ï¥© C ï¤­èª¿æ•´è¨“ï¦–ï¨ï¨èˆ‡é–“éš”ä¹‹é–“çš„å¹³è¡¡ã€‚åŒæ™‚é€é
é€™å€‹æ¦‚ï¦£ï¼Œæå‡ºï¦ºä»¥å€‹åˆ¥èª¿æ•´æ‡²ç½°ï¤­æ¸›å°‘å…·å¹²æ“¾æ¨£æœ¬å½±éŸ¿çš„æ–¹æ³•ï¼Œå› ç‚ºå…·å¹²æ“¾æ¨£æœ¬é€šå¸¸
å…·æœ‰æˆ–å¤šæˆ–å°‘çš„å¹²æ“¾é›œè¨Šï¼Œæ‰€ä»¥å¦‚ä½•æ¸›ä½å…·å¹²æ“¾æ¨£æœ¬çš„å½±éŸ¿æ˜¯æœ€ç›´æ¥èƒ½å¼·åŒ–åˆ†ï§å™¨çš„æ–¹
å¼ï¼Œæ›è¨€ä¹‹ï¼Œå¢é€²ï¦ºåˆ†ï§å™¨çš„æ³›åŒ–èƒ½ï¦Šã€‚ 
2. ç ”ç©¶æ–¹æ³• 
2.1. ï§åˆ¥æ¨™ç±¤æ¨¡ç³ŠåŒ–æ¨¡å‹ 
ç‚ºï¦ºå¾æ–°è¨ˆç®—å°å…·å¹²æ“¾æ¨£æœ¬çš„æ‡²ç½°ï¼Œæˆ‘å€‘å»ºï§·ï¦ºæ¨™ç±¤æ¨¡ç³ŠåŒ–æ”¯æŒå‘ï¥¾æ©Ÿä»¥é”åˆ°é€™æ¨£
çš„æ•ˆæœï¼Œé€™å€‹è¨ˆç®—å¾ç›£ç£å¼æ©Ÿå™¨å­¸ç¿’é–‹å§‹ï¼Œé¦–å…ˆåœ¨æ¨£æœ¬ç©ºé–“ä¸­å–å¾—ä¸€å€‹è¨“ï¦–é›† Tï¼ŒT = {(xi, 
yi)}, i = 1, 2, â€¦, nï¼Œåœ¨æ­¤è¨“ï¦–é›†ä¸­ï¼Œå„æ¨£æœ¬è³¦äºˆä¸€å€‹æ¨™ç±¤ yiï¼Œyiâˆˆ{-1, +1}ï¼Œä»¥é€²ï¨ˆäºŒå…ƒåˆ†
ï§(Binary Classification)ï¼Œå°‡æ¨£æœ¬åˆ†ï§åˆ°ï¥¸å€‹ï¥§åŒçš„ï§åˆ¥ã€‚éå»æˆ‘å€‘æ›¾æ¢è¨éå¦‚ä½•å°‡å—é™
åˆ¶çš„ï§åˆ¥æ¨™é–“çµ¦äºˆä¸€è¼ƒå¯¬é¬†çš„ç¯„åœå€¼[6-7]ï¼Œé€™æ¨£çš„æ–¹å¼èƒ½è®“è¨“ï¦–é»å…·æœ‰ï¤å¤šé—œæ–¼è‡ªèº«æ‰€
ä»£è¡¨çš„è³‡è¨Šï¼Œé€éæ­¤ç¨®æ¨¡ç³ŠåŒ–çš„æ–¹å¼ï¼Œæ¨£æœ¬çš„ï§åˆ¥æ¨™ç±¤å°‡å¯¦éš›çš„å¾åŸæœ¬é™åˆ¶çš„å›ºå®šå€¼è½‰
æ›æˆä¸€æ¨¡ç³Šé›†åˆï¼Œè¨“ï¦–é›†åˆå› æ­¤æ”¹è®Šç‚ºï¼š 
. ..., ,3 ,2 ,1     )}, ,{(' nizT ii == x  (4) 
åœ¨(4)ä¸­ ziä»£è¡¨ï¦ºæ¨£æœ¬ i çš„å½±éŸ¿ï¦Šï¼Œç‚ºä¸€æ¬Šé‡ï¼Œå°‡å…¶æ¬Šé‡è¡¨é€¹ç‚ºç¶“æ¨¡ç³ŠåŒ–å¾Œçš„ç³¢ç³Šæ¨™ç±¤ï¼Œ
é€™æ¨£çš„ç›®çš„æ˜¯ç‚ºï¦ºèƒ½ä»¥æ­¤æ¸›å°‘å…·å¹²æ“¾æ¨£æœ¬çš„æ¬Šé‡ï¤­æ¸›ä½å…¶å½±éŸ¿ï¦Šã€‚ä¸¦å°‡ ziä»¥çµåˆä¸€å€‹æ¨¡
ç³Šä¿‚ï¥© siï¤­è¡¨ç¤ºï¼š 
.iii ysz =  (5) 
æ¨¡ç³Šï¥© si å¯ç¶“ç”±è¨±å¤šï¥§åŒçš„æ¯”ï¦µç¸®æ¸›å‡½ï¥©ï¤­ç²å¾—ã€‚æœ€ç¶“å¸¸çš„ï§ºæ³æ˜¯ç”±ä¸€å€‹ï¦¸å±¬å‡½ï¥©
(Membership Function, MF)ç”¨ä»¥æ¨ä¼°æ¨£æœ¬(xi, yi)èˆ‡å…¶æ¯æ—ç¾¤ä¹‹é–“çš„é—œä¿‚ï¤­ç²å¾—ï¼š 
)).(,( iii fyfzs x=  (6) 
ç‚ºï¦ºè¡¨é€¹ä¸€å€‹é©ï¨€çš„èˆ‡æ¯æ—ç¾¤ä¹‹é–“é—œä¿‚çš„ï¦šç¹«ï¼Œä¸€å€‹ç”¨ä»¥è¡¨é€¹å…¶é—œä¿‚å¼·å¼±çš„ï¥¾æ¸¬å¿…
é ˆå…ˆçµ¦å®šã€‚æˆ‘å€‘æœŸæœ›é€™å€‹ï¥¾æ¸¬å¯ä»¥ç·šæ€§åœ°å°‡é—œä¿‚è½‰æ›æˆä¸€å€‹ï¦šçºŒçš„æŒ‡æ¨™å€¼ã€‚åœ¨ä¸€èˆ¬çš„èª
å®šï§¨ï¼Œé–“éš” Î¸i = yif(xi)æ˜¯é€™å€‹ï¥¾æ¸¬ä¸€å€‹ç›¸ç•¶ï¥§éŒ¯çš„é¸æ“‡ã€‚ï¥§åªæ˜¯å› ç‚º Î¸iå¯ä»¥é‘‘åˆ¥å…¶å€¼æ˜¯
å¦ç‚ºè² ï¼Œè€Œä¸”å®ƒå°šå¯åæ‡‰æ¨£æœ¬åœ¨ç©ºé–“ä¸­èˆ‡åˆ†ï§è¶…å¹³é¢çš„è·ï§ªï¼Œä¸€å€‹æ¨£æœ¬ï¥´å…¶ Î¸iè² å€¼è¶Šå¤§
5 
 
æˆï¼š 
. ..., 2, 1,     ,1),( nibsy iii =â‰¥+xw  (11)ï€© 
åœ¨æ¨™ç±¤ yiåªèƒ½ä»¥+1 æˆ–-1 è¡¨ç¤ºçš„æ¢ä»¶ä¸‹ï¼Œæˆ‘å€‘ä¹Ÿå¯å°‡ï¥§ç­‰å¼åˆ†é–‹è¡¨ç¤ºç‚ºä¸‹é¢ï¥¸å€‹å¼å­ 
.1for      /1,
and ,1for      /1,
âˆ’=âˆ’â‰¤+
+=+â‰¥+
iii
iii
ysb
ysb
xw
xw
 (12)ï€© 
å°ç…§åŸæœ¬æ¨™æº–çš„ SVM ï¼š 
,1for      1,
and ,1for      1,
âˆ’=âˆ’â‰¤+
+=+â‰¥+
ii
ii
yb
yb
xw
xw
 (13)ï€© 
åœ¨(12)å¼ä¸­ï¼Œå› ç‚ºå°å…¥ï¦º si æ‰€ä»¥æˆ‘å€‘å¯ä»¥å°‡é™åˆ¶å¼å¯¦ï¥©åŒ–ï¼Œsi ç‚ºä¸€æ¨¡ç³Šå€¼ï¼Œæ•…å…¶å€’ï¥©å¿…
å®šæœƒå¤§æ–¼ 1ã€‚è€Œåœ¨å¯¦éš›çš„æ‡‰ç”¨ä¸Šï¥§ç­‰å¼(12)ä¸­çš„ä¹Ÿæ”¹è®Šï¦ºåŸå§‹çš„å¼å­ä¸­çš„é™åˆ¶å¼ï¼Œæ¦‚æ‹¬
è€Œè¨€ï¼Œä»¥ ziï¤­ä»£æ›¿ yièƒ½å½±éŸ¿åŸå§‹é™åˆ¶çš„é‚Šç•Œæ¢ä»¶ï¼Œä½¿å…¶å¾â€œÂ±1â€å»¶ä¼¸ç‚ºâ€œÂ±1/siâ€ï¼Œæ¯”è¼ƒåŸå§‹
çš„ SVMï¼Œsiåœ¨æœ€ä½³åŒ–çš„éç¨‹ä¸­æ‰€å…·æœ‰ä¸€å€‹æœ€é‡è¦çš„åƒ¹å€¼å³åœ¨æ–¼è®“æˆ‘å€‘èƒ½ä»¥ä¿‚ï¥©è®ŠåŒ–çš„æ–¹
å¼ï¤­èª¿æ•´å°æ‡‰æ¯ä¸€é™åˆ¶å¼çš„å¯ï¨ˆå€åŸŸ(Feasible Region)ã€‚ 
ä»¿ä¸Šè¿°æ¨¡ç³ŠåŒ–æ¨™ç±¤ ziçš„å½±éŸ¿ï¼Œæˆ‘å€‘é‡æ–°ä¿®æ­£ï¦ºè»Ÿé–“éš”æ”¯æŒå‘ï¥¾æ©Ÿï¼Œå…·Î¾içš„é™åˆ¶å¼æ”¹
è®Šå¦‚ä¸‹ï¼š 
     ibz iii âˆ€âˆ’â‰¥+     1),( ,Î¾xw  (14) 
é€éçµåˆé™åˆ¶å¼(14)èˆ‡ç›®æ¨™å‡½ï¥©(1)ï¼Œåœ¨ç¶“é yièˆ‡ ziè½‰æ›åŠä»¿çš„åŸå§‹ SVM æ¨æ¼”éç¨‹ï¼Œæˆ‘
å€‘å¯ä»¥å¾—åˆ°ä¸€å€‹èˆ‡åŸå§‹ SVM ï§ä¼¼çš„å¼å­ï¼Œæœ€å¾Œç¶“ç”±ï§ä¼¼çš„æ­¥é©Ÿè½‰æ›æˆä¸€å€‹ä»¥å°å¶è®Šï¥©
è¡¨é€¹çš„çš„äºŒæ¬¡æœ€ä½³åŒ–å•é¡Œï¼š 
,,
2
1)( max
11 1
âˆ‘âˆ‘âˆ‘
== =
+âŒªâŒ©âˆ’=
n
i
i
n
i
n
j
jijijid zzJ Î±Î±Î± xxÎ±  (15) 
é™åˆ¶å¼ 
.0 Ci â‰¤â‰¤ Î±  (16)ï€© 
è§€å¯Ÿ(15-16)ï¼Œé€™ï§¨æå‡ºçš„æ¨¡ç³ŠåŒ–ï§åˆ¥æ¨™ç±¤æ”¯æŒå‘ï¥¾æ©Ÿèˆ‡åŸå§‹çš„æ”¯æŒå‘ï¥¾æ©Ÿ[1]å¹¾è¿‘
ç›¸åŒï¼Œå·®åˆ¥åªåœ¨æ–¼å°‡ yiä»¥ ziåšå–ä»£ã€‚åŒæ™‚ä¹Ÿèˆ‡å…ˆå‰ Lin èˆ‡ Wang æ‰€æå‡ºçš„æ¨¡ç³Šæ”¯æŒå‘ï¥¾
æ©Ÿ(FVM)ï¥§åŒ[8-9]ï¼ŒFSVM å¼•ç”¨ï¦ºå¤–éƒ¨çš„æ¨¡ç³Šå€¼ï¤­æ¨æ¼”å…¶é—œä¿‚ï¼Œåœ¨æœ€ä½³åŒ–çš„éç¨‹ä¸­ä»¥å¤–
éƒ¨çš„æ¨¡ç³Šå€¼åšç‚ºèª¿æ•´é¬†å¼›è®Šï¥©Î¾içš„æ¬Šé‡å› å­ã€‚ 
7 
 
  
(a) ï¥§å…·æœ‰ï¦¸å±¬å‡½ï¥©åŸå§‹ SVM æ¨¡å‹       (b) ä½¿ç”¨å¸¸ï¥©ï¦¸å±¬å‡½ï¥©æ¨¡ç³ŠåŒ–è½‰æ›å¾Œæ¨¡å‹ 
 
(c) ä½¿ç”¨ç·šæ€§ï¦¸å±¬å‡½ï¥©æ¨¡ç³ŠåŒ–è½‰æ›å¾Œæ¨¡å‹ 
åœ– 3. æ¯”è¼ƒåˆ†ç”±å¸¶ï¦¸å±¬å‡½ï¥©æˆ–ï¥§å¸¶ï¦¸å±¬å‡½ï¥©ï¥§åŒæ¨¡å‹ç”¢ç”Ÿçš„æ±ºç­–é‚Šç•Œèˆ‡åˆ†ï§é–“éš” 
æ­¤å¤–ï¼Œæˆ‘å€‘ä½¿ç”¨ä¸€å€‹ k æ‘ºäº¤å‰é©—è­‰(k-fold cross-validation)ï¤è©³ç´°çš„æ¸¬è©¦æ¨¡ç³ŠåŒ–è½‰æ›
å¾Œæ¨¡å‹çš„æ€§èƒ½ã€‚è¨­ k = 10ï¼Œå°ç­‰åœ°è¨­å®šï¥«ï¥© Î¸ã€Ï„ï¼Œä¸¦ä»¥æ ¼é»{1x102, 1x103, 1x104}ï¤­è¨­å®š
Cï¼ŒRBF æ ¸å‡½ï¥©ä¸­çš„ï¥«ï¥© Ïƒ å‰‡çµ±ä¸€æ¡ç”¨ 1ï¼Œå° Twonorm è³‡ï¦¾é›†é€²ï¨ˆæ ¼é»æœå°‹é…åˆäº¤å‰é©—
è­‰çš„ç¨‹åºï¼Œï¤å»£æ³›çš„ï¦ºè§£ä½¿ç”¨ï¥§åŒï¦¸å±¬å‡½ï¥©åˆ†ï§å™¨çš„ï¨ˆç‚ºã€‚åœ– 4 ç¹ªå‡ºå¯¦é©—å¾Œå…¶æ€§èƒ½æ¯”è¼ƒã€‚ 
åœ– 4 æ¯ä¸€å­åœ–ä¸­ï¼Œå¯¦ç·šåŠå…¶ä¸Šçš„å¯¦å¿ƒç¬¦è™Ÿé»èˆ‡è™›ç·šåŠå…¶ä¸Šçš„ç©ºå¿ƒç¬¦è™Ÿé»åˆ†åˆ¥è¡¨ç¤ºå…·
ï¦¸å±¬å‡½ï¥©çš„æ¨¡ç³ŠåŒ–è½‰æ›å¾Œæ¨¡å‹èˆ‡ï¥§å…·ï¦¸å±¬å‡½ï¥©çš„åŸå§‹æ¨¡å‹çš„æ€§èƒ½è¶¨å‹¢æ¯”è¼ƒï¼Œæ¯”è¼ƒåŸå§‹æ¨¡
å‹ï¼Œé›–ç„¶æ¨¡ç³ŠåŒ–è½‰æ›å¾Œæ¨¡å‹åœ¨ä¸Šè¿°å°ç­‰è¨­å®šä¸‹ä»¥è¼ƒä½çš„ C å€¼å¯é€¹åˆ°è¼ƒä½çš„äº¤å‰é©—è­‰èª¤
å·®ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œæ¡ç”¨å¸¸ï¥©ï¦¸å±¬å‡½ï¥©çš„æ¨¡å‹ï¼Œå…¶äº¤å‰é©—è­‰èª¤å·®çš„ï¨‰ä½ä¸¦ï¥§æ˜é¡¯(åœ– 4a)ï¼Œä½†
æ¡ç”¨ç·šæ€§ï¦¸å±¬å‡½ï¥©çš„æ¨¡å‹ï¼Œå°±æœ‰å¯è§€è€Œæ˜é¡¯çš„ï¨‰ä½(åœ– 4c)ã€‚é€™å€‹è¼ƒä½çš„äº¤å‰é©—è­‰èª¤å·®è­‰
æ˜æ¨¡ç³ŠåŒ–è½‰æ›å¾Œæ¨¡å‹å…·æœ‰å¤§é–“éš”åˆ†ï§å™¨çš„å„ªè¶Šç‰¹æ€§ã€‚è€Œæœ€ä½çš„äº¤å‰é©—è­‰èª¤å·®ä¸¦éå‡ºç¾åœ¨
è®ŠåŒ–ï¥¡ä¿‚ï¥© Î¸ åŠ Ï„ æ¡ç”¨æ¥µç«¯çš„è¨­å®šå€¼ï¼Œé€™è¡¨ç¤ºæº«å’Œçš„æ¸›ä½å…·å¹²æ“¾æ¨£æœ¬çš„å½±éŸ¿ï¦Šè¼ƒå¥½ï¼Œå¤ª
æ¿€ï¦Ÿçš„æ¸›ä½å½±éŸ¿ï¦Šåè€Œä½¿å¾—åˆ†ï§å™¨å­¸ç¿’ï¥§è¶³(Under-fitting)ï¼Œå°è‡´æ³›åŒ–æ€§èƒ½ï¨‰ä½ã€‚åœ– 4b
åŠåœ– 4d äºŒå­åœ–å‰‡ç¹ªå‡ºéš¨è®ŠåŒ–ï¥¡ä¿‚ï¥© Î¸ åŠ Ï„ å¢åŠ çš„é–“éš”è®ŠåŒ–è¶¨å‹¢ï¼Œç›¸å°æ–¼äº¤å‰é©—è­‰èª¤å·®
æˆ–å¤šæˆ–å°‘çš„éš¨è®ŠåŒ–ï¥¡ä¿‚ï¥©å¢åŠ è€Œæ¸›å°‘ï¼Œåœ– 4b åŠåœ– 4d è¡¨é€¹ä¸€å€‹ä¸€è‡´çš„é–“éš”å¢åŠ è¶¨å‹¢ï¼Œé–“
éš”æŒçºŒåœ°å¢åŠ ç›´åˆ°å…¶åˆ°é€¹ä¸€å€‹é«˜ç•Œï¼Œé›–ç„¶æ­¤é«˜ç•Œåœ¨ä½¿ç”¨å¸¸ï¥©ï¦¸å±¬å‡½ï¥©çš„æ¨¡å‹ä¸­å› ï¥«ï¥©å€¼
å°šæœªåˆ°é€¹é£½å’Œè€Œç„¡å¾è§€å¯Ÿ(åœ– 4b)ï¼Œä½†åœ¨ä½¿ç”¨ç·šæ€§ï¦¸å±¬å‡½ï¥©çš„æ¨¡å‹å°±å¯æ˜é¡¯è§€å¯Ÿåˆ°(åœ–
4d)ã€‚ç„¶è€Œäº‹å¯¦ä¸Šï¼Œåœ¨æ­¤é«˜ç•Œæ‰€æœ‰çš„å…·å¹²æ“¾æ¨£æœ¬çš„å½±éŸ¿ï¦Šè¢«æ¶ˆæ»…æ®†ç›¡ï¼Œåè€Œå¦‚å‰æ‰€è¿°å› 
9 
 
[2]  C. Cortes and V.N. Vanik, â€œSupport vector networks,â€ Machine Learning, Vol. 20, 1995, 
pp 273-297. 
[3]  T. Hastie, R. Tibshirani, and J. Friedman, The Eelements of Statistical Learning, 
Springer-Verlag, New York, 2001. 
[4]  J.D. McAuliffe, M.I. Jordan and P.L. Bartlett, â€œConvexity, classification, and risk bounds,â€ 
Tech. Report 638, Dept. of Statistics, UC Berkeley, 2003, CA. 
[5]  J. Shawe-Taylor, M. Anthony, P.L. Bartlett, and R.C. Williamson, â€œStructural risk minimi-
zation over data-dependent hierarchies,â€ IEEE Transactions on Information Theory, Vol. 
44, No. 5, 1998, pp 1926-1940. 
[6]  C.-Y. Yang, â€œSupport vector classifier with a fuzzy-value class label,â€ Lecture Notes in 
Computer Science, Vol. 3173, 2004, Springer-Verlag, Berlin, pp 506-511. 
[7]  C.-Y. Yang, â€œGeneralization Ability in SVM with Fuzzy Class Labels.â€ Proceedings of In-
ternational Conference on Computational Intellignece and Security 2006, Guangzhou, 
China, Nov. 2006, pp 97-100. 
[8]  C.-F. Lin, and S.-D. Wang, â€œFuzzy support vector machines,â€ IEEE Transactions on Neu-
ral Network, Vol. 13, No. 2, 2002, pp 464-471. 
[9]  C.-F. Lin, and S.-D. Wang, â€œTraining algorithms for fuzzy support vector machines with 
noisy data,â€ Pattern Recognition Letters, Vol. 25, No. 14, 2004, pp 1647-1656. 
[10]  L. Breiman, â€œBias, variance and arcing classifiers,â€ Tech. Report 460, Dept. of Statistics, 
UC Berkeley, 1996, CA. 
[11]  B. SchÃ¶lkopf, D. Schuurmans, P.L. Bartlett, and A.J. Smola, Advances in Large Margin 
Classifiers, MIT press, Cambridge, MA, 2000. 
åŸ·ï¨ˆæœ¬è¨ˆç•«ç›¸é—œä¹‹è‘—ä½œ 
[1]  Chan-Yun Yang, Che-Chang Hsu, Jr-Syu Yang. Learning SVM with Varied Example Cost: 
A kNN Evaluating Approach. Lecture Notes in Artificial Intelligence. (To appear) 
[2]  Chan-Yun Yang. Generalization Ability in SVM with Fuzzy Class Labels. In Proceedings 
of International Conference on Computational Intellignece and Security 2006 (CIS 2006), 
Guangzhou, China, pp.97-100, 2006,11. (EI) 
[3]  Chan-Yun Yang, Che-Chang Hsu, Jr-Syu Yang. A Novel SVM to Improve Classification 
for Heterogeneous Learning Samples. In Proceedings of International Conference on 
Computational Intellignece and Security 2006 (CIS 2006), Guangzhou, China, pp.172-175, 
2006,11. (EI) 
[4]  Che-Chang Hsu, Chan-Yun Yang, Jr-Syu Yang. Associating kNN and SVM for higher 
classification accuracy. Lecture Notes in Artificial Intelligence, 3801 (2005) 550-555. 
(SCIE) 
[5]  Chan-Yun Yang. Support vector classifier with a fuzzy-value class label. Lecture Notes in 
Computer Science, 3173 (2004) 506-511. (SCIE) 
[6]  Chan-Yun Yang. Highlighting heterogeneous samples in support vector machines' training. 
Submitted to Neurocomputing. (Revised) 
[7]  Chan-Yun Yang, Che-Chang Hsu, Jr-Syu Yang. Emphasizing heterogeneities in SVM clas-
sification by an embedded kNN preprocessor. Submitted to Pattern Recognition Letters. 
(Under revision) 
[8]  Chan-Yun Yang. Learning support vector machines with fuzzified class labels. Submitted 
to Fuzzy Sets and Systems. (Under review) 
è¨ˆç•«æˆæœè‡ªè©• 
å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
                                                             
è¨ˆç•«ç·¨è™Ÿ NSC 95-2221-E-149-016 
è¨ˆç•«åç¨± æ¨¡ç³Šèª¿æ§æ”¯æŒå‘ï¥¾æ©Ÿ - ï§åˆ¥æ¨™ç±¤æ¨¡ç³ŠåŒ– 
å‡ºåœ‹äººå“¡å§“å 
æœå‹™æ©Ÿé—œåŠè·ç¨± 
æ¥Šæ£§é›², ï¥£å°ç£ç§‘å­¸æŠ€è¡“å­¸é™¢, å‰¯æ•™æˆ 
æœƒè­°æ™‚é–“åœ°é» è‡ª 95ï¦ 11æœˆ 03æ—¥è‡³ï¦ 11æœˆ 06æ—¥, ä¸­åœ‹å¤§ï§“å»£æ±ï¥­å»£å·å¸‚ 
æœƒè­°åç¨± 2006 International Conference on Computational Intelligence and Security 
ç™¼è¡¨ï¥æ–‡é¡Œç›® 
1. Generalization Ability in SVM with Fuzzy Class Labels 
2. A Novel SVM to Improve Classification for Heterogeneous Learning 
Samples. 
 
ä¸€ã€ï¥«åŠ æœƒè­°ç¶“é 
å»ï¦æˆ‘å€‘ï¥«åŠ é CIS2005çš„æœƒè­°ï¼Œä¸¦èˆ‡èˆ‡æœƒçš„è¨±å¤šå­¸è€…è¨ï¥äº¤æ›æ„ï¨Šï¼Œä»Šï¦ CIS2005
å†ï¨ï¤­å‡½é‚€ç¨¿ï¼Œæˆ‘å€‘æ¬£ç„¶åŒæ„çš„æŠ•ç¨¿ï¼Œåœ¨ç¶“éå†—é•·çš„å¯©ç¨¿éç¨‹å¾Œï¼Œæˆ‘å€‘çš„æ–‡ç« å„Œå€–ç²å¾—
ï¤¿å–ï¼Œï¥§æœŸåœ‹ç§‘æœƒè¨ˆç•«é€šéï¼ŒåŸç·¨ï¦œæœ‰å‡ºå¸­åœ‹éš›æœƒè­°çš„ç¶“è²»ï¼Œä½¿å¾—å‡ºåœ‹çš„ç¶“è²»æœ‰è‘—ï¤˜ï¼Œ
è¬è¬åœ‹ç§‘æœƒçš„æ”¯æŒï¼é›£èƒ½å¯è²´çš„æ˜¯ï¼Œæˆ‘å€‘æœ‰ï¥¸ç¯‡æ–‡ç« ç²å¾—æ­¤ç ”è¨æœƒåˆŠï¤¿ç™¼è¡¨ï¼Œæ”¶ï¤¿åˆ° IEEE 
Exploreçš„è³‡ï¦¾åº«ä¸­ï¼Œæ˜¯ EIæ”¶ï¤¿çš„ï¥æ–‡é›†ï¼Œ ä¸¦ä¸”åœ¨æœƒä¸­å—é‚€æ“”ä»»â€œSession Chairâ€ï¼Œé€™æ˜¯
å¤§æœƒçµ¦äºˆçš„æ¦®è­½ï¼Œä¹Ÿå°å€‹äººç ”ç©¶å—åˆ°é‡è¦–æœ‰ä¸€é»çš„æ„Ÿåˆ°å…‰æ¦®ï¼Œæˆ–è¨±é€™å¯ç”¨ä»¥è¡¨å¾µæˆ‘å€‘é•·
æœŸçš„è€•è€˜å—åˆ°ï¦ºè‚¯å®šã€‚ç•¶ç„¶æˆ‘å€‘ä¹Ÿåœ¨æ–‡ç« çš„èªŒè¬ä¸­æ¨™ä¸Šï¦ºåœ‹ç§‘æœƒçš„è¨ˆç•«ç·¨è™Ÿ
ã€ŒNSC95-2221-E-149-016ã€ï¼Œä»¥å½°é¡¯åœ‹ç§‘æœƒçµ¦æˆ‘å€‘çš„æ”¯æŒï¼ 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
æ”¯æŒå‘ï¥¾æ©Ÿæ˜¯æ©Ÿå™¨å­¸ç¿’åŠè³‡ï¦¾æ¢å‹˜ï¦´åŸŸè¼ƒæ–°çš„ä¸€å€‹ï§¤ï¥ï¼Œåœ¨ï§¤ï¥æˆ–æ‡‰ç”¨ï¨¦èˆ‡æ™‚ä¿±é€²
æ¯æ¯ï¨¦æœ‰ï¥§æ–·çš„å‰µæ–°ï¼Œï¥«åŠ å¦‚æ­¤åœ‹éš›æ€§ç ”è¨æœƒä¸€ï¤­å¯ä»¥ï¤æ¥è¿‘çš„ï¦ºè§£åœ‹éš›ä¸Šä¸€äº›æ–°ç™¼å±•
çš„å‰æ²¿ï¼ŒäºŒï¤­ç™¼è¡¨å€‹äººçš„ç ”ç©¶å¿ƒå¾—ï¼Œï¥«èˆ‡é€™å€‹åœ‹éš›æ€§çš„ç«¶çˆ­ï¼Œä»¥è¡¨é”æˆ‘å€‘çš„ç ”ç©¶ï¼Œå»ºï§·
ä¿¡å¿ƒã€‚æ­¤æ¬¡æœƒè­°çš„å°ˆå®¶å¤šç‚ºä¸–ç•Œå„åœ‹æ­¤ï¦´åŸŸä¹‹å°ˆå®¶ï¼Œå› æ­¤åœ¨é€™ï§¨å¯ä»¥å……åˆ†æ¥æ”¶åˆ°æœ‰é—œçš„
æœ€æ–°ç ”ç©¶åŠå…¶ç›®å‰ç™¼å±•çš„è¨Šæ¯ã€‚æˆ‘å€‘å¾ˆé«˜èˆˆä¸‰ï¦ï¤­çš„ç ”ç©¶æ­£ï§“çºŒåœ¨é–‹èŠ±çµæœä¸­ï¼Œæœ‰ï¦ºåœ‹
ç§‘æœƒçš„æ”¯æŒï¼Œæˆ‘å€‘æ¯”è¼ƒèƒ½æœ‰å°ˆæ³¨æŠ•æ³¨åœ¨ç ”ç©¶ä¸Šï¼è¬è¬åœ‹ç§‘æœƒï¼ç›®å‰æˆ‘å€‘æ­£åœ¨åŠªï¦Šæ“´å¤§æˆ‘
å€‘ç ”ç©¶ï¼ŒåŒ…æ‹¬å…¶æ‰€ç«™ï§·çš„åŸºç¤èˆ‡å»¶ä¼¸æ‡‰ç”¨ï¼Œä¹Ÿè©¦åœ–åœ¨æˆ‘æ ¡å»ºï§·ä¸€å€‹ç›¸é—œçš„ç ”ç©¶åœ˜éšŠï¼Œç•¶
ç„¶é€™éœ€è¦æŒçºŒçš„é—œæ³¨ï¼å¦å¤–ï¼Œæ™ºæ…§å‹æ§åˆ¶åœ¨æœ¬æ ¡ä¹‹æ©Ÿæ¢°ç³»åŠæ©Ÿé›»æ‰€æ˜¯å·²ï¨ˆä¹‹æœ‰ï¦çš„èª²ç¨‹ï¼Œ
ç”±æ–¼é€™æ–¹é¢çš„èª²ç¨‹å…§å®¹ï¥‹ï¥‹å‰µæ–°ï¼Œå¿…é ˆç¶“å¸¸å¸æ”¶å„ç¨®æœ€æ–°æŠ€è¡“ä»¥ç¶­æŒæ•™å­¸å“è³ªèˆ‡å…§å®¹ä¸Š
çš„è·Ÿä¸Šæ½®ï§Šã€‚æ­¤æ¬¡ç ”è¨æœƒå»£æ³›å¸æ”¶åˆ°çš„ä¸€äº›çœ‹æ³•åŠç ”ç©¶æ­£å¥½å¯ä»¥ç”¨ä»¥è£œå……é€™æ–¹é¢çš„èª²ç¨‹
éœ€æ±‚ï¼Œè—‰ä»¥è±å¯Œèª²ç¨‹å…§å®¹ï¼Œç•¢ç«Ÿé€™äº›è±å¯Œçš„è³‡ï¦¾ï¨¦æ˜¯ç•¶ä»Šæœ€æ–°çš„ç™¼å±•ã€‚å°‡ï¤­æˆ–å¯è¡¨ç¾åœ¨
æ©Ÿé›»æ•´åˆç ”ç©¶æ‰€æ™ºæ…§å‹æ§åˆ¶èª²ç¨‹æˆ–å…¶ä»–èª²ç¨‹çš„ç›¸é—œä¸»é¡Œä¸­ã€‚ 
In general, the surrogate of loss function always 
focuses on the examples with negative margin and tries 
to penalize these examples under the optimization. The 
degree of penalization depends on the goal of accuracy 
that the classification ties to. For good classification 
accuracy, the surrogate, forcing the optimizer to focus 
on the misclassified examples, should penalize the 
examples with negative margin more heavily than the 
ones with positive margin. In contrast to the goal of 
generalization performance, small but sufficient 
penalties for the examples with negative margin to 
grant the margin of a classifier as large as possible are 
usually demanded. The regularization between the 
classification accuracy and the generalization 
performance depends on the penalty parameter C in (1). 
Based on the idea, a method try to reduce the influence 
of such hard examples by adjusting the penalties was 
proposed. Since most of hard examples contained quite 
amount of noisy information, reduction of their 
influence is an efficient way to enhance the robustness 
of the classifier, in other words, to give the classifier 
good generalization ability with large margin.  
2. Relaxation of hard-limited class label 
In order to re-evaluate the penalties of hard 
examples, a theory of fuzzy class label for SVM was 
developed. The fuzzification can start with the problem 
in supervised machine learning. A training set T, T = 
{(xi, yi)}, i = 1, 2, â€¦, n, should first be drawn from the 
sample space. In the training set, the class label yi
given as a crisp value, yiÂ{-1, +1} for a binary 
classification, represents the class belonging of 
example i. Motivated by the hard-limited crisp values 
for the class label, a proposition to allow some 
relaxation from the crisp value to a fuzzy value has 
been studied in the authorâ€™s previous work [6]. The 
relaxation urges the training example to carry more 
information itself about the organization of the training 
set. Based on the fuzzification, the class label is 
changed from the crisp label set to a fuzzy set 
essentially. The training set is therefore changed as 
 ....,,3,2,1)},,{(' nizT ii   x  
In (4), zi denotes a fuzzified class label, a scaling 
weight to represent the exact influence of example i.
The intention of this fuzzification is to weaken the 
influence by reducing the weights of the hard examples. 
Hence, zi can be given as a reduction version of yi by a 
scaling fuzzy number si:
 .iii ysz   
The fuzzy number si can be obtained using many 
scaling algorithms. Usually, it is used to be assigned by 
a membership function (MF) to assess the relationship 
between the example (xi, yi) and the exact class the 
example belongs to: 
 )).(,( iii fyfzs x  
In order to present a proper modification of 
relationship to the native class, a metric expressing the 
relationship strength of the hard examples should be 
given in advance. The metric should be capable to 
linearly convert the relationship into continuous 
indices. As generally adopted, the margin yif(xi) was an 
excellent choice of the metric. For the reason that not 
only it can identify the hard examples with negative 
value, but also it can reflect the distance measuring 
from example location to the separating hyperplane. 
The larger the negative margin of an example, the 
weaker the relationship to its native class. The choice 
of the membership function depends on the problem 
that the application wants to solve and the capability 
that the classifier wants to achieve. For example, a 
typical linear membership function is chosen as a 
demonstration: 

Â°Â¯
Â°
Â®
Â­
!

tÂ˜
 
,0)(for,1
,/1)(for,
,0)(/1for,0),(1
))(,(
ii
ii
iiii
ii
fy
fy
fyfy
fyfz
x
x
xx
x
WH
WWW  
where Ä² denotes a rate constant, and Ä° denotes a 
sufficient small constant to avoid singular 
configurations in the matrix operation. With the 
parameterized zi, the given label of each example in the 
training set is no longer a crisp value. Instead, it 
became a positive or a negative fuzzy value: 
 11 dd iz  
Due to the change from yi to zi, the corresponding 
canonical constraints in the separable case of SVM is 
also changed from yi(<w, xi>+b) Â• 1 to  
 ....,2,1,,1),( nibz ii  txw  
where w and b resulting from zi is denoted the weights 
and bias as they have been formerly characterized in 
SVM, and the expression of <Â·, Â·> denotes a kernel 
function. The inequalities of (9) can be recovered by 
replacing zi with yi and si:
1-4244-0605-6/06/$20.00 Â©2006 IEEE. 98
Furthermore, a set of k-fold cross-validation 
procedures setting k=10 was set up to investigate 
further details for the performance of the fuz-SVM. 
The procedures were starting with a grid-search of 
parameter Ä² in the membership function. In order to 
widely inspect the behavior of the membership 
functions, a neat change of C in the set of {1x102,
1x103, 1x104} was issued. Beside that, the parameter Ä±
involving in the validation process was identically set 
to 1. Panels in Fig. 3 show the results of the validations 
on the Twonorm dataset. In Fig. 3, both of the panels, 
consisting of three pairs of solid and dashed lines with 
the respective solid and hollow legends, illustrate the 
tendency of the performance issue. Comparing to the 
standard SVM, the model has the ability to reach the 
maximal error reduction with a small value of C. With 
the error reduction, the fuz-SVM presents its 
excellence for being a large margin classifier. The 
maximal error reduction does not occur at the 
extremely setting of the parameter Ä². Only the 
intension to moderately weaken the influence of the 
hard examples is a good policy. The adequate value for 
the parameter Ä² depends on the complexity of dataset. 
In spite of the error more or less reduced with the 
increasing parameter setting, Fig. 3 shows an 
increasing trend towards an agreement of the large 
margin. The margin continuously grows until it 
reaches an upper bound. The scale of growth is very 
significant. As illustrated [1, 4], a large margin reduces 
the complexity in VC dimension, and makes the 
induced classifier has a higher capacity of 
generalization performance. Hence, we can conclude 
that the evidence of margin growth is a crucial point 
for the success of the fuz-SVM. 
 13
 14
 22
 24
 32
 38
 41
 43
 64
 84
101
114
Non-Fuzzy
Point in Class A              
Point in Class B              
Misclassified Point in Class A
Misclassified Point in Class B
  7
 14
 16
 20
 22
 24
 32
 36
 37
 38
 52
 61
 64
 65
 83
100
101
113
114
Linear MF, W = 32
Point in Class A              
Point in Class B              
Misclassified Point in Class A
Misclassified Point in Class B
(a) Without MF                         (b) With MF
Figure 2. Decision boundaries generated by 
comparative models 
0 2 4 6 8 10 12
18
20
22
24
26
28
30
C
la
s
s
if
ic
a
ti
o
n
 E
rr
o
r 
(%
)
log
2
(W)
Fuz-SVM (C = 1e2)
Standard SVM (C = 1e2)
Fuz-SVM (C = 1e3)
Standard SVM (C = 1e3)
Fuz-SVM (C = 1e4)
Standard SVM (C = 1e4)
0 2 4 6 8 10 12
0
0.05
0.1
0.15
0.2
0.25
M
a
rg
in
log
2
(W)
Fuz-SVM (C = 1e2)
Standard SVM (C = 1e2)
Fuz-SVM (C = 1e3)
Standard SVM (C = 1e3)
Fuz-SVM (C = 1e4)
Standard SVM (C = 1e4)
(a) Classification error                 (b) Margin
Figure 3. Generalization performance by 
cross-validation 
5. Conclusion 
With the most crucial property of the potential for 
generalization, the method of the fuz-SVM, 
introducing an idea of fuzzy class membership to the 
examples of training set, was successfully developed. 
Both the classification error and margin were 
examined by a k-fold cross-validation to confirm its 
generalization performance. The model of the fuz-
SVM stepped into the reconfiguration of the decision 
function by the involving fuzzy membership among 
the training examples and reformed a more 
generalization hyperplane. Since change of zi is 
substantially intrinsic and elementary, the idea may 
broadly extend to different prototypes of SVM if it is 
not conflict with the original idea of the prototype. The 
beautiful formulation of subjected minimization with 
the ordered replacements of yi with zi renders the class 
membership fuzzification advantageous for further 
developments or modifications. 
Acknowledgment 
This work was supported by the National Science 
Council, Taiwan, ROC, under Contract NSC 95-2221-
E-149-016. 
References 
 [1] Vapnik, V.N., The nature of statistical learning theory,
Springer-Verlag, New York, 1995. 
[2] C. Cortes, and V.N. Vapnik, â€œSupport vector networksâ€, 
Machine Learning, vol. 20, 1995, pp. 273-297. 
[3] Hastie, T., R. Tibshirani, and J. Friedman, The elements 
of statistical learning, Springer-Verlag, New York, 2001. 
[4] P.L. Bartlett, M.I. Jordan, and J.D. McAuliffe, 
â€œConvexity, classification, and risk boundsâ€, Tech. Report 
638, Dept. of Statistics, UC Berkeley, CA, 2003. 
[5] J. Shawe-Taylor, P.L. Bartlett, R.C. Williamson, and M. 
Anthony, â€œStructural risk minimization over data-dependent 
hierarchiesâ€, IEEE Transactions on Information Theory, vol. 
44, no. 5, 1998, pp. 1926-1940. 
[6] C.-Y. Yang, â€œSupport vector classifier with a fuzzy-value 
class labelâ€, Lecture Notes in Computer Science, vol. 3173, 
Springer-Verlag, Berlin, 2004, pp. 506-511. 
[7] C.-F. Lin, and S.-D. Wang, â€œFuzzy support vector 
machinesâ€, IEEE Transactions on Neural Networks, vol. 13, 
no. 2, 2002, pp. 464-471. 
[8] C.-F. Lin, and S.-D. Wang, â€œTraining algorithms for 
fuzzy support vector machines with noisy dataâ€, Pattern 
Recognition Letters, vol. 25, no. 14, 2004, pp. 1647-1656. 
[9] L. Breiman, â€œBias, variance and arcing classifiersâ€, Tech. 
Report 460, Dept. of Statistics, UC Berkeley, CA, 1996. 
[10] Smola, A.J., P.L. Bartlett, B. SchÃ¶lkopf, and D. 
Schuurmans, Advances in Large Margin Classifiers, MIT 
press, Cambridge, MA, 2000. 
1-4244-0605-6/06/$20.00 Â©2006 IEEE. 100
learning technique, which is learning directly from a 
set of available examples, and interpreting the results 
statistically. Instead of trying to create rules, the kNN 
approaches work directly from examples themselves. 
Suppose an unlabeled example x is placed among a set 
of n training examples in a region of volume V, and it 
captures k neighboring examples. Counting the 
neighboring examples, a portion of kj examples turns 
out to be in class labeled È¦j. The joint density function 
of x and È¦j can be approximated  
 .
/
),(
V
nk
p jj  Zx  
From Bayes rule, 
 ),,()()( jj ppP ZZ xxx   
The posteriori probability of P(È¦j|x) can be 
obtained by  
 ,
)/(
)/(
),(
),(
)(
11
k
k
V
nk
V
nk
p
p
P jc
t
t
j
c
t
t
j
j    
Â¦Â¦
  
Z
Z
Z
x
x
x  
where c denotes the number of classes. With (3), one 
can estimate P(È¦j|x) by the fraction of examples 
captured in a local region labeled È¦j. Since it does not 
need to build a classifier actually, the set of training 
examples should be very representative for inference. 
The term of â€œprototypesâ€ is used for the crucial set of 
examples. In the kNN classification, the kNN rules 
only require: an ordinary odd integer k, a family of 
prototypes, and a metric to measure â€œclosenessâ€ to 
collect the closest patterns for decision-making. Using 
the prototypes, the class belonging of a new arrival 
query example is determined locally by the k nearest 
neighboring prototypes around the query example. It is 
a simple and intuitive method to classify unlabeled 
examples based on the similarity to the neighbors in 
the feature space. 
In general, difficult examples may lie distant from 
the gathering area of the examples with the same class 
label, or may locate close to the border of adjacent 
overlapped region in which the examples come from 
different class are resided together. However, the non-
parametric method which captures the local structure 
of a little part of the underlying prototypes is quite 
suitable to employ as a preprocessor for filtering the 
individual difficult examples. 
2.2. SVMs with weighted class labels 
The support vector machines were developed based 
on the structural risk minimization principle of 
statistical learning theory [1-5] by learning from the 
training samples, the decision function therefore can be 
obtained. The basic form of decision function in the 
SVM is given as f(x) = sign(<w, x> + b) which is 
described by a vector of orientation weights w and a 
bias term b. The goal of the SVM is to find a 
separating hyperplane with maximal margin while the 
classification error can be minimized by the training 
samples. With the notation of the input training set, S =
{(xi, yi)}, a proposition starts with the change of S [12]: 
 ....,2,,1)},~,{(~ niyS ii   x  
In the expression, Í¿i, having its sign identical to yi in 
the training set S, denotes a relaxed real-valued class 
label to represent the potential weights that sample i
should be taken. The expression of SËœ tries to carry 
more information about the training set, regardless 
both S and SËœ contains a set of the same patterns xi. The 
change of S in SËœ involves a remapping regarding to the 
idea of assigning various weights for the samples in 
different situations. In (4), class label Í¿i is no longer a 
discrete value; instead it becomes a real-value to 
represent an implicit relationship to the sampleâ€™s native 
class. Incorporating the idea of kNN, the value of Í¿i
can be obtained from the idea of kNN 
 ,
)|(
~
ii
i
i P
yy
xZ
K  
where P(È¦i|xi) is the posteriori probability denoted in 
(3). The method of (5), called kNN emphasizer, adopts 
an inverted scheme to scale the value of Í¿i. The 
essential in the expression of Í¿i can be discovered by 
the ratio of magnification 1/P(È¦i|xi). Generally, the 
value of the ratio will be greater than 1. Our intention 
is to use the ratio to magnify yi. Parameter È˜ in the 
expressions, called an acceleration factor, should be a 
positive real and greater than 1 to ensure that | Í¿i | Â• 1. 
The scaled-up real-valued class label Í¿i provides a 
stricter penalty in the optimization, and is able to 
conduct the classification more accurate. Especially, 
the improvement will be more significant in a much 
more confused dataset with many difficult examples. 
By the magnification, difficult examples are designed 
to carry heavier weight in order to receive more 
penalties in the optimization. However, a set of 
canonical constraints is setup with the primal objective 
1-4244-0605-6/06/$20.00 Â©2006 IEEE. 173
