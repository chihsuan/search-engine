  
è¡Œæ”¿é™¢åœ‹å®¶ç§‘å­¸å§”å“¡æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•« 
â–¡æˆæœå ±å‘Š   
â– æœŸä¸­é€²åº¦å ±å‘Š 
 
æ”¯æ´ Cloud-awareåµŒå…¥å¼è¡Œå‹•å¤šæ ¸å¿ƒå¹³å°--å­è¨ˆç•«ä¸‰ï¼šæ•´åˆåµŒå…¥å¼
ç³»çµ±èˆ‡é›²ç«¯è¨ˆç®—çš„éŸ³æ¨‚èˆ‡èªéŸ³æœå‹™ 
 
è¨ˆç•«é¡åˆ¥ï¼šâ–¡å€‹åˆ¥å‹è¨ˆç•«   â– æ•´åˆå‹è¨ˆç•« 
è¨ˆç•«ç·¨è™Ÿï¼šNSC 100ï¼2219ï¼Eï¼007ï¼008 
åŸ·è¡ŒæœŸé–“ï¼š2011å¹´ 5 æœˆ 1 æ—¥è‡³ 2013å¹´ 4 æœˆ 30 æ—¥ 
 
åŸ·è¡Œæ©Ÿæ§‹åŠç³»æ‰€ï¼šæ¸…è¯å¤§å­¸è³‡è¨Šå·¥ç¨‹ç³» 
 
è¨ˆç•«ä¸»æŒäººï¼šå¼µæ™ºæ˜Ÿ 
å…±åŒä¸»æŒäººï¼š 
è¨ˆç•«åƒèˆ‡äººå“¡ï¼šè‘‰å­é›‹ã€ä»»ä½³æ°‘ã€å³ç¦æµ·ã€é‚±è‰å©·ã€é™³æ°èˆˆã€å½­éƒé›… 
 
 
 
æˆæœå ±å‘Šé¡å‹(ä¾ç¶“è²»æ ¸å®šæ¸…å–®è¦å®šç¹³äº¤)ï¼šâ–¡ç²¾ç°¡å ±å‘Š  â– å®Œæ•´å ±å‘Š 
 
æœ¬è¨ˆç•«é™¤ç¹³äº¤æˆæœå ±å‘Šå¤–ï¼Œå¦é ˆç¹³äº¤ä»¥ä¸‹å‡ºåœ‹å¿ƒå¾—å ±å‘Šï¼š 
â–¡èµ´åœ‹å¤–å‡ºå·®æˆ–ç ”ç¿’å¿ƒå¾—å ±å‘Š 
â–¡èµ´å¤§é™¸åœ°å€å‡ºå·®æˆ–ç ”ç¿’å¿ƒå¾—å ±å‘Š 
â–¡å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
â–¡åœ‹éš›åˆä½œç ”ç©¶è¨ˆç•«åœ‹å¤–ç ”ç©¶å ±å‘Š 
 
 
è™•ç†æ–¹å¼ï¼šé™¤åˆ—ç®¡è¨ˆç•«åŠä¸‹åˆ—æƒ…å½¢è€…å¤–ï¼Œå¾—ç«‹å³å…¬é–‹æŸ¥è©¢ 
            â–¡æ¶‰åŠå°ˆåˆ©æˆ–å…¶ä»–æ™ºæ…§è²¡ç”¢æ¬Šï¼Œâ–¡ä¸€å¹´â–¡äºŒå¹´å¾Œå¯å…¬é–‹æŸ¥è©¢ 
 
ä¸­   è¯   æ°‘   åœ‹   101 å¹´  3  æœˆ  17  æ—¥ 
II 
 
æ‘˜è¦ï¼š 
1. ä¸­æ–‡æ‘˜è¦ï¼š 
éš¨è‘—ç„¡ç·šç¶²ï¤·ä»¥åŠè¡Œå‹•è£ç½®æ’­æ”¾éŸ³æ¨‚åŠŸèƒ½çš„æ™®åŠï¼Œä½¿éŸ³æ¨‚èˆ‡èªéŸ³æœå‹™æˆç‚ºè¡Œå‹•åŠ å€¼çš„åŸº
æœ¬æ‡‰ç”¨ä¹‹ä¸€ï¼Œè€Œæä¾›æœå‹™çš„å½¢å¼äº¦æ—¥è¶¨å¤šå…ƒï¼Œåœ¨èªéŸ³æ–¹é¢ï¼Œï¥§ç®¡æ˜¯åˆ©ç”¨è¡Œå‹•è£ç½®æœå°‹ç¶²ï¤·ä¸Š
çš„é—œéµå­—æˆ–æ˜¯ç·¨å¯«æ‰‹æ©Ÿç°¡è¨Šï¼Œäººå€‘å¸Œæœ›å¯ä»¥åˆ©ç”¨èªéŸ³è¼¸å…¥å–ä»£è¡Œå‹•è£ç½®çš„æŒ‰éµè¼¸å…¥ï¼Œå°æ–¼é€™
é …æ‡‰ç”¨ï¼ŒèªéŸ³è¾¨ï§¼çš„é€Ÿåº¦åŠæº–ç¢ºæ€§å‰‡è‡³é—œé‡è¦ï¼›åœ¨éŸ³æ¨‚æ–¹é¢ï¼Œèˆ‡å¯¦é«”éŸ³æ¨‚ CDç›¸è¼ƒï¼Œäººå€‘ï¤
å¸Œæœ›èƒ½éš¨æ™‚éš¨åœ°æ‰¾åˆ°æœ€æƒ³è½åˆ°çš„éŸ³æ¨‚ã€‚å› æ­¤ï¼Œæˆ‘å€‘å¸Œæœ›å»ºæ§‹ä¸€å€‹æœ‰æ•ˆï¥¡çš„ç³»çµ±ï¼Œå¯ä»¥åœ¨è¡Œå‹•
è£ç½®ä¸Šé€éå„ç¨®æª¢ï¥ªæ–¹å¼æœå°‹ä¸Šè¬é¦–çš„æ­Œæ›²è³‡ï¦¾åº«ï¼Œæ‰¾åˆ°ä½¿ç”¨è€…æƒ³è½çš„éŸ³æ¨‚ï¼Œå†åŠ ä¸Šå¦‚ç¯€å¥ã€
æ›²é¢¨ç­‰ç‰¹å¾µçš„è¼”åŠ©ï¼Œæ¨è–¦ä½¿ç”¨è€…å¯èƒ½æœƒå–œæ­¡çš„æ›²ç›®ï¼Œä»¥æ»¿è¶³ä½¿ç”¨è€…å€‹äººå–œå¥½ã€‚ 
æœ¬è¨ˆç•«æ“¬åˆ©ç”¨è¡Œå‹•å¤šæ ¸å¿ƒå¹³å°ç™¼å±•éŸ³æ¨‚èˆ‡èªéŸ³ç›¸é—œä¹‹æ‡‰ç”¨æœå‹™ï¼Œä½¿ç”¨è€…å¯é€éå„ç¨®æ–¹å¼
æœå°‹éŸ³æ¨‚ï¼Œå¦‚è—‰ç”±å“¼å”±éŸ³æ¨‚çš„ç‰‡æ®µã€èªéŸ³è¼¸å…¥æ­Œåã€æƒ…ç·’æˆ–æ•²æ“Šç¯€å¥é€²è¡Œæœå°‹ã€‚é¦–å…ˆç”±æ‰‹æŒ
å¼è£ç½®å°è¼¸å…¥çš„éŸ³è¨Šé€²è¡Œå‰ç«¯è™•ç†ä»¥å–å¾—éŸ³è¨Šç‰¹å¾µï¼Œä¹‹å¾Œåˆ©ç”¨é›²ç«¯æŠ€è¡“åœ¨ä¼ºæœç«¯é€²è¡Œè³‡ï¦¾åº«
æ¯”å°ï¼ŒåŒæ™‚æœå°‹ç›¸è¿‘æ›²é¢¨çš„æ­Œæ›²æ¨è–¦çµ¦ä½¿ç”¨è€…ï¼Œæœ€å¾Œå°‡æ‰€æœ‰è³‡è¨Šå›é€è‡³ç”¨æˆ¶ç«¯ä¸¦å‘ˆç¾çµ¦ä½¿ç”¨
è€…ã€‚æ­¤æ¶æ§‹é¿å…ï¦ºç›´æ¥åœ¨å‰ç«¯è£ç½®ä¸Šé€²è¡Œæ¯”å°çš„é¾å¤§é‹ç®—ï¥¾ä»¥åŠè³‡ï¦¾åº«çš„å„²å­˜ç©ºé–“ï¼Œå› æ­¤å¯
ä»¥é©ç”¨æ–¼å¤§éƒ¨åˆ†çš„æ‰‹æŒå¼è£ç½®ï¼Œæ­¤å¤–ï¼Œæœ‰é‘‘æ–¼æŠ½å–æŸäº›éŸ³è¨Šç‰¹å¾µçš„é‹ç®—ï¥¾å°æ–¼æ‰‹æŒå¼è£ç½®çš„
è² æ“”ä»å¤ªé‡ï¼Œå› æ­¤åˆ©ç”¨å¤šæ ¸å¿ƒç³»çµ±é€²è¡Œå¹³è¡Œè™•ç†ï¼Œé æœŸå¯å¤§å¹…æ”¹å–„è¨ˆç®—æ‰€éœ€çš„æ™‚é–“ï¼ŒåŠ é€Ÿæ•´
é«”è™•ç†çš„é€Ÿåº¦ï¼Œå¢é€²æ‡‰ç”¨æœå‹™çš„åƒ¹å€¼ã€‚ç ”ç©¶çš„å·¥ä½œé …ç›®å¦‚ä¸‹ï¼š 
 
1. å»ºç«‹å¤šé‡ç‰¹å¾µè¾¨ï§¼çš„éŸ³æ¨‚æœå°‹ç³»çµ± 
2. è’é›†åšç‚ºè¾¨ï§¼ç”¨é€”çš„éŸ³æ¨‚åŠèªï¦¾ 
3. å¯¦ä½œå€‹äººåŒ–éŸ³æ¨‚æ¨è–¦ç³»çµ± 
4. æ•´åˆç³»çµ±ä¸¦å¯¦ä½œæ–¼å¤šæ ¸å¿ƒåµŒå…¥å¼å¹³å° 
5. æ¸¬è©¦åŠè©•ä¼°æ‡‰ç”¨æœå‹™æ–¼é›²ç«¯é‹ç®—çš„æ•ˆèƒ½ 
 
é—œéµè©ï¼šèªéŸ³è¾¨ï§¼ã€å“¼å”±æª¢ï¥ªã€æ•²æ“Šæª¢ï¥ªã€æ›²é¢¨åˆ†é¡ã€é›²ç«¯é‹ç®—ã€å¹³è¡Œè™•ç† 
 
2. è‹±æ–‡æ‘˜è¦ï¼š 
Due to the rapid advances of wireless networks and embedded mobile devices, more and more 
applications and services have emerged for peopleâ€™s daily needs. Music service is one of them that 
have been receiving more and more attention. In particular, people want to be able to search music 
in an efficient way, Moreover people want to find songs that are similar to their favorite ones. As a 
result, the goal of this project is to develop an integrated music service system that fulfill common 
peopleâ€™s needs, with an emphasis on the synergism between the client-side embedded devices and 
the server-side cloud computing. Specific tasks include: 
 
1 
 
ä¸€ã€ å‰è¨€èˆ‡ç ”ç©¶ç›®çš„ 
éŸ³æ¨‚æœå°‹åŠå…¶æ‡‰ç”¨æœå‹™ä¸€ç›´æ˜¯ä¸€å€‹å……æ»¿é–‹ç™¼æ½›ï¦Šçš„ï¦´åŸŸï¼Œç›¸å°æ–¼å‚³çµ±æ–‡å­—çš„æœå°‹è€Œè¨€ï¼Œä½¿
ç”¨è€…å¯ä»¥ä½¿ç”¨å„ç¨®æ–¹æ³•æœå°‹åˆ°æƒ³è½çš„æ­Œæ›²ï¼Œåƒæ˜¯å“¼å”±æˆ–æ•²æ“ŠæŸå€‹æ—‹ï§˜çš„ç‰‡æ–·ã€èªéŸ³è¼¸å…¥æ­Œæ›²å
ç¨±ï¼Œå³å¯æœå°‹åˆ°æƒ³è¦çš„æ­Œæ›²ï¼Œæ˜¯ä¸€å€‹è¼ƒç›´è¦ºä¹Ÿè¼ƒäººæ€§åŒ–çš„æ–¹æ³•ã€‚åœ¨å“¼å”±æª¢ï¥ªæ–¹é¢ï¼Œç³»çµ±å¿…é ˆè¿½
è¹¤ä½¿ç”¨è€…å“¼å”±çš„æ—‹ï§˜çš„éŸ³é«˜ï¼Œä¸¦èˆ‡è³‡ï¦¾åº«å…§æ­Œæ›²çš„éŸ³é«˜ï§—ï¨‹é€²è¡Œæ¯”å°ã€‚æ­¤å¤–ï¼Œè—‰ç”±æ•²æ“ŠéŸ³ç¬¦æˆ–
ç¯€æ‹çš„èµ·å§‹ä½ç½®ï¼Œå¯ä»¥æè¿°æ­Œæ›²çš„éƒ¨åˆ†æ—‹ï§˜ç‰¹å¾µï¼Œä¸¦åˆ©ç”¨é€™äº›ç‰¹å¾µæœå°‹è³‡ï¦¾åº«ã€‚ç•¶ç„¶ï¼Œç³»çµ±ä¹Ÿ
å…è¨±ç›´æ¥ç”¨èªéŸ³è¼¸å…¥é—œéµå­—é€²è¡Œæœå°‹ï¼Œæ­¤éƒ¨åˆ†å‰‡éœ€è¦èªéŸ³è¾¨ï§¼çš„ç›¸é—œæŠ€è¡“ã€‚æœ¬è¨ˆç•«å°‡è¦åŠƒæ•´åˆ
ä»¥ä¸ŠæŠ€è¡“ï¼Œè£½ä½œä¸€å€‹å¤šé‡ç‰¹å¾µè¾¨ï§¼çš„éŸ³æ¨‚æœå°‹ç³»çµ±ï¼Œä»¥ç¬¦åˆä½¿ç”¨è€…çš„å„ç¨®éœ€æ±‚ï¼Œå…¶ç›¸é—œæ‡‰ç”¨åˆ—
è¡¨å¦‚ä¸‹ï¼š 
 
1. èˆ‡è¦–è½ç³»çµ±çµåˆï¼Œå¦‚ KTVçš„é»æ­Œç³»çµ±ï¼Œä½¿æ“ä½œçš„éç¨‹ï¤ç‚ºç°¡ï¥¥ä¸”äººæ€§åŒ–ã€‚ 
2. å°‡éŸ³æ¨‚æœå°‹æŠ€è¡“æ¿ƒç¸®æˆä¸€å€‹æ™¶ç‰‡ï¼Œï¥¥å¯è£ç½®åœ¨äº’å‹•å¼çš„ç©å¶ä¸Šï¼Œç©å¶å³å¯è¾¨èªä½¿ç”¨è€…çš„
è²éŸ³é€²è€Œç”¢ç”Ÿï¥§åŒçš„åæ‡‰ã€‚ 
3. éŸ³æ¨‚æœå°‹å…·æœ‰åˆ†è¾¨éŸ³æ¨‚çš„èƒ½ï¦Šï¼Œå› è€Œä¹Ÿå¯å°‡éŸ³æ¨‚æœå°‹æŠ€è¡“åŒ…è£æˆéŸ³æ¨‚å­¸ç¿’ã€æ•™å”±æˆ–è©•åˆ†
çš„è»Ÿé«”ã€‚ 
 
äºŒã€ ç ”ç©¶æ–¹æ³• 
åœ¨æœ¬æœŸä¸­å ±å‘Šå…§ï¼Œç ”ç©¶æ–¹æ³•ä¸»è¦åŒ…å«ä¸‰å€‹éƒ¨ä»½ï¼Œåˆ†åˆ¥ç‚º CASA (Computational auditory scene 
analysisï¼Œè¨ˆç®—è½è¦ºå ´æ™¯åˆ†æ)çš„æ”¹é€²ï¼ŒQBSH (Query by singing/hummingï¼Œå“¼å”±æœå°‹)åœ¨é›²ç«¯
ç³»çµ±ä¸Šçš„å¯¦ä½œï¼ŒåŠä½¿ç”¨ GPU (Graphic Processing Unitï¼Œåœ–å½¢è™•ç†å™¨)å¹³è¡Œè™•ç†å“¼å”±æœå°‹çš„æ¼”
ç®—æ³•ï¼Œä»¥åŠ é€Ÿå“¼å”±æœå°‹çš„é€Ÿåº¦ï¼Œä»¥ä¸‹å°‡åˆ†åˆ¥å°±æ­¤ä¸‰éƒ¨ä»½é€²è¡Œä»‹ç´¹ã€‚ 
 
1. CASA çš„æ”¹é€² 
 
åœ¨æœ¬æœŸä¸­ï¼Œæˆ‘å€‘ç™¼å±•ï¦ºä¸€å€‹æ··æˆæ³•æ”¹é€²ï¦º CASAçš„è¾¨ï§¼ï¥¡ï¼Œä¸¦å°‡æ­¤ç ”ç©¶çµæœæŠ•ç¨¿è‡³ ICASSP 
(International Conference on Acoustics, Speech, and Signal Processing) ç²å¾—æ¥å—ï¼Œ
ä»¥ä¸‹æ˜¯è«–æ–‡ä¸­çš„æ–¹æ³•æ‘˜éŒ„ï¼š 
 
In this work, we propose a hybrid method for singing pitch extraction from polyphonic audio 
music. We have observed several kinds of pitch errors made by a previously proposed algorithm 
based on trend estimation. We also noticed that other pitch tracking methods tend to have other 
types of pitch error. Then it becomes intuitive to combine the results of several pitch trackers to 
achieve a better accuracy. In this paper, we adopt 3 methods as a committee to determine the pitch, 
including the trend-estimation-based method for forward and backward signals, and training-based 
HMM method. Experimental results demonstrate that the proposed approach outperforms the best 
algorithm for the task of audio melody extraction in MIREX 2010. 
In the proposed hybrid method, trend estimation plays an important role in determining 
3 
 
pitch errors are likely to happen at the beginning of a music phrase. If we reverse the input signals 
in time axis and send it for pitch tracking, the pitch errors occur elsewhere. As a result, forward 
and backward (in time) signals are likely to generate complementary results. This observation 
motivates us to combines pitch contours obtained from forward and backward signals to achieve a 
better accuracy. 
 
 
Fig. 3.1.3 Illustrations of a typical pitch error and its recovery. (a) The ground-truth pitch contour, 
and the one generated by the forward trend-estimation-based method.  (b) The result of merging 
pitch contours generated by 3 different methods, which can recover from the pitch error displayed in 
(a). 
 
HMM-based methods have been widely used for audio melody extraction. In this study, we 
use the maximum 2 values of the NSHS (normalized sub-harmonics summation) map and their 
locations (frequencies) as the features for HMM. Fig. 3.1.4(a) illustrates the scatter plot of the first 
two dimensions of the features, including the maximum value of a frameâ€™s NSHS and the 
corresponding index. Fig. 3.1.4(b) shows the corresponding NSHS curves for (a). As can be 
observed in the Fig. 3.1.4, the max values and the corresponding indexes are mostly likely to be 
around the state bin(indicated as the middle bar in both Fig. 3.1.4(a) and Fig. 3.1.4(b)). Let the 
feature vector set be denoted as V = {v0, â€¦, vt, â€¦}, our target is to find the most likely state 
sequence S = {S0, â€¦, St, â€¦}: 
5 
 
 
Fig. 3.1.5 Evaluation results of different pitch combination methods 
 
After obtaining multiple pitch contours, we need to combine them in an optimal sense. We can 
arrange these pitch contours into a matrix C, where each row is a pitch contour and each column is 
possible semitones of a frame. Here we adopt 3 different methods to derive the final pitch contour: 
1. Median method: The optimum pitch contour Pğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘›(ğ‘—) at frame ğ‘— is expressed as: 
 
Pğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘›(ğ‘—) = {ğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘›
ğ‘–
(ğ¶ğ‘–,ğ‘—), ğ‘— = 1~ğ‘€},             (2) 
           
where ğ‘€ is the number of frames. 
2. Mean method: The optimum pitch contour Pğ‘šğ‘’ğ‘ğ‘›(ğ‘—) at frame j is expressed as: 
 
Pğ‘šğ‘’ğ‘ğ‘›(ğ‘—) = {ğ‘šğ‘’ğ‘ğ‘›
ğ‘–
(ğ¶ğ‘–,ğ‘—), ğ‘— = 1~ğ‘€}                  (3) 
 
3. DP-based method: The recurrence function of this method can be formed as follows: 
 
ğ·(ğ‘–, ğ‘—) = ğ¶(ğ‘–, ğ‘—) âˆ’ min (ğ¶ğ‘—âˆ’1 âˆ’ ğ·ğ‘—âˆ’1)                 (4) 
ğ·(ğ‘–, 1) = 0, âˆ€ğ‘– = 1~ğ‘,                            (5) 
 
where ğ‘ is the number of pitch contour candidates. We can easily obtain the optimal path Pğ‘‘ğ‘ 
by the backtracking, which is a common method in dynamic programming. 
The evaluation results of the above methods are illustrated in Fig. 3.1.5. As can be easily 
observed, the result of median method is the best among all 3 methods. Thus, the median method 
will be adopted for the rest of the evaluations presented below.  
The dataset for our evaluation is MIR-1k, which contains 1000 pop-song singing mixture with 
leading vocals and music accompanies. A 5-fold singer-specific cross validation is then employed 
to obtain the average raw-pitch accuracy for the proposed method. 
Fig. 3.1.6 shows the evaluation results of raw-pitch accuracy with a pitch tolerance of 0.5 
semitones. The proposed method outperforms Hsuâ€™s trend-estimation-based method (the best 
7 
 
 
åœ– 3.2.1 client ç«¯çš„ç³»çµ±æ¶æ§‹ 
 
 Serverç«¯çš„ä¼ºæœå™¨å‰‡æ˜¯é€éç¶²é æœå‹™ä¾†å›æ‡‰ä½¿ç”¨è€…çš„æ—‹ï§˜æœå°‹éœ€æ±‚ï¼Œç•¶ç¶²é æœå‹™æ¥æ”¶
åˆ°ï¦ºä½¿ç”¨è€…å“¼å”±çš„éŸ³é«˜å¾Œï¼Œæœƒå°‡éŸ³é«˜é€è‡³ä¸»ä¼ºæœå™¨ï¼Œä¸»ä¼ºæœå™¨æœƒåˆ†é…æ­Œæ›²è³‡ï¦¾åº«çš„è¾¨ï§¼ç¯„åœ
äº¤çµ¦ GPUé€²è¡Œé‹ç®—ï¼Œä¸‹åœ–é¡¯ç¤ºçš„æ˜¯ç³»çµ±å¾Œç«¯çš„é‹ä½œæµç¨‹ã€‚ 
 
 
åœ– 3.2.2 Server ç«¯çš„ç³»çµ±æ¶æ§‹ 
 
ä½¿ç”¨è€…åœ¨å“¼å”±é¸æ­Œçš„æµç¨‹ä¸­ä¸»è¦æœƒç¶“éå››å€‹æ­¥é©Ÿï¼Œå¦‚åœ– 3.2.3ã€‚åœ¨ Clientç«¯å¿…é ˆå–å¾—
ä½¿ç”¨è€…å“¼å”±çš„æ—‹ï§˜é€²è€Œå¾—åˆ°æ—‹ï§˜ä¹‹éŸ³é«˜ï¼Œå–å¾—éŸ³é«˜çš„æ–¹å¼æ˜¯ä½¿ç”¨ç”± Lawrencer R.Rabineræ‰€
æå‡ºçš„ ACF (Autocorrelation Function)ï¼Œè€Œå¾Œé€é Web service å°‡éŸ³é«˜è³‡ï¦¾é€åˆ°ä¼ºæœå™¨
å»åšè¾¨ï§¼çš„è¤‡é›œé‹ç®—ï¼Œæœ€å¾ŒæŠŠæœ€ç¬¦åˆä½¿ç”¨è€…å“¼å”±çš„æ­Œæ›²è¾¨ï§¼çµæœé€å›çµ¦ä½¿ç”¨è€…ã€‚ 
 
9 
 
 
åœ– 3.2.5 ä½¿ç”¨è€…åœ¨å“¼å”±é¸æ­Œæ™‚éŒ„éŸ³çš„ä½¿ç”¨ä»‹é¢ 
 
   åœ¨ GPUå¯¦ä½œæ–¹é¢ï¼Œç›¸é—œç´°ç¯€åƒ…åœ¨ä¸‹ä¸€ç¯€è¨è«–ï¼Œåœ¨æ­¤æš«ï¥§è«–è¿°ã€‚ 
 
3. ä»¥ GPU å¹³è¡Œè™•ç†å“¼å”±æœå°‹çš„æ¼”ç®—æ³• 
 
å‘ˆä¸Šæ‰€è¿°ï¼Œå°æœ¬è¨ˆç•«è€Œè¨€å¾ˆé‡è¦çš„å¦ä¸€å€‹æˆæœå³ç‚º GPUå¹³è¡Œè™•ç†æ¼”ç®—æ³•ï¼Œé€™éƒ¨ä»½èƒ½ä½¿æˆ‘
å€‘åŸæœ¬ä½¿ç”¨çš„æ¼”ç®—æ³•ä¹‹è¨ˆç®—æ™‚é–“å¤§å¤§é™ä½ï¼Œæ­¤æˆæœäº¦æŠ•ç¨¿è‡³ ICASSPï¼Œä¸¦ç²å¾—æ¥å—ï¼Œä»¥ä¸‹æ‘˜
éŒ„è«–æ–‡ç‰‡æ®µä¾†è§£é‡‹æ­¤éƒ¨ä»½çš„å¯¦ä½œæˆæœï¼š 
 
In this work, we develop a QBSH system which starts the comparison from anywhere in a 
song where notes begin, and the schemes of parallelization are directly designed for searching 
many songs in the database. The proposed method is used to improve a web-deployed QBSH 
system called MIRACLE which won the championship of 2011 CUDA programming contest in 
Taiwan, hosted by NVIDIA. 
We use linear scaling as the comparison method in our work. Linear scaling is a simple yet 
effective method for query by singing/humming. Since the keys and tempos may be different 
between the input pitch vector and the songs in the database, we need to transpose the key and 
scale the tempo of the input vector. Key transposition can be simply handled by shifting the mean 
values of pitch vectors of query input and the intended songs in the database to the same value. 
Usually we shift one of them to match another when comparing these two vectors. For tempo 
scaling, since tempo variation is usually linear, we can apply LS (linear scaling) to the input pitch 
vector for comparison. Assuming that the input pitch vector has a duration of ğ‘‘ seconds, then we 
need to compress or stretch the vector to obtain ğ‘Ÿ versions of the original vector, with durations 
equally spaced between ğ‘ ğ‘šğ‘–ğ‘› Ã— ğ‘‘  and ğ‘ ğ‘šğ‘ğ‘¥ Ã— ğ‘‘ , where ğ‘ ğ‘šğ‘–ğ‘›  (<1) and ğ‘ ğ‘šğ‘ğ‘¥  (>1) are the 
minimum and maximum of the scaling factor, respectively. The distance between the input pitch 
vector and a particular song is then the minimum of the ğ‘Ÿ distances between the ğ‘Ÿ vectors and 
the song, as shown in figure 3.3.1 where we compress/stretch the ğ‘‘-second vector to obtain 5 
vectors with lengths equally spaced between 0.5 Ã— ğ‘‘ and 1.5 Ã— ğ‘‘. The best result is obtained 
when the scaling factor is 1.25. 
11 
 
optimum parallelization, as shown next. 
 
ï¬ In the first scheme, we simply launch ğ‘ threads for comparing  ğ‘ different songs in 
the database. The current song segment for comparison is copied to local storages for 
speeding up the computing. 
 
ï¬ In the second scheme, we launch ğ‘Ÿ threads for one song. These ğ‘Ÿ threads are grouped 
into a block, with ğ‘ blocks in total. However, though the degree of parallelization is higher, 
the computation time is even longer. The reason is that there are many blocks but only a few 
of threads within one block â€“ the SPs are not fully utilized. 
 
ï¬ In the third scheme, we still have ğ‘ blocks for ğ‘ songs, but now each block has ğ‘˜ 
threads in a block. The computation tasks starting at different notes in a song are equally 
distributed to the ğ‘˜ threads. Since ğ‘˜ is usually larger than ğ‘Ÿ, the utilization of SPs is better 
than that of scheme 2. Moreover, since there are multiple threads for one song, we obtain the 
minimum distance between the input pitch vector and this song in parallel by using these 
threads.  
 
After obtaining the distance between the query input and each of the songs in the database, we 
then sort all the distances on CPU to obtain the top-n list. We did try the sorting using GPU, but 
the performance is not satisfactory due to excessive access time over the global memory. 
We used the public corpus MIR-QBSH for our experiments of QBSH with GPU. Note that in 
this corpus, the anchor positions for all queries are from the beginning of a song. In order to test 
the accuracy of "anchor anywhere", we duplicate the last one fourth of each song and prepend it to 
the beginning of the song. The corpus contains 6197 clips which correspond to 48 children's songs. 
To increase the complexity of the comparison, we added 12887 noise songs (which correspond to 
pop songs in the past decades) to the database, such that the number of songs in the database is 
12935. Figure 3.3.3 shows the distribution of song lengths, in terms of number of music notes and 
number of pitch points, respectively. This plot indicates the complexity of our task. The number of 
music notes indicates how many positions we need to start the comparison algorithm, while the 
number of pitch points represents how long the sequence we need to run through the comparison 
algorithm. 
 
13 
 
 
Figure 3.3.5 The computation time per query with respect to the number of songs in the database 
for the three different schemes of parallelization. The number of threads in a block is 1024 for 
schemes 1 and 3, and 31 for scheme 2. 
 
 
 
 
 
Figure 3.3.6 The computation time per query with respect to the number of songs in the database 
for the three different schemes of parallelization. The number of threads in a block is 128 for 
schemes 1 and 3, and 31 for scheme 2. 
 
The above figures suggest that number of threads in a block is an important factor for scheme 
3. Thus we investigated the effect of number of threads in a block for scheme 3, as shown in figure 
3.3.7. The best performance is achieved when there are 128 threads in a block. Utilization of cores 
in GPU will be lower if we launch fewer threads in a block. On the other hand, if we have more 
threads than 128, then it becomes time consuming to compute the minimum distance for a song 
which is obtained from these threads. The snapshot of our system is shown in figure 3.3.8.  
 
15 
 
singing melody transcription system. Li et al. in proposed a method that first filtering and selecting 
channels from the spectrogram. Peaks in these selected channels are then extracted as the feature to 
train HMMs, where the transition probabilities are estimated from the pre-labeled training data. Both 
studies focus on the contextual relationship between neighboring music notes. However, the 
concurrent pitches produced by music accompany or chord, which may critically influence the overall 
vocal pitch estimation, were not dealt with in details. To address this problem, Hsu et al. introduced a 
method based on trend estimation, where the singing voices were enhanced first and the concurrent 
pitches were eliminated by vibrato and tremolo features. After that, pitch range of each frame was 
identified by finding a coarse path (pitch trend) within high-magnitude T-F (time-frequency) blocks. 
The use of such a pitch trend can remove the undesirable concurrent pitches produced by accompanied 
music. 
 
å¦ä¸€äº›å¯åƒè€ƒçš„æ–‡ç»å¦‚ä¸‹åˆ—è¡¨ï¼š 
 
1. C. K.Wang, R. Y. Lyu, and Y. C. Chiang, â€œAn automatic singing transcription system with 
multilingual singing lyric recognizer and robust melody tracker,â€ in Proc. 8th European Conf. on 
Speech Communication and Technology, Geneva, Switzerland, 2003. 
2. T. Zhang, â€œSystem and method for automatic singer identification,â€ in Proc. IEEE International 
Conference on Multimedia and Expo (ICME), pp. 33-36, 2003. 
3. D. Yang and W. Lee. â€œDisambiguating music emotion using software agents,â€ In Proc. Symp. 
Music Inf. Retrieval (ISMIRâ€™04), Barcelona, Spain, pp. 52-57, 2004. 
4. S. Vembu and S. Baumann, â€œSeparation of vocals from polyphonic audio recordings,â€ in Proc. Int. 
Symp. Music Inf. Retrieval (ISMIRâ€™05), pp. 337â€“344, 2005. 
5. T. Virtanen, A. Mesaros, and M. RyynÃ¤nen, "Combining Pitch-Based Inference and Non-Negative 
Spectrogram Factorization in Separating vocals from polyphonic music," in Proc. ISCA Tutorial 
and Research Workshop on Statistical and Perceptual Audition (SAPA'08), Brisbane, Australia, 
September 2008. 
6. A. Ozerov, P. Philippe, F. Bimbot and R. Gribonval, "Adaptation of Bayesian models for single 
channel source separation and its application to voice / music separation in popular songs," IEEE 
Trans. on Audio, Speech and Lang. Proc., special issue on Blind Signal Proc. for Speech and Audio 
Applications, vol. 15, no. 5, pp. 1564-1578, 2007. 
7. B. Raj, Smaragdis, P., Shashanka, M.V. and Singh, R., â€œSeparating a Foreground Singer from 
Background Music.â€, Intl Symposium on Frontiers of Research on Speech and Music (FRSM), 
Mysore, India, Jan 2007. 
8. G. J. Brown and D. L. Wang, â€œTiming is of the essence: Neural oscillator models of auditory 
grouping,â€ in Listening to Speech: An Auditory Perspective, S. Greenberg and W. Ainsworth Ed, 
Lawrence Erlbaum, Mahwah NJ, 2006. pp. 375-392. 
9. G. Hu and D.L. Wang, â€œAn auditory scene analysis approach to monaural speech segregation,â€ in 
Acoustic Echo and Noise Control, E. Hansler and G. Schmidt Ed., Heidelberg: Springer, 2006, pp. 
485-515. 
10. D.L. Wang, "On ideal binary mask as the computational goal of auditory scene analysis," in Speech 
17 
 
A similar study of accelerating QBSH on GPUs is proposed by Ferrao et al. However, the 
comparison only started from the beginning of a song in the database, which may violate peopleâ€™s 
singing habits. In this paper, comparison starts from anywhere in a song where notes begin, and the 
schemes of parallelization are directly designed for searching many sequences (i.e. many songs in the 
database). The proposed method is used to improve a web-deployed QBSH system called MIRACLE 
which won the championship of 2011 CUDA programming contest in Taiwan, hosted by NVIDIA. 
 
é—œæ–¼ QBSHå…¶ä»–å¯ä»¥åƒè€ƒçš„æ–‡ç»å¦‚ä¸‹ï¼š 
 
1. A. Ghias, J. Logan, D. Chamberlain, B. C. Smith, â€œQuery by humming-musical information 
retrieval in an audio databaseâ€, ACM Multimedia â€™95 San Francisco, 1995. 
2. Rodger J. McNab, Lloyd A. Smith, Jan H. Witten, â€œTowards the Digital Music Library: Tune 
Retrieval from Acoustic Inputâ€ ACM, 1996. 
3. Rodger J. McNab, Lloyd A. Smith, Jan H. Witten, â€œSignal Processing for Melody Transcriptionâ€ 
Proceedings of the 19th Australasian Computer Science Conference, 1996. 
4. Rodger J. McNab, Lloyd A. Smith, â€œMelody transcription for interactive applicationsâ€ Department 
of Computer Science University of Waikato, New Zealand. 
5. Yianilos, Peter N. â€œData structures and algorithms for nearest neighbor search in general metric 
spaces,â€ In Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, 
pages 311-321, Austin, Texas, 25-27 January 1993 
6. J.-S. Roger Jang and Chuen-Tsai Sun and Eiji Mizutani, "Neuro-Fuzzy and Soft Computing: A 
Computational Approach to Learning and Machine Intelligence", MATLAB Curriculum Series, 
Upper Saddle River, NJ, Prentice Hall, 1997. (ISBN: 0-13-261066-3) 
7. Jyh-Shing Roger Jang, Hong-Ru Lee, Jiang-Chuen Chen, and Cheng-Yuan Lin, "Research and 
Development of an MIR Engine with Multi-modal Interface for Real-world Applications", Journal 
of the American Society for Information Science and Technology, 2003. 
8. J.-S. Roger Jang, and Yung-Sen Jang, "On the Implementation of Melody Recognition on 8-bit 
AND 16-bit Microcontrollers", The Fourth IEEE Pacific-Rim Conference on Multimedia, 
Singapore, December 2003. 
9. J.-S. Roger Jang and Shiuan-Sung Lin, "Optimization of Viterbi Beam Search in Speech 
Recognition", International Symposium on Chinese Spoken Language Processing, Taiwan, August 
2002. 
10. J.-S. Roger Jang, Hong-Ru Lee, "Hierarchical Filtering Method for Content-based Music Retrieval 
via Acoustic Input", The 9th ACM Multimedia Conference (Oral paper, acceptance rate 16%), PP. 
401-410, Ottawa, Ontario, Canada, September 2001. 
11. J.-S. Roger Jang, Jiang-Chun Chen, Ming-Yang Kao, "MIRACLE: A Music Information Retrieval 
System with Clustered Computing Engines", 2nd Annual International Symposium on Music 
Information Retrieval 2001, Indiana University, Bloomington, Indiana, USA, October 2001. 
12. J.-S. Roger Jang, Hong-Ru Lee, Ming-Yang Kao, "Content-based Music Retrieval Using Linear 
Scaling and Branch-and-bound Tree Search", IEEE International Conference on Multimedia and 
Expo, Waseda University, Tokyo, Japan, August 2001. 
19 
 
 
 ä»»æ„è™•æ¯”å°çš„è¾¨ï§¼ï¥¡åŠè¾¨ï§¼æ™‚é–“æ¯”è¼ƒï¼š 
 
è¾¨è­˜æ–¹æ³• ç§»èª¿æ–¹æ³• Pitch scale Top-10 è¾¨è­˜ç‡ è¾¨è­˜æ™‚é–“(ç§’) 
2-Stage LS Median 10 77.08% 6.82 
2-Stage LS Median 1024 77.83% 6.91 
2-Stage LS Mean 10 75.09% 2.45 
2-Stage LS Mean 1024 76.13% 2.73 
2-Stage LS 
optimized 
Median 10 77.08% 4.87 
2-Stage LS 
optimized 
Median 1024 77.83% 4.84 
2-Stage LS 
optimized 
MedianByVote 10 77.08% 3.16 
2-Stage LS 
optimized 
Mean 10 75.09% 1.65 
2-Stage LS 
optimized 
Mean 1024 76.13% 1.69 
è¡¨ 4.1 è¾¨ï§¼ï¥¡åŠæ™‚é–“åˆ—è¡¨ 
 
 ä½¿ç”¨ SEV (Sorted error vector)æ–¼ LSä»»æ„è™•æ¯”å°è¾¨ï§¼ï¥¡åŠè¾¨ï§¼æ™‚é–“æ¯”è¼ƒï¼š 
 æœªä½¿ç”¨ SEVçš„è¾¨ï§¼ï¥¡ï¼š77.08% 
 ç•¶ sevBoundè¨­ç‚º 0.08ï¼Œä½¿ç”¨ median ç§»èª¿æ™‚çš„è¾¨ï§¼ï¥¡ï¼š80.28%ï¼ˆè¾¨ï§¼ï¥¡å¢åŠ  3.2%ï¼‰ 
 
21 
 
 
åœ– 4.2 ä½¿ç”¨ SEV åœ¨ï¥§åŒä¹‹è·é›¢è¨ˆç®—æ³•ä¸‹ä¹‹è¨ˆç®—æ™‚é–“æ¯”è¼ƒ 
 
 ç¬¬äºŒéƒ¨ä»½çš„æ¸¬è©¦ç’°å¢ƒèˆ‡èªï¦¾ä»‹ç´¹å¦‚ä¸‹ï¼š 
 æ¸¬è©¦ç’°å¢ƒï¼š 
Windows 7 Enterprise, 64-bit 
IntelÂ® Coreâ„¢ i7-2600 Processor (8M Cache, 3.40 GHz) 
16GB Main Memory 
Geforce GTX 560 Ti (384 core, 1024MB RAM GDDR5), compute Capability 2.1 
CUDA Toolkit 4.0 for Windows 
NVIDIA Driver 270.81 
 æ¸¬è©¦èªï¦¾ï¼š 
  ç¸½æ­Œæ›²æ•¸ï¼š6,197é¦– 
  ç”± 264äººæ‰€éŒ„è£½çš„å…’æ­Œå“¼å”±ç‰‡æ®µ 
 å“¼å”±å–æ¨£è¦æ ¼ï¼š 
  éŒ„éŸ³é•·åº¦ï¼š8ç§’ 
  å–®è²é“ 
  å–æ¨£é »ï¥¡ï¼š16,000 
  è§£æåº¦ï¼š16bits 
 è³‡ï¦¾åº«ï¼š 
  ç¸½æ­Œæ›²æ•¸ï¼š12,935 é¦– 
 
 å¾é ­æ¯”å°çš„è¾¨ï§¼ï¥¡æ¯”è¼ƒï¼š 
ç„¡ç ”ç™¼æˆæœæ¨å»£è³‡æ–™ 
å…¶ä»–æˆæœ 
(ç„¡æ³•ä»¥é‡åŒ–è¡¨é”ä¹‹æˆ
æœå¦‚è¾¦ç†å­¸è¡“æ´»å‹•ã€ç²
å¾—çé …ã€é‡è¦åœ‹éš›åˆ
ä½œã€ç ”ç©¶æˆæœåœ‹éš›å½±éŸ¿
åŠ›åŠå…¶ä»–å”åŠ©ç”¢æ¥­æŠ€
è¡“ç™¼å±•ä¹‹å…·é«”æ•ˆç›Šäº‹
é …ç­‰ï¼Œè«‹ä»¥æ–‡å­—æ•˜è¿°å¡«
åˆ—ã€‚) 
2011 å°ç£ CUDA ç¨‹å¼è¨­è¨ˆæ¯”è³½å† è» 
2011 é›»ä¿¡å‰µæ–°æ‡‰ç”¨å¤§è³½ï¼ˆè¡Œå‹•æ‡‰ç”¨ å‰µæ–°éŠæˆ²é¡ æ ¡åœ’çµ„ï¼‰äºè» 
2012 ç¬¬äºŒå±†ç¾å¾‹é›»è²è«–æ–‡çé‡‘è³ªç 
 
 
 
 æˆæœé …ç›® é‡åŒ– åç¨±æˆ–å…§å®¹æ€§è³ªç°¡è¿° 
æ¸¬é©—å·¥å…·(å«è³ªæ€§èˆ‡é‡æ€§) 0  
èª²ç¨‹/æ¨¡çµ„ 0  
é›»è…¦åŠç¶²è·¯ç³»çµ±æˆ–å·¥å…· 0  
æ•™æ 0  
èˆ‰è¾¦ä¹‹æ´»å‹•/ç«¶è³½ 0  
ç ”è¨æœƒ/å·¥ä½œåŠ 0  
é›»å­å ±ã€ç¶²ç«™ 0  
ç§‘ 
æ•™ 
è™• 
è¨ˆ 
ç•« 
åŠ  
å¡« 
é … 
ç›® è¨ˆç•«æˆæœæ¨å»£ä¹‹åƒèˆ‡ï¼ˆé–±è½ï¼‰äººæ•¸ 0  
 
