 1
ï¨ˆæ”¿é™¢åœ‹å®¶ç§‘å­¸å§”å“¡æœƒå°ˆé¡Œç ”ç©¶è¨ˆç•«æˆæœå ±å‘Š 
 
0Bé«˜æº–ç¢ºæ€§ä¹‹è‡ªå‹•åŒ–è¦–è¨Šå»èƒŒçš„ç ”ç©¶ 
 
è¨ˆç•«ç·¨è™Ÿï¼šNSC100-2221-E-346-008 
åŸ·ï¨ˆæœŸé™ï¼š100 ï¦ 8 æœˆ 1 æ—¥è‡³ 101 ï¦ 7 æœˆ 31 æ—¥ 
ä¸»æŒäººï¼šèƒ¡æ­¦èªŒ   åœ‹ï§·æ¾æ¹–ç§‘æŠ€å¤§å­¸è³‡è¨Šå·¥ç¨‹ç³» 
 
ä¸€ã€ä¸­æ–‡æ‘˜è¦ 
è¦–è¨Šå»èƒŒæ–¹æ³•æ˜¯è¦–è¨Šåˆæˆã€è¦–è¨Šå‰µä½œã€
é›»å½±(é›»è¦–)ç‰¹æ•ˆè£½ä½œä¸Šéå¸¸é‡è¦ä¸”å…·é«˜åƒ¹å€¼
çš„æŠ€è¡“ï¼Œé€éè¦–è¨Šå»èƒŒæ–¹æ³•å¯ä»¥ä¿æœ‰è¦–è¨Šä¸­
å‰æ™¯ç‰©ä»¶çš„é€æ˜ï¨ä¸¦è®“å¾ŒçºŒçš„è¦–è¨Šè™•ï§¤æ‰€
å¾—åˆ°çš„çµæœï¤ç‚ºçœŸå¯¦ã€‚ç”±æ–¼ç›®å‰æ‰€æå‡ºæœ‰é—œ
è¦–è¨Šå»èƒŒçš„æ–¹æ³•çµ•å¤§å¤šï¥©å‡å±¬æ–¼éè‡ªå‹•åŒ–
æˆ–åŠè‡ªå‹•åŒ–çš„è¦–è¨Šå»èƒŒæ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯éœ€ç”±ä½¿
ç”¨è€…ç”±äººå·¥æ–¹å¼åŠ å…¥ä¸‰å€åŸŸåœ–æˆ–äººå·¥å¡—é´‰
ï¤­æ¨™ç¤ºå‡ºå‰æ™¯å€åŸŸåŠèƒŒæ™¯å€åŸŸæ‰èƒ½ç²å¾—é€
æ˜åœ–å±¤é®ç½©ï¼Œå› æ­¤å°æ–¼ä½¿ç”¨è€…è€Œè¨€æ˜¯ä»¶éå¸¸
è€—æ™‚ä¸”ç¹ç‘£çš„å·¥ä½œã€‚æ‰€ä»¥ï¼Œæœ¬ç ”ç©¶æå‡ºï¦ºä¸€
å€‹æ¤åŸºæ–¼æ··åˆå¼è¦–è¨Šç‰©ä»¶åˆ†å‰²åŠå°é–‰å‹å»
èƒŒæ³•çš„è‡ªå‹•åŒ–è¦–è¨Šå»èƒŒæ–¹æ³•ï¼Œä¸»è¦ç›®çš„æ˜¯è¦
æå‡ºï¥§éœ€è—‰ç”±ä½¿ç”¨è€…äººå·¥çµ¦å®šä¸‰å€åŸŸåœ–æˆ–
äººå·¥å¡—é´‰ï¤­æ¨™ç¤ºå‡ºå‰æ™¯å€åŸŸåŠèƒŒæ™¯å€åŸŸè€Œ
èƒ½é”åˆ°è‡ªå‹•åŒ–çš„è¦–è¨Šå»èƒŒçµæœä¸”ä¿æœ‰é«˜æº–
ç¢ºæ€§çš„ç›®çš„ã€‚å¯¦é©—æ¨¡æ“¬çµæœå¯è­‰å¯¦æœ¬ç ”ç©¶æ‰€
æå‡ºçš„æ–¹æ³•æ•ˆèƒ½å„ªæ–¼å…¶ä»–ï§ä¼¼çš„æ–¹æ³•ã€‚ 
 
é—œéµè©ï¼šè¦–è¨Šå»èƒŒã€è¦–è¨Šç‰©ä»¶åˆ†å‰²ã€é€æ˜åœ–
å±¤é®ç½©ã€å°é–‰å‹å»èƒŒæ³• 
 
Abstract 
Video matting is an important and 
high-value technology in video composition, 
video editing, and special effects of film and 
TV. Using video matting can maintain the 
foreground objectâ€™s transparency (alpha matte) 
in order to obtain more realistic result in the 
subsequent video processing. Most of the 
suggested approaches of video matting obtain 
alpha matte using usersâ€™ specified trimaps or 
scribbles to mark the foreground and 
background, thus they are non-automatic or 
semi-automatic video matting. Userâ€™s 
specified trimaps or scribbles is a 
time-consuming and troublesome process for 
users. Therefore, this project proposes an 
automatic video matting based on hybrid 
video object segmentation and closed-form 
matting. The main goal is to propose the 
accurate and automatic matting without usersâ€™ 
specified trimaps or scribbles Experimental 
results show that the proposed method 
outperforms state-of-the-art methods. 
 
Keywords: video matting, video object 
segmentation, alpha matte, closed-form 
matting. 
 
äºŒã€ç·£ç”±èˆ‡ç›®çš„ 
è¦–è¨Šå»èƒŒæ–¹æ³•æ˜¯è¦–è¨Šåˆæˆã€è¦–è¨Šå‰µä½œã€
é›»å½±(é›»è¦–)ç‰¹æ•ˆè£½ä½œä¸Šéå¸¸é‡è¦ä¸”å…·é«˜åƒ¹å€¼
çš„æŠ€è¡“ï¼Œé€éè¦–è¨Šå»èƒŒæ–¹æ³•å¯ä»¥ä¿æœ‰è¦–è¨Šä¸­
å‰æ™¯ç‰©ä»¶(ï¦µå¦‚ä¸»è§’æ¼”å“¡)çš„é€æ˜ï¨ä¸¦è®“å¾ŒçºŒ
çš„è¦–è¨Šè™•ï§¤(ï¦µå¦‚å ´æ™¯ç½®æ›)æ‰€å¾—åˆ°çš„çµæœï¤
ç‚ºçœŸå¯¦ã€‚ä»¥å¾€é›»å½±ç§‘æŠ€ä¸­ç‰¹æ•ˆåˆæˆå•é¡Œçš„è§£
æ±ºæ–¹æ³•æ˜¯åœ¨æ”å½±æ£šå…§å»ºï¤£å¹•(blue screen)çš„
èƒŒæ™¯ï¼Œå†å°‡å‰æ™¯ç‰©ä»¶ç½®æ–¼å…¶ä¸­ï¤­æ‹æ”å½±ç‰‡å¾Œ
å†å°‡å‰æ™¯ç‰©ä»¶æ“·å–å‡ºï¤­ï¼Œä»¥é”åˆ°è¦–è¨Šå»èƒŒçš„
ç›®çš„ï¼›ä½†æ­¤æ–¹æ³•éœ€èŠ±è²»æ˜‚è²´çš„æ”å½±æ£šè¨­ç½®æˆ
æœ¬ã€‚æ­¤å¤–ï¼Œè¦–è¨Šå»èƒŒèˆ‡å‚³çµ±çš„è¦–è¨Šç‰©ä»¶åˆ†å‰²
(video object segmentation)æœ‰æ‰€ï¥§åŒï¼Œè¦–è¨Šç‰©
ä»¶åˆ†å‰²å±¬æ–¼äºŒå…ƒåˆ†å‰²(binary segmentation)
æ–¹æ³•ï¼Œç„¡æ³•ä¿æœ‰è¦–è¨Šç‰©ä»¶åŸæœ‰çš„é€æ˜ï¨ï¼Œæ•…
è©²æ–¹æ³•ä¸¦ï¥§é©åˆæ‡‰ç”¨åœ¨å…·é€æ˜ï¨çš„ç‰©ä»¶
ä¸Šï¼Œï¦µå¦‚é«®çµ²ã€æ¯›é«®ã€é›²å½©ç­‰çš„åˆ†å‰²ã€‚å› æ­¤ï¼Œ
ä¿æœ‰ç‰©é«”é€æ˜ï¨çš„è¦–è¨Šå»èƒŒæ–¹æ³•åœ¨éš¨è‘—è¦–
è¨Šåˆæˆã€å‰µä½œã€å£“ç¸®â€¦ç­‰æ‡‰ç”¨ï¦´åŸŸçš„ç™¼å±•åŠ
éœ€æ±‚ä¸‹ï¤é¡¯å¾—å…¶å¿…è¦æ€§èˆ‡é‡è¦æ€§ã€‚ 
è¦–è¨Šå»èƒŒæ–¹æ³•æ˜¯å½±åƒå»èƒŒæ–¹æ³•çš„å»¶ä¼¸
æŠ€è¡“ï¼Œå½±åƒå»èƒŒæ–¹æ³•çš„ï§¤ï¥èƒŒæ™¯æ˜¯å»ºæ§‹åœ¨ï¥´
å½±åƒä¸­çš„åƒç´ å—åˆ°å…‰ç·šæŠ˜å°„çš„å½±éŸ¿ï¼Œå‰‡åŒæ™‚
æœƒæ¥æ”¶åˆ°å‰æ™¯å’ŒèƒŒæ™¯çš„å…‰ç·šï¼Œæ­¤ç¨®æ··åˆçš„æ¯”
 3
åœ¨æ“·å–å‰æ™¯ç‰©ä»¶é®ç½©ä¸Šï¼Œæ‰€æå‡ºçš„æ··åˆ
å¼è¦–è¨Šç‰©ä»¶åˆ†å‰²æ–¹æ³•ä¸­æ˜¯å…ˆæ¡ç”¨ç§»å‹•åµæ¸¬
(motion detection) [7]ï¤­æ±‚ç®—è¦–è¨Šç‰©ä»¶çš„ç§»
å‹•è³‡è¨Šã€‚åœ¨æœ¬ç ”ç©¶ä¸­åªæ¢è¨ç§»å‹•çš„å‰æ™¯ç‰©
ä»¶ã€éœæ…‹çš„èƒŒæ™¯åŠå›ºå®šçš„æ”å½±æ©Ÿçš„æ¡ˆï¦µã€‚åœ–
2 ç‚ºç§»å‹•åµæ¸¬çš„çµæœï¼Œåœ– 2(a)ç‚ºç¬¬ k å¼µå½±æ ¼ã€
åœ– 2(b)ç‚ºç¬¬ k-1 å¼µå½±æ ¼ã€åœ– 2(c)å‰‡ç‚ºç§»å‹•åµ
æ¸¬çš„çµæœã€‚ 
 
 
    (a)           (b)          (c) 
åœ– 2. ç§»å‹•åµæ¸¬çš„ç¤ºæ„åœ–ã€‚ 
 
æ¢¯ ï¨ è®Š åŒ– åµ æ¸¬ (gradient-variation 
detection) [10]å‰‡æ˜¯ä½¿ç”¨ Sobel é‹ç®—åˆ†åˆ¥å°ç¬¬
k å¼µå½±æ ¼å’Œ k-1 å¼µå½±æ ¼ä»£å…¥å¼å­(2)é€²ï¨ˆé‹
ç®—ï¼Œå…¶ä¸­ ),( yxkG å’Œ ),(1 yxkâˆ’G åˆ†åˆ¥ç‚ºç¬¬ k å¼µå½±
æ ¼å’Œ k-1 å¼µå½±æ ¼çš„æ¢¯ï¨å¤§å°ã€‚åœ– 3(a)ç‚ºåœ– 2
çš„æ¢¯ï¨è®ŠåŒ–åµæ¸¬çµæœã€‚ 
Gkk Tyxyx â‰¥âˆ’ âˆ’ ),(),( 1GG  (2) 
 ç²—ç³™çš„ç§»å‹•ç‰©ä»¶é®ç½© (coarse moving 
object mask)å‰‡æ˜¯å°ç§»å‹•ç‰©ä»¶åµæ¸¬çµæœå’Œæ¢¯
ï¨è®ŠåŒ–åµæ¸¬çµæœé€²ï¨ˆ OR é‹ç®—ï¤­ç²å¾—ï¼Œå¦‚
),(),(),( yxyxyx Gk
M
k
C
k MOMOMO U= æ‰€ç¤ºï¼Œå…¶ä¸­
),( yxCkMO ã€ ),( yxMkMO å’Œ ),( yxGkMO åˆ†åˆ¥ä»£è¡¨
ç²—ç³™çš„ç§»å‹•ç‰©ä»¶é®ç½©ã€ç§»å‹•ç‰©ä»¶åµæ¸¬çµæœã€
æ¢¯ï¨è®ŠåŒ–åµæ¸¬çµæœã€‚åœ– 3(b)ç‚ºï§ç”¨åœ– 2(c)å’Œ
åœ– 3(a)æ‰€ç²å¾—çš„ç²—ç³™çš„ç§»å‹•ç‰©ä»¶é®ç½©çš„çµ
æœã€‚ç”±æ–¼ç²—ç³™çš„ç§»å‹•ç‰©ä»¶é®ç½©å¯èƒ½æœƒæœ‰é›œ
è¨Šï¼Œæ•…éœ€å†é€²ä¸€æ­¥é€²ï¨ˆå»é™¤é›œè¨Šçš„è™•ï§¤ï¤­ç²
å¾—ï¨ç·»çš„ç§»å‹•ç‰©ä»¶é®ç½©(fine moving object 
mask)ã€‚ 
 
  
        (a)                (b) 
åœ– 3. ç²—ç³™çš„ç§»å‹•ç‰©ä»¶é®ç½©ã€‚ 
 
èƒŒæ™¯ç›¸æ¸›æ³•å‰‡æ˜¯ï§ç”¨è§’ï¨æ¨¡çµ„æ³•
(angle-module rule) [8]ï¤­ç²å¾—å‰æ™¯ç‰©ä»¶ï¼Œæ¥
è‘—å†ï§ç”¨å¼å­(3)å’Œ(4)çš„é™°å½±åŠå…‰å½±åå°„å»
é™¤æ³•[8]ï¤­å»é™¤ç›¸é—œçš„é™°å½±åŠå…‰å½±åå°„ï¤­ç²
å¾—æœ€å¾Œçš„å‰æ™¯ç‰©ä»¶é®ç½©ã€‚ 
â©â¨
â§ â‹…â‰¥â‰¥â‹…â‰¤â‰¤=
otherwise ,0
)),((),()),(( and ),( if ,1
),( 2121
yxhyxyxhyxÎ˜
yx kshkkshshkshk
BIÎ’
Sh
Ï†Ï†
 
 (3) 
â©â¨
â§ â‹…â‰¥â‰¥â‹…â‰¤â‰¤=
otherwise ,0
)),((),()),(( and ),( if ,1
),( 2121
yxhyxyxhyxÎ˜
yx krfkkrfrfkrfk
BIB
Rf
Ï†Ï†
 (4) 
åœ¨èƒŒæ™¯å»ºï§·åŠï¤æ–°ä¸Šï¼Œåœ¨ç¬¬ä¸€å¼µå½±æ ¼ä¸­
èƒŒæ™¯å½±åƒæ˜¯è¢«è¨­å®šç‚ºç©ºçš„ï¼Œæ¥è‘—åœ¨å¾ŒçºŒçš„å½±
æ ¼ä¸­èƒŒæ™¯å½±åƒå†ç”±ç›¸é—œçš„åƒç´ (éç§»å‹•ç‰©ä»¶
åƒç´ )ï¤­å–ä»£ï¼›ä¹Ÿå°±æ˜¯ï§ç”¨å¼å­(5)ï¤­é€²ï¨ˆèƒŒ
æ™¯å»ºï§·åŠï¤æ–°ï¼Œå…¶ä¸­ ),( yxkMO æ˜¯ç§»å‹•ç‰©ä»¶é®
ç½©ã€ ),( yxMk æ˜¯æ¨™è¨˜å½±åƒ ( 0),( =yxMk å’Œ
1),( =yxMk åˆ†åˆ¥ä»£è¡¨è©²åƒç´ æ˜¯æœªæ¨™è¨˜åŠå·²æ¨™
è¨˜) 
âªâ©
âªâ¨
â§
==
==Ã—âˆ’+Ã—
=
=
âˆ’
âˆ’
1),(  1),(  ,),(
0),(  1),(  ,),()1(),(
0),(  ,),(
),(
1
1
yxandyxifyx
yxandyxifyxyx
yxifyx
yx
kk
kkk
k
k
MOMkB
MOMkBI
MkI
B Î»Î»
 (5) 
ç•¶èƒŒæ™¯ç›¸æ¸›æ³•æ‰€ç²å¾—çš„ç‰©ä»¶é®ç½©åŠç§»
å‹•ç‰©ä»¶åµæ¸¬æ³•æ‰€ç²å¾—çš„ç‰©ä»¶é®ç½©å‡è¢«ç²å¾—
å¾Œï¼Œæ¥è‘—ï¥¥ï§ç”¨å¼å­ (6)é€²ï¨ˆé®ç½©åˆä½µ
(merging)ï¼Œå…¶ä¸­ ),( yxkBS æ˜¯èƒŒæ™¯ç›¸æ¸›æ³•æ‰€ç²
å¾—çš„ç‰©ä»¶é®ç½©ã€ ),( yxkFD æ˜¯ç§»å‹•ç‰©ä»¶åµæ¸¬æ³•
æ‰€ç²å¾—çš„ç‰©ä»¶é®ç½©ã€‚åœ– 4(a)å‰‡ç‚ºé®ç½©åˆä½µçš„
çµæœåœ–ã€‚ 
âªâ©
âªâ¨
â§
âˆˆ
âˆˆ
=
else
Ryxifyx
Ryxifyx
yx movingt
stillt
Merge
t
 ,0
),(  ,),(
),(  ,),(
),( FD
BS
MO  (6) 
å¯¦å‹™ä¸Šï¼Œæœ‰å¯èƒ½æœƒæœ‰å¤šé¤˜çš„èƒŒæ™¯å€åŸŸï§
åœ¨æœ€å¾Œçš„é®ç½©åˆä½µåœ–ä¸­ï¼Œæ‰€ä»¥æœ€å¾Œé‚„è¦é€²ï¨ˆ
ç§»å‹•ç‰©ä»¶ï¨ï¦“ï¼Œç§»å‹•ç‰©ä»¶ï¨ï¦“å¯åˆ†ç‚ºï¨…å€åŸŸ
çš„å¡«å……åŠç‰©ä»¶é‚Šç·£ï¨ï¦“ï¼Œåœ¨ï¨…å€åŸŸçš„å¡«å……éƒ¨
åˆ†ï¼Œç•¶ï¨…å€åŸŸåœ¨å‰æ™¯ç‰©ä»¶é®ç½©å…§æ™‚ï¼Œå…ˆåˆ¤å®š
ï¨… å€ åŸŸ å…§ æ²’ æœ‰ èƒŒ æ™¯ åƒ ç´  çš„ å€ åŸŸ ( å³
0),( =yxkB ä¹‹å€åŸŸ)ï¼Œä¸¦å°‡å…¶è£œç‚ºå‰æ™¯åƒç´ ï¼Œ
å†åˆ¤æ–·å‰©é¤˜çš„ï¨…å€åŸŸï¼Œç•¶ï¨…å€åŸŸçš„é¢ç©å°æ–¼
ï¨…å€åŸŸå‘¨é•·æ™‚å‰‡åˆ¤å®šç‚ºå‰æ™¯ç‰©ä»¶ï¼Œæœ€å¾Œå†ä½¿
ç”¨ï¦šé€šæ¨™è¨˜åˆªé™¤éå°çš„å‰æ™¯ç‰©ä»¶å€åŸŸå’Œï¨…ã€‚ 
ç‰©ä»¶é‚Šç·£ï¨ï¦“æ˜¯ç‚ºï¦ºä½¿å‰æ™¯ç‰©ä»¶çš„é‚Š
ç·£ï¤åŠ æ½”æ·¨ï¼Œå¾è€Œç²å¾—æœ€å¾Œçš„å‰æ™¯ç‰©ä»¶é®
ç½©ï¼Œå…¶å‰æ™¯ç‰©ä»¶é‚Šç·£ä¸Šæ¯ä¸€é»çš„ï¨ï¦“å‰‡æ˜¯ä½¿
ç”¨å¼å­(7)ï¤­åˆ¤æ–·è©²åƒç´ é»è¦ï¥§è¦å¾å‰æ™¯ç‰©
ä»¶é®ç½©ä¸­åˆªé™¤ï¼Œå…¶ä¸­ ),( yxEkI æ˜¯å‰æ™¯ç‰©ä»¶é‚Šç·£
 5
  
(a) 
  
(b) 
  
(c) 
åœ– 8. Gupta ç­‰äººæ‰€æå‡ºæ–¹æ³•çš„æ¨¡æ“¬çµæœã€‚ 
 
 
  
(a) 
  
(b) 
  
(c) 
åœ– 9. Hu ç­‰äººæ‰€æå‡ºæ–¹æ³•çš„æ¨¡æ“¬çµæœã€‚ 
 
 
 
  
(a) 
  
(b) 
  
(c) 
åœ– 10. æœ¬ç ”ç©¶æ‰€æå‡ºæ–¹æ³•çš„æ¨¡æ“¬çµæœã€‚ 
 
å¯¦é©—æ¨¡æ“¬ä¸­æ˜¯æ¡ç”¨è‡ªï¨ˆæ‹æ”çš„å¤§å»³è¦–
è¨Š(Lobby sequence)ï¤­ä¼°ç®—æ‰€æå‡ºæ–¹æ³•çš„æ•ˆ
èƒ½ï¼Œå¤§å»³è¦–è¨Šå…±å«æœ‰ 250 å¼µå½±æ ¼ï¼Œæ¯å¼µå½±æ ¼
å¤§å°ç‚º 480640Ã— åƒç´ ã€‚å¤§å»³è¦–è¨Šçš„åŸºæº–æ¨£æ¿
(ground truth)æ˜¯æ¡ç”¨æ‰‹å·¥çµ¦å®šäººå·¥å¡—é´‰è³‡è¨Š
ä¸¦é€éå°é–‰å‹å»èƒŒæ³•ï¤­ç²å¾—ï¼Œå¦‚åœ– 7 æ‰€ç¤ºã€‚
åœ– 7(a)æ˜¯å¤§å»³è¦–è¨Šåºï¦œçš„ç¬¬ 48 å¼µåŠç¬¬ 141
å¼µå½±æ ¼ï¼Œåœ– 7(b)ç‚ºç›¸é—œçš„æ‰‹å·¥çµ¦å®šäººå·¥å¡—é´‰
ç¤ºæ„åœ–ï¼Œåœ– 7(c)æ˜¯æ‰€ç²å¾—çš„é€æ˜åœ–å±¤é®ç½©ï¼Œ
åœ– 7(d)å‰‡æ˜¯ï§ç”¨é€æ˜åœ–å±¤é®ç½©æå–å‡ºçš„å…·
é€æ˜ï¨å‰æ™¯ç‰©ä»¶ä¸¦åˆæˆåˆ°ä¸€å€‹ç´”ï¤½è‰²èƒŒæ™¯
çš„ç¤ºæ„åœ–ã€‚ 
å¦å¤–ï¼Œæœ¬ç ”ç©¶åˆ†åˆ¥å’Œ Gupta ç­‰äºº[4]åŠ
Hu ç­‰äºº[6]æ‰€æå‡ºçš„æ–¹æ³•é€²ï¨ˆæ•ˆèƒ½æ¯”è¼ƒï¼Œå…¶
ç›¸é—œç‚ºæ¨¡æ“¬çµæœå¦‚åœ– 8~åœ– 10 æ‰€ç¤ºã€‚ 
å¦å¤–ï¼Œç‚ºï¦ºé€²ä¸€æ­¥è©•ï¥¾æ–¹æ³•çš„æ•ˆèƒ½æŒ‡
æ¨™ï¼Œæ•…æ¡ç”¨ï¦ºå¹³å‡çµ•å°èª¤å·®(mean absolute 
error, MAE)ï¤­é€²ï¨ˆè©•ä¼°ï¼Œå…¶å…¬å¼å¦‚å¼å­(8)
æ‰€ç¤ºï¼Œå…¶ä¸­ ),( jitÎ± ç‚ºæ‰€ç²å¾—çš„é€æ˜åœ–å±¤é®
ç½©ã€ ),( jitÎ± å‰‡ç‚ºåŸºæº–æ¨£æ¿çš„é€æ˜åœ–å±¤é®ç½©ã€‚
ç¶“è¨ˆç®—å…¶è¦–è¨Šå½±æ ¼çš„å¹³å‡ MAE å¾Œï¼Œç²å¾—
Gupta ç­‰äººçš„æ–¹æ³•ç‚º 210512.7 âˆ’Ã— ã€Hu ç­‰äººçš„
æ–¹ æ³• ç‚º 210992.4 âˆ’Ã— ã€ æœ¬ ç ” ç©¶ çš„ æ–¹ æ³• ç‚º
210423.0 âˆ’Ã— ã€‚å¾ä¸Šè¿°çš„æ¨¡æ“¬çµæœåœ–åŠå¹³å‡
MAE å€¼å‡å¯è­‰æ˜æœ¬ç ”ç©¶æ‰€æå‡ºçš„æ–¹æ³•å…·æœ‰
å¾ˆå¥½çš„æ•ˆèƒ½ã€‚ 
0BAUTOMATIC VIDEO MATTING USING CLOSED-FORM MATTING BASED 
ON AUTOMATIC GENERATING TRIMAPS 
 
Wu-Chih Hu (èƒ¡æ­¦èªŒ)1, Jung-Fu Hsu (è¨±æ¦®å¯Œ)2, Deng-Yuan Huang (é»ƒç™»æ·µ)3 
 
1,2Department of Computer Science and Information Engineering, 
National Penghu University of Science and Technology, Taiwan 
{wchu, d98523002}@npu.edu.tw 
3Department of Electrical Engineering, 
Dayeh University, Taiwan 
kevin@mail.dyu.edu.tw 
 
ABSTRACT 
 
This paper proposes an automatic video matting scheme 
that uses closed-form matting based on automatic 
generating trimaps. In the proposed method, foreground 
masks are obtained using the hybrid video object 
segmentation. Next, the trimaps are automatically 
generated using two different scanning windows from 
the obtained foreground masks. Finally, closed-form 
matting is used with the automatically generated trimaps 
to yield the alpha mattes and the foreground objects 
with an opacity estimate. Experimental results show that 
the proposed method has good performance in 
automatic video matting. 
 
Keywords video matting; closed-form matting; alpha 
matte; opacity estimate; trimap. 
 
I. INTRODUCTION 
 
In video matting, the background is removed from a 
video sequence to obtain the foreground object along 
with an opacity (alpha) estimate for each pixel covered 
by the object. Therefore, video matting is not a binary 
segmentation. Video matting plays an important role in 
video editing applications, especially for commercial 
television and film production. 
Typically, the alpha matte is computed using a blue 
(or green) background for easier segmentation of the 
foreground object in video matting. However, this 
technique is not suitable in all situations since it requires 
a calibrated studio setup with special equipment. 
Video matting is an extension of image matting. 
Image matting was first mathematically established by 
Porter and Duff [1]. Image matting considers the 
problem of estimating the opacity of each pixel in a 
given image. The given image is assumed to be a 
composite of a foreground image and a background 
image using the compositing equation, where the color 
of the given image is assumed to be a convex 
combination of corresponding foreground and 
background colors with the associated opacity value 
(alpha value). 
For each pixel in a color image, the compositing 
equation produces 3 equations (RGB channels) in 7 
unknowns. Consequently, image matting is a highly 
under-constrained problem. In order to solve this 
problem, most existing methods of image matting 
require the user to provide additional constraints in the 
form of a trimap or a set of scribbles (brush strokes). 
Most existing methods of video matting are 
supervised video matting (non-automatic matting or 
semi-automatic matting) methods [2-6] that are time-
consuming and troublesome for the user. Therefore, 
automatic video matting (unsupervised video matting) is 
desirable. For automatic video matting to solve the 
alpha matte from a highly under-constrained problem 
without any user input is a great challenge. 
Object segmentation-based trimap estimation can be 
used for automatic video matting. Object segmentation 
can extract a foreground object from the background. A 
trimap is automatically constructed based on the 
obtained foreground object and background to solve the 
alpha matte. Therefore, automatic video matting can be 
performed using video object segmentation [7-10] and 
image matting. 
Closed-form matting [11] does not require exact 
estimates for the foreground and background; a few 
scribbles are sufficient for extracting a quality alpha 
matte. Methods that use video object segmentation and 
closed-form matting have been proposed to achieve 
automatic video matting [12-14]. 
Jain et al. [12] proposed an automatic scribbling 
approach based on the motion of the foreground object 
in a video sequence to obtain automatic video matting. 
A motion analysis technique [15] is used to determine 
the probability map, which defines the pixel as a 
foreground pixel or a background pixel. The corrupted 
probability map is then refined and reconstructed using 
a noise filter and morphological techniques to obtain a 
clear segmentation between the foreground and the 
background. Then, each frame is automatically 
15=GT  is set from experience. Fig. 3(a) shows the 
gradient-variation detection of Fig. 2(a). 
Gkk Tyxyx â‰¥âˆ’ âˆ’ ),(),( 1GG  (1) 
The coarse moving object mask is obtained using Eq. 
(2), where the operator U  is the Boolean operator â€œORâ€; 
),( yxCkMO , ),( yx
M
kMO , and ),( yx
G
kMO  are the 
coarse moving object mask, motion detection, and 
gradient-variation detection of point ),( yx  in the thk  
frame, respectively. Fig. 3(b) shows the detection of the 
coarse moving object mask using Fig. 2(c) and Fig. 3(a). 
),(),(),( yxyxyx Gk
M
k
C
k MOMOMO U=  (2) 
 
    
                  (a)                                          (b) 
Fig. 3. Coarse moving object mask. (a) Gradient-
variation detection; (b) detection of coarse moving 
object mask. 
 
The coarse moving object mask has background 
noise, thus the probability of noise distribution [10] is 
used to eliminate noise. 
In background subtraction, the angle-module rule [7] 
is used to obtain the foreground, where 00 8=Ï‰  and 
100 =h  are set from experience. Then, shadows and 
reflections are removed using Eqs. (3) and (4), 
respectively [7], where ),( yxkB  is the background 
image; 01 0=shÏ† , 02 2=shÏ† , 11 =shh , 5.02 =shh , 
0
1 0=rfÏ† , 02 2=rfÏ† , 21 =rfh , and 12 =rfh  are set from 
experience. 
â©â¨
â§ â‹…â‰¥â‰¥â‹…â‰¤â‰¤=
otherwise ,0
)),((),()),(( and ),( if ,1
),( 2121
yxhyxyxhyxÎ˜
yx kshkkshshkshk
BIÎ’
Sh
Ï†Ï†  
 (3) 
â©â¨
â§ â‹…â‰¥â‰¥â‹…â‰¤â‰¤=
otherwise ,0
)),((),()),(( and ),( if ,1
),( 2121
yxhyxyxhyxÎ˜
yx krfkkrfrfkrfk
BIB
Rf
Ï†Ï†  
 (4) 
In background initialization and background 
updating, the background image is void in the initial 
frame; that is, every pixel in the background image is 
unmarked. Then, the pixels in the background image are 
marked and replaced by the associated pixels in the 
current image except for the moving object mask.  
Background initialization and updating is performed 
using Eq. (5), where 5.0=Î»  is set from experience. 
),( yxkMO  is the fine moving object mask. ),( yxMk  
is the marked image, where 0),( =yxMk  and 
1),( =yxMk  denote that pixel ),( yx  is unmarked and 
marked in the background image, respectively.  
When pixels in the background image are all marked 
or not all marked in the last frame, the feedback scheme 
is used with the constructed background image to return 
to the first frame in a video sequence to obtain high-
accuracy video object segmentation. 
 
âªâ©
âªâ¨
â§
==
==Ã—âˆ’+Ã—
=
=
âˆ’
âˆ’
1),(  1),(  ,),(
0),(  1),(  ,),()1(),(
0),(  ,),(
),(
1
1
yxandyxifyx
yxandyxifyxyx
yxifyx
yx
kk
kkk
k
k
MOMkB
MOMkBI
MkI
B Î»Î»  
 (5) 
When the fine moving object mask and the 
foreground extraction are obtained using the frame 
difference scheme and background subtraction, 
respectively, the merging processing is performed. In 
the merging processing, the result obtained using 
background subtraction (foreground extraction mask) is 
used if the pixel is in the still region; the result obtained 
using the frame difference scheme (fine moving object 
mask) is used if the pixel is in the moving region; 
otherwise the pixel is marked in black, as defined in Eq. 
(6), where ),( yxkBS  is the background subtraction 
mask; ),( yxkFD  is the frame difference mask; stillR  is 
the still region; and movingR  is the moving region. Rules 
of object region determination [8] are used to detect the 
still region and moving region. Fig. 4(a) shows the 
result obtained using the merging processing. 
âªâ©
âªâ¨
â§
âˆˆ
âˆˆ
=
else
Ryxifyx
Ryxifyx
yx movingt
stillt
Merge
t
 ,0
),(  ,),(
),(  ,),(
),( FD
BS
MO  (6) 
 
    
                  (a)                                          (b) 
Fig. 4. Foreground mask. (a) result obtained using the 
merging processing; (b) result obtained using moving 
object refinement. 
 
In practice, a residual background region appears in 
the result obtained using the merging processing. 
Therefore, moving object refinement is proposed to 
remove the residual background region and to store 
hole-regions with the background information inside the 
foreground object. Moving object refinement mainly 
consists of hole-region filling, object boundary 
refinement, and residual region removal. In the hole-
region filling, the point in the hole-region inside the 
foreground object mask is first remarked in white if it is 
not the background pixel (namely, 0),( =yxkB ). Next, 
the residual hole-region inside the foreground object is 
filled if its area is less than its circumference. Finally, 
connected component labeling is used to label each 
respectively; the figures show the results of the th48  
and th141  frames of the Lobby sequence.  
 
    
(a) 
    
(b) 
    
(c) 
   
(d) 
Fig. 7. Ground truths of the Lobby sequence. (a) 
Original frames, (b) manual scribbles, (c) alpha mattes, 
(d) foreground objects with an opacity estimate with a 
green background. 
 
Fig. 8(a)-Fig. 10(a) show the automatic scribbling 
or trimaps. Fig. 8(b)-Fig. 10(b) show the obtained alpha 
mattes. Fig. 8(c)-Fig. 10(c) show the obtained 
foreground objects with an opacity estimate with a 
green background. In the methods proposed by Gupta et 
al., the check boundary r was set as 10 pixels; scribble 
size m was set as 3 pixels; and the number of 
normalized cuts was set as 5. 
The experimental results show that the proposed 
method obtains better automatic video matting, 
especially the matting of hole-regions with the 
background information inside the foreground object. 
Besides, the results obtained using the methods 
proposed by Gupta et al., and Hu et al. are very easily 
affected by the reflection. 
 
    
(a) 
    
(b) 
    
(c) 
Fig. 8. Video matting using the method proposed by 
Gupta et al. (a) Automatic scribbling, (b) alpha mattes, 
(c) foreground objects with an opacity estimate with a 
green background. 
 
    
(a) 
    
(b) 
    
(c) 
Fig. 9. Video matting using the method proposed by Hu 
et al. (a) Automatically generated trimaps, (b) alpha 
mattes, (c) foreground objects with an opacity estimate 
with a green background. 
 
algorithm based on change detection and 
background updating,â€ International Journal of 
Innovative Computing, Information and Control, 
Vol. 5, No.7, pp. 1797-1810, 2009. 
[9] L. Wang and N.H.C. Yung, â€œExtraction of moving 
objects from their background based on multiple 
adaptive thresholds and boundary evaluation,â€ 
IEEE Transactions on Intelligent Transportation 
Systems, Vol. 11, pp. 40-51, 2010. 
[10] W.-C. Hu, â€œReal-time on-line video object 
segmentation based on motion detection without 
background construction,â€ International Journal of 
Innovative Computing, Information and Control, 
Vol. 7, No. 4, pp. 1845-1860, 2011. 
[11] A. Levin, D. Lischinski, and Y. Weiss, â€œA closed-
form solution to natural image matting,â€ IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, Vol. 30, No. 2, pp. 228-242, 2008. 
[12] A. Jain, M. Agrawal, A. Gupta, V. Khandelwal, â€œA 
novel approach to video matting using automated 
scribbling by motion analysis,â€ Proceedings. of 
IEEE International Conference on Virtual 
Environments, Human-Computer Interfaces, and 
Measurement Systems, Istambul, Turkey, pp. 25-30, 
2008. 
[13] A. Gupta, S. Mangal, P. Nagori, A. Jain, and V. 
Khandelwal, â€œVideo matting by automatic 
scribbling using quadra directional filling of 
segmented frames,â€ Proceedings of 2nd IEEE 
International Conference on Computer Science and 
Information Technology, Beijing, China, pp. 336-
340, 2009. 
[14]W.-C. Hu, D.-Y. Huang, C.-Y. Yang, and J.-F. Hsu, 
â€œAutomatic video object segmentation with opacity 
estimate,â€ Proceedings of the 4th International 
Conference on Genetic and Evolutionary 
Computing, Shenzhen, China, pp. 683-686, 2010. 
[15] C.K. Williams and M.K. Titsias, â€œGreedy learning 
of multiple objects in images using robust statistics 
and factorial learning,â€ Nerual Computation, Vol. 
16, No. 5, pp. 1039-1062, 2004. 
[16] J. Shi and J. Malik, â€œNormalized cuts and image 
segmentation,â€ IEEE Transactions on Pattern 
Analysis and Machine Intelligence, Vol. 22, No. 8, 
pp. 888-905, 2000. 
 2
(Speaker: Prof. Hideyuki Takagi, Japan)ã€Learning from Ants and Bees: Nature 
Inspired Problem Solving (Speaker: Prof. Saman Halgamuge, Australia)åŠ
Hybrid ICT Platform for Intelligent Systems (Speaker: Prof. Dickson Lukose, 
Malaysia)ã€‚è€ƒï¥¾å€‹äººç ”ç©¶ï¦´åŸŸèˆ‡èˆˆè¶£ï¼Œæ•…åªï¥«èˆ‡ï¦º Prof. Hideyuki Takagi åŠ
Prof. Saman Halgamuge å‰ï¥¸å€‹çš„æ¼”è¬›å ´æ¬¡ï¼Œé€™ï¥¸å€‹å ´æ¬¡çš„è¬›é¡Œå…§å®¹å°æ–¼å€‹
äººåœ¨æ©Ÿå™¨å­¸ç¿’ï¦´åŸŸçš„ç ”ç©¶æœ‰ï¤é€²ä¸€æ­¥çš„ï¦ºè§£åŠï¥­æ€ï¼Œå†åŠ ä¸Šæœƒä¸­æœ‰å¤šä½èˆ‡
æœƒå­¸è€…çš„æå•ï¼Œè‘—å¯¦è®“æˆ‘å—ï¨—ï¥¼å¤šã€‚ 
æœ¬äººæ‰€ç™¼è¡¨çš„ï¥æ–‡å‰‡æ˜¯è¢«å®‰æ’åœ¨ 12/6æ—¥(ç¬¬äºŒå¤©)ä¸Šåˆ 11:45~13:15é€²ï¨ˆ
çš„ Session : Intelligent Data Analysis And Applications çš„ç¬¬ 3 ç¯‡ï¥æ–‡ç™¼è¡¨ã€‚åœ¨
è©² section ä¸­å…±æœ‰ï§‘ç¯‡ï¥æ–‡ï¼Œæ‰€æœ‰ä½œè€…å‡å‡ºå¸­è©² session ä¸¦ç™¼è¡¨ï¥æ–‡ï¼Œæœƒä¸­è¨
ï¥ç›¸ç•¶ç†±ï¦Ÿï¼Œç™¼è¡¨éç¨‹é †ï§ï¼Œä¹ŸåŒæ™‚å’Œèˆ‡æœƒå­¸è€…äº¤æ›ç ”ç©¶å¿ƒå¾—èˆ‡ç›¸é—œå•é¡Œ
è¨ï¥ï¼Œæ˜¯ä¸€å€‹æˆåŠŸçš„ï¥æ–‡ç™¼è¡¨æœƒã€‚ 
 
äºŒã€ èˆ‡æœƒå¿ƒå¾— 
æœ¬æ¬¡æœƒè­°æ¶µè“‹çš„ä¸»é¡Œç¯„åœéå¸¸å»£æ³›ï¼ŒåŒ…å«æœ‰æ··åˆæ™ºæ…§ç³»çµ±(hybrid 
intelligent systems)ã€å½±åƒèˆ‡è¨Šè™Ÿè™•ï§¤è»Ÿæ€§è¨ˆç®—(soft computing)ã€æ™ºæ…§å‹ç¶²ï¤·
å»ºæ¨¡èˆ‡é€šè¨Šã€æ™ºæ…§å‹è³‡ï¦¾æ¡ç¤¦(data mining)ã€æ§åˆ¶èˆ‡è‡ªå‹•åŒ–è»Ÿæ€§è¨ˆç®—ã€å¤šé‡
ä»£ï§¤ç³»çµ±èˆ‡æ‡‰ç”¨(multi agent systems and applications)â€¦ç­‰ä¸»é¡Œï¼Œæ˜¯ä¸€å€‹æ¢è¨
æ··åˆæ™ºæ…§ç³»çµ±çš„åœ‹éš›ç ”è¨æœƒã€‚æ­¤æ¬¡ç ”è¨æœƒé™¤ï¦ºå°ç£çš„å­¸è€…å¤–ï¼Œäº¦æœ‰ï¥§å°‘æ—¥
æœ¬ã€è‹±åœ‹ã€ç¾åœ‹ã€å—éã€æ¾³å¤§ï§äºâ€¦ç­‰åœ°ç´„ 40 å¹¾åœ‹çš„å­¸è€…èˆ‡æœƒã€‚é™¤å€‹äººï¥
 4
äº”ã€ æ”œå›è³‡ï¦¾åç¨±åŠå…§å®¹ 
æœƒè­°æ‘˜è¦ï¥æ–‡é›†ã€ï¥æ–‡å…‰ç¢Ÿç‰‡åŠæœƒè­°æ‰‹å†Šã€‚ 
Figure 1. The proposed method of gait recognition for different people 
groups 
A. Background Modeling 
The principle of background modeling is simple in that 
we can consider the pixel to be a background point if the 
change of gray intensity of that pixel between adjacent 
frames is small or approaching to zero. Therefore, the 
temporal difference of frames is utilized to find the pixels 
with small change in gray intensity between consecutive 
frames. The pixel in background is determined as. 
 11 ( , ) ( , ) ( , )( , )     1
0
t t
t
if F x y F x y Th x y
BP x y t
otherwise
Â­  Â° t  Â®
Â°Â¯
where  represents that the pixel of  in 
current frame t  is a background pixel if the intensity change 
is less than the threshold  of which the value is 
different pixel by pixel, and it is determined adaptively, 
t
( , ) 1tBP x y  
)
( , )p x y
( , )Th x y
( ,F x y  and 1( ,t )F x y
t

( , )x y
 are gay intensities in current frame 
t  and previous frame 1 , respectively. Therefore, the gray 
intensity of pixels in background for current frame t , i.e., 
, is calculated as. 

tBG
  ),(),(),( yxFyxBPyxBG ttt u 
The average of T frames for each pixel is further 
carried out to represent the image of absolute background 
.
( , )p x y
( , )TBG x y
  
1 1
( , ) ( , ) / ( , )
T T
T t t
t t
BG x y BG x y BP x y
  
 Â¦ Â¦
Figure 2. Result of background modeling. (a) Original image, and 
modeling absolute background using (b) 5 frames, and (c) 10 frames. 
To determine the threshold  adaptively, 
temporal update of this parameter is given as. 
( , )Th x y
 1 1( , ) ( 1) ( ( , ) ( , ))( , )   1t t tt
Th x y t F x y F x y
Th x y t
t
 u   Âª Âº tÂ« Â»Â« Â»
 
where the initial value is set to 0. The result of 
background modeling is shown in Fig. 2. 
0 ( , )Th x y
B. Motion Detection by Background Subtraction 
The purpose of motion detection is to segment the gait 
silhouette of a walking people from their absolute 
background. The idea of background subtraction is quite 
direct in that the change in gray intensity of pixels is 
noticeable if objects appear rightly in these pixels. Therefore, 
the shape of gait silhouette is detected as. 

1 ( , ) ( , )
( , )
0
t B
t
if F x y B x y Th
D x y
otherwise
Â­  t
 Â®
Â¯
 
where  is a gray level of absolute background at 
pixel ,
( , )B x y
( , )p x y BTh
t
 is a threshold value set to 23 by 
experiments, and D x  is a resulting binary image for 
extracting the gait silhouette of a walking people. However, 
since the segmented clusters in  are not always 
complete, morphological processing, including the methods 
of erosion and dilation, is further performed. The result of 
motion detection using background subtraction is shown in 
Fig. 3. 
( , )y
( , )tD x y
Figure 3. Result of motion detection. (a) Absolute image, (b) current 
image, (c) result of background subtraction, and (d) result after 
morphological processing in (c). 
C. Shadow Removal 
The foreground pixels can be considered as shadow 
pixels if their gray intensities are darker than those in 
background [12], which is primarily caused by non-uniform 
illumination on motion objects.  Shadow often leads to the 
deformation of the shape of human gait silhouette, and thus it 
unavoidably increases the erroneous recognition rate of 
human gait. Hence, the method inspired by [12] is proposed 
to effectively remove the influence of shadow on motion 
objects as. 
602 2011 11th International Conference on Hybrid Intelligent Systems (HIS)
gait silhouettes. The highest CRR of 81.0% in the case of 
n=20 were obtained, indicating the feasibility of the 
proposed method. 
Figure 5. Video sequences of 5 different people groups. (a) The pregnant, 
(b) the child, (c) the adult, (d) people with a walking stick, and (e) the aged. 
To analyze the misclassification of gait silhouettes for the 
5 different people groups, a confusion matrix of CRR is 
analyzed, as listed in Table I, from which the three groups of 
the adult, the people with a walking stick, and the aged are 
most confused of having the highest misclassification rate of 
16.6% due to their similar silhouettes in shape. For the 
people group with a walking stick, when the walking stick is 
occluded by walkerâ€™s body, this group can be easily 
recognized as the groups of the adult and the aged. Thus, 
high misclassification rates can arise. Moreover, the two 
groups of the adult and the children are similar in shape but 
different in scale. This result causes a misclassification rate 
of 13.3% between them since the shape of the two groups is 
similar described by the FDs that has the favorable property 
of scale invariance. 
TABLE I. CONFUSION MATRIX OF CORRECT RECOGNITION RATE FOR 
THE CASE OF n=20 FOURIER DESCRIPTORS
The 
pregnant 
The 
children
The 
adult
People/w
walking
stick
The aged
The 
pregnant 
93.4% 0 6.6% 0 0 
The 
children
0 86.7% 13.3% 0 0 
The 
adult
0 0 75.1% 16.6% 8.3% 
People/w
walking
stick
0 0 16.6% 66.8% 16.6% 
The aged 5.0% 0 11.6% 0 83.4% 
IV. CONCLUSION AND FUTURE WORK
In this paper, we have proposed a novel method of 
recognizing the gait silhouettes for different people groups. 
The correct recognition rate of 81% is obtained using only 
the first 20 coefficients of FDs, indicating the feasibility of 
the proposed method. However, the three groups of the adult, 
the people with a walking stick, and the aged for the test 
video sequences have a similar shape, especially when the 
walking stick is occluded by walkerâ€™s body, to cause an 
erroneous recognition rate of 20% between them. Hence, the 
increase of discriminability between the three groups can 
leave as the future work to improve the recognition rate of 
gait silhouettes for different people groups. 
ACKNOWLEDGMENT
This work is partly supported by a grant from National 
Science Council, Taiwan, under contract NSC-100-2221-E-
212-020. 
REFERENCES
[1] Z. Hong, Z. Jun, and Zhijing, â€œA new method of pedestrian gait 
classification,â€ in: Proc. IEEE Int. Conf. on Educational and 
Information Technology, Chongqing, China, 2010, Vol. 3, pp. 268-
272. 
[2] E.H. Zhang, H.B. Ma, J.W. Lu, and Y.J. Chen, â€œGait recognition 
using dynamic gait energy and PCA+LPP methodâ€, in: Proc. IEEE Int. 
Conf. on Machine Learning and Cybernetics, Baoding, China, 2009, 
pp. 50-53. 
[3] X.T. Chen, Z.H. Fan, H. Wang, and Z.Q. Li, â€œAutomatic gait 
recognition using kernel principal component analysisâ€, in: Proc. 
IEEE Int. Conf. on Biomedical Engineering and Computer Science, 
Wuhan, China, 2010. 
[4] S.D. Mowbray, and M.S. Nixon, â€œAutomatic gait recognition via 
Fourier descriptors of deformable objects,â€ LNCS 2688, pp. 566â€“573, 
2003. 
[5] Z. Ling, C. Zhao, Q. Pan, Y. Wang, and Y. Cheng, â€œAnalyzing 
human movements from silhouettes via Fourier descriptor,â€ in: Proc. 
IEEE Int. Conf. on Automation and Logistics, Jinan, China, 2007, pp. 
231-236. 
[6] D. Xiao, and L. Yang, â€œGait recognition using Zernike moments and 
BP neural network,â€ in: Proc. IEEE Int. Conf. on Networking, 
Sensing and Control, Sanya, China, 2008, pp. 418-423. 
[7] B. Ye, and Y.M. Wen, â€œGait recognition based on DWT and SVM,â€ 
in: Proc. IEEE Int. Conf. on Wavelet Analysis and Pattern 
Recognition, Beijing, China, 2007, Vol. 3, pp. 1382-1387. 
[8] X. Yang, J. Dai, Y. Zhou, and J. Yang, â€œGabor-based discriminative 
common vectors for gait recognition,â€ in: Proc. IEEE Int. Conf. on  
Image and Signal Processing, Sanya, China, 2008, Vol. 4, pp. 191-
195. 
[9] J. Wu, J. Wang, and L. Liu, â€œFeature extraction via KPCA for 
classification of gait patterns,â€ Human Movement Science, Vol. 26, 
No. 3, June 2007, pp. 393-411. 
[10] J. Wu, â€œA novel approach for discrimination of human gait using 
kernel learning algorithm,â€ in: Proc. IEEE Int. Conf. on Natural 
Computation, Fuzhou, China, 2010, Vol. 6, pp. 3253-3256. 
[11] P.R.G. Harding, and T.J. Ellis, â€œRecognizing hand gesture using 
Fourier descriptors,â€ in: Proc. IEEE Int. Conf. on Pattern Recognition, 
Cambridge, UK, 2004, Vol. 3, pp. 286-289. 
[12] R. Cucchiara, C. Grana, M. Piccardi, A. Prati, and S. Sirotti, 
â€œDetecting moving objects, ghosts, and shadows in video streams,â€ 
IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 
25, Oct. 2003, pp 1337-1342. 
[13] R.E. Fan, P.H. Chen, and C.J. Lin. â€œWorking set selection using 
second order information for training SVM,â€ Journal of Machine 
Learning Research, Vol. 6, 2005, pp. 1889-1918. 
604 2011 11th International Conference on Hybrid Intelligent Systems (HIS)
 2
Computation for Future Internet: Internet of Things (Speaker: Prof. Han-Chieh 
Chao, Taiwan)åŠ Hot Issues in Evolutionary Many-Objective Optimization 
(Speaker: Prof. Hisao Ishibuchi, Japan)ã€‚è€ƒï¥¾å€‹äººç ”ç©¶ï¦´åŸŸèˆ‡èˆˆè¶£ï¼Œæ•…åªï¥«èˆ‡
ï¦º Prof. Han-Chieh Chao åŠ Prof. Hisao Ishibuchi å¾Œï¥¸å€‹çš„æ¼”è¬›å ´æ¬¡ï¼Œé€™ï¥¸å€‹
å ´æ¬¡çš„è¬›é¡Œå…§å®¹å°æ–¼å€‹äººåœ¨ç‰©ï¦—ç¶²åŠå¤šç›®æ¨™æœ€ä½³åŒ–ç­‰ï¦´åŸŸçš„ç ”ç©¶æœ‰ï¤é€²ä¸€
æ­¥çš„ï¦ºè§£åŠï¥­æ€ï¼Œå†åŠ ä¸Šæœƒä¸­æœ‰å¤šä½èˆ‡æœƒå­¸è€…çš„æå•ï¼Œè‘—å¯¦è®“æˆ‘å—ï¨—ï¥¼å¤šã€‚ 
æœ¬äººæ‰€ç™¼è¡¨çš„ï¥æ–‡å‰‡æ˜¯è¢«å®‰æ’åœ¨8/28æ—¥(ç¬¬å››å¤©)ä¸Šåˆ10:30~12:20é€²ï¨ˆ
çš„ Session : Multimedia Innovative Computing çš„ç¬¬ 1 ç¯‡ï¥æ–‡ç™¼è¡¨ã€‚æœ¬äººç‚ºè©²
session çš„ chairï¼Œè©² section ä¸­å…±æœ‰ï§‘ç¯‡ï¥æ–‡ï¼Œæ‰€æœ‰ä½œè€…å‡å‡ºå¸­è©² session ä¸¦
ç™¼è¡¨ï¥æ–‡ï¼Œç™¼è¡¨éç¨‹ç›¸ç•¶é †ï§ï¼ŒåŒæ™‚ä¹Ÿå’Œèˆ‡æœƒå­¸è€…äº¤æ›ç ”ç©¶å¿ƒå¾—èˆ‡ç›¸é—œå•
é¡Œè¨ï¥ï¼Œå…¶ä¸­ç¬¬ä¸€ä½ Keynote Speaker Prof. Valentina E. Balas å…¨ç¨‹ï¥«èˆ‡æœ¬
sessionï¼Œæ•…æ˜¯ä¸€å€‹æˆåŠŸçš„ï¥æ–‡ç™¼è¡¨æœƒã€‚ 
 
äºŒã€ èˆ‡æœƒå¿ƒå¾— 
æœ¬æ¬¡æœƒè­°æ¶µè“‹çš„ä¸»é¡Œç¯„åœéå¸¸å»£æ³›ï¼ŒåŒ…å«æœ‰å¤šåª’é«”çš„é€²åŒ–è¨ˆç®—ã€é›²ç«¯
è¨ˆç®—çš„è³‡å®‰ã€äººå·¥æ™ºæ…§æ‡‰ç”¨æ–¼é›»å­æœå‹™ã€æ··åˆå¼é€²åŒ–æ¼”ç®—æ³•ã€ï¨ˆå‹•æœå‹™ã€
é›»è…¦ç¶²ï¤·çš„é€²åŒ–è¨ˆç®—â€¦ç­‰ä¸»é¡Œï¼Œæ˜¯ä¸€å€‹æ¢è¨éºå‚³èˆ‡é€²åŒ–è¨ˆç®—åœ‹éš›ç ”è¨æœƒã€‚
æ­¤æ¬¡ç ”è¨æœƒé™¤ï¦ºå°ç£çš„å­¸è€…å¤–ï¼Œäº¦æœ‰ï¥§å°‘æ—¥æœ¬ã€ç¾åœ‹ã€ï¤é¦¬å°¼äºã€å¤§ï§“â€¦
ç­‰åœ°çš„å­¸è€…èˆ‡æœƒã€‚é™¤å€‹äººï¥æ–‡ç™¼è¡¨å ´æ¬¡å¤–ï¼Œäº¦ï¥«èˆ‡ï¦ºå…¶ä»–å ´æ¬¡çš„ï¥æ–‡ç™¼è¡¨ï¼Œ
æœƒä¸­é™¤ï¦ºèˆ‡ï¥æ–‡ä½œè€…æœ‰äº¤è«‡ã€äº’æ›ç ”ç©¶å¿ƒå¾—ã€‚æœ¬æ¬¡å‡ºå¸­æœƒè­°çš„å°ç£å­¸è€…(å«
 Novel Detection of Image Forgery for Exchanged Foreground and Background Using 
Image Watermarking Based on Alpha Matte 
 
Wu-Chih Hu 
Department of Computer 
Science & Information 
Engineering, 
National Penghu University 
of Science and Technology, 
Penghu, Taiwan 
wchu@npu.edu.tw 
Wei-Hao Chen 
Graduate Institute of 
Electrical Engineering and 
Computer Science, 
National Penghu University 
of Science and 
Technology,Penghu, Taiwan 
d99523005@npu.edu.tw  
Deng-Yuan Huang 
Department of Electrical 
Engineering, 
Dayeh University, 
Changhua, Taiwan 
kevin@mail.dyu.edu.tw 
 
Ching-Yu Yang 
Department of Computer 
Science & Information 
Engineering, 
National Penghu University 
of Science and Technology, 
Penghu, Taiwan 
chingyu@npu.edu.tw 
 
 
Abstractâ€”This paper proposes an novel method to detect image 
forgery for exchanged foreground and background by using image 
watermarking based on alpha matte. In the proposed method, the 
DWT-DCT-SVD-based image watermarking is used and applied 
on the alpha mattes obtained using image matting algorithms. The 
difference of singular values is used to determine the exchanged 
foreground or exchanged background. Experimental results show 
that the proposed method can obtain accurate detection of image 
forgery for exchanged foreground or background. Therefore, the 
proposed method is a high-value tool of the image forgery 
detection. 
Keywords-image watermarking; alpha matte; image matting; 
exchanged image;image forgery 
I.  INTRODUCTION 
With the rapid development of image processing 
technology and powerful editing of multimedia tools, it has 
become easier to duplicate and manipulate images without 
degrading the quality and leaving any obvious visual clues. 
Abusive use of image forgery has become a serious problem, 
thus the authentication of images is an important study issue. 
Therefore, it is also a challenging task to obtain accurate and 
robust detection of image forgery. 
Image forensics can be divided into active forensics and 
passive forensics [1, 2]. The active forensic approach is a 
non-blind approach. Image watermarking [3-5] is a popular 
technique among the non-blind approaches. Image 
watermarking embeds a watermark at the recording time and 
extracts hidden message later to verify the image 
authenticity. 
The passive forensic approach is a blind approach. No 
supplementary information is used in the passive forensic 
approaches, such as copy-move forgery detection [6-8] and 
tampering detection of composite images [1, 2, 9]. Therefore, 
the passive forensic approaches are more realistic than the 
active forensic approaches. 
Image matting is the process of extracting the foreground 
object from an image along with an opacity estimate for 
each pixel covered by the object. Although image matting 
has been studied for more than two decades, image matting 
has received increasing attention in the last decade and many 
methods have been proposed for image matting [10]. It is 
worth mentioning that state-of-the-art matting algorithms 
were proposed in the last five years, such as easy matting 
[11], closed-form matting [12], spectral matting [13], and 
modified spectral matting [14, 15]. 
Using image matting can obtain the more realistic image 
composition of the new background image and the extracted 
foreground object with the alpha matte. However, the copy-
move forgery detection can not work well for the above 
composite images. Furthermore, the tampering detection of 
composite images uses the content consistency of the image 
to obtain the forgery detection, thus it also can not obtain the 
accurate forgery detection of the composite image based on 
the alpha matte. Moreover, the image watermarking is used 
to obtain authentication of the whole image, thus it is not 
suitably to detect image forgery for exchanged foreground or 
exchanged background. 
In this paper, we propose a novel image forensic method 
to detect the exchanged foreground and background using 
image watermarking based on alpha matte. In the proposed 
method, the DWT-DCT-SVD-based image watermarking [4] 
is used to embed the watermark, and the alpha matte is 
obtained using the known image matting algorithm. Two 
different watermarks are embedded into the foreground 
image and background image based on obtained alpha matte. 
Then, the difference of singular values is used to determine 
the exchanged foreground or exchanged background of an 
image. Experimental results show that the proposed method 
has good performance on image forgery detection. 
The rest of this paper is organized as follows. The 
proposed image forensic method of exchanged foreground 
and background detection is described in Section II. Section 
III presents experimental results and their evaluations. 
Finally, the conclusion is given in Section IV. 
II. IMAGE FORENSIC METHOD OF EXCHANGED 
FOREGROUND AND BACKGROUND DETECTION 
In the DWT+DCT+SVD-based watermarking method 
[4], the cover image is transformed from the RGB color 
space into the YCbCr color space to obtain the gray-level 
image. The DWT is then applied to the gray-level image to 
obtain the LL subband image. The DCT is then applied to 
2012 Sixth International Conference on Genetic and Evolutionary Computing
978-0-7695-4763-3/12 $26.00 Â© 2012 IEEE
DOI 10.1109/ICGEC.2012.114
245
 III. EXPERIMENTAL RESULTS 
Experiments were conducted on a computer with an 
Intel(R) Core(TM)2 Quad Q8200 @2.33 GHz CPU and 
2GB of RAM. The algorithms were implemented in Matlab 
R2011a. The watermarks NPU-logo and CSIE-logo were 
used to embed into the foreground and background images, 
respectively, as shown in Fig. 3(a) and Fig. 3(b). The 
threshold 10=Th  was set by experience. The alpha matte of 
the tested image was obtained using the component-hue-
difference-based spectral matting [15]. 
 
       
                                 (a)                                              (b) 
Figure 3. Used watermarks: (a) NPU-logo, (b) CSIE-logo. 
Six tested images were used to evaluate the performance 
of the proposed image forgery detection. Figs. 4(a)-4(f) 
show images of a face, woman, bear, Amira, Kim, and 
pharos, respectively. Figs. 5(a)-5(f) show the alpha mattes of 
Figs. 4(a)-4(f), respectively. 
Figs. 6-11 are the results of image forgery detection 
using the proposed method, where Figs. 6(a)-11(a) are the 
detected results of image forgery for exchanged background, 
and Figs. 6(b)-11(b) are the detected results of image forgery 
for exchanged foreground. Experimental results show that 
using the proposed method can obtain the accurate detection 
of image forgery for exchanged foreground or background. 
Therefore, the proposed method has good performance on 
image forgery detection. 
 
   
               (a)                                     (b)                                       (c) 
   
              (d)                                       (e)                                          (f) 
Figure 4. Tested images: (a) face, (b) woman, (c) bear, (d) Amira, (e) Kim, 
(f) pharos. 
   
               (a)                                     (b)                                       (c) 
   
              (d)                                       (e)                                          (f) 
Figure 5. Alpha mattes: (a) face, (b) woman, (c) bear, (d) Amira, (e) Kim, 
(f) pharos. 
 
  
Figure 6. Forgery detection of the face image: (a) result of exchanged 
background, (b) result of exchanged foreground. 
 
  
Figure 7. Forgery detection of the woman image: (a) result of exchanged 
background, (b) result of exchanged foreground. 
 
  
Figure 8. Forgery detection of the bear image: (a) result of exchanged 
background, (b) result of exchanged foreground. 
247
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«è¡ç”Ÿç ”ç™¼æˆæœæ¨å»£è³‡æ–™è¡¨
æ—¥æœŸ:2012/10/08
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«
è¨ˆç•«åç¨±: é«˜æº–ç¢ºæ€§ä¹‹è‡ªå‹•åŒ–è¦–è¨Šå»èƒŒçš„ç ”ç©¶
è¨ˆç•«ä¸»æŒäºº: èƒ¡æ­¦èªŒ
è¨ˆç•«ç·¨è™Ÿ: 100-2221-E-346-008- å­¸é–€é ˜åŸŸ: å½±åƒè™•ç†
ç„¡ç ”ç™¼æˆæœæ¨å»£è³‡æ–™
å…¶ä»–æˆæœ 
(ç„¡æ³•ä»¥é‡åŒ–è¡¨é”ä¹‹æˆ
æœå¦‚è¾¦ç†å­¸è¡“æ´»å‹•ã€ç²
å¾—çé …ã€é‡è¦åœ‹éš›åˆ
ä½œã€ç ”ç©¶æˆæœåœ‹éš›å½±éŸ¿
åŠ›åŠå…¶ä»–å”åŠ©ç”¢æ¥­æŠ€
è¡“ç™¼å±•ä¹‹å…·é«”æ•ˆç›Šäº‹
é …ç­‰ï¼Œè«‹ä»¥æ–‡å­—æ•˜è¿°å¡«
åˆ—ã€‚) 
ç„¡ 
 æˆæœé …ç›® é‡åŒ– åç¨±æˆ–å…§å®¹æ€§è³ªç°¡è¿° 
æ¸¬é©—å·¥å…·(å«è³ªæ€§èˆ‡é‡æ€§) 0  
èª²ç¨‹/æ¨¡çµ„ 0  
é›»è…¦åŠç¶²è·¯ç³»çµ±æˆ–å·¥å…· 0  
æ•™æ 0  
èˆ‰è¾¦ä¹‹æ´»å‹•/ç«¶è³½ 0  
ç ”è¨æœƒ/å·¥ä½œåŠ 0  
é›»å­å ±ã€ç¶²ç«™ 0  
ç§‘ 
æ•™ 
è™• 
è¨ˆ 
ç•« 
åŠ  
å¡« 
é … 
ç›® è¨ˆç•«æˆæœæ¨å»£ä¹‹åƒèˆ‡ï¼ˆé–±è½ï¼‰äººæ•¸ 0  
 
