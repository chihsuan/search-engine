II 
 
ÊëòË¶Å 
Âú®Êú¨Ë®àÁï´‰∏≠ÔºåÊàëÂÄëÈáùÂ∞çÂ§öÊ®°ÂºèÈü≥Ô§îÊÉÖÁ∑íËæ®ÔßºÈÄ≤Ô®àÁ†îÁ©∂ÔºåÁ∂ìÈÅé‰∏âÔ¶éÁöÑÂä™Ô¶äÔºåÂÖ±Áç≤Âæó‰∏ãÔ¶ú‰∏ª
Ë¶ÅÁ†îÁ©∂ÊàêÊûúÔºö 
 
1. Áõ∏Â∞çÊÉÖÁ∑íÊ®ôÂÆöÂèäÊ®°ÂûãÂª∫Ôß∑ (IEEE TASLP 2011) [1] 
ÊàëÂÄëÂ∞áÈü≥Ô§îÊÉÖÁ∑íË°®Á§∫Âú®Áî±Ê≠£ÂêëÔ®ÅÂèäÊøÄÊòÇÔ®ÅÂ±ïÈñãÁöÑ‰∫åÁ∂≠Âπ≥Èù¢‰πã‰∏ÄÈªûÔºåÊèêÂá∫ÈÄèÈÅéÊØîËºÉÈü≥Ô§îÁöÑ
Áõ∏Â∞çÊÉÖÁ∑íÔ§≠ÂÅöÊÉÖÁ∑íÊ®ôÂÆö‰ª•ÂèäÊ®°ÂûãÂª∫Ôß∑ÁöÑÁ≥ªÁµ±ÔºåÁî®‰ª•Ê∏õ‰ΩéÈü≥Ô§îÊ®ôË®ªÁöÑÈõ£Ô®Å‰∏¶ÊèêÂçáÊÉÖÁ∑íËæ®Ôßº
‰πãÊ∫ñÁ¢∫Ô®Å„ÄÇ 
2. ‰ª•Ëø¥Ê≠∏Ê≥ïËæ®ÔßºÈü≥Ô§îÊÉÖÁ∑í (IEEE TASLP 2008) [2] 
ÊàëÂÄëÂ∞áÈü≥Ô§îÊÉÖÁ∑í‰ª•ÊøÄÊòÇÔ®ÅËàáÊ≠£ÂêëÔ®ÅË°®Á§∫Ôºå‰∏¶ÔßùÁî®Ëø¥Ê≠∏Ê≥ïÂàÜÂà•Ëæ®ÔßºÂá∫ÊØè‰∏ÄÈ¶ñÊ≠åÊõ≤ÁöÑÊøÄÊòÇÔ®Å
ËàáÊ≠£ÂêëÔ®Å„ÄÇ 
3. Èü≥Ô§îÊÉÖÁ∑íÊ©üÔ•°ÂàÜ‰ΩàÈ†êÊ∏¨ (IEEE TASLP 2011) [3] 
Áî±ÊñºÊØèÂÄã‰∫∫Â∞çÈü≥Ô§îÊÉÖÁ∑íÁöÑÊÑüÂèóÔ•ßÂêåÔºåÊâÄ‰ª•Ô•¥Âè™Â∞áÈü≥Ô§îÊÉÖÁ∑í‰ª•‰∏ÄÈªûË°®Á§∫ÊòØÔ•ßË∂≥ÁöÑ„ÄÇÁÇ∫Ô¶∫Ëß£
Ê±∫ÂÄã‰∫∫ÂåñÁöÑÂïèÈ°åÔºåÊàëÂÄëÂ∞áÈü≥Ô§îÊÉÖÁ∑í‰ª•Ê©üÔ•°ÂàÜ‰ΩàÁöÑÊñπÂºèË°®Á§∫Ôºå‰∏¶ÊèêÂá∫‰∏ÄÂ•óÈ†êÊ∏¨Èü≥Ô§îÊÉÖÁ∑íÊ©ü
Ô•°ÂàÜ‰ΩàÁöÑÊñπÊ≥ï„ÄÇ 
4. ÔßùÁî®ÊÉÖÁ∑íÊêúÂ∞ãÈü≥Ô§î (IEEE MMTC E-Letter 2009) [4] 
ÊàëÂÄëÂ∞áÈü≥Ô§îÊÉÖÁ∑íË°®Á§∫Âú®Áî±Ê≠£ÂêëÔ®ÅÂèäÊøÄÊòÇÔ®ÅÂ±ïÈñãÁöÑ‰∫åÁ∂≠Âπ≥Èù¢‰∏äÔºåÊ≠§‰∫åÁ∂≠Âπ≥Èù¢ÂèØ‰ΩúÁÇ∫‰∏ÄÂÄãÁ∞°
ÂñÆÊñπÔ••ÁöÑÈü≥Ô§îÁÆ°Ôß§„ÄÅÁÄèË¶Ω„ÄÅÊ™¢Ô•™ÁöÑ‰ªãÈù¢Ôºå‰∏¶‰∏îÁõ∏Áï∂ÈÅ©Áî®ÊñºÊô∫ÊÖßÂûãÊâãÊ©ü‰∏ä„ÄÇ 
 
Êú¨Ë®àÁï´ÁöÑÁ†îÁ©∂ÊàêÊûúË¢´ÈÅ¥ÈÅ∏ÁÇ∫ 100 Ô¶é 6 Êúà 8 Êó•ÂúãÁßëÊúÉÊàêÊûúÁôºË°®Ë®òËÄÖÊúÉ‰πã‰∏ªÈ°å„ÄÇÊ≠§Ë®àÁï´‰πãÁî¢
Âá∫ÂåÖÊã¨Èü≥Ô§îÊÉÖÁ∑íËæ®ÔßºÔ¶¥ÂüüÁöÑÁ¨¨‰∏ÄÊú¨Â∞àÊõ∏„ÄÅÂõõÁØáÊúüÂàäÔ•ÅÊñá„ÄÅ20 ÁØáÂúãÈöõÁ†îË®éÊúÉÔ•ÅÊñáÂèä‰∏ÄÈ†ÖÂ∞àÔßù„ÄÇ 
 
ÈóúÈçµË©ûÔºöÈü≥Ô§îÊÉÖÁ∑íËæ®Ôßº„ÄÅËø¥Ê≠∏Ê≥ï„ÄÅÂÄã‰∫∫Âåñ„ÄÅÈü≥Ô§îÊ™¢Ô•™ 
 
IV 
 
ÁõÆÔ§ø 
1. ÈÄèÈÅéÊØîËºÉÈü≥Ô§îÁöÑÁõ∏Â∞çÊÉÖÁ∑í‰πãÊÉÖÁ∑íÊ®ôÂÆöÂèäÊ®°ÂûãÂª∫Ôß∑Á≥ªÁµ±‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ 1
2. ÈÄèÈÅéËø¥Ê≠∏Ê≥ïËæ®ÔßºÈü≥Ô§îÊÉÖÁ∑í‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ 19
3. È†êÊ∏¨Èü≥Ô§îÊÉÖÁ∑íÁöÑÊ©üÔ•°ÂàÜ‰Ωà‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ 33
4. ÔßùÁî®ÊÉÖÁ∑íÂπ≥Èù¢ÊêúÂ∞ãÈü≥Ô§î‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ 50
5. Ô•´ËÄÉÊñáÁçª‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ 53
 
 
 
2 
 
emotion perception, it is difficult to accurately compute the emotion values [23]. Existing methods, 
which use the mean square error (MSE) between the estimated emotion values and the ground truth as 
the objective function to train a model [23], [29], [31], face a problem that the range of the emotion 
estimate is much smaller than that of the ground truth, as an inaccurate model tends to make 
conservative estimate (see Fig. 2 for illustration). The ranking-based approach is free of this issue 
because songs associated with topmost/lowermost rankings would be assigned with the highest/lowest 
emotion values, producing a full coverage of the 2-D emotion space. 
The contributions of the work are summarized as follows: 
ÔÅ¨ We propose a new approach that ranks rather than rates the emotion of music pieces for 
music organization and retrieval. 
ÔÅ¨ We develop a novel emotion annotation method called music emotion tournament to relieve 
the burden of emotion annotation on the subjects and enhance the quality of the ground truth 
(Section 1.3). 
 
 
Fig. 1. with the and each is represented as a point in the 2-D emotion space [17], 
where a user can specify points or draw trajectories to retrieve songs of certain emotions [19]‚Äì[23]. 
 
Fig. 2. Distributions of the ground truth (squares) and the recognition result circles) of the method 
in [19]. It can be that, due to the difficulty of accurately computing the emotion values, the range of the 
emotion estimate is much smaller than that of the ground truth. The ranking- based approach is free of this problem because 
songs associated with topmost/ lowermost rankings are assigned with the highest/lowest emotion values, producing a full 
coverage of the 2-D emotion space. 
4 
 
observations [35]. Given N inputs (xi, yi), where xi is the feature vector of an object di and yi is the real 
value to be predicted, a regressor      is created by minimizing the MSE 
 
  
 
 
    
 (        )
 
                               (1) 
 
where  is the recognition result for  . Many good regression algorithms are available. For MER, 
the support vector regression (SVR) [39] is found to have superior performance [23]. It nonlinearly maps 
input feature vectors to a higher dimensional feature space  by the so-called ‚Äïkernel trick [35]‚Äñ 
and learns a nonlinear function by a linear learning machine   ùíô    
         in the 
kernel-induced feature space, where data are more separable. In our evaluation, we compare the 
performance of SVR and the proposed learning-to-rank algorithm both quantitatively and qualitatively. 
To summarize, the categorical approach and the dimensional approach offer different advantages 
that are complementary to each other. We can imagine a mobile device that employs both to facilitate 
music retrieval. 
1.3 Ranking-Based Emotion Annotation 
The cognitive load of rating emotion could introduce serious user fatigue that reduces the reliability 
of the annotations [33]. To address this issue, we propose a ranking measure and have the subjects only 
make pairwise comparisons of the emotions of songs. Since it is a lengthy process to determine the 
straight order of music pieces (requiring n(n-1)/2 comparisons), we design a music emotion tournament 
scheme to reduce the burden on subjects. As Fig. 3 shows, the music pieces are organized into n-1 
matches, which form a hierarchy of log2n levels. A subject listens to two music pieces in a match and 
decides which one has larger emotion value (e.g., more positive valence or higher arousal). The result of 
a pairwise comparison is incorporated to an N √ó N binary preference matrix P, with each entry Pi,j 
representing whether object di is ranked higher than object dj, as exemplified in Fig. 3. N denotes the 
total number of songs in the database and n denotes the number of songs to be annotated by a subject; 
usually N >> n. 
We can then use the greedy algorithm proposed in [40] to efficiently approximate a global order œÄ 
from the preference matrix. The intuition is simple: The more objects an object di dominates (ranked 
higher), or the less objects that di is dominated by, the greater global order di would have. Specifically, 
each object is assigned a potential value 
 
ùúå  ‚àëùëëùëó‚ààùëâ ùëÉ ,ùëó  ùëÉùëó,                                 (2) 
 
where V denotes the set of objects whose global order have not been determined. The greedy algorithm 
picks an object dt that has the maximum potential and assigns it a rank œÄùë°  ùëÅ  |ùëâ|  1, which 
 
Fig. 3. Left: the proposed music emotion tournament, which groups eight randomly chosen music pieces in a tournament of 
seven matches. We use a bold line to indicate the winner of each match. Right: the corresponding preference matrix, with 
the entry (i, j) painted black to indicate that the object di is ranked higher than the object dj. The global order 
f>b>c=h>a=d=e=g can be estimated by the greedy algorithm described in Table I. 
6 
 
approach [42]. The former takes object pairs as learning instances, formulates the problem as the 
classification of object pairs into two categories (correctly and incorrectly ranked), and trains 
classification models for ranking. For example, RankSVM adapts the support vector machines to 
classify object pairs in consideration of large margin rank boundaries [41]. The pairwise approach, 
however, ignores the fact that ranking is essentially an estimation task applied to a list of objects. 
Moreover, taking every possible pair is of complexity O(N
2
M), which is exceedingly time consuming 
when N is large. 
The listwise approach ListNet proposed in [42] conquers the shortcomings of the pairwise approach 
by using score lists directly as learning instances and minimizing the listwise loss between the ground 
truth ranking list and the estimated one. In this way, the optimization is performed directly on the list 
and the computational cost is reduced to O(NM). More specifically, to define a listwise loss function, the 
top-one probability is employed to transform a list of ranking scores into a probability distribution [38]. 
The top one probability P(si) of object di, defined as follows, represents the probability of the object 
being ranked on the top: 
 
ùëÉ ùë†   
     
 ùëó  
    ùëó 
 
       
 ùëó  
    (  )
                          (5) 
 
where i and j are object indices and      is an increasing and strictly positive function such as the 
exponential function. Modeling the list of scores as a probabilistic distribution, a metric such as the cross 
entropy can be used to measure the distance (listwise loss) between the ground truth list and the 
estimated one 
 
 ( , ùëì   )   ‚àë   
 ùëÉ ùë†    ùëîùëÉ ùëì                           (6) 
 
Because this cost function is differentiable, we can minimize it by the gradient descent procedure [43]. 
In [42], the authors adopt the linear neural network model as the ranking model, with the constant b 
omitted 
 
ùëì      
                                    (7) 
 
and update the weights w at a learning rate   as follows: 
        ,  
 
Fig. 4. Architecture of a radial basis neural network model, which is used as the ranking model in the proposed RBF-ListNet 
algorithm. 
8 
 
 
1.5 System Overview 
A schematic diagram of the MER system is shown in Fig. 5. The music emotion tournament 
scheme described in Section 1.3 is performed in the subjective test to collect emotion annotation. The 
RBF-ListNet algorithm proposed in Section 1.4.2 is adopted to train a ranking model. The ground truth 
needed by this algorithm is obtained by approximating the global order and then computing the ranking 
scores using (4). After training, the ranking model takes the features of a new song as input and outputs 
the estimated ranking score of the song. The ranking scores of a music collection are then mapped to the 
2-D emotion space to generate a dimensional representation. The songs which are ranked topmost 
(bottommost) are assigned with the maximal (minimal) valence or arousal values, then for simplicity the 
remaining ones are mapped linearly. 
1.6 Experiment Setup 
Data Collection 
As summarized in Table 3, two datasets are utilized in the performance study. The first dataset, db1, 
Table 2 
PROPOSED RBF-LISTNET  
 
 
 
Fig. 5. Schematic diagram of the music emotion recognition system 
Table 3 
STATISTICS OF THE TWO DATASETS UTILIZED IN THIS  
 
10 
 
annotation process begins. 
A questionnaire is presented after the annotation process with three inquiries regarding the 
cognitive load of annotation (see Section 1.7.1) and the following three inquiries regarding the factors 
that influence emotion perception. 
ÔÅ¨ How much does the acoustic property of music (melody, rhythm, and timbre) influence your 
emotion perception? (five-point scale, weak to strong). 
ÔÅ¨ How much do the lyrics of music influence your emotion perception? (five-point scale, weak to 
strong). 
ÔÅ¨ What factors of music, besides the above two, influence your emotion perception? (free text; 
optional problem). 
The average ratings of the 622 subjects that answer the first two questions are 4.46 and 3.33, 
respectively. Both the acoustic property and the lyrics are considered important for emotion perception, 
but clearly the former exerts a stronger effect than the latter. This is reasonable since the power of music 
is universal; we can perceive emotion from songs that are purely instrumental or in an unfamiliar 
language [52]. According to this result, we would focus on modeling emotion perception using the 
acoustic property of music in this work. 
Many observations can be made from the 179 responses to the third question, which are shown in 
Table 4. First, vocal timbre also influences the emotion perception of music and therefore speech 
features such as formants and prosody contours [53], [54] may be useful.7 Second, emotion perception 
is indeed subjective and is under the influence of listening context, mood, familiarity, memory, and 
preference. Finally, subjects report that song titles should not be shown during annotation and that a 
proper segment selection should be done to capture the most representative part of a song. These 
comments could be considered by future MER systems. 
Feature Extraction 
The experience of music listening is multidimensional. Different emotion perceptions are usually 
associated with different patterns of acoustic cues [52], [55]‚Äì[57]. For example, arousal is often 
correlated with tempo (fast/slow), pitch (high/low), loudness (high/low), and timbre (bright/soft), while 
valence is related to musical mode (major/minor) and harmony (consonant/dissonant) [57]. It is also 
noted that emotion perception is rarely dependent on a single music factor but a combination of them 
[56]. For example, loud chords and high-pitched chords may suggest more positive valence than soft 
chords and low-pitched chords, irrespective of mode. See [57] for an overview of the empirical research 
concerning the influence of different music factors on emotion perception. In this work, we extract the 
following features (summarized in Table 5) to represent the three perceptual dimensions of music 
listening: melody, timbre, and rhythm, which are all closely related to emotion perception. 
Melody: We use the MIR toolbox [58] to generate two pitch features (salient pitch and chromagram 
center) and three tonality features (key clarity, mode, harmonic change). MIR toolbox estimates the 
pitch, or the perceived fundamental frequency, of each short time frame (50 ms, 1/2 overlapping) based 
on the multipitch detection algorithm described by Tolonen and Karjalainen [62]. The algorithm 
Table 5 
FEATURE  
 
12 
 
shown relevant to both valence and arousal perception in [9]. 
1.7 Performance Study 
We run a series of experiments to study the performance of the proposed methods. First, we 
compare the cognitive load of emotion annotation of the conventional rating measure and that of the 
proposed ranking measure. Second, we compare the accuracy of emotion recognition using different 
feature sets, learning algorithms, and parameter settings. Feature selection is also conducted to obtain 
the best feature representation. Finally, we evaluate the subjective satisfaction of the recognition result 
of our MER system. 
Cognitive Load of Annotation 
The following three evaluation inquiries are presented to the subjects after the annotation process. 
ÔÅ¨ Easiness. How easy it is to perform the annotation. 
ÔÅ¨ Within-subject reliability. The possibility that the annotations are nearly the same a month later. 
ÔÅ¨ Between-subject reliability. The possibility that the annotations are nearly the same as those of 
others.  
All answers are on a five-point scale ranging from 1 to 5 (strongly disagree to strongly agree). 
Table 6 shows the average answers of the 603 subjects that answer the three questions. It can be found 
that the ranking measure is indeed much easier to use than its rival; this validates our claim that it is 
easier for human to rank rather than rate the music emotion. The subjects also express a high confidence 
level of both within- and between-reliability when the ranking measure is used, indicating its ability to 
reduce the inconsistency of emotion annotation. The fact that all the results of the rating measure are 
below 3 further confirms the importance of the proposed method. We have swapped the order of the 
tasks and found that the results are nearly the same. For all the three inquiries the differences between 
ranking and rating are significant under the two-tailed t-test value (p-value<0.01). 
Next, we conduct a test-retest reliability study to examine whether the annotations of a subject are 
similar after a span of time [67]. This way, we can objectively evaluate the within-subject reliability by 
computing the correlation between the two courses of annotations. More specifically, we conduct a 
second subjective test one month after the first one and invite 98 subjects to annotate the pieces they 
have annotated again. We compute the Spearman rank correlation coefficient [44] for every subject. The 
value is in [-1, 1]; a higher value indicates a higher correlation. As Table 6 shows, the average 
correlation of the ranking measure (0.708) is indeed higher than that of the rating measure (0.635). 
However, unlike the subjective evaluation, the difference between the ranking and rating measures is not 
significant in the objective evaluation. A possible explanation is that in the subjective evaluation the 
subjects overestimated the within reliability because they found the annotation easy to perform. 
We also use the ratings to order music pieces and compare the resulting order to that of the ranking 
Table 7 
GAMMA STATISTIC OF A SVR MODEL USING DIFFERENT FOR VALENCE AND AROUSAL 
 
 
14 
 
to 73.6% and 87.6%, respectively, which are comparable to that of the state-of-the-art music emotion 
classification systems [9], [10]. 
2) Comparison of Different Learning Algorithms: We then compare the performance of SVR and 
ListNet with different kernels/neural network functions using the following four settings. In the first db1
‚Üí db1 setting, we use one of the four subsets of db1 for testing and the remaining ones for training. In 
the second db2‚Üídb2 setting, we randomly sample one-fourth of db2 for testing and the remaining ones 
for training. The above procedure is repeated 100 times to get the average result. In the third db2‚Üídb1 
setting, we use the whole db2 as the training set and the whole db1 for testing. Likewise, in the fourth 
db1‚Üídb2 setting, we use db1 for training and db2 for testing. We regard the result of the last two 
settings as more important because they evaluate how well an MER system can be trained and evaluated 
using separate datasets. 
As arousal recognition has been shown easier, we focus on valence recognition and use the top 70 
features selected by RReliefF from {melody, timbre, rhythm} as the feature representation. In all 
settings, feature normalization is done by standardizing the training data to zero mean and unit variance 
and applying the normalization parameters to the test set. The convergent threshold Œ¥ is set to 1e-4, the 
learning rate   is set to 0.01 and 10000/N for ListNet and RBF-ListNet, and the number of radial basis 
functions K is set to 10 (a parameter sensitivity test will be presented later). In addition, we set the 
maximum number of iterations of ListNet and RBF-ListNet to 40 to prevent overfitting. We also 
implement a baseline method that randomly orders the music pieces. 
Table 8 shows the experimental results. For db1‚Üídb1, the SVR of RBF kernel performs the best. 
The algorithms that employ RBF function generally perform better than those that use linear function; 
this validates previous findings that the characteristics of music are often better modeled with a 
nonlinear function. All the four algorithms outperform the random permutation baseline by a great 
margin. 
For db2‚Üídb2, however, the performance difference between the four algorithms becomes small, 
suggesting that valence recognition for this setting is relatively easier. The large number of training data 
in this setting (930 pieces) may account for this result. 
Table 9 
ESTIMATED ORDER (POSITIVE TO NEGATIVE VALENCE) OF SONGS OF db1 BY THE MER SYSTEM TRAINED WITH db2 ( db2‚Üídb1) THE 
NUMBER IN THE INDICATES THE AVERAGE GROUND TRUTH RATING OF THE  
 
 
Fig. 6. Gamma statistic for valence recognition with different numbers of training data under the db2‚Üídb1 setting. Each 
error bar represents one standard error of 100 trials. 
 
16 
 
degrades the performance. Moreover, we have found that the optimal value of   depends on the number 
of training data N. Setting   to 1e4/N seems to perform well in all four settings (for db2‚Üídb1, 
 ‚âÖ 10.8). The value of also K affects the recognition accuracy. Since the time complexity increases 
linearly with K, we set K to 10 in balance of computational time and accuracy. The computational cost 
of RBF-ListNet is slightly higher than SVR; for db2‚Üídb1 RBF-ListNet and SVR take 4.2 and 1.6 s, 
respectively. 
Subjective Evaluation of the Recognition Result 
We further evaluate the subjective satisfaction of the recognition result of MER by conducting 
another subjective evaluation that places 15 randomly chosen music pieces from db2 on a displayed axis 
according to valence values. Subjects are asked to listen to the pieces and then express the satisfaction of 
the distribution of the pieces on an 11-point scale ranging from 0 to 10 (strongly unsatisfactory to 
strongly satisfactory).We train the MER system under the db2‚Üídb2 setting and compare the following 
four methods of obtaining the valence values: 1) a baseline method that determines the valence value 
randomly, 2) the ground truth ranking-based annotations, 3) the recognition result of ListNet using 
ranking-based annotations, and 4) the recognition result of SVR using rating-based annotations. For 
methods 2) and 3) the rankings are converted to ratings in a linear fashion, as described in Section 1.5.  
Table 11 shows the average result of the 134 subjects. All methods score higher than 5 except for 
the random baseline. Both SVR and ListNet outperform the ground truth one, likely because of the 
excessive number of ties of the ground truth that reduces the satisfaction. More importantly, it is noted 
that the satisfaction of ListNet and SVR are comparable; both of them are around 6.45. This result 
implies that a good emotion-based visualization can be obtained by focusing on the relative emotion 
values only. 
1.8 Discussion 
The dimensional approach to MER aims at representing each song as a point in the 2-D emotion 
space. However, it is unclear whether human perceptually evaluates the accuracy of such representation 
with respect to the exact position of each song or the position relative to one another. In fact, as emotion 
perception is subjective, it is also unclear whether the exact valence and arousal values of a song can 
even be determined. This work explores the possibility of representing songs in the emotion space 
according to the relative emotion rankings, which simplifies both the annotation and model training 
processes of MER (both the human subjects and computer need to pay attention to the relative rankings 
only). However, the strategy of converting emotion rankings to emotion values in a linear way may be 
overly simplified. For future work we may consider using emotion ratings to regulate the conversion. 
For example, to determine which songs have neutral emotion. 
As for the emotion tournament scheme, we can include a reverse tournament to enhance the 
resolution in the lowermost ranks of the ground truth. For example, we can ask half of the subjects to 
rank songs in ascending order of emotion values and half in descending order. We can also allow the 
result of a match to be a tie. For example, if a and b in Fig. 3 are tied, we can randomly select one of 
them as a surrogate to compare with the winner between c and d. This would further relieve the burden 
on the subjects since in some cases the emotions of two songs are just not comparable. Moreover, in this 
work we do not explicitly deal with contradictions. It is possible that is ranked higher than by one 
subject and ranked lower by another. This is not an issue in this work because the songs annotated by 
two subjects seldom overlap, but may deserve attention in future research. 
How the categorical approach and the dimensional approach to MER can benefit each other 
deserves attention. Being two extreme scenarios (discrete and continuous), categorical MER and 
dimensional MER actually share a unified goal of predicting the affective content of music pieces. 
Therefore, success in one scenario may benefit the other. For example, the ordinal relationship among 
the emotion classes (e.g., exciting is more aroused than cheerful than peaceful) can be incorporated to 
improve categorical MER. More research in this direction is worth pursuing. 
Emotion perception of a song is in nature subjective and is under the influence of personal factors 
18 
 
 ùë§ùëö  ‚àë( ùëÉ ùë†   ùëÉ ùíò
    )  ùëö
 
   
 
 ‚àë ùõø ùëì    , ùë†  ‚àáùë§ùëöùëì  ùê¢ 
 
                     (13) 
 
where ùõø ùëì    , ùë†   ùëÉ(ùëì    )  ùëÉ ùë†   is the normalized error for the ith object. 
Alternatively, when the cosine RBF neural network model [47] is employed as the ranking function 
ùëì     ‚àëùëò  
ùêæ ùúÜùëòùëïùëò     ‚àë Œªk
ùõºùëò
(||  ‚àíùêØùëò||
2
+ùõºùëò
2)
 2‚ÅÑ
ùêæ
ùëò            (14) 
the gradients can be derived in a similar way as follows: 
 ùúÜùëò  ‚àëùõø ùëì    , ùë†  ‚àáùúÜùëòùëì  ùê¢ 
 
   
 
 ùêØùëò  ‚àëùõø ùëì    , ùë†  ‚àáùêØùëòùëì  ùê¢ 
 
   
 
 ùõºùëò  ‚àë ùõø ùëì    , ùë†  ‚àáùõºùëòùëì  ùê¢ 
 
                    (15) 
and 
‚àáùúÜùëòùëì  ùê¢  ùëïùëò    , 
‚àáùêØùëòùëì     ùúÜùëò‚àáùêØùëòùëïùëò     ùúÜùëò‚àáùêØùëò
ùõºùëò
 ùëò 
 
 ùúÜùëò
 ùõºùëò
 ùëò 
 ‚àáùêØùëò ùëò  ùúÜùëò
ùõºùëò
 ùëò 
3
    ùêØùëò  
 ùúÜùëò
ùëïùëò    
3
ùõºùëò
 
    ùêØùëò , 
‚àáùõºùëòùëì     ùúÜùëò‚àáùõºùëòùëïùëò     
 ùúÜùëò
 ùëò  ùõºùëò‚àáùõºùëò ùëò 
 ùëò 
  ùúÜùëò
 ùëò 
  ùõºùëò
 
 ùëò 
3  
 ùúÜùëò
‚Ñéùëò    
ùõºùëò
 1  ùëïùëò    
                  (16) 
where  ùëò  (||   ùêØùëò||
 
 ùõºùëò
 )
  ‚ÅÑ
, ùëïùëò     ùõºùëò  ùëò ‚ÅÑ , and 
‚àáùêØùëò ùëò       ùêØùëò  ùëò 
‚àí  
‚àáùõºùëò ùëò  ùõºùëò ùëò 
‚àí                             (17) 
The gradients are used in the model training process of RBF-ListNet as described in (10).   
20 
 
allows thorough performance study, and generally exhibits reliable prediction performance. The other 
main issue, the dependency between arousal and valence, is addressed by reducing the data correlation 
by principal component analysis [81].  
An extensive performance study is conducted to evaluate the prediction accuracy of the proposed 
regression approach by using different combination of data spaces, feature spaces, and regression 
algorithms. Support vector regression [82] is found to produce better prediction accuracy than linear 
regression [35] and AdaBoost.RT [83]. The R2 statistics [35] reaches 58.3% for arousal and 28.1% for 
valence. Because there are no other existing systems viewing MER from a continuous perspective, we 
apply the regression approach to detect the emotion variation within music selections and find it is 
superior to the one proposed in [31]. 
In summary, the primary contributions of the work include: 
ÔÅ¨ To our best knowledge, this work represents one of the first attempts that develop an MER system 
from a continuous perspective and represent each song as a point in the emotion plane. This 
approach is free of the ambiguity issue of MER. 
ÔÅ¨ A novel AV computation method based on the regression theory is proposed. Principal component 
analysis [81] is employed to reduce the data correlation, and RReliefF [71] is utilized for feature 
selection (Sections 2.3 and 2.4). 
ÔÅ¨ An extensive performance study is conducted to demonstrate the accuracy and effectiveness of the 
regression approach for both music emotion recognition and music emotion variation detection 
(Section 2.5). 
ÔÅ¨ A group-wise MER scheme is proposed to solve the subjectivity issue of MER (Section 2.6). 
2.2 Related Work 
Despite a great deal of effort has been made for MER in recent years [9], [12], [33], [72]‚Äì[76], little 
attention has been paid to view the emotion plane from a continuous perspective. Some exceptions can 
be found in the music emotion variation detection (MEVD) field [31], [32], [76], [77], where the 
 
Fig. 7. Thayer‚Äòs arousal-valence emotion plane. 
TABLE 12 
COMPARISON OF WORKS ON MUSIC EMOTION 
Field Perspective Description 
MER [9], [12], [33], 
[72]‚Äì[76] 
categorical 
Classifying music selections into several 
classes based on emotion. 
MEVD [31], [32], 
[76], [77] 
continuous 
Detecting the emotion variation within a 
music selection. 
MER (this work) continuous 
Representing each music selection as a point 
in the emotion plane. 
 
22 
 
The System Identification Approach (System ID) 
A systematic approach for MEVD is proposed in [31], where the system identification technique is 
utilized to model the music emotion as a function of 18 musical features. The ground truth data are 
collected every second, so the music selections are also segmented every second before feature 
extraction is performed. Six western classical music selections of various moods form the dataset. 
Results demonstrate that system identification provides a means to the generalization of the emotional 
content for a genre of music (western classical music), and the reported average R
2
 statistics [35] is 
78.4% for arousal and 21.9% for valence.  
However, the system ID approach, as well as the time series analysis approach proposed earlier by 
Schubert [32], computes the AV values by exploiting the temporal relationship between music segments, 
which is absent for MER.  
A comparison of the reviewed methods in terms of the ability to compute the AV values is 
summarized in Table 13. To sum up, a robust AV computation algorithm should have a sound theoretical 
foundation and allows quantitative performance study. Among the reviewed methods, only the system 
ID approach embeds a theoretical structure, yet it utilizes the temporal information which is not 
available for MER. In addition, the computation of AV values should be made without applying any 
geometric operation between arousal and valence due to their inexact relationship. It is also favorable to 
have the computation algorithm directly predict real values (i.e., arousal or valence) from extracted 
features. 
2.3 The Regression Approach 
Regression theory is a well-studied theory aiming at predicting a real value from observed variables 
(or features). It has a sound theoretical foundation, allows easy performance analysis and optimization, 
and generally provides reliable prediction performance [35]. Besides, no temporal information or 
geometric operation is needed. Therefore, formulating MER as a regression problem seems to be a 
promising approach. Below we first describe how the formulation is made, then present the system 
description in detail. The performance study of the regression approach is reported in Section 2.5. 
Given N inputs (xi, yi), 1‚â§ i ‚â§N, where xi is a feature vector for the ith input sample, and yiÔÉé‚Ñù (‚Ñù 
denotes a set of real values) is the real value to be predicted for the ith sample, the regression system 
trains a regression algorithm (regressor) R( ÔÉó ) such that the mean squared error Œµ is minimized [35]: 
 
2
1
1
( ( ))
N
i ii
y R x
N
ÔÅ•
ÔÄΩ
ÔÄΩ ÔÄ≠ÔÉ•
,                            (21) 
 
where R(xi) is the prediction result for the ith sample. 
Since the AV values are viewed upon as real values from the continuous perspective, the regression 
theory can be well applied to directly predict arousal and valence. To formulate MER as a regression 
problem, the following considerations are taken into account:  
1) Domain of ‚Ñù: The Thayer‚Äòs emotion plane is viewed as a coordinate space spanned by arousal 
and valence, where each value is confined within [‚Äì1, 1].  
2) Ground truth: The ground truth is set via a subjective test by averaging the subjects‚Äò opinions 
about the AV values of each music sample (see Section 2.4.3).  
3) Feature extraction: The extracted features need to be relevant to emotion perception for the 
regressor to be accurate (see Sections 2.4.2 and 2.5.3).  
4) Regression algorithm: Although regression theory has been well studied and many good 
regressors are readily available [78], the performance of a regressor is case dependent. A number of 
regressors should be adopted and compared to find the best one (see Section 2.4.4). 
24 
 
 
Feature Extraction 
After preprocessing, we use the spectral contrast algorithm [9], DWCH algorithm [73], and two 
computer programs PsySound [89] and Marsyas [60] to extract musical features and construct a 
114-dimension feature space, which is referred to as ALL hereafter. The extracted features, which are 
described in detail below, have been used for MER in pervious works. See Table 14 for denotations and 
brief descriptions. 
As the name indicates, PsySound aims to model parameters of auditory sensation based on some 
psychoacoustic models [89], [90]. Four types of measures are output by PsySound: loudness, level, 
dissonance, and pitch. Loudness measures include loudness, sharpness (sound brightness), and timbral 
width (sound flatness). Level measures include sound pressure level, background noise level etc. 
Dissonance measures are related to the perception of short irregularities in a sound; any note in music 
that does not fall within the prevailing harmony is considered dissonant. Pitch measures are related to 
the perceived fundamental frequency of a sound. Because of this psychoacoustical foundation, the 
features extracted by PsySound have been found much related to emotion perception, especially 15 of 
them [76]. Therefore, we utilize these 15 features to form a second feature space called Psy15. See Table 
15 for the description of Psy15. 
Marsyas is a free software framework for rapid development and evaluation of computer audition 
applications [60]. It generates 19 timbral texture features (spectral centroid, spectral rolloff, spectral flux, 
time domain zero-crossing and MFCC), 6 rhythmic content features (by beat and tempo detection) and 5 
pitch content features (by multi-pitch detection). Spectral centroid, spectral rolloff, and spectral flux 
describe spectral shape properties, zero-crossing measures the noisiness of the signal, and MFCC 
TABLE 14 
THE ADOPTED FEATURE EXTRACTION ALGORITHMS 
Method 
Number of 
feature 
Description 
PsySound [89] (P) 44 
Extracts features including loudness, level, pitch multiplicity, 
and dissonance based on psychoacoustic models. 
Marsyas [60] (M) 30 
Extracts timbral texture, rhythmic content and pitch content 
features. It has been shown useful in music genre classification. 
Spectral contrast [9] 
(SC) 
12 
Represents the relative characteristics of each spectral subband, 
and reflects the distribution of harmonic components. 
DWCH [73] (D) 28 
Daubechies wavelets coefficient histogram, which has better 
ability in representing both local and global information. 
Total (ALL) 114  
 
TABLE 15 
THE 15 PSYSOUND FEATURES (PSY15) RECOMMENDED IN [76] 
 Feature Description 
1 Spectral Centroid The centroid of spectral density function. 
2 Loudness Human perception of sound intensity. 
3, 41 Sharpness A pitch-like (low-high) aspect of timbre. 
5 Timbral Width The flatness of a loudness function. 
6 Volume Human perception of the size of sound. 
7, 81 Spectral Dissonance Roughness of all spectrum components. 
9,101 Tonal Dissonance Roughness of just the tonal components. 
11 Pure Tonal The audibility of the spectral pitches. 
12 Complex Tonal The audibility of the virtual pitches. 
13 Multiplicity The number of pitches heard. 
14 Tonality Major-minor tonality, e.g., A major. 
15 Chord Musical pitches sounded simultaneously. 
1 Two algorithms are used to extract the feature. 
26 
 
estimates the linear relationship by a least squares estimator. We treat MLR as the baseline approach for 
its simplicity. 
Comparatively, SVR nonlinearly maps input feature vectors to a higher dimensional feature space 
by the kernel trick, and yields prediction functions that are expanded on a subset of support vectors [82]. 
As its name indicates, SVR is an extension of the famous support vector classification, which has been 
found in many cases superior to existing machine learning methods. A number of previous works have 
adopted support vector classification for MER and reported excellent classification performance [12], 
[33], [73]. 
BoostR is another nonlinear regression algorithm in which a number of regression trees are trained 
iteratively and weighted according to the prediction accuracy. After the iterative process, the prediction 
result of each regression tree is combined (weighted mean) to form the final hypothesis. The basic 
underlining concept of the boosting process is based on the observation that finding a number of weak 
predicting rules is much easier than finding a single, highly accurate one [83]. Boosting algorithms, 
which are the state-of-the-art methods for face detection [84], have been successfully applied in many 
machine learning problems. 
Emotion Visualization 
Associated with the AV values, each music sample is visualized as a point in the emotion plane, and 
the similarity between music samples can be estimated by computing the Euclidean distance in the 
emotion plane. A user interface that supports music retrieval/recommendation by specifying a point in 
the emotion plane can be realized without further labeling the unseen music samples (different from that 
of Musicovery [91]). Such a user interface can be of great use in managing large scale music databases.  
2.5 Performance Study 
We run a series of experiments to evaluate the performance of the regression approach. Different 
 
 (a)                           (b) 
Fig. 9. (a) Histogram of standard deviations for arousal and valence of 195 songs in the first course of subjective test. (b) Histogram of absolute difference 
for arousal and valence of 220 data pairs in the test-retest stability study. 
 
Fig. 10. Comparison of the root mean squared error using different feature dimensions for AV and PC. The features are the top ones selected by RReliefF. 
28 
 
 
 
2.5.2 Data Space 
As mentioned in Section 2.1, a major issue of the continuous perspective is the dependency 
between the two dimensions arousal (a) and valence (v) in the arousal-valence (denoted as AV) data 
space. The Pearson's linear correlation coefficient between a and v in our dataset can reach 0.3368; 
therefore, it is interesting to see whether the performance of regression can be improved by reducing the 
data correlation first. 
A common method for reducing the correlation between variables is the principal component 
analysis (PCA) [81], which entails the computation of a loading matrix L to transform original data Y to 
principal components U such that 
 
( ( ))U L Y mean YÔÄΩ ÔÄ≠ ,                               (23) 
          1 ( )Y L U mean YÔÄ≠ÔÄΩ ÔÄ´ ,                               (24) 
 
where U is the representation of Y in the principal component space. By PCA, we are able to transform 
the original data space AV to the principal component space (denoted as PC) where the correlation 
between the two resulting dimensions is reduced to nearly zero. Therefore, besides training RV and RA, 
we train another two regressors Rp and Rq in the PC space (where p and q denote the two dimensions of 
PC), and then transform the data space of the prediction results back to AV by (24). Though Rp and Rq 
are also trained independently as RV and RA, the underlying dimensions p and q are nearly uncorrelated. 
2.5.3 Feature Space 
From the machine learning point of view, features are not necessarily of equal importance or quality, 
and irrelevant or redundant features may lead to inaccurate conclusion. Although domain knowledge 
helps identify good features, there is only limited understanding of how music evokes emotion. One 
solution for addressing this problem is to extract a number of musical features and then use a feature 
selection algorithm (FSA) to identify good features [85].  
The purpose of FSA is to find the optimal feature subset that gives the maximal prediction accuracy 
and keeps the feature dimension minimal. For its simplicity and effectiveness, RReliefF [93] is adopted 
in our work. It evaluates the features one by one and assigns a real number to each feature to indicate its 
importance. Because RReliefF takes feature interrelation- ship into account, it is better than other 
TABLE 16 
THE FEATURE SPACES USED IN THE PERFORMANCE STUDY 
Name Dimension Description 
ALL 114 / 114 Use the features in Table III. 
Psy15 15 / 15 Use the features in Table IV. 
RRFm,n 
AV: 8 / 3 
PC: 18 / 15 
Features selected from ALL by RReliefF, the dimension is 
selected to minimize Œµ. 
  
TABLE 17 
TOP FIVE SELECTED FEATURES BY RRELIEFF FOR AV AND PC DATA SPACES 
Arousal Valence 
Name Extractor Weight Name Extractor Weight 
stdFlux M 0.0123  spectral diss(S) P* 0.0223  
tonality P* 0.0115  tonality P* 0.0210  
multiplicity P* 0.0095  sum of beat hist M 0.0167  
meanFlux M 0.0093  chord P* 0.0129  
meanRolloff M 0.0092  sum of pitch hist M 0.0108  
* Psy15 features. 
30 
 
the regressor. Result shown in Table 19 indicates: 
1) The best combination of data and feature space by summing the R
2
 of arousal and valence 
directly is PC+RRF18,15, and the resulting R
2
 reaches 58.3% for arousal and 28.1% for valence.  
2) Transforming the data to PC does not make significant difference to the prediction accuracy.  
This is interesting, since reducing the correlation between arousal and valence seems to have little 
influence. One possible reason to this phenomenon is that subjects can independently annotate arousal 
and valence to a certain extent, but it remains to validate this argument more rigorously. 
3) Selecting features by RRF greatly improves the accuracy (especially for valence), which shows 
the importance of feature selection. Generally the performance of adopted feature spaces is: RRF > 
Psy15 > ALL. 
4) Using Psy15 as the feature space rather than ALL does not exhibit evident accuracy 
improvement for PC, which may be reasonable because the psychoacoustic meaning of the Psy15 
features might be lost in the principal space. 
5) Recall when we use AV+RRF, only 3 features are used to predict valence (see Table 16); 
however, the reported R
2
 25.4% is high enough compared to the best one 28.1%. This finding implies 
most of the 114 extracted features may not be so relevant to valence. 
We also show the distributions of ground truth and prediction result for PC+RRF18,15+SVR in Fig. 
 
Fig. 11. Distributions of ground truth (blue point) and prediction result (red circle) for PC+RRF18,15+SVR. It can be observed 
that the distributions are similar. For a closer look, see Fig. 6. 
 
Fig. 12. Distributions of ground truth values (blue stars) with lines connecting to the corresponding predicted values (red 
circles) by PC+RRF18,15+SVR For space limitation, only 100 songs are shown. 
32 
 
and since the regressors are trained based upon the average opinions of the subjects. It remains needed to 
address the subjectivity issue more effectively.  
We develop a group-wise MER scheme (GWMER) to resolve this issue. The main idea is to divide 
the users into a variety of user groups and train regressors for each group. The groups can be defined 
according to user information [93], [94] such as generation, sex, occupation, personality, etc., to reduce 
the individual differences for each group. The training process is similar to what we describe in Section 
2.4 except that the subjects are specified according to the target user group. The number of necessary 
subjects grows proportionally with the number of defined groups. After a collection of regressors have 
been trained, we can choose the most suitable RA and RV to respond to each user according to his/her 
personal information. In this way, the effect of a great number of individual factors is eliminated. See 
Fig. 13 for the system diagram. 
Personalization offers an alternative way to resolve the subjectivity issue. However, building a 
personalized MER system is difficult because emotion perception may be too subtle to be understood 
and described quantitatively. Another issue for the personalization is the extra user burden. 
GWMER represents a compromise between the general MER system presented in this work and a 
personalized one. It can alleviate the subjectivity issue without resorting to too much user burden. 
Further investigation is worthwhile. 
2.7 Conclusion 
In this work, a music selection is quantified as a point in the arousal-valence emotion plane. This 
continuous view of music emotion makes the proposed MER system free of the inherent ambiguity issue 
from which conventional categorical approaches suffer. In addition, because there is more freedom in 
describing a song, the subjectivity issue is alleviated to some extent. 
The accuracy of the AV computation determines the viability of the MER system. We formulate 
MER as a regression problem and adopt the support vector regression for direct estimation of the AV 
values. Comparing to existing AV computation algorithms, the regression approach has a sound 
theoretical foundation, exhibits promising prediction accuracy, and needs no temporal information or 
geometric operations.  
Through an extensive performance study on the selection of data space, feature space, and regressor, 
we have demonstrated the effectiveness of the regression approach. The R
2
 statistics reaches 58.3% for 
arousal and 28.1% for valence. We have also shown that the regression approach can be applied to 
MEVD and that it outperforms the approach proposed in [31]. 
Since arousal and valence may be dependent, we apply principal component analysis on the ground 
truth data to reduce the data correlation. The resulting little accuracy improvement implies the subjects 
can label arousal and valence quite independently. We also employ RReliefF to select features and find 
the accuracy greatly improved. 
Future work will focus on exploiting features about lyrics and singing of a song, evaluating the 
regression approach on a large-scale database, and realizing the GWMER scheme to further address the 
subjectivity issue. 
 
  
34 
 
emotion and the design of an emotion-based music retrieval system. 
Many existing systems, however, do not take the subjectivity issue into account and assume that 
underlying the individual responses is a universal model of emotional response that characterizes how a 
typical listener will respond [99]. In the short term, this approach paid off by allowing prototypes to be 
built rapidly [9]‚Äì[13]. However, as pointed out in [104], ignoring the important issues inherent to 
emotion perception has significant limitations because a functional MER system cannot be completely 
divorced from underlying emotion theory. The study presented in [99] also indicates a performance limit 
of the universal model; they have considered a variety of algorithms at each stage of the training process 
of MER, from preprocessing to feature selection and model selection, but failed to significantly improve 
the prediction accuracy. 
To better address the subjectivity issue, in this work we propose a computational model that 
predicts the motion distribution of a clip directly from music features. Specifically, given a music signal, 
our goal is to predict its emotion mass at discrete samples of the emotion plane, with the values summed 
to one. Here the term emotion mass refers to the probability of perceiving a specific emotion 
(represented as a discrete sample of the emotion plane) while listening to a clip. We also discuss the 
implication and application of this approach for MER and personalized retrieval. 
In summary, the main contributions of this work are as follows. 
ÔÅ¨ We propose a novel approach that considers the perceived emotions of a clip as a probabilistic 
distribution rather than a single point (Section 3.3). 
ÔÅ¨ We present the methodology for automatically predicting the emotion distribution of a clip directly 
from music features. Novel ways of ground truth collection, model training, and model fusion are 
developed (Section 3.4). 
ÔÅ¨ We conduct an extensive performance study to evaluate the proposed system both quantitatively 
and qualitatively (Section 3.5). 
3.2 Related Work 
Many efforts have been made by MIR researchers to automate MER and the type of music under 
study has gradually shifted over the past few years from symbolic music [11], [12] to raw audio signals 
and from Western classical music [9], [12], [13] to popular music. The training of an automatic 
recognition model is typically achieved by a standard machine learning technique: A number of features 
are extracted to represent the music signal, the ground truth emotion labels, or emotion values are 
collected from human annotators and a learning algorithm is applied to learn the relationship between 
music features and emotion labels/values. After training, the model is applied to predict the emotion of 
an input clip. 
The major advantage of the categorical approach to MER is that it is easy to be incorporated into a 
text-based or metadata-based retrieval system. Similar to other music metadata such as genres and 
instrumentations, emotion labels provide an atomic description of music that allows users to retrieve 
music through a few keywords. Following this direction, many systems that assign an emotion label to a 
clip have been developed [9], [10]. Some key issues of MER have also been identified, such as the 
granularity and ambiguity issues, the subjectivity of emotion perception, the semantic gap between the 
object feature level and the human cognitive level of emotion perception and the lack of a standard 
 
Fig. 14. Emotion annotations in the 2-D valence-arousal emotion plane [21] for four clips: (a) Smells Like Teen Spirit by 
Nirvana; (b) A Whole New World by Peabo Bryson and Regina Belle; (c) The Rose by Janis Joplin; (d) Tell Laura I Love Her 
by Ritchie Valens. Each circle corresponds to a subject‚Äòs annotation of the clip [26]. It can be observed that emotion 
perception is indeed subjective and different subjects‚Äò annotations of a clip constitute an ‚Äïemotion distribution‚Äñ in the 
emotion plane. 
36 
 
practice. While efforts have been made to take the subjectivity issue into account [26], [27], a 
computational framework as presented in this work has not been developed. We model the perceived 
emotions of a clip as a probabilistic distribution in the emotion plane and develop a methodology to 
compute the probability of perceiving an emotion while listening to a song, somewhat similar to the idea 
of soft labeling of the fuzzy approach [15]. 
To summarize, the categorical approach and the dimensional approach offer different advantages 
that are complementary to each other. We can imagine a mobile device that employs both approaches to 
facilitate music retrieval. Interested readers may refer to [98], [106] for detailed reviews of MER.  
 
3.3 Emotion Distribution Prediction 
Given the feature representation ùíô  of a clip ùëë , our goal is to predict the probability of 
perceiving a speciÔ¨Åc emotion while listening to the clip. As an emotion corresponds to a point from the 
dimensional perspective, this would require an inÔ¨Ånite number of predictions. To make the problem 
tractable, we quantize each dimension by G equally spaced discrete samples (emotion points), leading to 
a ùê∫ √ó ùê∫ grid representation of the 2-D emotion plane, or ùêû   ùê© ùëó  ,ùëó  , 
ùê∫,ùê∫
, where ùêû denotes the set of 
discrete samples. SpeciÔ¨Åcally, the VA values of a point ùê© ùëó are  ùëñ  1/2 ‚àÜ  1 and  ùëñ  1/2 ‚àÜ  1, 
respectively, where ‚àÜ 2/ùê∫ is the space between two neighboring points. This way, the task becomes 
the prediction of ùê≤  [  ,  ,  ,   , ùëó,    ,ùê∫ùê∫], subject to 
 
                        (25) 
 
where   , ùëó, or the emotion mass, denotes the probability of perceiving the emotion ùê© ùëó  when listening 
to the clip ùëë . Because   , ùëó is a real value in the range [0, 1], we can apply regression algorithms [146] 
and train ùê∫  regressors for each of the emotion points, under the assumption that the emotion mass at 
different points are independent. We denote ùëì ùëó ‚ãÖ  as the regressor for predicting the emotion mass 
corresponding to ùê© ùëó. The estimate is therefore expressed as   ,ùëñùëóÃÇ  ùëì ùëó ùíô  . 
The new problem formulation of MER leads to the following two issues. First, how to obtain the 
ground truth data needed to train the regressors? We need to have ground truth emotion mass for each of 
the discrete samples. Second, how to choose a regression algorithm to train the regressors and how to 
choose the feature representation of music? 
We build a system that addresses all the above issues. As Fig. 16 shows, we Ô¨Årst collect the ground 
 
Fig. 16. Schematic diagram of emotion distribution prediction. The ground truth is obtained by applying density estimation 
to the annotations collected through a subjective test. Based on the ground truth and extracted features, we conduct regressor 
training and fusion to learn the mapping between feature space and emotion distribution. Given a test clip, we extract 
features and apply the trained regressors to compute its emotion distribution. 
38 
 
                           (26) 
 
where  is a kernel function, typically chosen as a bivariate Gaussian with zero mean and diagonal 
covariance 
 
                          (27) 
 
where    i g [  ,   ]  and   ,    are scaling parameters that can be optimally chosen [110]. It can 
be found that the density is estimated by summing the affinity of subjects‚Äò annotations around ùê© ùëó, 
weighted inversely proportionally to the scaled distance between the annotation ùê™ ùë¢ and ùê© ùëó. The value 
of   , ùëó is large when the annotations are concentrated around ùê© ùëó. The resulting 2-D histogram is then 
normalized to give a probability distribution. Fig. 17(b) shows a sample result. Note that the kernel density 
estimation can be regarded as an extension of the classical Parzen window method [81], which simply 
counts the number of annotations that fall within an L2 ellipse centered at ùê© ùëó, with horizontal axis of 
length    and vertical axis of length   . 
3.4.2 Regressor Training 
Under the independence assumption, we train  regressors for each of the emotion points. Given 
N inputs  ùíô ,    , ùëó , s ‚àà  1, ,ùëÅ , where ùíô  is the feature vector and is the real-valued emotion 
mass to be predicted, a regressor ùëì ùëó ‚ãÖ is created by minimizing the mismatch  (i.e., mean squared error) 
between ground truth and prediction [35] 
 
                         (28) 
 
The regression theory has been well studied and many good regression algorithms are readily 
available. We employ support vector regression (SVR) in this work for its superior performance for 
MER [2], [99]. 
Since the 1990s, support vector machines (SVMs) have been widely used in different classification 
and regression tasks [39], [115]. SVM nonlinearly maps an input feature vector ùíô  to a higher 
dimensional feature space   ùíô  by the so-called ‚Äïkernel trick‚Äñ and learns a nonlinear function by a 
linear learning machine in the kernel-induced feature space, where data are more separable [39]. For 
regression, we look for a function ùëì ùíô    
   ùíô     that has at most Œµ deviation from the ground 
truth for all the training data and meanwhile is as flat as possible (i.e.,     is small) [115]. Moreover, 
under the soft margin principle [43], we introduce slack variables ùúâ  and ùúâ 
‚àó to allow the error to be 
greater than Œµ. Consequently, we have the following optimization problem: 
 
                     (29) 
 
40 
 
load sharing in distributed computer systems [118]. 
An alternative is to weight regressors according to the R
2
 statistics, another standard metric for 
evaluating regressors [35]. The R
2
 statistics, or the coefficient of determination, measures the proportion 
of the underlying data variation that is explained by the fitted regression model [92] 
 
                               (32) 
 
R
2
=1 means the model perfectly fits the data, while R
2
=0 indicates no linear relationship between the 
ground truth and the estimate. Accordingly, we set 
 
                               (33) 
 
where ùíö ùëó  [ùíö , ùëó,  , ùíö , ùëó]
 denotes the ground truth and ùëì ùëó
ùë° ùëøùë°  [ùëì ùëó
ùë°   
ùë° ,  , ùëì ùëó
ùë°   
ùë°  ]  the 
estimates, respectively. In practice, we measure Œª or ùëÖ  from the training data and apply the weights 
to the test data. We call the two methods weighted-by-Œª and weighted-by-ùëÖ , respectively. 
An algorithmic description of emotion distribution prediction is shown in Table 21. 
3.5 Experiment Setup 
3.5.1 Data Collection 
The dataset studied in this work consists of 60 famous popular songs from English albums. These 
songs are listed in Table 22 along with their IDs. For fair comparison, every song is converted to a 
uniform format (22,050 Hz, 16 bits, and mono channel PCM WAV) and normalized to the same volume 
level. Besides, to reduce the emotion variation and lessen the burden on the subjects, each song is 
TABLE 21 
OF EMOTION 
 
42 
 
features to represent the five perceptual dimensions of music listening (summarized in Table 23) and 
train regressors using features of each perceptual dimension separately. 
1) Melody/harmony: We use the MIR toolbox [58] to generate four pitch features (mean and standard 
deviation of salient pitch and chromagram center) and six tonality features (mean and standard 
deviation of key clarity, mode and harmonic change). MIR toolbox estimates the pitch, or the 
perceived fundamental frequency, of each short time frame (50 ms, 1/2 overlapping) based on the 
multipitch detection algorithm de- scribed in [62]. The algorithm decomposes an audio waveform 
into two frequency bands (below and above 1 kHz), computes the autocorrelation function of the 
envelop in each subband and finally produces pitch estimates by picking the peaks from the sum of 
the two autocorrelation functions. The pitch estimate corresponding to the highest peak is returned 
as the salient pitch. MIR toolbox also computes the wrapped chromagram, or the pitch class profile, 
for each frame (100 ms, 1/8 overlapping) and uses the centroid (center of gravity) of the 
chromagram as another pitch feature. Each bin of the chromagram corresponds to one of the 12 
pitch classes in the Western 12-tone equal temperament scale. By comparing a chromagram with 
the 24 major and minor key profiles [63], we can estimate the strength of the frame in association 
with each key (e.g., C major). The strength associated with the best key, i.e., the one with the 
highest strength, is returned as the key clarity. The difference between the best major key and the 
best minor key in key strength is returned as the estimate of the musical mode; the higher its value 
the more major the frame would be. MIR toolbox also utilizes the algorithm described in [64] to 
detect the harmonic changes (e.g., chord change) of a clip from its chromagram. High harmonic 
change indicates large difference in harmonic content between consecutive frames. These 
short-term features are aggregated by taking the mean and standard deviation [55]. 
2) Spectral: We use Marsyas [121] to extract 32 spectral flat- ness measures (SFM) and 32 spectral 
crest factors (SCF), which are both related to the noisiness of audio signal [65]. Noisiness is related 
to valence perception; angry pieces are often noise-like, while joyful and peaceful pieces are not 
[123]. SFM is the ratio between the geometric mean of the power spectrum and its arithmetic mean, 
whereas SCF is the ratio between the peak amplitude and the root-mean-square amplitude. They are 
extracted by computing the values in 17 subbands for each frame and then taking the mean and 
standard deviation for each second. The sequence of feature vectors is then collapsed into a single 
vector representing the entire signal by taking again the mean and standard deviation. We discard 
the values in the last nine subbands for they contain less information. In addition, we use the MA 
toolbox [61] to extract Mel-frequency cepstral coefficients (MFCCs), the coefficients of the 
discrete cosine transform of each short-term log power spectrum expressed on a nonlinear 
perceptual-related Mel-frequency scale, to represent the shape of the spectrum. We then take the 
mean and standard deviation of the first 13 MFCC coefficients of each frame for temporal 
integration. 
To avoid using too many features for a small-scale dataset and to make the size of each feature 
set roughly even, we employ principal component analysis (PCA) [81] to reduce the feature 
dimension of the spectral features from 90 to 10. Specifically, we compute a few orthogonal linear 
combinations (principal components) of the original features with the largest variance from the 
training data and then apply the transformation to both the training and the test data. 
TABLE 23 
EXTRACTED FEATURE SETS
 
44 
 
where vec ‚ãÖ  denotes the vectorization of a matrix. Being an extension of the well-known Kullback‚Äì
Leibler divergence, the Jensen‚ÄìShannon divergence is symmetric, nonnegative, and small when the two 
distributions are similar. 
The performance is evaluated by the leave-one-out (LOO) cross validation technique, which is 
known to provide an almost unbiased estimate of the generalization error even with a small dataset [81]. 
As its name suggests, LOO uses one clip as the test data and the remainder as training data in each run 
of the evaluation. The procedure is repeated N runs until each clip is tested once. In each run, feature 
normalization is done by standardizing the training data to zero mean and unit variance and applying the 
normalization parameters to the test data. We report the mean R
2
 and DJS of the N runs. 
3.6.1 Comparison of Different Distribution Modeling Methods 
We first compare different distribution modeling methods, including the proposed KDE-based 
approach with different sample sizes ( =6, 8, 16, 64, and 100) and the simple single-Gaussian approach. 
The former requires  regressors while the latter requires five. We fix the feature representation to 
melody/harmony and employ SVR to train the regressors. Our implementation of SVR is based on the 
free library LIBSVM [68], along with the RBF kernel and a grid parameter search to find the best 
parameters for each distribution modeling method independently. We also implement a baseline method 
that randomly assigns emotion mass values. 
Table 24 shows that the proposed approach outperforms the random baseline regardless of the value 
of G; ùëÖ  is improved from 0 to 0.5, whereas DJS is reduced from 0.24 to 0.10. The proposed approach 
attains the best ùëÖ  (0.5057) and DJS (0.0990) when G is set to 8. This is reasonable as smaller G cannot 
offer sufficient resolution for distribution modeling and larger G results in unnecessary complexity. In 
addition, setting G to 64 or 100 deteriorates the prediction accuracy as the number of discrete samples 
greatly exceeds the number of user annotations for each clip. We also notice that the approach KDE 
(G=8) is fairly efficient and takes less than five seconds to finish the cross validation process. On the 
other hand, although single-Gaussian is more efficient, its prediction accuracy is lower than the 
proposed approach. The difference in ùëÖ  is significant (p-value<5%) under the two-tailed t-test [92]. 
This is perhaps because the proposed approach is specifically optimized to predict the emotion mass, 
whereas single-Gaussian is not. 
To qualitatively evaluate the result, we apply Gaussian process regression (GPR) [114] to 
interpolate the emotion mass of a 100√ó100 finer grid from the 8√ó8 one for visualization purpose. One 
regressor is trained for each clip, using the locations of the discrete samples defined over the coarser grid 
as input variables and the corresponding estimated emotion mass as target variables. The regressor is 
then applied to predict the emotion mass at the discrete samples defined over the finger grid for the same 
clip. In our implementation, which is based on the GPML (Gaussian process for machine learning) 
toolkit [114], the interpolation process of a clip takes less than one second on a regular dual-core 
Pentium PC. 
The result is shown in Fig. 18, in which the clips are sorted from left to right in ascending order of 
the mean ground truth valence value and from bottom to top in ascending order of mean arousal value. 
It can be found that the predictions of both approaches resemble the ground truth ones, but the ground 
truth distribution is denser. Comparing Figs. 18(b) and 18(c), we see that the distributions generated by 
single-Gaussian are denser in the upper plane yet sparser in the lower plane. This is largely due to the 
difficulty of computing the mean valence values and the covariance parameters. Using melody/harmony 
as the feature representation, the best R
2
 (after parameter tuning) for valence mean, arousal mean, 
valence variance, arousal variance, and the covariance between valence and arousal are 0.1673, 0.7329, 
0.1376, 0.0411, and 0.2230, respectively (the R
2
 reported here are with respect to each Gaussian 
parameter rather than the emotion distribution). It seems that inaccurate estimation of the mean valence 
values and the covariance parameters exerts a much more negative effect for emotion distribution 
prediction, comparing to that of inaccurate emotion mass estimation. 
In summary, our evaluation shows that KDE (G=8) outperforms other distribution modeling 
methods in terms of both R
2
 and DJS, with only a modest increase in computational cost. Qualitative 
evaluation also shows the superiority of KDE (G=8) in emotion distribution prediction. 
46 
 
well in the first quadrant. To sum up, Fig. 19 shows that the feature sets are related to different emotion 
perceptions of music and that a proper fusion method should aggregate the advantage of them and 
improve the accuracy. 
3.6.3 Evaluation of Regressor Fusion 
Finally, we compare different methods of regressor fusion, including weighted-by- Œª, 
weighted-by-R
2
, and three baseline methods: the melody/harmony baseline, the early (feature) fusion 
baseline that concatenate all the features into a single 46-D feature vector before model training and the 
late (decision) fusion baseline that simply averages the result of different regressors (namely,                       
in (30)). Result shown in Table 26 leads to the following observations: First, the performance of early 
fusion is even inferior to that of the melody/harmony baseline, showing that directly concatenating 
features of different perceptual dimensions is not effective. Second, simply averaging the result of the 
feature sets (i.e., late fusion) improves R
2
 but not DJS. Third, by assigning weights to each feature set in a 
location-dependent fashion, the proposed methods better aggregate the advantage of the feature sets and 
outperform the baseline methods in both R
2
 and DJS. The R
2
 and DJS of weighted-by-R
2
 are 0.5439 (7.6% 
relative gain over melody/harmony) and 0.0960 (3.1% improvement), respectively. By comparing Fig. 
19(a) and (f), we see that weighted-by- R
2
 improves the R
2
 for almost all locations. Salient improvement 
is found in the first and the second quadrants, where feature sets such as spectral and rhythmic 
outperform melody/harmony. Though the improvement is not significant under the t-test, this result 
shows that the proposed fusion methods are indeed effective in aggregating different dimensions of 
music listening. 
3.7 Discussion 
To our best knowledge, this work represents the first attempt that predicts the emotion distribution 
of a clip. Therefore, there is no reference with which we can compare our result. However, it should be 
fair to say that our result is promising, considering the result of existing work on predicting the mean VA 
values of music signals. For example, the R
2
 reported in [26] is 0.17 for valence and 0.80 for arousal, 
while that reported in [99] is 0.26 for valence and 0.70 for arousal. The finding that valence prediction is 
TABLE 25 
OF DIFFERENT FEATURE 
 
TABLE 26 
 
 
Fig. 19. (a)‚Äì(e) The R2 at each emotion point, using different feature sets as input to SVR for model training. It can be found 
that the feature sets lead to better performance in different regions (brighter regions). (f) Using the proposed regressor fusion 
algorithm weighted-by- R
2
, we combine the advantage of the feature sets and improve the R
2
 from 0.5057 (melody/harmony) 
to 0.5439 (fused) and the DJS from 0.0990 to 0.0960. (a) Melody/harmony. (b) Spectral. (c) Temporal. (d) Rhythmic. (e) 
Lyrics. (f) Fused. 
48 
 
can organize and represent clips in the emotion plane. An intuitive next step would be to address the 
retrieval part; that is, after a user specifies certain emotion e, return him or her a list of clips ranked in 
descending order of P(d|e,u). According to Bayes‚Äò rule, we have 
 
                            (36) 
 
where P(d|e,u)~P(e|d) is a term of personal perception of music emotion, can be regarded as the 
preference of the user with respect to music type and P(e|d) is the emotion preference. This establishes a 
probabilistic framework of personal- ized emotion-based music retrieval. Both P(d|u) and P(e|u) can be 
learnt from the user‚Äòs listening history. We can consider an emotion distribution P(e|d) as a collection of 
people‚Äòs perceived emotions of a clip and the perceived emotion of the user P(e|d,u)  as a sample of the 
distribution. However, developing a computational model for P(e|d,u) may be challenging and needs 
further research [26], [27]. 
In addition to personalized emotion-based music retrieval, modeling music emotion as a probability 
distribution has application in enhancing our understanding of music emotion. For example, we can 
compute the pairwise similarity between the emotion distributions of clips using 1-DJS and then cluster 
the clips using a clustering algorithm such as spectral clustering [127]. Fig. 20 shows the result when we 
group clips into nine clusters (each column corresponds to a cluster). Note the number of clusters is set 
to the one with the highest within-cluster sums of point-to-centroid distances [81] among 2 to 20. 
Clearly the clusters correspond to different patterns of emotion distribution. Clips in clusters A and B are 
generally of high arousal and negative valence, but clips in A (such as Smells Like Teen Spirit) can also 
be perceived as of positive valence, while clips in B (such as Sweet Dreams) can be perceived as of low 
arousal. Clusters H and I consist of clips whose emotion is of high arousal and positive valence, but clips 
in H (Are We the Waiting) are more likely to be perceived as of negative valence than clips in I (Oh 
Happy Day). Cluster C (Barriers) corresponds to clips of low arousal and negative valance, while 
cluster G (I Will Always Love You) corresponds to clips of low arousal and positive valance. We observe 
that the emotion distributions of clips of low arousal are generally sparser than those of high arousal. 
Finally, clips belong to the middle three clusters are of neutral valence, but largely differ in arousal. 
Clips of cluster F (Sweet Child O‚Äô Mine) have the highest arousal, while clips of cluster E (As Time Goes 
By) have the lowest arousal. Clips of cluster D (such as The Drugs Don‚Äôt Work) have fairly sparse 
emotion distributions, showing their emotion perceptions are very subjective. We would not have such a 
rich description of music emotion if we represent clips as points rather than distributions. 
Another example application is to investigate the relationship between the Euclidean distance in the 
emotion plane and the perceptual distance. Although being widely adopted in the literature, it is unclear 
whether the emotion plane is Euclidean perceptually. For example, is the distance between two valence 
values 0.6 and 0.8 the same as that between two other valence values 0.1 and 0.1 in a listener‚Äòs mind? As 
the emotion distributions of clips of neutral valence are often sparser, the latter may have a smaller 
perceptual distance. We use a mass, spring and damper force-based algorithm [128] to study this. 
Specifically, considering that clips with similar emotion distributions should be perceptually similar, we 
initialize the position of each clip by its mean VA values and then apply an electrostatic-like force 
between every pair of clips that is proportional to the in- verse of their JS divergence. This way, clips 
with similar emotion distributions will be drawn closer while still trying to stick to the initial positions. 
We implement the algorithm using the NEATO utility of the Graphviz package, which runs an iterative 
solver to minimize a global energy function of the spring system [128]. Results are shown in Fig. 21, 
with the grid lines deformed accordingly using a moving least squares method of rigid transformation 
[129]. By comparing the original and modified positions of the clips, we see that the range of valence 
values in the lower plane is compressed from 1 to around 0.7, showing that the perceptual distance is 
shorter than the Euclidean distance in the lower plane. We also observe that the clips in the first quad- 
rant are drawn close to the origin, whereas clips in the second quadrant undergo less changes (possibly 
because their distributions are relatively distinct from others). This result shows that the perceptual 
distance between clips is different from the Euclidean distance of the mean VA values. 
50 
 
4. Searching Music in the Emotion Plane 
Music plays an important role in human‚Äòs history, even more so in the digital age. Never before has 
such a large collection of music been created and accessed daily by people. Because almost all music is 
created to convey emotion, music organization and retrieval by emotion is a meaningful way for 
accessing music information. The proliferation of tiny mobile devices and the like also calls for 
content-based retrieval of music through a small display space.  
Music emotion recognition (MER) aims at recognizing the affective content of music signals. A 
typical approach is to categorize emotions into a number of classes (e.g., happy, angry, sad and relaxing) 
and apply machine learning techniques to train a classifier [9], [10], [13]. This approach, though widely 
adopted, faces the granularity issue in practice, because classifying emotions into only a handful of 
classes cannot meet the user demand for effective information access. Using a finer granularity for 
emotion description does not necessarily address the issue since language is inherently ambiguous, and 
the description for the same emotion varies from person to person. 
Instead, we propose to view emotions from a dimensional perspective and define emotions in a 2-D 
plane in terms of arousal (how exciting or calming) and valence (how positive or negative), the two 
emotion dimensions found to be most fundamental by cognitive study [22]. In this way, MER becomes 
the prediction of the arousal and valence (AV) values of a song corresponding to a point in the emotion 
plane [2], [24], [25], [27]. The granularity and ambiguity issues associated with emotion classes no 
longer exists since no categorical classes are needed. Moreover, because the 2-D emotion plane provides 
a simple means for user interface, novel emotion-based music organization, browsing, and retrieval can 
be easily created for mobile devices. 
4.1 Emotion-Based Retrieval 
The advantages of the emotion-based approach are that each music sample can be represented as a 
point in the emotion plane and that the similarity between music samples can be measured by Euclidean 
distance. As shown in Fig. 23, a user can retrieve music of a certain emotion by simply specifying a 
point in the emotion plane. The system then returns the music samples whose AV values are close to the 
point. A user can also generate an emotion-based playlist by drawing a trajectory in the emotion plane. 
This way, songs of various emotions corresponding to different points on the trajectory are added to the 
playlist and played back in order.  
One can also couple other musical metadata such as artist name, genre, or lyrics with emotion to 
narrow down the search range. For example, one can specify an artist, and the system would display all 
songs of the artist in the emotion plane. It is also possible to playback music that matches the user‚Äòs 
mood detected by using physiological, prosodic, or facial cues [132]. This retrieval paradigm is 
functionally powerful since people‚Äòs criterion is often related to the emotion state at the moment of 
music selection [6]. 
4.2 Emotion Recognition 
MER can be formulated as a regression problem [2] by viewing arousal and valence as real values 
in [-1, 1].  Then a regression model can be trained to predict the AV values. More specifically, given N 
inputs (xi, yi), 1‚â§ i ‚â§N, where xi is a feature vector of the ith input sample, and yi is the real value to be 
predicted, a regression model (regressor) R(¬∑) is created by minimizing the mismatch (i.e., mean squared 
difference) between the predicted and the ground truth values. Many good regression algorithms, such as 
support vector regression (SVR) or Gaussian process regression [35], are readily available. In [2], two 
SVR models are trained for arousal and valence respectively. A schematic diagram of this MER system 
is shown in Fig. 24. 
Usually, timbral, rhythmic, melodic, and harmonic features of music are extracted to represent the 
acoustic property of a song. Because of its ability to model auditory sensation based on psychoacoustic 
models, the computer program PsySound  [89] is often employed for feature extraction. The use of 
mid-level features such as chord progression or genre metadata has also been explored [17], [18]. Many 
52 
 
 
The subjectivity issue can be addressed by personalizing the MER system [27], [26]. We can ask a 
user to annotate a small number of songs and use the annotations to train a personalized model. A 
two-stage personalization scheme is proposed in [27]. Two models are trained: one for predicting the 
general perception of a song, and the other for predicting the difference between the general perception 
and a user‚Äòs individual perception. This is a simple personalization process because the music content 
and the individuality of the user are treated separately. To make it more sophisticated, one can take into 
account the demographic property, music preference, or listening context of the user in the process. 
4.3.2 Difficulty of Emotion Annotation 
The emotion annotation process of MER requires the subjects to rate the emotion in a continuum.  
But it has been found that such rating imposes a heavy cognitive load to the subjects [24]. In addition, it 
is difficult to ensure a consistent rating scale between and within the subjects [34]. As a result, the 
quality of the ground truth varies, which in turn degrades the accuracy of MER. 
To address this issue, ranking-based emotion annotation is proposed [24]. A subject is asked to 
compare the affective content of two songs and determine, for example, which song has a higher arousal 
value, instead of the exact emotion values. The rankings of music emotion are then converted to 
numerical values by a greedy algorithm [40]. Empirical evaluation shows that this scheme relieves the 
burden of emotion annotation on the subjects and enhances the quality of the ground truth. It is also 
possible to use an online game to harness the so-called human computation and make the annotation 
process more engaging [36]. 
4.3.3 Semantic Gap between Audio Signal and Human Perception 
The viability of an MER system largely lies in the accuracy of emotion recognition. However, due 
to the semantic gap between the object feature level and the human cognitive level of emotion 
perception, it is difficult to accurately compute the emotion values, especially the valence values. What 
intrinsic element of music, if any, causes a listener to create a specific emotional response is still far 
from well-understood. While mid-level audio features such as chord, rhythmic patterns, and 
instrumentation carry more semantic information, robust techniques for extracting such features need to 
be developed. 
Available data for MER are not limited to the raw audio signal. Complementary to music signal, 
lyrics are semantically rich and have profound impact on human perception of music [133]. It is often 
easy for us to tell from the lyrics whether a song expresses sadness or happiness. Incorporating lyrics to 
MER is feasible because most popular songs sold in the market come with lyrics [134]. One can analyze 
lyrics using natural language processing to generate textual feature descriptions of music. It has been 
shown that using lyrics indeed improves valence recognition [16], [19]. 
4.4 Conclusion 
The past decade has witnessed a growing interest in analyzing the affective content of music. In this 
article, we have described a new music retrieval paradigm that allows users to search music in the 
emotion plane. It opens up a new playground for advanced research on music emotion recognition and 
understanding.   
 
Fig. 25. Emotion annotations in the emotion plane for four songs: (a) Smells like teen spirit by Nirvana, (b) A whole new 
world by Peabo Bryson and Regina Belle, (c) The rose by Janis Joplin, and (d) Tell Laura I love her by Ritchie Valens. Each 
circle corresponds to the annotation of a song by a subject [27]. 
54 
 
217‚Äì238, 2004. 
[21] J. A. Russell, ‚ÄïA circumplex model of affect,‚Äñ J. Personal. Social Sci., vol. 39, no. 6, pp. 1161‚Äì
1178, 1980. 
[22] R. E. Thayer, The Biopsychology of Mood and Arousal. New York: Oxford Univ. Press, 1989. 
[23] Y.-H. Yang, Y.-C. Lin, Y.-F. Su, and H. H. Chen, ‚ÄïA regression approach to music emotion 
recognition,‚Äñ IEEE Trans. Audio, Speech, Lang. Process., vol. 16, no. 2, pp. 448‚Äì457, Feb. 
2008. 
[24] Y.-H. Yang and H. H. Chen, ‚ÄïMusic emotion ranking,‚Äñ in Proc. IEEE Int. Conf. Acoust., Speech, 
Signal Process., 2009, pp. 1657‚Äì1660. 
[25] Y.-H. Yang, Y.-C. Lin, H.-T. Cheng, and H. H. Chen, ‚ÄïMr. Emo: Music retrieval in the emotion 
plane,‚Äñ in Proc. ACM Int. Conf. Multimedia, 2008, pp. 1003‚Äì1004. 
[26] Y.-H. Yang, Y.-F. Su, Y.-C. Lin, and H. H. Chen, ‚ÄïMusic emotion recognition: The role of 
individuality,‚Äñ in Proc. ACM Int. Workshop Human-Centered Multimedia, 2007, pp. 13‚Äì21. 
[27] Y.-H. Yang, Y.-C. Lin, and H. H. Chen, ‚ÄïPersonalized music emotion recognition,‚Äñ in Proc. 
ACM Int. Conf. Inf. Retrieval, 2009, pp. 748‚Äì749. 
[28] S. Jun, S. Rho, B.-J. Han, and E. Hwang, ‚ÄïA fuzzy inference-based music emotion recognition 
system,‚Äñ in Proc. Vis. Inf. Eng., 2008, pp. 673‚Äì677. 
[29] K. F. MacDorman, S. Ough, and C.-C. Ho, ‚ÄïAutomatic emotion prediction of song excerpts: 
Index construction, algorithm design, and empirical comparison,‚Äñ J. New Music Res., vol. 36, 
no. 4, pp. 281‚Äì299, 2007.  
[30] T. Eerola, O. Lartillot, and P. Toiviainen, ‚ÄïPrediction of multidimensional emotional ratings in 
music from audio using multivariate regression models,‚Äñ in Proc. Int. Conf. Music Inf. Retrieval, 
2009, pp. 621‚Äì626. 
[31] M. D. Korhonen, D. A. Clausi, and M. E. Jernigan, ‚ÄïModeling emotional content of music using 
system identification,‚Äñ IEEE Trans. Syst., Man, Cybern., vol. 36, no. 3, pp. 588‚Äì599, Jun. 2006. 
[32] E. Schubert, ‚ÄïMeasurement and time series analysis of emotion in music,‚Äñ Ph.D. dissertation, 
School of Music Education, Univ. New South Wales, Sydney, Australia, 2006. 
[33] D. Yang and W.-S. Lee, ‚ÄïDisambiguating music emotion using soft- ware agents,‚Äñ in Proc. Int. 
Conf. Music Inf. Retrieval, 2004, pp. 52‚Äì58. 
[34] S. Ovadia, ‚ÄïRatings and rankings: Reconsidering the structure of values and their measurement,‚Äñ 
Int. J. Social Res. Methodol., vol. 7, no. 5, pp. 403‚Äì414, 2004. 
[35] A. Sen and M. Srivastava, Regression Analysis: Theory, Methods, and Applications. New York: 
Springer, 1990. 
[36] Y. E. Kim, E. Schmidt, and L. Emelle, ‚ÄïMoodswings: A collaborative game for music mood 
label collection,‚Äñ in Proc. Int. Conf. Music Inf. Retrieval, 2008, pp. 231‚Äì236. 
[37] M. Mandel and D. Ellis, ‚ÄïA web-based game for collecting music meta- data,‚Äñ J. New Music 
Res., vol. 37, no. 2, pp. 151‚Äì165, 2008. 
[38] E. Law, K. West, M. Mandel, M. Bay, and J. S. Downie, ‚ÄïEvaluation of algorithms using games: 
The case of music annotation,‚Äñ in Proc. Int. Conf. Music Inf. Retrieval, 2009, pp. 387‚Äì392. 
[39] C. Cortes and V. Vapnik, ‚ÄïSupport vector networks,‚Äñ Mach. Learn., vol. 20, no. 3, pp. 273‚Äì297, 
1995. 
[40] W. W. Cohen, R. E. Schapire, and Y. Singer, ‚ÄïLearning to order things,‚Äñ J. Artif. Intell. Res., vol. 
10, pp. 243‚Äì270, 1999. 
[41] R. Herbrich, T. Graepel, and K. Obermayer, ‚ÄïSupport vector learning for ordinal regression,‚Äñ in 
Proc. Int. Conf. Artif. Neural Netw., 1999, pp. 97‚Äì102. 
56 
 
[61] E. Pampalk, ‚ÄïA Matlab toolbox to compute music similarity from audio,‚Äñ in Proc. Int. Conf. 
Music Inf. Retrieval, 2004 [Online]. Available: http://www.ofai.at/elias.pampalk/ma/ 
[62] T. Tolonen and M. Karjalainen, ‚ÄïA computationally efficient multipitch analysis model,‚Äñ IEEE 
Trans. Speech Audio Process., vol. 8, no. 6, pp. 708‚Äì716, Nov. 2000. 
[63] E. G√≥mez, ‚ÄïTonal description of music audio signal,‚Äñ Ph.D. dissertation, Univ. Pompeu Fabra, 
Barcelona, Spain, 2006. 
[64] C. Harte, M. Sandler, and M. Gasser, ‚ÄïDetecting harmonic change in musical audio,‚Äñ in Proc. 
ACM Workshop Audio Music Comput. Multimedia, 2006, pp. 21‚Äì26. 
[65] E. Allamanche, J. Herre, O. Helmuth, B. Fr√∂ba, T. Kasten, and M. Cremer, ‚ÄïContent-based 
identification of audio material using MPEG-7 low level description,‚Äñ in Proc. Int. Conf. Music 
Inf. Retrieval, 2001, pp. 197‚Äì204. 
[66] A. Klapuri, ‚ÄïSound onset detection by applying psychoacoustic knowledge,‚Äñ in Proc. Int. Conf. 
Acoust., Speech, Signal Process., 1999, pp. 3089‚Äì3092. 
[67] R. Cohen and M. Swerdlik, Psychological Testing and Assessment: An Introduction to Tests and 
Measurement. Mountain View, CA: May- field, 2002. 
[68] C.-C. Chang and C.-J. Lin, ‚ÄïLIBSVM: A library for support vector machines,‚Äñ 2001 [Online]. 
Available: http://www.csie.ntu.edu.tw/cjlin/libsvm 
[69] I. Guyon and A. Elisseeff, ‚ÄïAn introduction to variable and feature selection,‚Äñ J. Mach. Learn. 
Res., vol. 3, pp. 1157‚Äì1182, 2003. 
[70] X. Geng, T.-Y. Liu, T. Qin, and H. Li, ‚ÄïFeature selection for ranking,‚Äñ in Proc. ACM Int. Conf. 
Inf. Retrieval, 2007, pp. 407‚Äì414. 
[71] M. R. SÀá ikonja and I. Kononenko, ‚ÄïTheoretical and empirical analysis of ReliefF and RReliefF,‚Äñ 
Mach. Learn., vol. 53, pp. 23‚Äì69, 2003. 
[72] Y. Feng, Y. Zhuang, and Y. Pan, ‚ÄïPopular music retrieval by detecting mood,‚Äñ Proc. ACM SIGIR, 
pp. 375‚Äì376, 2003. 
[73] T. Li and M. Ogihara, ‚ÄïContent-based music similarity search and emotion detection,‚Äñ Proc. Int. 
Conf. Acoustic, Speech, and Signal Processing, Toulouse, France, pp. 17‚Äì21, 2004. 
[74] T.-L. Wu and S.-K. Jeng, ‚ÄïExtraction of segments of significant emotional expressions in music,‚Äñ 
Proc. Int. Workshop on Computer Music and Audio Technology, pp. 76‚Äì80, 2006. 
[75] V. Carvalho and C. Chao, ‚ÄïSentiment retrieval in popular music based on sequential learning,‚Äñ 
Proc. ACM SIGIR, 2005. 
[76] Y.-H. Yang, C.-C Liu, and H. H. Chen, ‚ÄïMusic emotion classification: A fuzzy approach,‚Äñ Proc. 
ACM Multimedia, Santa Barbara, USA, pp. 81‚Äì84, 2006. 
[77] A. Hanjalic and L.-Q. Xu, ‚ÄïAffective video content representation and modeling,‚Äñ IEEE Trans. 
Multimedia, vol. 7, no. 1, pp. 143‚Äì154, 2005. 
[78] P. J. Lang, ‚ÄïThe emotion probe, ‚Äñ American Psychologist, vol. 50, no. 5,  pp. 372‚Äì385, 1995. 
[79] J. A. Russell, A. Weiss, and G. A, Mendelsohn, ‚ÄïAffect grid: A single-item scale of pleasure and 
arousal,‚Äñ J. of Personality and Social Psychology, vol. 57, no. 3, pp. 493‚Äì502, 1989. 
[80] R. Cowie, E. Douglas-Cowie, S. Savvidou, E. McMahon, M. Sawey, and M. Schr√∂der, 
‚Äï‚ÄóFEELTRACE‚Äò: An instrument for recording perceived emotion in real time,‚Äñ Proc. Speech 
and Emotion, ISCA Tutorial and Research Workshop, Newcastle, U.K., pp. 19‚Äì24, 2000. 
[81] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Recognition, New York, John Wiley & Sons, 
Inc., 2000. 
[82] A. J. Smola and B. Sch√∂lkopf, ‚ÄïA tutorial on support vector regression,‚Äñ Statistics and 
Computing, 2004. 
[83] D.P. Solomatine and D.L. Shrestha, ‚ÄïAdaBoost.RT: A boosting algorithm for regression 
problems,‚Äñ Proc. IEEE Int. Joint Conf. Neural Networks, pp. 1163‚Äì1168, 2004. 
[84] P. Viola and M. J. Jones, ‚ÄïRobust real-time face detection,‚Äñ Int. J. of Computer Vision, vol. 57,  
no. 2, pp. 137‚Äì154, 2004. 
58 
 
[109] M. Zaanen and P. Kanters, ‚ÄïAutomatic mood classification using tf*idf based on lyrics,‚Äñ in Proc. 
Int. Conf. Music Inf. Retrieval, 2010, pp. 75‚Äì80. 
[110] B. Schuller, C. Hage, D. Schuller, and G. Rigoll, ‚Äï‚ÄóMister D.J., cheer me up!‚Äò: musical and 
textual features for automatic mood classification,‚Äñ J. New Music Res., vol. 39, no. 1, pp. 13‚Äì34, 
2010. 
[111] B. Schuller, J. Dorfner, and G. Rigoll, ‚ÄïDetermination of nonprototypical valence and arousal in 
popular music: Features and performances,‚Äñ EURASIP J. Audio, Speech, Music Process., p. 19, 
2010, Article ID 735854. 
[112] A. W. Bowman and A. Azzalini, Applied Smoothing Techniques for Data Analysis.   New York: 
Oxford Univ. Press, 1997. 
[113] Z. I. Botev, J. F. Grotowski, and D. P. Kroese, ‚ÄïKernel density estimation via diffusion,‚Äñ Ann. 
Statist., 2009. 
[114] C. Rasmussen and C. Williams, Gaussian Processes for Machine Learning.   Cambridge, MA: 
MIT Press, 2006 [Online]. Available: http://www.gaussianprocess.org/gpml/ 
[115] B. Sch√∂lkopf, A. Smola, R. Williamson, and P. Bartlett, ‚ÄïNew support vector algorithms,‚Äñ 
Neural Comput., vol. 12, pp. 1207‚Äì1245, 2000. 
[116] J. Kittler, M. Hatef, R. Duin, and J. Matas, ‚ÄïOn combining classifiers,‚Äñ IEEE Trans. Pattern 
Anal. Mach. Intell., vol. 20, no. 3, pp. 226‚Äì239, Mar. 1998. 
[117] O. R. Terrades, E. Valveny, and S. Tabbone, ‚ÄïOptimal classifier fusion in a non-Bayesian 
probabilistic framework,‚Äñ IEEE Trans. Pattern Anal. Mach. Intell., vol. 31, no. 9, pp. 1630‚Äì
1644, Sep. 2009. 
[118] S. A. Banawan and N. M. Zeidat, ‚ÄïA comparative study of load sharing in heterogeneous 
multicomputersystems,‚Äñ in Proc. Annu. Simulat. Symp., 1992, pp. 22‚Äì31. 
[119] F. E. Grubbs, ‚ÄïProcedures for detecting outlying observations in samples,‚Äñ Technometrics, vol. 
11, no. 1, pp. 1‚Äì21, 1969. 
[120] E. Benetos, M. Kotti, and C. Kotropoulos, ‚ÄïLarge scale musical instrument identification,‚Äñ in 
Proc. Int. Conf. Music Inf. Retrieval, 2007 [Online]. Available: 
http://www.ifs.tuwien.ac.at/mir/muscle/del/audiotools.html#SoundDescrToolbox 
[121] T. Lidy and A. Rauber, ‚ÄïEvaluation of feature extractors and psycho- acoustic transformations 
for music genre classification,‚Äñ in Proc. Int. Conf. Music Inf. Retrieval, 2005, pp. 34‚Äì41 
[Online]. Available: http:// www.ifs.tuwien.ac.at/mir/audiofeatureextraction.html 
[122] T. Hofmann, ‚ÄïProbabilistic latent semantic indexing,‚Äñ in Proc. ACM Int. Conf. Inf. Retrieval, 
1999, pp. 50‚Äì57. 
[123] W. F. Thompson and B. Robitaille, ‚ÄïCan composers express emotions through music?,‚Äñ 
Empirical Studies of the Arts, vol. 10, pp. 79‚Äì89, 1992. 
[124] S. O. Ali, ‚ÄïSongs and emotions: Are lyrics and melodies equal partners,‚Äñ Psychol. Music, vol. 
34, no. 4, pp. 511‚Äì534, 2006. 
[125] F. Sebastiani, ‚ÄïMachine learning in automated text categorization,‚Äñ ACM Comput. Surveys, vol. 
34, no. 1, pp. 1‚Äì47, 2002. 
[126] Y. Rubner, C. Tomasi, and L. J. Guibas, ‚ÄïThe earth mover‚Äòs distance as a metric for image 
retrieval,‚Äñ Int. J. Comput. Vis., vol. 40, no. 2, pp. 99‚Äì121, 2000. 
[127] A. Y. Ng, M. I. Jordan, and Y. Weiss, ‚ÄïOn spectral clustering: Analysis and an algorithm,‚Äñ Proc. 
Neural Inf. Process. Syst., pp. 849‚Äì856, 2002.  
[128] J. Ellson, E. R. Gansner, E. Koutsofios, S. C. North, and G. Woodhull,   ‚ÄïGraphviz‚Äîopen   
source   graph   drawing   tools,‚Äñ   in Proc. Graph Drawing, 2001, pp. 483‚Äì484 [Online]. 
Available: http://www.graphviz.org/ 
Âá∫ÂúãÂ†±Âëä(Ô•´Ë®™„ÄÅÊúÉË≠∞) 
 
Âá∫Âúã‰∫∫Âì°ÂßìÂêç Èô≥ÂÆèÈäò 
ÊúçÂãôÊ©üÈóúÂèäËÅ∑Á®± Âè∞ÁÅ£Â§ßÂ≠∏Èõª‰ø°ÊâÄÊïôÊéà 
Ë®àÁï´Á∑®ËôüÂèäÂêçÁ®± NSC 97-2221-E-002-111-MY3 Â§öÊ®°ÂºèÈü≥Ô§îÊÉÖÁ∑íËæ®ÔßºÔºåÈÇÅÈ†ÇË®àÁï´ÔºåÊãîÂ∞ñË®à
Áï´ AE01-05 
ÁôºË°®ÊºîÔ•ØÈ°åÁõÆ Our recent progress in audio and video signal processing 
Âá∫ÂúãÊúüÈñìÂèäÂú∞Èªû 8/1-4/11, Maui, HI, USA  
8/21-24/11, San Diego, CA, USA 
8/25/11, Santa Barbara, CA, USA 
Â†±ÂëäÊó•Êúü 8/30/11 
 
1. Ô•´ËàáÊúÉË≠∞Á∂ìÈÅé 
 
In this trip, I attended two international conferences and visited UCSB.  My first stop in this 
trip was Maui, Hawaii, where I attended the 20th International Conference on Computer 
Communications and Networks (ICCCN) and presented the following two papers: 
[1] Enhancement of LCD images illuminated with dim backlight 
[2] Local Dimming of liquid crystal display using visual attention prediction model  
 
at the Workshop on Multimedia Computing and Communications (MCC) of this conference. In 
addition, I served as the Award Co-Chair for the conference. My second stop in this trip was 
San Diego, where I attended the SPIE Conference XXXIV Applications of Digital Image 
Processing as part of 2011 Optics+Photonics and presented the following invited paper: 
[3] Color enhancement for portable LCD displays in low-power mode. 
 
All these papers were well received, and many interesting discussion were inspired. On August 
23, I was invited for a short visit to Qualcomm and had a dinner with Dr. Chien-Chang Chang, 
a VP at Qualcomm, to discussion a potential partnership between Qualcomm, NTU, and 
ChungHua Telecom. My last stop in this trip was UCSB, wehre I visited three professors, 
Yuan-Fang Wang, Jerry Gibson, and Hua Lee, and had in-depth technical discussions on 
various topics, including rate-distortion bounds for perceptual video coding, integration of 3D 
face modeling with depth from focus, and music retrieval by emotion. 
 
2. ËàáÊúÉÂøÉÂæó 
ICCCN celebrated its 20th anniversary in Hawaii. People came here to share ideas and latest 
research outcomes in a wide spectrum of communications and networks areas. But having a 
conference in Hawaii is both good and bad, good because it has nice weather and beautiful 
scenery and bad because most people spent time outside the conference to enjoy the nice 
weather and beautiful scenery. As a result, only a handful of people showed up in some 
afternoon sessions. Nonetheless, it is a fruitful trip because the conference chair introduced me 
to a company in China who in interested in our video coding technology. We are currently 
following up the initial contact and trying to provide test results. The paper we presented at the 
Âá∫ÂúãÂ†±Âëä(Ô•´Ë®™„ÄÅÊúÉË≠∞) 
 
Âá∫Âúã‰∫∫Âì°ÂßìÂêç Èô≥ÂÆèÈäò 
ÊúçÂãôÊ©üÈóúÂèäËÅ∑Á®± Âè∞ÁÅ£Â§ßÂ≠∏Èõª‰ø°ÊâÄÊïôÊéà 
Ë®àÁï´Á∑®ËôüÂèäÂêçÁ®± NSC 97-2221-E-002-111-MY3 Â§öÊ®°ÂºèÈü≥Ô§îÊÉÖÁ∑íËæ®ÔßºÔºåÈÇÅÈ†ÇË®àÁï´ÔºåÊãîÂ∞ñË®à
Áï´ AE01-05 
ÁôºË°®ÊºîÔ•ØÈ°åÁõÆ Our recent progress in audio and video signal processing 
Âá∫ÂúãÊúüÈñìÂèäÂú∞Èªû 8/1-4/11, Maui, HI, USA  
8/21-24/11, San Diego, CA, USA 
8/25/11, Santa Barbara, CA, USA 
Â†±ÂëäÊó•Êúü 8/30/11 
 
1. Ô•´ËàáÊúÉË≠∞Á∂ìÈÅé 
 
In this trip, I attended two international conferences and visited UCSB.  My first stop in this 
trip was Maui, Hawaii, where I attended the 20th International Conference on Computer 
Communications and Networks (ICCCN) and presented the following two papers: 
[1] Enhancement of LCD images illuminated with dim backlight 
[2] Local Dimming of liquid crystal display using visual attention prediction model  
 
at the Workshop on Multimedia Computing and Communications (MCC) of this conference. In 
addition, I served as the Award Co-Chair for the conference. My second stop in this trip was 
San Diego, where I attended the SPIE Conference XXXIV Applications of Digital Image 
Processing as part of 2011 Optics+Photonics and presented the following invited paper: 
[3] Color enhancement for portable LCD displays in low-power mode. 
 
All these papers were well received, and many interesting discussion were inspired. On August 
23, I was invited for a short visit to Qualcomm and had a dinner with Dr. Chien-Chang Chang, 
a VP at Qualcomm, to discussion a potential partnership between Qualcomm, NTU, and 
ChungHua Telecom. My last stop in this trip was UCSB, wehre I visited three professors, 
Yuan-Fang Wang, Jerry Gibson, and Hua Lee, and had in-depth technical discussions on 
various topics, including rate-distortion bounds for perceptual video coding, integration of 3D 
face modeling with depth from focus, and music retrieval by emotion. 
 
2. ËàáÊúÉÂøÉÂæó 
ICCCN celebrated its 20th anniversary in Hawaii. People came here to share ideas and latest 
research outcomes in a wide spectrum of communications and networks areas. But having a 
conference in Hawaii is both good and bad, good because it has nice weather and beautiful 
scenery and bad because most people spent time outside the conference to enjoy the nice 
weather and beautiful scenery. As a result, only a handful of people showed up in some 
afternoon sessions. Nonetheless, it is a fruitful trip because the conference chair introduced me 
to a company in China who in interested in our video coding technology. We are currently 
following up the initial contact and trying to provide test results. The paper we presented at the 
ÂúãÁßëÊúÉË£úÂä©Ë®àÁï´Ë°çÁîüÁ†îÁôºÊàêÊûúÊé®Âª£Ë≥áÊñôË°®
Êó•Êúü:2009/03/17
ÂúãÁßëÊúÉË£úÂä©Ë®àÁï´
Ë®àÁï´ÂêçÁ®±: Â§öÊ®°ÂºèÈü≥Ê®ÇÊÉÖÁ∑íËæ®Ë≠ò
Ë®àÁï´‰∏ªÊåÅ‰∫∫: Èô≥ÂÆèÈäò
Ë®àÁï´Á∑®Ëôü: 97-2221-E-002-111-MY3 Â≠∏ÈñÄÈ†òÂüü: Ë®äËôüËôïÁêÜ 
Á†îÁôºÊàêÊûúÂêçÁ®±
(‰∏≠Êñá) ‰ª•ÊÉÖÁ∑íÂπ≥Èù¢Ê™¢Á¥¢ÔºåÁÆ°ÁêÜÂèäÁÄèË¶ΩÈü≥Ê®Ç
(Ëã±Êñá)
ÊàêÊûúÊ≠∏Â±¨Ê©üÊßã
ÂúãÁ´ãËá∫ÁÅ£Â§ßÂ≠∏ ÁôºÊòé‰∫∫
(Ââµ‰Ωú‰∫∫)
Èô≥ÂÆèÈäò,Ê•äÂ•ïËªí
ÊäÄË°ìË™™Êòé
(‰∏≠Êñá) Êú¨ÁôºÊòé‰øÇ‰∏ÄÁ®ÆÂ§öÂ™íÈ´îÊ™¢Á¥¢Á≥ªÁµ±„ÄÅÂª∫Á´ãË©≤Á≥ªÁµ±ÁöÑÊñπÊ≥ïÂèäÂÖ∂ÊáâÁî®ÊñπÊ≥ïÔºå‰øÇÂà©Áî®Èü≥Ê®ÇÊÉÖ
Á∑í‰ΩúÁÇ∫ 
Ê™¢Á¥¢Ê¢ù‰ª∂‰ª•Âπ´Âä©‰ΩøÁî®ËÄÖÊ™¢Á¥¢Âá∫ÊâÄÈúÄ‰πãÈü≥Ê®ÇÂèäÂÖ∂Áõ∏ÈóúË≥áË®äÔºåÂÖ∂‰∏≠ÔºåË©≤Â§öÂ™íÈ´îÊ™¢Á¥¢Á≥ª
Áµ±‰øÇÂåÖ 
Êã¨ÊúâÔºö‰∏ÄÁ≥ªÁµ±Á´ØÔºå‰øÇËá≥Â∞ëÂÑ≤Â≠òÊúâË§áÊï∏ÂÄãÈü≥Ê®ÇÊÉÖÁ∑íÂ∫ßÊ®ôÂÄºÂíåÂÖ∂Â∞çÊáâÁöÑÈü≥Ê®ÇÂèäÂÖ∂Áõ∏Èóú
Ë≥áË®äÔºõ 
‰∏ÄÁî®Êà∂Á´ØÔºå‰øÇÊèê‰æõ‰∏ÄÊ™¢Á¥¢‰ªãÈù¢‰ª•‰æõ‰ΩøÁî®ËÄÖÈÄ≤Ë°åÈü≥Ê®ÇÊ™¢Á¥¢ÔºõË©≤Âª∫Á´ãÂ§öÂ™íÈ´îÊ™¢Á¥¢Á≥ªÁµ±
ÁöÑÊñπÊ≥ï 
‰øÇÂà©Áî®‰∫∫Â∑•Êô∫ÊÖßÊäÄË°ì‰∏≠Ê©üÂô®Â≠∏ÁøíÁöÑÊñπÂºèÔºåÈÄèÈÅéË§áÊï∏ÂÄãË©¶ËÅΩÈü≥Ê®Ç‰ΩúÁÇ∫Ê©üÂô®Â≠∏ÁøíÁöÑË≥á
ÊñôÊ®° 
ÂûãÔºåÂ∞á‰∏ÄÈü≥Ê®ÇÂÆöÁæ©Êàê‰∏ÄÂÄã‰∫åÁ∂≠ÁöÑÊÉÖÁ∑íÂ∫ßÊ®ôÂÄºÊñº‰∏ÄÊÉÖÁ∑íÂπ≥Èù¢‰∏äÔºõË©≤ÊáâÁî®Â§öÂ™íÈ´îÊ™¢Á¥¢
Á≥ªÁµ±ÁöÑ 
ÊñπÊ≥ïÔºå‰øÇ‰æõ‰ΩøÁî®ËÄÖÊñº‰∏ÄÂ§öÂ™íÈ´îÊ™¢Á¥¢Á≥ªÁµ±Áµ¶‰∫àÁâπÂÆöÁöÑÊÉÖÁ∑íÂ∫ßÊ®ôÂÄº‰ΩúÁÇ∫Ê™¢Á¥¢Ê¢ù‰ª∂ÔºåÂÜç
ËóâÁî±Ë©≤ 
ÊÉÖÁ∑íÂ∫ßÊ®ôÂÄºÂ∞ãÊâæÂá∫Â∞çÊáâÁöÑÈü≥Ê®ÇÊàñÂÖ∂Áõ∏ÈóúË≥áË®äÔºåÊ†πÊìö‰∏çÂêåÊÉÖÁ∑íÂ∫ßÊ®ôÂÄºÁµ¶ÂÆöÁöÑÊñπÂºèÔºå
ÂèØ‰ª•Áç≤ 
Âæó‰∏çÂêåÁöÑÊ™¢Á¥¢ÁµêÊûú„ÄÇ
(Ëã±Êñá)
Áî¢Ê•≠Âà• Ë≥áË®äÊúçÂãôÊ•≠
ÊäÄË°ì/Áî¢ÂìÅÊáâÁî®ÁØÑÂúç
ÊäÄË°ìÁßªËΩâÂèØË°åÊÄßÂèä
È†êÊúüÊïàÁõä
Ë®ªÔºöÊú¨È†ÖÁ†îÁôºÊàêÊûúËã•Â∞öÊú™Áî≥Ë´ãÂ∞àÂà©ÔºåË´ãÂãøÊè≠Èú≤ÂèØÁî≥Ë´ãÂ∞àÂà©‰πã‰∏ªË¶ÅÂÖßÂÆπ„ÄÇ
ÂÖ∂‰ªñÊàêÊûú 
(ÁÑ°Ê≥ï‰ª•ÈáèÂåñË°®ÈÅî‰πãÊàê
ÊûúÂ¶ÇËæ¶ÁêÜÂ≠∏Ë°ìÊ¥ªÂãï„ÄÅÁç≤
ÂæóÁçéÈ†Ö„ÄÅÈáçË¶ÅÂúãÈöõÂêà
‰Ωú„ÄÅÁ†îÁ©∂ÊàêÊûúÂúãÈöõÂΩ±Èüø
ÂäõÂèäÂÖ∂‰ªñÂçîÂä©Áî¢Ê•≠ÊäÄ
Ë°ìÁôºÂ±ï‰πãÂÖ∑È´îÊïàÁõä‰∫ã
È†ÖÁ≠âÔºåË´ã‰ª•ÊñáÂ≠óÊïòËø∞Â°´
Âàó„ÄÇ) 
N/A 
 ÊàêÊûúÈ†ÖÁõÆ ÈáèÂåñ ÂêçÁ®±ÊàñÂÖßÂÆπÊÄßË≥™Á∞°Ëø∞ 
Ê∏¨È©óÂ∑•ÂÖ∑(Âê´Ë≥™ÊÄßËàáÈáèÊÄß) 0  
Ë™≤Á®ã/Ê®°ÁµÑ 0  
ÈõªËÖ¶ÂèäÁ∂≤Ë∑ØÁ≥ªÁµ±ÊàñÂ∑•ÂÖ∑ 0  
ÊïôÊùê 0  
ËàâËæ¶‰πãÊ¥ªÂãï/Á´∂Ë≥Ω 0  
Á†îË®éÊúÉ/Â∑•‰ΩúÂùä 0  
ÈõªÂ≠êÂ†±„ÄÅÁ∂≤Á´ô 0  
Áßë 
Êïô 
Ëôï 
Ë®à 
Áï´ 
Âä† 
Â°´ 
È†Ö 
ÁõÆ Ë®àÁï´ÊàêÊûúÊé®Âª£‰πãÂèÉËàáÔºàÈñ±ËÅΩÔºâ‰∫∫Êï∏ 0  
 
