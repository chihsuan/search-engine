Abstract
Algebraic-geometry (AG) codes open new and rather exciting possibility in coding theory
for finding a sequence of asymptotically good linear codes that meet or exceed the Gilbert-
Varshamov bound. To make AG codes competitive in real world applications, the theory of
AG codes must be made easier to be understood and the encoding and decoding complexity of
AG codes be reduced to rival that of Reed-Solomon codes. The decoding of AG codes follows
the same four-step procedure as that in the decoding of Reed-Solomon codes: syndrome gen-
eration, error-locator polynomial finding, error-location search, and error-value evaluation.
The main goal of this project is to develop efficient algorithms and hardware architectures
for the encoding and decoding of existent AG codes, such as Hermitian codes, and by ex-
ploiting a special tower of function fields, to construct a series of new AG codes with good
asymptotical behaviors. In this project, a series-in-series-out hardware architecture for a sys-
tematic encoding scheme of Hermitian codes is developed with complexity similar to that for
a systematic encoder of classical cyclic codes. The upper bounds of the numbers of memory
elements and constant multipliers in the proposed architecture are shown both proportional
to O(n), where n is the codeword length. And to encode a codeword of length n, this archi-
tecture takes n clock cycles without any latency. A parallel early stopped Berlekamp-Massey
(PESBM) algorithm is developed in this project for finding error-locator polynomials in the
decoding of one-point AG codes. The computation complexity of the PESBM algorithm is
O(Œ≥n2), where n is the codeword length and Œ≥ is the smallest nonzero nongap of the point
on the algebraic curve over which the code is defined. Due to the early stopped property, the
PESBM algorithm is the most efficient algorithm compared with those in the literature. A
pipeline hardware architecture of the PESBM algorithm via one-dimensional systolic array
is also developed in this project. This implementation requires only a series of t+ ‚åäg‚àí1
2
‚åã+1
processing elements, called PE cells, g delay units, called D cells, and Œ≥ operation elements,
Contents
Abstract 1
Contents i
List of Figures vi
List of Tables ix
1 Introduction 1
1.1 Algebraic-Geometry Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Systematic Encoding of Hermitian Codes . . . . . . . . . . . . . . . . . . . . 5
1.3 Determination of Error-Locator Polynomials . . . . . . . . . . . . . . . . . . 8
1.4 Implementation of Algorithms for Finding Error-Locator Polynomials . . . . 10
1.5 Implementation of Error-Value Evaluator for Hermitian Codes . . . . . . . . 11
1.6 Construction of New Classes of Algebraic-Geometry Codes . . . . . . . . . . 13
2 A Serial-In-Serial-Out Systematic Encoder for Hermitian Codes 14
i
4.1.3 Description of the PESBM Algorithm . . . . . . . . . . . . . . . . . . 67
4.2 A Systolic Array Implementation . . . . . . . . . . . . . . . . . . . . . . . . 70
4.2.1 An Implementation of Left-column Replacement Algorithm via Sys-
tolic Array . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.2.2 Reduction of Hardware Complexity . . . . . . . . . . . . . . . . . . . 74
4.3 Cell Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.4 A Design Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.5 Analysis of Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5 An Efficient Architecture of Error-Value Evaluator for Hermitian Codes 95
5.1 A Generalized Forney Formula for Error-Value Evaluation of Hermitian Codes 95
5.1.1 Theories for the Calculation of the Separating Functions . . . . . . . 97
5.2 An Algorithm for the Error-value Evaluation of Hermitian Codes with One
Error-Locator Polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
5.3 An Efficient Hardware Architecture for the Error-Value Evaluation of Hermi-
tian Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
5.3.1 Functions of the Error-Value Solver . . . . . . . . . . . . . . . . . . . 100
5.3.2 Main Circuits of the Error-Value Evaluator . . . . . . . . . . . . . . . 103
5.3.3 The Division Circuit in the Error-Value Evaluator . . . . . . . . . . . 110
5.4 Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
iii
.2 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
Bibliography 164
v
4.1 The extended syndrome matrix M of the Hermitian code. . . . . . . . . . . 72
4.2 Data dependency graph of Gaussian elimination without column exchange:
(a) single elimination subgraph with k = 0 of the given example; (b) unrolled
space-time solid. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.3 The data flows of two-dimensional array and one-dimensional array of Gaus-
sian elimination without column exchange. . . . . . . . . . . . . . . . . . . . 92
4.4 The input and output definitions of a PE cell and a D cell. . . . . . . . . . . 93
4.5 The input and output definitions of a OE cell. . . . . . . . . . . . . . . . . . 93
4.6 The circuit for computing data discrepancies and available candidates in the
cell OE(i). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
4.7 The circuit for updating coefficient vectors in the cell OE(i). . . . . . . . . . 94
4.8 The proposed implementation of the PESBM algorithm via systolic array for
the Hermitian code C‚Ñ¶(23Q). . . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.1 Inputs and Outputs of the error-value solver. . . . . . . . . . . . . . . . . . . 101
5.2 A Horner‚Äôs loop. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
5.3 A circuit for calculating the coefficients œÜ
(NI)
NP,a,b and œÜ
(NI)
NP,a+q+1,b of the polyno-
mial œÜ(x, y) for a fixed a, 0 ‚â§ a ‚â§ a(NI‚àí1)NP . . . . . . . . . . . . . . . . . . . . 107
5.4 A circuit for calculating the error value eNP . . . . . . . . . . . . . . . . . . . 110
5.5 A three-dimensional representation of data dependencies of computations in
the division algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
vii
List of Tables
2.1 A list of the values of di, Œ¥ij and Œ¥
‚Ä≤
ij in the (64,48) Hermitian code. . . . . . . 27
4.1 The truth table of E(IR)j and k(IR)j for the cell PE(j), for a given Hermitian
code C‚Ñ¶(23Q) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.2 The transition tables of the state machine in OE cells, for a given Hermitian
code C‚Ñ¶(23Q). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
5.1 Hardware and time complexities of each step in Function I. . . . . . . . . . . 122
ix
Generation and Error Locator Search in the decoding of Hermitian codes‚Äù (with project
code NSC 89-2213-E-007-105), we have proposed the architectures for syndrome generation
and error location search in the decoding of Hermitian codes, which is published in [6].
The remaining works in the decoding of Hermitian codes are determining the error-locator
polynomial(s) and evaluating the error values.
1.1 Algebraic-Geometry Codes
In this section, we will give a trimmer presentation of AG codes on plane curves to introduce
basic concepts related to our work. For more details on the theory of AG codes, please
refer to the articles and the references therein in the special issue on AG codes of the IEEE
Transactions on Information Theory in November, 1995 or the books [5, 7].
Let GF (q) be a finite field with q elements. On the set GF (q)3 \ {(0, 0, 0)} of nonzero
3-tuples over GF (q), an equivalent relation ‚Äú‚â°‚Äù is defined as (a0, a1, a2) ‚â° (b0, b1, b2) if and
only if there exists a nonzero Œª in GF (q) such that Œªai = bi for i = 0, 1, 2. The equivalent
class of (a0, a1, a2) is denoted by (a0 : a1 : a2). Let P
2 be the set of all such equivalent
classes, usually called a projective plane over GF (q). And let A2 be the set GF (q)2 of all
2-tuples over GF (q), usually called an affine plane over GF (q). Each affine point Pa in A2
with Pa = (a¬Ø1, a¬Ø2) can be identified with a unique equivalent class (or called projective point)
P = (a0 : a1 : a2) in P
2 with a0 6= 0 by letting a¬Øi = ai/a0 for i = 1, 2. The set of all the other
points P = (a0 : a1 : a2) with a0 = 0 forms the so-called line at the infinity in the projective
plane P2.
Let F (x0, x1, x2) be a homogeneous polynomial over GF (q) in three indeterminants. A
projective curve is defined by the following equation: F (x0, x1, x2) = 0. We say that F (P ) =
0, i.e., P is a zero of F , if F (a0, a1, a2) = 0, where P = (a0 : a1 : a2). And such a point
P = (a0 : a1 : a2) in the projective plane P
2 over GF (q) is called a rational point of the
projective curve defined by F . Dividing each term of F by the monomial xl0 with l the
2
Let L(mQ) denote the vector space over GF (q) consisting of all polynomials f(x, y) in
x and y over GF (q) such that the pole order of f(x, y) at Q is less than or equal to m.
Hereafter, we shall assume that m ‚â• 2g ‚àí 1. And by the Riemann-Roch Theorem, the
dimension of L(mQ) is m‚àíg+1. Consider a set B(m) = {foi(x, y)}1‚â§i‚â§m‚àíg+1 of monomials
foi(x, y) with pole order oi at Q such that o1 < o2 < ¬∑ ¬∑ ¬∑ < om‚àíg+1. It is clear that for each
nongap j, j ‚â§ m, there is exactly one monomial foi in B(m) such that oi = j. A monomial
xayb of the same pole order oi at Q as foi is called a consistent term of foi if x
ayb 6= foi .
Such a consistent term xayb can be expressed as a linear combination of fok ‚Äôs, 1 ‚â§ k ‚â§ i [9].
Thus B(m) is a basis of the vector space L(mQ) over GF (q). Let Œ≥ be the smallest nonzero
nongap at Q. The basis B(m) = {foi}1‚â§i‚â§m‚àíg+1 of L(mQ) is called a standard basis if ‚Ñì and
‚Ñì‚àí Œ≥ are nongaps at Q, then [10]
f‚Ñì = fŒ≥f‚Ñì‚àíŒ≥. (1.2)
Notice that for any nonnegative integers m and m‚Ä≤ with m < m‚Ä≤, we have L(mQ) ‚äÜ L(m‚Ä≤Q),
and thus we can construct standard bases B(m) and B(m‚Ä≤) for L(mQ) and L(m‚Ä≤Q) such
that B(m) ‚äÜ B(m‚Ä≤). Thus a tower B(2g ‚àí 1) ‚äÜ B(2g) ‚äÜ B(2g + 1) ‚äÜ ¬∑ ¬∑ ¬∑ of standard bases
can be constructed for L(mQ)‚Äôs, m ‚â• 2g ‚àí 1. For the Hermitian curve of degree 5 in (1.1),
the vector space L(mQ) with m = 23 has a standard basis B(23) consisting of m‚àíg+1 = 18
monomials:
B(23) =
{
1, x, y, x2, xy, y2, x3, x2y, xy2, y3, x4, x3y, x2y2, xy3, x5, x4y, x3y2, x2y3
}
. (1.3)
Note that the monomial y4 with pole order 20 is absent and is a linear combination y + x5
of the two monomials y and x5.
Suppose that there are n finite rational points Pi over the symbol field GF (q), 1 ‚â§ i ‚â§ n,
on the curve. With m ‚â• 2g ‚àí 1 assumed, the m‚àí g + 1 monomials foi, 1 ‚â§ i ‚â§ m‚àí g + 1,
of the basis B(m) of the vector space L(mQ) can be used to construct an (m‚àí g + 1)√ó n
4
over the same symbol field. For example, a (64, 32, 27) Hermitian code and a (16, 8, 9) ex-
tended Reed-Solomon code, both over the same symbol field GF (16), have the same rate
1/2. But over practical channels, the (64, 32, 27) Hermitian code has far better performance
than the (16, 8, 9) extended Reed-Solomon code [11]. In general, Hermitian codes are q times
longer than Reed-Solomon codes over the same symbol field GF (q2) [13]. Thus the goal of
this paper is to develop a serial-in-serial-out architecture for systematic encoding of Hermi-
tian codes that is about q times larger than for Reed-Solomon codes and is able to target a
low area of implementation.
Let Hq denote the Hermitian curve yq + y = xq+1 over the field GF (q2) with genus g =
q(q ‚àí 1)/2. It is well known that Hq has a single rational point Q at infinity and q3 finite
rational points Pi = (Œ±i, Œ≤i), i = 0, ¬∑ ¬∑ ¬∑ , q
3 ‚àí 1, with Œ≤qi + Œ≤i = Œ±
q+1
i [5, 7]. Let GF (q
2)(x, y)
be the algebraic function field associated with the Hermitian curve Hq : yq + y = xq+1 over
GF (q2), called the Hermitian function field over GF (q2). For any non-negative integer m,
let L(mQ) denote the vector space over GF (q2) consisting of all rational functions in the
Hermitian function field GF (q2)(x, y) whose pole divisors are upper bounded by mQ. For
m > 2g ‚àí 2, the dimension of L(mQ) over GF (q2) is equal to m ‚àí g + 1. Since the pole
orders ox and oy of the rational functions x and y at Q are q and (q + 1) respectively, the
elements foi = x
aiybi with 0 ‚â§ ai ‚â§ q, 0 ‚â§ bi and oi = aiq + bi(q + 1) ‚â§ m form a basis of
L(mQ). The Hermitian code Cm of length n = q3 is defined to be
Cm = CL(D,mQ) ‚â° {(f(P0), ¬∑ ¬∑ ¬∑ , f(Pn‚àí1)) ‚àà GF (q
2)n|f ‚àà L(mQ)},
where D =
‚àëq3‚àí1
i=0 Pi is the sum of all places of degree one (except Q) of the Hermitian
function field GF (q2)(x, y). For 2g ‚àí 2 < m ‚â§ q3, the dimension k(m) of Cm is equal to
dimL(mQ) = m‚àí g + 1, i.e.,
k(m) = m‚àí g + 1. (1.5)
The minimum distance dmin(m) of Cm satisfies
dmin(m) ‚â• q
3 ‚àím. (1.6)
6
Unlike the brute-force matrix multiplication or the encoding schemes in [13,14], a novel and
elegant approach for the systematic encoding of Hermitian codes is developed in [15] by em-
ploying a GF (q2)[t]-module structure of Hermitian codes. However, to our best knowledge,
there is no serious discussion on the hardware implementation complexity of this systematic
encoding scheme in the literature. Therefore in Chapter 2, we will develop a serial-in-serial-
out hardware architecture, similar to a classical cyclic encoder, for the systematic encoding
scheme proposed in [15] and to demonstrate that the hardware complexity of this archi-
tecture is much less than that of the brute-force serial-in-serial-out systematic encoding by
matrix multiplication.
1.3 Determination of Error-Locator Polynomials
The algebraic decoding procedure by searching the error-locator polynomial(s) and then
the error locations is the common principle of the available decoding algorithms for AG
codes. Justesen et al. [16] first contributed to the simplified understanding of AG codes and
presented an algorithm for decoding codes constructed from nonsingular irreducible algebraic
plane curves [5, 7]. This algorithm can correct up to d‚àó/2 ‚àí k2/4 errors with complexity of
order O(n3), where n is the code length, d‚àó is the designed minimum distance, and k is the
degree of the curve.
Based on the Gaussian elimination and by iteratively determining unknown syndromes
from the known syndromes, Feng and Rao proposed an efficient decoding algorithm in [9],
usually called the Feng-Rao algorithm, which can correct up to ‚åä(d‚àó‚àí1)/2‚åã errors with com-
plexity of order O(n3). The Feng-Rao algorithm applies the modified fundamental iterative
algorithm (FIA) [17] followed by the majority voting scheme (MVS) [9]. The execution of
the modified FIA on a syndrome matrix enclosing unknown entries is similar to the opera-
tion of Gaussian elimination on columns without column exchange [18]. After applying the
modified FIA, there are (at least one) candidate values for the (first) confronted unknown
8
where Œ≥ < k is the order of the first nonzero nongap in the function space associated with
the code. The Ko¬®tter‚Äôs algorithm has a regular and simple structure and is suitable for VLSI
implementation [10].
Among these algorithms of determining the error-locator polynomial(s), one can distin-
guish them into two types: the Gaussian-elimination-type algorithm, such as the Feng-
Rao algorithm, and the Berlekamp-Massey-type algorithm, such as the Sakata et al.‚Äôs algo-
rithm. The Berlekamp-Massey-type algorithm seems to be more efficient than the Gaussian-
elimination-type algorithm, but the latter is more easily understanding than the former.
Recently, we introduced an excursion from a restricted Gaussian elimination to several
aspects of the Berlekamp-Massey algorithm [24], where it can be seen that the prominent
features of the Berlekamp-Massey algorithm can be fully reached naturally by operating a
restricted Gaussian elimination on an appropriate syndrome matrix which has structural
properties. The formulation taken in [24] makes the deep insights of the Berlekamp-Massey
algorithm crystal clear. By applying and extending the formulation in [24] for the decoding
of BCH codes, a simple and efficient decoding algorithm for AG codes, called parallel early
stopped Berlekamp-Massey (PESBM) algorithm, will be developed in Chapter 3 of this
report.
1.4 Implementation of Algorithms for Finding Error-
Locator Polynomials
An implementation of the Sakata et al.‚Äôs algorithm has been reported in [25,26]. Based on the
Sakata et al.‚Äôs algorithm, an algorithmic and architectural result for a decoder of Hermitian
codes has been presented in [27]. Moreover, by applying a set of parallel Berlekamp-Massey
algorithms, an implementation of the parallel Sakata et al.‚Äôs algorithm has been designed by
Ko¬®tter in [10].
10
the decoding of BCH codes and Reed-Solomon codes. For each potential error point Q, a
separating function œÉQ is constructed with the property that it vanishes at all potential error
points except the point Q. Then the error value eQ at the point Q is equal to S(œÉQ)/œÉQ(Q),
where S(œÉQ) can be calculated from coefficients in the function œÉQ and some syndromes.
Several ways to construct the separating function œÉQ from a Gro¬®bner basis of the error
locator ideal are firstly proposed in [29]. Then an efficient algorithm for the calculation
of error values using the separating functions derived in [29] are developed in [30]. Since
the above approach involves changing the Gro¬®bner bases of the error-locator ideal with
respect to different monomial orderings in a multi-variate polynomial ring, the process for
calculating the separating functions is messy and may not have efficient hardware architecture
for practical implementations.
Instead of changing the Gro¬®bner bases, two simple methods which can either construct
the separating function œÉQ for the (potential) error point Q or generate a new error-locator
polynomial with a small degree of vanishing in Q is derived in [31]. Also, an algorithm for
evaluating the error values based on the above method is proposed in [31]. Although the
algorithm still requires the knowledge of a reduced Gro¬®bner basis for the error-locator ideal,
it can be easily modified to the case when only one error locator polynomial is given. Thus
the process for calculating the error-locator ideal can be stopped immediately once an error
locator polynomial is found, and the time and hardware complexities for searching the error
locator-ideal and solving the error locations will be greatly reduced. Therefore, in Chapter
5, we will modify the algorithm in [31] and propose an efficient architecture via systolic
arrays for the determination of error values in the decoding of Hermitian codes when only
one error-locator polynomial is given.
12
Chapter 2
A Serial-In-Serial-Out Systematic
Encoder for Hermitian Codes
In this chapter, we first review the basic properties of Gro¬®bner bases for ideals and modules
and present the systematic encoding scheme of Hermitian codes as developed in [15]. We
then develop a serial-in-serial-out hardware architecture, similar to a classical cyclic encoder,
for this systematic encoding scheme. The complexity of our architecture will be analyzed.
2.1 Gro¬®bner bases and Systematic Encoding of Hermi-
tian Codes
In this section, we first describe a module structure of Hermitian codes Cm under a given per-
mutation [12,15]. Then we review some basics of Gro¬®bner bases of modules over polynomial
rings [15, 36]. Finally we present a systematic encoding scheme for Cm by the construction
of a reduced Gro¬®bner basis for the module structure of Cm as developed in [15].
2.1.1 Module Structures of Hermitian Codes
Let PGF (q2)(x,y) and DGF (q2)(x,y) be the set of all places and the divisor group of the Hermi-
tian function field GF (q2)(x, y) over GF (q2) respectively. Let Sn be the symmetric group
14
arbitrary place in Oi as Pi,0, and set the places Pi,j in orbit Oi as Pi,j = œÉ
j(Pi,0), 1 ‚â§ j ‚â§ ni.
Then we have Pi,ni = Pi,0 and in general, we define Pi,j = Pi,j(mod ni)
, ‚àÄj ‚àà Z. It is clear
that Pi,j‚àí1 = œÉ
‚àí1(Pi,j). Then the components of codewords corresponding to the places Pi,j
in the same Oi can be permuted cyclically by œÉ.
To construct a module structure of the Hermitian code Cm, we first rearrange the compo-
nents of a codeword according to the new labeling of places Pi,j. Then a codeword v in Cm
can be represented as an ‚Ñì-tuple of polynomials in GF (q2)[t]:
v = (v1(t), v2(t), . . . , v‚Ñì(t)), (2.3)
where vi(t) =
‚àëni‚àí1
j=0 f(Pi,j)t
j and f ‚àà L(mQ). Since
tvi(t)(mod t
ni ‚àí 1) =
‚àëni‚àí1
j=0 f(Pi,j)t
j+1(mod tni ‚àí 1)
=
‚àëni‚àí1
j=0 f(Pi,j‚àí1)t
j =
‚àëni‚àí1
j=0 f(œÉ
‚àí1(Pi,j))t
j (2.4)
for each i, we can see from (2.1) that multiplying a codeword v = (v1(t), v2(t), . . . , v‚Ñì(t))
in Cm by t is equivalent to permuting the codeword locally cyclically by œÉ. Thus Cm is
closed under multiplication by t modulo tni ‚àí 1 at each ith coordinate in (2.3). Since Cm is
GF (q2)-linear, Cm can be represented as a submodule of the GF (q
2)[t]-module
M = ‚äï‚Ñìi=1(GF (q
2)[t]/(tni ‚àí 1)), (2.5)
which is the direct sum of the quotient rings GF (q2)[t]/(tni ‚àí 1).
For convenience to compute the Gro¬®bner bases, we consider the following canonical epi-
morphism
œÄ : GF (q2)[t]‚Ñì ‚Üí ‚äï‚Ñìi=1(GF (q
2)[t]/(tni ‚àí 1)).
Let ei be the ith standard basis vector in the GF (q
2)[t]-module GF (q2)[t]‚Ñì and X i =
(tni ‚àí 1)ei, for i = 1, ¬∑ ¬∑ ¬∑ , ‚Ñì. Define C¬Øm ‚â° œÄ‚àí1(Cm) which is a submodule of the free
GF (q2)[t]-module GF (q2)[t]‚Ñì and is generated by all codewords in Cm (regarded as vec-
tors in GF (q2)[t]‚Ñì) and all X i‚Äôs. Note that Cm is isomorphic to the quotient module
C¬Øm/(X1, . . . ,X‚Ñì).
16
are called the nonstandard monomials forW and the monomials not in LT (W) the standard
monomials. As can be seen in [36], every non-zero submodule W of M¬Ø has a Gro¬®bner basis
G, which can be computed by Buchberger‚Äôs algorithm, and is generated by G, i.e., W = (G).
A Gro¬®bner basis G = {g1, . . . , gs} is called reduced if lc(gi) = 1 for all i and for each i,
no term in gi is divisible by any lm(gj) for j 6= i. With the POT ordering (in fact, any
monomial ordering), every Gro¬®bner basis can be transformed to a reduced Gro¬®bner basis,
which is unique for every non-zero submodule W of M¬Ø [36]. Such a reduced Gro¬®bner basis
G = {g1, . . . , gs} cannot have two i 6= j such that ek divides both lm(gi) and lm(gj) for
some k. Otherwise, lm(gi) = lt(gi) = t
nek and lm(gj) = lt(gj) = t
mek, and the leading
monomial of gi will divide the leading term of gj if n ‚â§ m or the leading monomial of gj
will divide the leading term of gi if n ‚â• m, a contradiction. By the pigeon-hole principle,
the size of the reduced Gro¬®bner basis of a submodule of M¬Ø is no more than ‚Ñì [15]. From
now on, we assume all the Gro¬®bner bases we use are reduced.
Now consider the submodule C¬Øm of M¬Ø associated with the Hermitian code Cm. Since the
generators of the submodule C¬Øm include X i = (t
ni ‚àí 1)ei, the (reduced) Gro¬®bner basis G
(with respective to the POT ordering) of C¬Øm must contain exactly ‚Ñì vectors of the form
g1 = (g11(t), g12(t), . . . , g1‚Ñì(t)),
g2 = (0, g22(t), . . . , g2‚Ñì(t)),
...
g‚Ñì = (0, 0, ¬∑ ¬∑ ¬∑ , 0, g‚Ñì‚Ñì(t)),
(2.7)
where deg gii ‚â§ ni for all i and deg gij < deg gjj for all 1 ‚â§ i < j. Once the (reduced)
Gro¬®bner basis G = {g1, . . . , g‚Ñì} of C¬Øm is computed, a division algorithm with respect to G
can be applied to a vector v = (v1(t), . . . , v‚Ñì(t)) ‚àà M¬Ø to obtain the following representation
v = q1(t)g1 + ¬∑ ¬∑ ¬∑+ q‚Ñìg‚Ñì + v¬Ø, (2.8)
where qi(t) ‚àà GF (q2)[t] for all i and v¬Ø is a unique liner combination of standard monomials
for C¬Øm. The vector v¬Ø in (2.8) is called the remainder of v with respect to G. The division
algorithm is described as follows. At the first step, we let v = v1 = (v11(t), . . . , v1‚Ñì(t)) and
divide v11(t) by g11(t) to obtain a quotient q1(t) and a remainder r1(t) with r1(t) = 0 or
18
in C¬Øm, a contradiction. Conversely, given a coset leader u = (u1(t), . . . , u‚Ñì(t)) in M¬Ø with
ui(t) =
‚àëni‚àí1
j=ri
uijt
j , we can compute its remainder u¬Ø = (u¬Ø1(t), . . . , u¬Ø‚Ñì(t)), u¬Øi(t) =
‚àëri‚àí1
j=0 u¬Øijt
j ,
with respective to G, such that v = u‚àí u¬Ø is a coset leader which is in C¬Øm and corresponds to
a codeword in Cm. Note that v and u have the same nonstandard terms uijt
jei, 1 ‚â§ i ‚â§ ‚Ñì,
ri ‚â§ j ‚â§ ni‚àí1. In summary, each codeword in Cm corresponds uniquely to a specification of
nonstandard terms to a coset leader in M¬Ø. Since there are ki = ni‚àíri nonstandard terms in
the ith coordinate, we have the GF (q2)-dimension of Cm to be k(m) =
‚àë‚Ñì
i=1 ki = n‚àí
‚àë‚Ñì
i=1 ri.
Thus we can regard the coefficients and the monomials of the nonstandard terms in a
codeword of Cm as the information digits and the information positions, respectively. And
we regard the coefficients and the monomials of the standard terms in a codeword of Cm as
the parity-check digits and parity-check positions, respectively, where the number of parity-
check digits for Cm is r(m) =
‚àë‚Ñì
i=1 ri.
With u = (u1(t), . . . , u‚Ñì(t)), where ui(t) =
‚àëni‚àí1
j=ri
uijt
j , and v = u ‚àí u¬Ø, where u¬Ø =
(u¬Ø1(t), . . . , u¬Ø‚Ñì(t)), u¬Øi(t) =
‚àëri‚àí1
j=0 u¬Øijt
j , is the remainder of u with respective to the (reduced)
Gro¬®bner basis G, the mapping u 7‚Üí v is a systematic encoding of the Hermitian code Cm
since v is a codeword in Cm (as a submodule ofM = ‚äï‚Ñìi=1(GF (q
2)[t]/(tni ‚àí 1))) and u and
v have the same nonstandard terms.
Example 1. Consider the Hermitian code C53 defined by the Hermitian curve H4 = x5 ‚àí
y4‚àí y over GF (42). We have n = 64, k(53) = 48, r(53) = 16 and dmin(53) = 11 in this case.
Let œÉ be the GF (42)-automorphism of the Hermitian function field GF (42)(x, y)
œÉ =
{
x 7‚Üí Œ±x
y 7‚Üí Œ±5y,
(2.9)
where Œ± is a primitive element of GF (42) and then Œ±5 is a primitive element of the subfield
GF (4) [37]. Under the action of œÉ, supp(D) can be decomposed into six orbits
O1 = {œÉj((1, Œ±1))|j = 0, . . . , 14}, O2 = {œÉj((1, Œ±2))|j = 0, . . . , 14},
O3 = {œÉ
j((1, Œ±4))|j = 0, . . . , 14}, O4 = {œÉ
j((1, Œ±8))|j = 0, . . . , 14},
O5 = {œÉj((0, Œ±0))|j = 0, 1, 2}, O6 = {(0, 0)}.
20
cyclic encoder, for the systematic encoding scheme of Hermitian codes, we introduce here a
serial representation for the information block u and the codeword v as follows. We define
the information polynomial and the codeword polynomial as
u(t) =
‚Ñì‚àë
i=1
ui(t) t
‚àë‚Ñì
j=i+1 nj and v(t) =
‚Ñì‚àë
i=1
vi(t) t
‚àë‚Ñì
j=i+1 nj ,
which correspond to the vectors in GF (q2)n
u = (0, . . . , 0, u‚Ñì,r‚Ñì, . . . , u‚Ñì,n‚Ñì‚àí1, . . . , 0, . . . , 0, u2,r2, . . . , u2,n2‚àí1,
0, . . . , 0, u1,r1, . . . , u1,n1‚àí1) and
v = (v‚Ñì,0, . . . , v‚Ñì,n‚Ñì‚àí1, . . . , v2,0, . . . , v2,n2‚àí1, v1,0, . . . , v1,n1‚àí1),
(2.11)
in the sense that we will serial in the coefficients of the information polynomial u(t) (with
appropriately padded zero symbols) to the encoder and serial out the coefficients of the
codeword polynomial v(t) from the encoder both in the descendent degree order.
The systematic encoding scheme amounts to compute the remainder u¬Ø of the information
vector u in M with respective to G by the division algorithm described in Subsection 2.1.2.
To have the serial-in-serial-out feature, when the first miniblock u1 = (0, . . . , 0, u1,r1, . . . , u1,n1‚àí1)
of the information block u in (2.11) is shifted into the encoder from u1,n1‚àí1 to u1,0 = 0, the
first miniblock v1 = (v1,0, . . . , v1,n1‚àí1) of the codeword v should be shifted out of the encoder
simultaneously from v1,n1‚àí1 to v1,0. During this first period [1, n1] of times, we need to calcu-
late the parity-check symbols v1j = ‚àíu¬Ø1j for 0 ‚â§ j ‚â§ r1‚àí1, where u¬Ø1j is the coefficient of the
first component u¬Ø1(t) of the remainder vector u¬Ø = (u¬Ø1(t), . . . , u¬Ø‚Ñì(t)) of u = (u1(t), . . . , u‚Ñì(t))
(as a vector in M¬Ø) with respective to the (reduced) Gro¬®bner basis G. By the division algo-
rithm described in Subsection 2.1.2, u¬Ø1(t) is the remainder of u1(t) divided by g11(t) with
quotient q1(t) whose degree deg q1(t) ‚â§ k1 ‚àí 1. This is similar to the function of the sys-
tematic encoder for a classical cyclic code [38] and can be implemented by a divide-by-g11(t)
circuit as shown in Figure 2.1 with i = 1.
In general, during the ith period [n1+¬∑ ¬∑ ¬∑+ni‚àí1+1, n1+¬∑ ¬∑ ¬∑+ni] of times, 2 ‚â§ i ‚â§ ‚Ñì, we need
to calculate the parity-check symbols vij = ‚àíu¬Øij for 0 ‚â§ j ‚â§ ri‚àí1, where u¬Øij is the coefficient
of the ith component u¬Øi(t) of the remainder vector u¬Ø. And by the division algorithm described
22
‚â§ max{k1 ‚àí 2, k2 ‚àí 2, . . . , ki‚àí1 ‚àí 2, ki ‚àí 1}+ (nj ‚àí kj ‚àí 1), (2.13)
since deg gij(t) < deg gjj(t) = nj ‚àí kj for all i < j. Now consider the case of i = m. Since
qm(t) is the quotient of u
‚Ä≤
m(t) = um(t)‚àí
‚àëm‚àí1
i=1 qi(t)gim(t) divided by gmm(t), we have
deg qm(t) = deg u
‚Ä≤
m(t)‚àí deg gmm(t)
‚â§ max{deg um(t), max
1‚â§i‚â§m‚àí1
{deg(qi(t)gim(t))}} ‚àí (nm ‚àí km)
‚â§ max{nm ‚àí 1, max
1‚â§i‚â§m‚àí1
{max{k1 ‚àí 2, k2 ‚àí 2, . . . , ki‚àí1 ‚àí 2, ki ‚àí 1}
+(nm ‚àí km ‚àí 1)}} ‚àí (nm ‚àí km)
= max{k1 ‚àí 2, k2 ‚àí 2, . . . , km‚àí1 ‚àí 2, km ‚àí 1},
where the second inequality is from (2.13). This completes the proof. 2
Now, we let di = max{k1 ‚àí 2, k2 ‚àí 2, . . . , ki‚àí1 ‚àí 2, ki ‚àí 1}, the upper bound of deg qi(t)
in (2.12). Note that di ‚â• ki ‚àí 1. Then the degree of u‚Ä≤i(t) will be upper bounded by
deg u‚Ä≤i(t) = deg qi(t) + deg gii(t) ‚â§ di + ri.
Now the second difficulty can be easily removed by closing the switch, which is controlled
by the signal sci and is initially open, in the divide-by-gii(t) circuit shown in Figure 2.1
at the time t = n1 + ¬∑ ¬∑ ¬∑ + ni‚àí1 ‚àí (di ‚àí ki), just (di ‚àí ki + 1) clocks before the ith period
[n1 + ¬∑ ¬∑ ¬∑ + ni‚àí1 + 1, n1 + ¬∑ ¬∑ ¬∑ + ni] of times. The coefficients of the quotient qi(t) will be
generated during the period [n1 + ¬∑ ¬∑ ¬∑+ ni‚àí1 ‚àí (di ‚àí ki), n1 + ¬∑ ¬∑ ¬∑+ ni‚àí1 + ki]. The switch of
the divide-by-gii(t) circuit will be open again at the time t = n1+ ¬∑ ¬∑ ¬∑+ni‚àí1+ki+1 in order
to produce the remainder u¬Øi(t), i.e., the parity-check symbols.
Next, we deal with the first difficulty, i.e., the synchronization problem. We note that
the coefficient of the term tkj‚àí1 of the quotient qj(t) is generated when the information
symbol uj,nj‚àí1 is input to the encoder at the time t = n1 + ¬∑ ¬∑ ¬∑ + nj‚àí1 + 1. If we were
starting to multiply qj(t) by gji(t), j < i, at the same time as the coefficients of qj(t) begin
to be generated, the coefficient of the term tkj‚àí1+deg gji(t) of the product qj(t)gji(t) would
24
as the quotient output of the division of u‚Ä≤i(t) by gii(t), by a cyclic shift register of length
di + 1 as shown in Figure 2.3. The multiplexer, controlled by the signal mci, in the cyclic
shift register will select its input from the quotient output of the divide-by-gii(t) circuit while
the coefficients of the quotient qi(t) are being generated. After the computation of qi(t) is
completed, the multiplexer will select its input from the cyclic feedback of the shift register.
Now to put Œ¥ij delay elements for the multiply-by-gij circuit, i < j, to delay its input qi(t) is
equivalent to connect the input of the multiply-by-gij circuit to the signal xŒ¥‚Ä≤ij of the cyclic
shift register in Figure 2.3, where Œ¥‚Ä≤ij = Œ¥ij mod (di+1). The serial-in-serial-out architecture
for the systematic encoding of the ith information miniblock ui = (0, . . . , 0, ui,ri, . . . , ui,ni‚àí1)
is shown in Figure 2.4, together with the multiplication circuits for later use. The overall
circuit in Figure 2.4 is called the ith level of the serial-in-serial-out architecture for the
systematic encoding. Since the output vij = uij for ri ‚â§ j ‚â§ ni ‚àí 1 and vij = ‚àíu¬Øij for
0 ‚â§ j ‚â§ ri ‚àí 1, the switch in front of the output vij can be controlled by the same signal sci
used in the divide-by-gii(t) circuit.
idii
qq
,0,
 
1,0, ‚àíirii
uu 
isc
imc
1,, ‚àíii niri
uu 
1,0, ‚àí‚àí‚àí irii
uu 
1,, ‚àíii niri
uu 
1‚àí


)()( 1 tgtq iii + )()( tgtq ii 
isc

‚àí
=
+
1
1
1 (t) )(
i
j
ijj gtq )()(
1
1
tgtq
i
j
jj

‚àí
= 
	
‚àí
=
1
1
)()(
i
j
jij tgtq

 

)()(
1
 1 tgtq
i
j
ijj

=
+ 
=
i
j
jj tgtq
1
)()( 

)( of storage tqi
)(-by-Divide tgii 1,,1,0, ‚àí‚àí iii niririi
vvvv 
)(-by-multiply 1 tgii+ )(-by-multiply tgi
Figure 2.4: The ith level of the serial-in-serial-out architecture for the systematic encoding.
Example 2. Consider again the (64,48) Hermitian code C53 over GF (16) in Example 1.
Since supp(D) is decomposed into six orbits under the action of the œÉ given in (2.9), the
systematic encoder has six levels. The values of di, Œ¥ij and Œ¥
‚Ä≤
ij are given in Table 2.1. A
mark √ó in Table 2.1 represents a value that is not used. Since g56(t) in the Gro¬®bner basis G
26
Consider the ith level of the proposed serial-in-serial-out architecture. As can be seen from
Figure 2.4, the numbers of memory elements needed in the divide-by-gii(t) circuit, in the
(‚Ñì‚àí i) multiply-by-gij(t) circuits, i < j ‚â§ ‚Ñì, and in the (cyclic) shift-register storage for qi(t)
are ri,
‚àë‚Ñì
j=i+1 deg gij(t), and at most (di + 1) respectively. Thus, the total number Dm of
memory elements for the systematic encoding of Hermitian code Cm is
Dm ‚â§
‚Ñì‚àë
i=1
ri +
‚Ñì‚àí1‚àë
i=1
‚Ñì‚àë
j=i+1
deg gij(t) +
‚Ñì‚àí1‚àë
i=1
(di + 1)
‚â§ (n‚àí k(m)) +
‚Ñì‚àë
j=2
(j ‚àí 1)max(nj ‚àí kj ‚àí 1, 0)
+
‚Ñì‚àí1‚àë
i=1
(max{k1 ‚àí 2, k2 ‚àí 2, ¬∑ ¬∑ ¬∑ , ki‚àí1 ‚àí 2, ki ‚àí 1}+ 1), (2.15)
since
‚àë‚Ñì
i=1 ri = r(m) = n ‚àí k(m), deg gij(t) < deg gjj(t) = rj = nj ‚àí kj, i < j, and from
Theorem 1. Similarly, the total number Mm of constant multipliers needed in the proposed
architecture for Hermitian code Cm is
Mm ‚â§
‚Ñì‚àë
i=1
ri +
‚Ñì‚àí1‚àë
i=1
‚Ñì‚àë
j=i+1
(deg gij(t) + 1)
‚â§ (n‚àí k(m)) +
‚Ñì‚àë
j=2
(j ‚àí 1)(nj ‚àí kj). (2.16)
From (2.15) and (2.16), it can be seen that Dm and Mm increase as ‚Ñì increases in general.
Thus to construct a systematic encoder with less complexity, we tend to use a permutation
which results in the minimum number of orbits in the decomposition of supp(D) among all
possible permutations.
Note that for any Hermitian code Cm over GF (q
2), there always exists a permutation œÉ
œÉ :
{
x 7‚Üí Œ±x
y 7‚Üí Œ±q+1y,
(2.17)
where Œ± is a primitive element in GF (q2) and Œ±q+1 is a primitive element in the subfield
GF (q), by taking Œ¥ = ¬µ = 0 and «´ = Œ± in (2.2). The permutation œÉ defined in (2.17)
28
by
Dm ‚â§ (n‚àí k(m)) +
‚àë
j=2,¬∑¬∑¬∑ ,q+2; nj‚àíkj>0
(j ‚àí 1)(nj ‚àí kj ‚àí 1)
+
‚àëq+1
i=1 max{k1 ‚àí 2, ki ‚àí 1}+ (q + 1)
‚â§ (n‚àí k(m)) +
‚àëq+2
j=2(j ‚àí 1)(nj ‚àí kj) + (q + 1)(k1 ‚àí 1) + (q + 1)
‚â§ (n‚àí k(m)) + (q + 1)(n‚àí k(m)‚àí r1) + (q + 1)k1
= (q + 2)(n‚àí k(m)) + (q + 1)(k1 ‚àí r1),
(2.19)
where we have used Theorem 2 in the first two inequalities, and from (2.16), the total number
Mm of finite-field constant multipliers can be further upper-bounded by
Mm ‚â§ (n‚àí k(m)) +
‚àëq+2
j=2(j ‚àí 1)(nj ‚àí kj)
‚â§ (n‚àí k(m)) + (q + 1)(n‚àí k(m)‚àí r1)
= (q + 2)(n‚àí k(m))‚àí (q + 1)r1.
(2.20)
In general, with the permutation in (2.17) and for high rate Hermitian codes Cm with n = q
3
and m = q3 ‚àí O(q2) (and then k(m) = m + 1 ‚àí q(q+1)
2
= q3 ‚àí O(q2)), the upper bounds of
Dm, Mm, D
‚Ä≤
m, M
‚Ä≤
m in (2.19), (2.20) and (2.18) are of the order
Dm = O(q
3) = D‚Ä≤m, and Mm = O(q
3)‚â™M ‚Ä≤m = O(q
5).
Hence the longer high rate Hermitian codes are, the greater the reduction of the space
complexity is in the proposed hardware architecture for a systematic encoder.
30
10 20 30 40 50 60 70
0
200
400
600
800
1000
1200
m = 12 to 63 
co
m
pl
ex
ity
M
m
,
M
m
D
m
,
D
m
Figure 2.6: Comparison of hardware complexity between the proposed architecture and the
brute-force matrix multiplication for a serial-in-serial-out systematic encoder of a Hermitian
code Cm over GF (4
2) with 12 ‚â§ m ‚â§ 63.
32
3.1 Left-column Replacement Algorithm
In this section, we will present a basic type of column operations performed in the funda-
mental iterative algorithm (FIA) [17] or in [24] for determining the linear dependency of a
column on its previous (i.e. left) columns in a matrix. Related concepts and useful results
will be further developed.
To determine the linear dependency of a column on its previous columns in a given matrix
A, a restricted Gaussian elimination on columns of the matrixA can be performed as follows.
For each non-zero column aj of the matrix A, we subtract a multiple of a previous column
av, v < j, of the matrix A from column aj to eliminate the leading (i.e. the topmost)
non-zero entry of column aj to lower down the position of the leading non-zero entry of the
newly resulted column, if possible. More precisely, if the leading non-zero entry of the jth
column aj is the ith entry ai,j and there is a previous column av, v < j, which has a leading
non-zero entry ai,v also at the ith position, then we replace column aj by aj ‚àí (ai,j/ai,v)av.
The replaced column aj ‚àí (ai,j/ai,v)av is either a zero vector or has a leading non-zero
entry below the ith position. For convenience, we will call the above operation a left-column
replacement operation and denote it by
aj ‚Üê aj + Œ≤av with v < j, (3.1)
where Œ≤ is a constant.
A matrix A‚Ä≤ is called a left-reduced matrix of A if it is obtained by applying a series of
left-column replacement operations in (3.1) on the matrix A and the leading non-zero entry
of each column of the matrix A‚Ä≤, if exists, cannot be further eliminated by any left-column
replacement operation. The leading non-zero entry of a column in a left-reduced matrixA‚Ä≤ is
called a pivot. Note that we have restricted ourselves to left-column replacement operations
in applying Gaussian elimination on columns of the matrix A, i.e. without any column
exchange and without any column replacement by a multiple of a right column.
34
defined by
Soi =
n‚àë
k=1
ekfoi(Pk), ‚àÄ i. (3.4)
Since
‚àën
k=1 ckfoi(Pk) = 0 for all 1 ‚â§ i ‚â§ m ‚àí g + 1 by the parity-check matrix H in (1.4),
the first m ‚àí g + 1 (one-dimensional) syndromes can be calculated from the received word
r as follows: Soi =
‚àën
k=1 rkfoi(Pk), ‚àÄ 1 ‚â§ i ‚â§ m ‚àí g + 1, which will be called known (one-
dimensional) syndromes with respect to the received vector r and the parity check matrix
H . Similarly, for each ordered pair {oi, oj} of nongaps at Q, a (two-dimensional) syndrome
Soi,oj with order oi + oj can be defined by
Soi,oj =
n‚àë
k=1
ekfoi(Pk)foj (Pk) =
n‚àë
k=1
ekfoifoj (Pk). (3.5)
As discussed in [9], a (two-dimensional) syndrome Soi,oj may not be equal to the (one-
dimensional) syndrome Soi+oj of the same order. If Soi,oj 6= Soi+oj , Soi,oj is called a consistent
term of Soi+oj and, in this case, Soi,oj can be obtained by a linear combination of Sk‚Äôs with
0 ‚â§ k ‚â§ oi + oj. Consequently, with r and H , Soi,oj ‚Äôs are known only when oi + oj ‚â§ m.
Consider the (m‚àí g+1)√ó (m‚àí g+1) syndrome matrix S = [Soi,oj ]1‚â§i,j‚â§m‚àíg+1. A column
SCoj of the syndrome matrix S is said to be a partial linear combination of its previous
columns up to the first ‚Ñì components if there exists a linear combination coefficient vector
œÉ
(‚Ñì)
j = (œÉ
(‚Ñì)
j,1, œÉ
(‚Ñì)
j,2, . . . , œÉ
(‚Ñì)
j,j ) with œÉ
(‚Ñì)
j,j 6= 0 for the column S
C
oj
such that
j‚àë
k=1
œÉ
(‚Ñì)
j,kSoi,ok = 0, ‚àÄ1 ‚â§ i ‚â§ ‚Ñì. (3.6)
Suppose that all entries of the matrix S are known. The following theorem, whose proof can
be found in [40, Theorem 1], provides the kernel of the algebraic decoding of the AG code
Cm.
Theorem 4. Suppose that the Hamming weight of the error vector e is no greater than
‚åäd
‚àó‚àí1
2
‚åã, where d‚àó = m ‚àí 2g + 2 is the designed minimum distance. If a column SCoj of
the syndrome matrix S is a partial linear combination of its previous columns up to the
36
need to know the exact values of Soi,oj in the matrix S of order less than or equal to m+ g.
Unfortunately, not all of them in the syndrome matrix S are known. As mentioned before,
with the received vector r and the parity check matrix H , the value of the syndrome Soi,oj
in the matrix S is known only if oi + oj ‚â§ m.
3.2.1 An extended syndrome matrix
Define an (m + 1)√ó n matrix X = [Xk,i]0‚â§k‚â§m,1‚â§i‚â§n with Xk,i = foj (Pi), 1 ‚â§ i ‚â§ n, if k is
a nongap oj , less than or equal to m, at Q and Xk,i := ‚ñ≥, 1 ‚â§ i ‚â§ n, if k is a gap at Q.
The symbol ‚Äú‚ñ≥‚Äù stands for an unspecified or blank entry in the matrix X. Let Y be the
diagonal matrix
Y =
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£∞
e1 0 ¬∑ ¬∑ ¬∑ 0
0 e2 ¬∑ ¬∑ ¬∑ 0
...
...
. . .
...
0 0 ¬∑ ¬∑ ¬∑ en
Ô£πÔ£∫Ô£∫Ô£∫Ô£ª
with error values ei‚Äôs on the diagonal. Then an (m+1)√ó (m+1) matrixM = [Mk,l]0‚â§k,l‚â§m
can be defined as
Mk,l =
{ ‚àën
i=1 eifk(Pi)fl(Pi), if both k and l are nongaps at Q,
‚ñ≥, otherwise,
by regarding the matrix M as the product XYXt. Note that if k = oi and l = oj are
nongaps at Q, then we have Moi,oj = Soi,oj . We thus call the matrix M as an extended
syndrome matrix. Each non-blank entry Mi,j , i.e. Mi,j 6= ‚ñ≥, will be called an effective entry
of the matrixM . And if Mi,j is effective, then Mi,j is either a consistent term of Si+j or just
the Si+j. The non-effective data, ‚ñ≥‚Äôs, are not able and not necessary to be determined. Note
that, by deleting all its non-effective entries, the extended syndrome matrixM reduces to the
syndrome matrix S. For example, consider the Hermitian code C23 of length 64 over GF (4
2)
defined by taking B(23) in (1.3) as a standard basis of L(23Q), the extended syndrome
38
first b + 1 columns of the matrix M . Since the extended syndrome matrix M is near in a
Hankel form, the following result is useful.
Lemma 5. Suppose that j and j‚Ä≤ are two nongaps at Q with j‚Ä≤ < j and j‚Ä≤ ‚â° j(mod Œ≥),
where Œ≥ is the smallest nonzero nongap at Q. If the (j‚Ä≤ + 1)th column MCj‚Ä≤, as a whole, is
a linear combination of its previous columns, then the (j + 1)th column MCj is also a linear
combination of its previous columns.
Proof. Since the column MCj‚Ä≤ is a linear combination of its previous columns by the as-
sumption, there exists a vector œÉ
(m)
j‚Ä≤ for the column M
C
j‚Ä≤ such that
j‚Ä≤‚àë
v=0
Mu,vœÉ
(m)
j‚Ä≤,v = 0, for all nongap u, 0 ‚â§ u ‚â§ m, at Q. (3.10)
We note here that if the entryMu,v is equal to‚ñ≥ for some 0 ‚â§ v ‚â§ j‚Ä≤, then, in our convention,
we have œÉ
(m)
j‚Ä≤,v = 0. Thus, we shall formally define the multiplication 0 ¬∑ ‚ñ≥ of 0 and ‚àÜ to be 0
and then the formula in (3.10) is well defined. In fact, with this coefficient vector œÉ
(m)
j‚Ä≤ , the
range of u, 0 ‚â§ u ‚â§ m, in (3.10) can be extended to all u ‚â• 0 by the Remark after Theorem
4. Because the basis B(m) is standard and then satisfies (1.2), we have
Mu,v =
n‚àë
i=1
eifu(Pi)fv(Pi) =
n‚àë
i=1
ei(fu‚àíŒ≥fŒ≥)(Pi)fv(Pi)
=
n‚àë
i=1
eifu‚àíŒ≥(Pi)(fvfŒ≥)(Pi) =
n‚àë
i=1
eifu‚àíŒ≥(Pi)fv+Œ≥(Pi) =Mu‚àíŒ≥,v+Œ≥ ,
where u ‚àí Œ≥, v are nongaps. Then for column MCj‚Ä≤+Œ≥, we have
‚àëj‚Ä≤
v=0Mu‚àíŒ≥,v+Œ≥œÉ
(m)
j‚Ä≤,v =‚àëj‚Ä≤
v=0Mu,vœÉ
(m)
j‚Ä≤,v = 0, for all nongaps u ‚àí Œ≥, 0 ‚â§ u ‚àí Œ≥ ‚â§ m, by (3.10). This implies that
the column MCj‚Ä≤+Œ≥ is a linear combination of its previous columns in the matrix M if the
column MCj‚Ä≤ is. Since j = j
‚Ä≤ + ‚ÑìŒ≥ for some ‚Ñì ‚â• 1 and by induction, the column MCj is a
linear combination of its previous columns in the matrix M .
Suppose that there exists a coefficient vector œÉ
(a‚àí1)
b for the non-blank column M
C
b such
that
b‚àë
j=0
Mi,jœÉ
(a‚àí1)
b,j = 0, if 0 ‚â§ i ‚â§ a‚àí 1; but
b‚àë
j=0
Ma,jœÉ
(a‚àí1)
b,j = Da,b 6= 0. (3.11)
40
of Da,b and Da‚àíŒ≥,b+Œ≥ as well as their corresponding coefficient vector œÉ
(a‚àí1)
b and œÉ
(a‚àíŒ≥‚àí1)
b+Œ≥ .
If a ‚àí Œ≥ is a gap at Q, then the (a ‚àí Œ≥ + 1)th row MRa‚àíŒ≥ of the matrix M is a blank row.
Hence, we only know that with the coefficient vector œÉ
(a‚àíŒ≥‚àí1)
b+Œ≥ = (0
Œ≥ ,œÉ
(a‚àí1)
b ), the (b+Œ≥+1)th
column MCb+Œ≥ of the matrix M is a partial linear combination of its previous columns at
least up to the first a‚àí Œ≥ + 1 components. Hence, we have the following result.
Lemma 6. Suppose that a‚àíŒ≥ is a nongap at Q and there exists a nonzero discrepancy Da,b
at the position (a, b) of the matrix M with respect to the coefficient vector œÉ
(a‚àí1)
b . Then
the coefficient vector œÉ
(a‚àíŒ≥‚àí1)
b+Œ≥ = (0
Œ≥,œÉ
(a‚àí1)
b ) gives a nonzero discrepancy Da‚àíŒ≥,b+Œ≥ at the
position (a‚àí Œ≥, b+ Œ≥) of the matrix M and, moreover, Da‚àíŒ≥,b+Œ≥ = Da,b.
If the nonzero discrepancy Da,b cannot be reduced to zero by the left-column replacement
algorithm, then we say that there is a pivot position, at the (a, b)-th position of the matrix
M , on the column MCb . And, by Lemma 3, if the column M
C
b has a pivot position, then it
is linearly independent of its previous columns. Note that for each r, m + 1 ‚â§ r ‚â§ m + g,
the Sr-slanted diagonal does not cross the first r ‚àím columns in the matrix M , however,
we shall regard every entry in these r ‚àím columns as being above the Sr-slanted diagonal.
Lemma 7. Given an r, 0 ‚â§ r ‚â§ m+ g, and if the non-blank column MCj of the matrix M
has a pivot position on or above the Sr-slanted diagonal, then every non-blank column M
C
j‚Ä≤
of the matrixM to the left of the column MCj with j
‚Ä≤ ‚â° j(mod Œ≥) also has a pivot position
on or above the Sr-slanted diagonal.
Proof. If j = 0, then we have done. Suppose that j > 1 and that there is a non-blank
column MCj‚Ä≤ , where j
‚Ä≤ < j and j‚Ä≤ ‚â° j(mod Œ≥), which does not have any pivot position on
or above the Sr-slanted diagonal. If r ‚â• (‚åä
m+1
2
‚åã + g) + j‚Ä≤, i.e. the first ‚åäm+1
2
‚åã + g entries
of the column MCj‚Ä≤ are above the Sr-slanted diagonal and (by the assumption) among these
positions there is no pivot at all. Then, by Theorem 4, the column MCj‚Ä≤, as a whole, is a
linear combination of its previous columns. Hence, by Lemma 5, the column MCj is also
a linear combination of its previous columns and then does not have any pivot position,
42
have Œ≥ = 4, ‚Ñì0 = 0, ‚Ñì1 = 5, ‚Ñì2 = 10, ‚Ñì3 = 15. Then, without loss of generality, we shall assign
Lr(i) = ‚Ñìi if r ‚â§ ‚Ñìi for the initial condition.
Since the column MCLr‚àí1(i) of the matrix M does not have a pivot position on or above
the Sr‚àí1-slanted diagonal which is trivially above the Sr-slanted diagonal, we have
Lr‚àí1(i) ‚â§ Lr(i), ‚àÄ r, and 0 ‚â§ i ‚â§ Œ≥ ‚àí 1. (3.17)
By the definition of Lr(i) and with the Lr(i) > ‚Ñìi assumed, the column M
C
Lr(i)‚àíŒ≥ is the
rightmost column such that its column index satisfies (Lr(i) ‚àí Œ≥) ‚â° i(mod Œ≥) and it has
a pivot position on or above the Sr-slanted diagonal. Then, by Lemma 7, each non-blank
column MCj , where j < Lr(i) ‚àí Œ≥ and j ‚â° i(mod Œ≥), of the matrix M also has a pivot
position on or above the Sr-slanted diagonal. Consequently, we have the following result.
Lemma 8. For an r, 0 ‚â§ r ‚â§ m+ g, an i, 0 ‚â§ i ‚â§ Œ≥‚àí1, and with Lr(i) > ‚Ñìi assumed, there
are exactly (Lr(i) ‚àí ‚Ñìi)/Œ≥ pivot positions of the matrix M within the different non-blank
columns MCj ‚Äôs, where j ‚â§ Lr(i) ‚àí Œ≥ and j ‚â° i(mod Œ≥). And, in particular, there are in
different non-blank rows.
3.4 The Decoding Algorithm
Recall that, by the definition of Lr‚àí1(i), the columnM
C
Lr‚àí1(i) of the matrixM , 0 ‚â§ i ‚â§ Œ≥‚àí1,
does not have a pivot position on or above the Sr‚àí1-slanted diagonal. Thus, there is a
coefficient vector œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
such that the columnMCLr‚àí1(i) is a partial linear combination
of its previous columns up to the entry Mr‚àí1‚àíLr‚àí1(i),Lr‚àí1(i). To find Lr(i)‚Äôs from Lr‚àí1(i)‚Äôs,
we shall pay our attention to the following columns of the matrix M : MCLr‚àí1(0), M
C
Lr‚àí1(1),
. . ., and MCLr‚àí1(Œ≥‚àí1).
If r ‚àí Lr‚àí1(i) < 0 for some r and i, then we say that the column M
C
Lr‚àí1(i) of the matrix
M is still in the waiting state at the iteration (or step) of r, and we let Lr(i) = Lr‚àí1(i) and
44


 ED
'
Œ≥Œ≥ OEOD' +‚àí 


ED
'

( )



‚àíD
E
h
( )( )




‚àíD
E
O
h
Œ≥

( )‚àíD
E
h
( ) ( ) ( ) ( )( )











EDEDD
E
O
ED
ED
D
E
D
E
'
' +‚àí+‚àí‚àí
‚àí=  hhh
Œ≥
( )Œ≥PRG
DQG


 DDEDEDDD ‚â°+<+‚â•
GLDJRQDO
VODQWHG


 ED
6 +
GLDJRQDO
VODQWHG
ED
6 +
0PDWUL[V\QGURPHH[WHQGHG7KH
Œ≥O
Figure 3.2: A general case of how to reduce the nonzero discrepancy Da,b to zero.
We summarize what we have done in the following lemma.
Lemma 9. Suppose that there are coefficient vectors œÉ
(a‚àí1)
b and œÉ
(a‚Ä≤‚àí1)
b‚Ä≤ for two effective
columns MCb and M
C
b‚Ä≤ of the matrix M , respectively, and in addition, both the next dis-
crepancy Da,b and the next discrepancy Da‚Ä≤,b‚Ä≤ are nonzeros. Suppose moreover that the
indices a, b, a‚Ä≤, and b‚Ä≤ satisfy a‚Ä≤ ‚â• a, a‚Ä≤ + b‚Ä≤ < a+ b, and a‚Ä≤ ‚â° a( mod Œ≥). Then the nonzero
discrepancy Da,b can be reduced to zero by the left-column replacement algorithm and a
coefficient vector œÉ
(a)
b for the column M
C
b of the matrix M can be obtained by updating
the coefficient vector œÉ
(a‚àí1)
b as shown in (3.19).
Let √óRr (i), initially set to ‚àû, denote the row-index of the pivot position of the column
MCLr‚àí1(i) of the matrix M for each r, 0 ‚â§ r ‚â§ m+ g. Note that if, within the iteration of r
in the left-column replacement algorithm, the column MCLr‚àí1(i) of the matrix M does have
a pivot position, then
√óRr (i) = r ‚àí Lr‚àí1(i). (3.20)
46
√óRs (u) and then we have ‚Ñì
√ó
i =
√óRr (i)‚àí√ó
R
s (u)
Œ≥
. (Note that √óRs (u) must be less than
√óRr (i), otherwise the nonzero discrepancy at the pivot position (√ó
R
r (i), Lr‚àí1(i)) would
be reduced to zero by the nonzero discrepancy at the pivot position (√óRs (u), Ls‚àí1(u)),
a contradiction.)
2. If there is no s, u with s < r and 0 ‚â§ u ‚â§ Œ≥ ‚àí 1 such that √óRs (u) 6= ‚àû and √ó
R
s (u) ‚â°
√óRr (i)(mod Œ≥), then ‚Ñì
√ó
i is the first integer such that √ó
R
r (i) ‚àí ‚Ñì
√ó
i Œ≥ is a gap at Q or
√óRr (i)‚àí ‚Ñì
√ó
i Œ≥ < 0.
Then, we have Lr(i) = Lr‚àí1(i) + ‚Ñì
√ó
i Œ≥. Moreover, by Lemma 6 and Lemma 9, a coefficient
vector œÉ
(r‚àíLr(i))
Lr(i)
for the column MCLr(i) can also be obtain by updating the coefficient vector
œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
by
œÉ
(r‚àíLr(i))
Lr(i)
= (0‚Ñì
√ó
i Œ≥ ,œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
)‚àí
Dr‚àíLr‚àí1(i),Lr‚àí1(i)
Ds‚àíLs‚àí1(u),Ls‚àí1(u)
(œÉ
(s‚àí1‚àíLs‚àí1(u))
Ls‚àí1(u)
, 0r‚àís) (3.21)
for the first case, as summarized in Figure 3.3 where √óRs (u) = s ‚àí Ls‚àí1(u) and √ó
R
r (i) =
r ‚àí Lr‚àí1(i), and
œÉ
(r‚àíLr(i))
Lr(i)
=
{
(0‚Ñì
√ó
i Œ≥ ,œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
), if √óRr (i)‚àí ‚Ñì
√ó
i Œ≥ is a gap at Q,
(0Lr(i), 1), if √óRr (i)‚àí ‚Ñì
√ó
i Œ≥ < 0,
(3.22)
for the second case.
With the obtain coefficient vector œÉ
(r‚àíLr(i))
Lr(i)
, the left-column replacement algorithm can
go on one step further from r to r + 1 to examine the next discrepancy at the position
(r + 1‚àí Lr(i), Lr(i)) of the matrix M . If there is one coefficient vector œÉ
(r‚àíLr(i))
Lr(i)
such that
r ‚àí Lr(i) = ‚åä
m+1
2
‚åã + g ‚àí 1, then the algorithm stops. Since the column MCLr(i) is a partial
linear combination of its previous columns up to the first ‚åäm+1
2
‚åã+ g entries and by Theorem
4, the column MCLr(i), as a whole, is a linear combination of its previous columns in the
matrix M and then an error-locator polynomial is determined.
We summarize our decoding algorithm in the following, called Algorithm I.
Algorithm I:
48
Update √óRr (i) = r ‚àí Lr‚àí1(i),
store D√óRr (i),Lr‚àí1(i) and œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
.
If there is at least one pair of s, u with s < r and 0 ‚â§ u ‚â§ Œ≥ ‚àí 1
such that √óRs (u) ‚â• 0 and √ó
R
s (u) ‚â° √ó
R
r (i)(mod Œ≥), then we select
such a pair of s, u with the largest √óRs (u) and then
‚Ñì√ói =
√óRr (i)‚àí√ó
R
s (u)
Œ≥
,
Lr(i) = Lr‚àí1(i) + ‚Ñì
√ó
i Œ≥,
œÉ
(r‚àíLr(i))
Lr(i)
= (0‚Ñì
√ó
i Œ≥ ,œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
)‚àí
Dr‚àíLr‚àí1(i),Lr‚àí1(i)
D
√ó
R
s (u),Ls‚àí1(u)
(œÉ
(s‚àí1‚àíLs‚àí1(u))
Ls‚àí1(u)
, 0r‚àís).
Else if ‚Ñì√ói is the smallest positive integer such that √ó
R
r (i)‚àí ‚Ñì
√ó
i Œ≥
is a gap at Q, then
Lr(i) = Lr‚àí1(i) + ‚Ñì
√ó
i Œ≥,
œÉ
(r‚àíLr(i))
Lr(i)
= (0‚Ñì
√ó
i Œ≥ ,œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
).
Else if ‚Ñì√ói is the smallest positive integer such that √ó
R
r (i)‚àí‚Ñì
√ó
i Œ≥ <
0, then
Lr(i) = Lr‚àí1(i) + ‚Ñì
√ó
i Œ≥,
œÉ
(r‚àíLr(i))
Lr(i)
= (0Lr(i), 1).
Before describing how to treat the unknown syndromes Sm+1, Sm+2, . . . , we consider the
following two examples for the demonstration of the proposed algorithm.
Example 3. Consider a BCH code over GF (24) with the designed error-correcting capability
t = 3, which is the same example taken in [17, Example 3]. Suppose a vector r is received
and the following (one-dimensional) syndromes Sr, 0 ‚â§ r ‚â§ 5, are known:
S0 = Œ±
7, S1 = Œ±
12, S2 = Œ±
6, S3 = Œ±
12, S4 = Œ±
14, S5 = Œ±
14.
And now we have g = 0, and there is no gap at all. Since Œ≥ = 1, the index i in Algorithm I
can be omitted. The execution of the proposed algorithm is briefly described in the following.
For r = 0 , the discrepancy Dr‚àíLr‚àí1,Lr‚àí1 = D0,0 = 1 ¬∑M0,0 = S0 6= 0. This discrepancy
cannot be reduced to zero, hence, we update √óR0 = r ‚àí Lr‚àí1 = 0 and store D0,0 and
œÉ
(‚àí1)
0 = (1). The integer ‚Ñì
√ó = 1 then L0 = L‚àí1 + 1 = 1 and we obtain œÉ
(r‚àíLr)
Lr
=
œÉ
(‚àí1)
1 = (0, 1). There is a pivot at the position (0, 0) of the matrix M .
50
S9 = 0, S10 = Œ±
5, S11 = Œ±
2, S12 = Œ±
4, and S13 = Œ±
5.
Then, the extended syndrome matrix M is
M =
Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ±4 ‚ñ≥ ‚ñ≥ Œ± ‚ñ≥ Œ±3 Œ±6 1 Œ±6 0 Œ±5 Œ±2 Œ±4 Œ±5
‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥
‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥
Œ± ‚ñ≥ ‚ñ≥ Œ±6 ‚ñ≥ Œ±6 0 Œ±5 Œ±2 Œ±4 Œ±5 # # #
‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥ ‚ñ≥
Œ±3 ‚ñ≥ ‚ñ≥ Œ±6 ‚ñ≥ Œ±6 Œ±2 Œ±4 Œ± # # # # #
Œ±6 ‚ñ≥ ‚ñ≥ 0 ‚ñ≥ Œ±2 Œ±4 Œ±5 # # # # # #
1 ‚ñ≥ ‚ñ≥ Œ±5 ‚ñ≥ Œ±4 Œ±5 # # # # # # #
Œ±6 ‚ñ≥ ‚ñ≥ Œ±2 ‚ñ≥ Œ± # # # # # # # #
0 ‚ñ≥ ‚ñ≥ Œ±4 ‚ñ≥ # # # # # # # # #
Œ±5 ‚ñ≥ ‚ñ≥ Œ±5 ‚ñ≥ # # # # # # # # #
Œ±2 ‚ñ≥ ‚ñ≥ # ‚ñ≥ # # # # # # # # #
Œ±4 ‚ñ≥ ‚ñ≥ # ‚ñ≥ # # # # # # # # #
Œ±5 ‚ñ≥ ‚ñ≥ # ‚ñ≥ # # # # # # # # #
Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
where the M5,5 = Œ±
6 6= S10 is a consistent term of S10 as well as M8,5 and M5,8 are two
consistent terms of S13, and # denotes an unknown syndrome.
After executing the proposed algorithm to the iteration r = 13, we have the following
results:
L13(0) = 9, œÉ
(4)
9 = (Œ±
4, 0, 0, Œ±3, 0, 0, 1, 0, Œ±5, 1).
L13(1) = 7, œÉ
(6)
7 = (1, 0, 0, Œ±
2, 0, Œ±6, 0, 1).
L13(2) = 8, œÉ
(5)
8 = (Œ±
6, 0, 0, Œ±2, 0, Œ±5, Œ±, 0, 1).
Moreover, we have stored the following pivots:
(s, u) = (0, 0) with √óR0 (0) = 0, D0,0 = Œ±
4, and œÉ
(‚àí1)
0 = (1).
(s, u) = (6, 0) with √óR6 (0) = 3, D3,3 = Œ±, and œÉ
(2)
3 = (Œ±
4, 0, 0, 1).
(s, u) = (10, 2) with √óR10(2) = 5, D5,5 = Œ±, and œÉ
(4)
5 = (Œ±
1, 0, 0, Œ±1, 0, 1).
(s, u) = (12, 0) with √óR12(0) = 6, D6,6 = Œ±
6, and œÉ
(5)
6 = (Œ±
2, 0, 0, 1, 0, Œ±5, 1).
52
The idea of the proposed decoding algorithm is to iteratively find a coefficient vector œÉ
(a)
b
that solves the submatrixM (a,b) of the extended syndrome matrixM by increasing the order
r, where r = a+b and 0 ‚â§ r ‚â§ m+g. If there is no gap, the proposed algorithm is reduced to
the early stopped Berlekamp-Massey algorithm (ESBM) [24], since the entries of the matrix
M treated in this paper will have the back-shifted property in this case. The structural
properties of such a Hankel-type matrix have been exploited by Liu and Lu in [24]. With
these properties, for any an r, among those Sr‚Äôs lying on the Sr-slanted diagonal, there is
only one Sr being examined by the algorithm. With gaps, however, the back-shifted property
of the elements in the extended syndrome matrix M is ruined by blank columns and blank
rows. However, with the Œ≥-shifted structural properties of the matrix M , our proposed
decoding algorithm, which generalizes the Feng-Rao algorithm, interprets the whole process
of applying the left-column replacement algorithm on the matrix M into a set of Œ≥ parallel
ESBM algorithms. While, the Ko¬®tter‚Äôs algorithm, which is a parallel version of the Sakata
et al.‚Äôs algorithm, is a set of Œ≥ parallel conventional Berlekamp-Massey algorithm.
The proposed decoding algorithm can be regarded as a Gaussian-elimination-type algo-
rithm (and, hence, is simple) and as a Berlekamp-Massey-type algorithm (and, hence, is
efficient) simuiltaneously. We note here that the operations performed in each component
ESBM algorithm are independent of but related with each other. And, for any an r, among
those Sr‚Äôs (or their consistent terms) lying on the Sr-slanted diagonal, there are at most Œ≥
Sr‚Äôs being examined by the algorithm.
3.4.1 Majority Voting Scheme
As mentioned by Feng and Rao in [9], the key problem in decoding AG codes is to find
the real values of the (one-dimensional) syndrome Si for i = m + 1, m + 2, . . . , m + g and
a coefficient vector œÉ
(‚åä(m+1)/2‚åã+g‚àí1)
j which satisfies Theorem 4. Suppose that Algorithm I
has run up to the iteration of r = m. And now, consider the following confronted unknown
entry Mm+1‚àíLm(i),Lm(i) for some i, 0 ‚â§ i ‚â§ Œ≥ ‚àí 1. As mentioned before, if m + 1 ‚àí Lm(i)
54
@r‚àíLr‚àí1(i),Lr‚àí1(i) = @5,9 calculated by (3.23). Since we have a pair (s, u) = (10, 2) such
that √óR10(2) = 5 = r ‚àí Lr‚àí1(i). Hence, the candidate @5,9 is not available.
If i = 1, since r ‚àí Lr‚àí1(i) = 14‚àí 7 = 7 is a nongap at Q, then we may have the candidate
@r‚àíLr‚àí1(i),Lr‚àí1(i) = @7,7 calculated by (3.23). Since r ‚àí Lr‚àí1(i) = 7 ‚â° 1(mod 3) and
there is no a pair (s, u) such that √óRs (u) ‚â° 1(mod 3). Hence the candidate @7,7 is
indeed available. By (3.23), with the coefficient vector œÉ
(6)
7 we have the available
candidate @7,7 = Œ±
3. And by the Œ≥-shifted properties of the matrix M , with the
coefficient vector (0‚ÑìŒ≥,œÉ
(6)
7 ) we may have the available candidate @7‚àí‚ÑìŒ≥,7+‚ÑìŒ≥. Since
both r ‚àí Lr‚àí1(i)‚àí Œ≥ = 7‚àí 3 = 4 and r ‚àí Lr‚àí1(i)‚àí 2Œ≥ = 7‚àí 6 = 1 are gaps at Q, we
concludes that there is only one available candidate, which is related to @7,7, for the
confronted unknown syndrome. We note here that S7,7 6= S14 is a consistent term of
S14.
If i = 2, since r ‚àí Lr‚àí1(i) = 14‚àí 8 = 6 is a nongap at Q, then we may have the candidate
@r‚àíLr‚àí1(i),Lr‚àí1(i) = @6,8 calculated by (3.23). Since we have a pair (s, u) = (12, 0) such
that √óR12(0) = 6 = r ‚àí Lr‚àí1(i). Hence, the candidate @6,8 is not available.
The total number of the available candidates with the same values plays an important role
when the majority voting scheme is proceeded. In [9], Feng and Rao have shown that the total
number of correct candidates for the confronted unknown syndrome is greater than the total
number of the incorrect ones. Now, suppose that the candidate @m+1‚àíLm(i),Lm(i) is available
and there is a pair (s, u) such that √óRs (u) ‚â° m+1‚àíLm(i)(mod Œ≥) and √ó
R
s (u) < m+1‚àíLm(i),
then we will have the total number (m+ 1‚àíLm(i)‚àí√óRs (u))/Œ≥ of available candidates with
the same value @m+1‚àíLm(i),Lm(i). On the other hand, if the candidate @m+1‚àíLm(i),Lm(i) is
available but there is no a pair (s, u) such that √óRs (u) ‚â° m + 1 ‚àí Lm(i)(mod Œ≥), then we
will have the total number 1 + (m + 1 ‚àí Lm(i) ‚àí ‚Ñìj)/Œ≥ of available candidates with the
same value @m+1‚àíLm(i),Lm(i), where we assumed that m + 1 ‚àí Lm(i) ‚â° j(mod Œ≥). With the
consistent term being considered, where we note that there always exists a procedure, for
56
as the total number of nongaps ALr‚àí1(i) at Q in [0, Lr‚àí1(i) ‚àí 1]. The similar consideration
occurs in the calculation of updating coefficient vector as well as in the calculation of available
candidate. For updating coefficient vector, as shown in the Algorithm I, we must first take
one multiplication to calculate the Dr‚àíLr‚àí1(i),Lr‚àí1(i)/D√óRs (u),Ls‚àí1(u) and then multiply it to the
stored vector œÉ
(s‚àí1‚àíLs‚àí1(u))
Ls‚àí1(u)
. Hence, it requires totally ALs‚àí1(u) + 1 multiplications. Thus,
for the case of a specified i in the iteration of r, 0 ‚â§ r ‚â§ m, it requires totally at most
ALr‚àí1(i) + ALs‚àí1(u) + 1 ‚â§ 2ALr‚àí1(i) multiplications.
As described in Section 3.4, the proposed algorithm can be considered as Œ≥ independent (or
parallel) ESBM algorithms. In [24], Liu and Lu have shown that the worst case of the ESBM
algorithm for the decoding t-error-correcting BCH codes is that all the t pivot positions are
lying on the diagonal of the considered syndrome matrix. Since the proposed algorithm, the
Algorithm I, is just the extension of the ESBM algorithm to examine the extended syndrome
matrixM , consequently, the worst case of the Algorithm I is also that all the pivot positions
are lying on the diagonal line of the matrix M .
In the following, we will first consider the computation complexity of each separated ESBM
algorithm for a specified i, 0 ‚â§ i ‚â§ Œ≥ ‚àí 1, and then the total complexity of the proposed
algorithm. Recall that, if r ‚àí Lr‚àí1(i) < 0, then the algorithm is still in the waiting state.
Hence, we shall consider the range of r, rather than 0 ‚â§ r ‚â§ m + g, to be ‚Ñìi ‚â§ r ‚â§ m + g.
For the simplicity, we shall suppose that m ‚â• 4g ‚àí 1.
Now, let‚Äôs consider the case of i = 0. Since ‚Ñì0 = 0 obviously, hence there are totallym+g+1
iterations for examining the first m+ 1 known entries and the following g unknown entries.
For the worst case of the proposed algorithm, there are the following ‚ÑìŒ≥0 + 1 pivot positions
occurred: (0, 0), (Œ≥, Œ≥), . . . , (‚ÑìŒ≥0Œ≥, ‚Ñì
Œ≥
0Œ≥), which are on the diagonal, where ‚Ñì
Œ≥
0 = ‚åä(t+g)/Œ≥‚åã with
t = ‚åä(m ‚àí 2g + 1)/2‚åã. Since there is a pivot at the position (0, 0), then we have L0(0) = Œ≥.
By the Algorithm I, for the iterations from r = 1 through r = Œ≥‚àí1, we have Œ≥‚àí1 steps with
r‚àíLr‚àí1(i) < 0, i.e. there are Œ≥‚àí1 steps in the waiting state. Suppose that there are B‚ÑìŒ≥0Œ≥ gaps
58
then he must determine the real values of the unknown syndromes from Sm+1 to Sm+2g+2.
A rough bound of the ALr(i), m + 1 ‚â§ r ‚â§ m + 2g + 2, is that ALr(i) ‚â§ m ‚àí g. Then, the
total complexity of the proposed algorithm for examining unknown syndromes is at most
Œ≥(2g + 2)2(m‚àí g)(‚â§ O(Œ≥m2)).
Altogether, the complexity of the proposed algorithm is in the order O(Œ≥m2) ‚âÉ O(Œ≥n2),
since m ‚âÉ n for a long code, where n is the code length. From the upper bound, we can show
that our algorithm as efficient as the Ko¬®tter‚Äôs algorithm. Moreover, by storing the nonzero
discrepancy D√óRs (u),Ls‚àí1(u) together with the coefficient vector œÉ
(s‚àí1‚àíLs‚àí1(u))
Ls‚àí1(u)
, our algorithm is
superior for saving computation complexity. We note here that, how good the complexity is
depends on the code construction. For example, in the case of one-point Hermitian code, we
have Œ≥ = n1/3. Then the complexity of the proposed algorithm for the decoding of one-point
Hermitian code is O(n7/3). On the other hand, let‚Äôs consider the curve [42] in the affine
space GF (q)3, where q = k2, defined by yk+1 = xk + x and zk+1 = ‚àíxyk ‚àí yxk ‚àí 1.
If k ‚â° 1(mod 3), then the curve has (k2 ‚àí 1)2 rational points over GF (q) and the genus of
the curve g = k3 ‚àí k2 ‚àí k. The pole order of the functions x, y, and z at Q (the point at
the infinity) are (k + 1)2, k(k + 1), and k(k + 2) respectively. Then, we have Œ≥ ‚âÉ n1/2. And
the complexity of the proposed algorithm is O(n5/2).
60
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∏
Ô£∂
Ô£¨Ô£¨
Ô£¨Ô£¨
Ô£¨Ô£¨
Ô£¨Ô£¨
Ô£¨Ô£¨
Ô£¨Ô£¨
Ô£¨Ô£¨
Ô£¨Ô£¨
Ô£¨Ô£¨
Ô£¨Ô£¨
Ô£≠
Ô£´
‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ‚àÜ
‚àÜ‚àÜ‚àÜ











#
#
#
#
#
#
#

Œ±
Œ±
Œ±
Œ±Œ±
Œ±
Œ±Œ±Œ±
Œ±Œ±Œ±
Œ±Œ±Œ±Œ±
Œ±Œ±Œ±Œ±Œ±Œ±
Œ±Œ±Œ±Œ±Œ±Œ±Œ±
Œ±Œ±Œ±Œ±Œ±Œ±Œ±Œ±Œ±
GLDJRQDO
VODQWHG

‚àí+P6
( ) ZW +H
 == gP
FDQGLGDWHDYDLODEOHQRQ#
√ì
FDQGLGDWHDYDLODEOH
g+Ô£∫Ô£ª
Ô£∫Ô£ØÔ£∞
Ô£Ø +

P
Figure 3.4: An illustrative example to compare the proposed algorithm with the Ko¬®tter‚Äôs
algorithm.
62
4.1.1 Left-column Replacement Algorithm
Given a nonzero column aCj of a matrixA, the idea of the left-column replacement algorithm
is to subtract a multiple of a previous (i.e. left) column aCv , v < j, to eliminate the leading
(i.e. the topmost) non-zero entry of column aCj and thus to lower down the position of the
leading non-zero entry of the newly resulted column, if possible. If the leading nonzero entry
of aCj is the ith entry ai,j and there exits a previous column a
C
v with v < j and nonzero
leading entry ai,v, then we replace column a
C
j by a
C
j ‚àí (ai,j/ai,v)a
C
v . For convenience, we
denote the above operation by
aCj ‚Üê a
C
j + Œ≤a
C
v with v < j (4.1)
,where Œ≤ is a constant, and call it a left-column replacement operation. After applying a
series of left-column operations on a matrix A, a matrix A‚Ä≤ is called a left-reduced matrix
of A if the leading nonzero entry of each column of A‚Ä≤, if exists, cannot be further zeroed
out by any left-column operation. The leading nonzero entry of a column is called a pivot.
Note that all pivots must in different rows of the matrix A‚Ä≤.
Let a‚Ä≤Cj be the jth column of a left-reduced matrix A
‚Ä≤. Then a‚Ä≤Cj is resulted from a linear
combination of the first j columns aCv , 1 ‚â§ v ‚â§ j, of the original matrix A, i.e. there exists
a coefficient vector Œ≤j = (Œ≤j,1, ¬∑ ¬∑ ¬∑ , Œ≤j,j‚àí1, 1) satisfying
j‚àí1‚àë
v=1
Œ≤j,va
C
v + a
C
j = a
‚Ä≤C
j . (4.2)
From (4.2), we can see that if a‚Ä≤Cj is a zero vector, then a
C
j is a linear combination of its
previous columns in A. Otherwise, a‚Ä≤Cj must has a pivot at the ith coordinate for some i.
Since a‚Ä≤u,j = 0 for all 1 ‚â§ u ‚â§ i, we have
au,j = ‚àí
j‚àí1‚àë
v=1
Œ≤j,vau,v, 1 ‚â§ u ‚â§ i‚àí 1
from (4.2). Thus the column aCj is a partial linear combination of its previous columns up
64
(0, ¬∑ ¬∑ ¬∑ , 0Ô∏∏ Ô∏∑Ô∏∑ Ô∏∏
Œ≥
), gives a nonzero discrepancy Da‚àíŒ≥,b+Œ≥ at the position (a‚àí Œ≥, b+ Œ≥) of the matrix
M . Moreover, Da‚àíŒ≥,b+Œ≥ = Da,b.
Proof. Please see the paragraph before Lemma 6.
We say that there is a pivot at the position (a, b) in the column MCb of matrix M if the
nonzero discrepancy Da,b cannot be reduced to zero by the left-column replacement algo-
rithm. By Lemma 11, if the columnMCb has a pivot position, then it is linearly independent
of its previous columns. Let Sr-slanted diagonal in the matrix M be the slanted diago-
nal where all non-blank entries Sr‚Äôs or its consistent terms reside. Notice that for each r,
m + 1 ‚â§ r ‚â§ m + g, the Sr-slanted diagonal does not across the first r ‚àí m columns in
the matrix M . However, we regard every entry in these r ‚àím columns as being above the
Sr-slanted diagonal. The following lemmas are bases of the PESBM algorithm.
Lemma 13. Given an r, 0 ‚â§ r ‚â§ m+ g, and if the effective column MCb of the matrix M
has a pivot position on or above the Sr-slanted diagonal, then every non-blank column M
C
b‚Ä≤
of the matrixM to the left of the column MCb with b
‚Ä≤ ‚â° b(mod Œ≥) also has a pivot position
on or above the Sr-slanted diagonal.
Proof. Please see the proof of Lemma 7.
Lemma 14. Suppose there are coefficient vectors œÉ
(a‚àí1)
b and œÉ
(a‚Ä≤‚àí1)
b‚Ä≤ for the tow effective
columns MCb and M
C
b‚Ä≤ of the matrix M respectively, and both the discrepancies Da,b and
Da‚Ä≤,b‚Ä≤ are not zeros. Suppose that the indices a, b, a
‚Ä≤, and b‚Ä≤ satisfy a‚Ä≤ ‚â• a, a‚Ä≤+ b‚Ä≤ < a+ b and
a‚Ä≤ = a+ ‚ÑìŒ≥ for some ‚Ñì ‚â• 0. Then the nonzero discrepancy Da,b can be reduced to zero by the
left-column replacement algorithm and the coefficient vector œÉ
(a)
b for the column M
C
b can
be obtained by updating the coefficient vector œÉ
(a‚àí1)
b as shown in the following equation:
œÉ
(a)
b = œÉ
(a‚àí1)
b ‚àí
Da,b
Da‚Ä≤,b‚Ä≤
(0‚ÑìŒ≥ ,œÉ
(a‚Ä≤‚àí1)
b‚Ä≤ , 0
(a+b)‚àí(a‚Ä≤+b‚Ä≤)) (4.4)
Proof. Please see the paragraph before Lemma 9.
66
With the obtained coefficient vector œÉ
(r‚àíLr(i))
Lr(i)
, the algorithm goes one step further from
r to r + 1 to examine the discrepancy of th entry Mr+1‚àíLr(i),Lr(i). If there is one coefficient
vector œÉ
(r‚àíLr(i))
Lr(i)
such that r ‚àí Lr(i) = ‚åä
m+1
2
‚åã + g ‚àí 1, then by Theorem 4, the algorithm
terminates. The PESBM algorithm is summarized in Algorithm I. We shall assume that
every entry in the extended syndrome matrix M is known in Algorithm I.
Algorithm I:
Initial conditions :
For each i, 0 ‚â§ i ‚â§ Œ≥ ‚àí 1, set √óR‚àí1(i) =‚àû, L‚àí1(i) = ‚Ñìi, and œÉ
(‚àí1)
L‚àí1(i)
= (0L‚àí1(i), 1).
For r from 0 to m+ g
For i from 0 to Œ≥ ‚àí 1
If r ‚àí Lr‚àí1(i) < 0 (i.e. the column MCr‚àíLr‚àí1(i) is still in waiting state),
Set Lr(i) = Lr‚àí1(i) and œÉ
(r‚àíLr(i))
Lr(i)
= œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
.
Else (i.e. r ‚àí Lr‚àí1(i) ‚â• 0),
If r ‚àí Lr‚àí1(i) is a gap at Q,
Set Lr(i) = Lr‚àí1(i) and œÉ
(r‚àíLr(i))
Lr(i)
= œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
.
Else (i.e. r ‚àí Lr‚àí1(i) is a nongap at Q),
With œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
, calculate the discrepancy Dr‚àíLr‚àí1(i),Lr‚àí1(i) by (4.3), i.e.
Dr‚àíLr‚àí1(i),Lr‚àí1(i) = œÉ
r‚àí1‚àíLr‚àí1(i)
Lr‚àí1(i)
¬∑ (Mr‚àíLr‚àí1(i),0, . . . ,Mr‚àíLr‚àí1(i),Lr‚àí1(i)),
where the operation ‚Äù¬∑‚Äù denote the inner product of two vectors.
If Dr‚àíLr‚àí1(i),Lr‚àí1(i) = 0 and r ‚àí Lr‚àí1(i) = ‚åä
m+1
2
‚åã+ g ‚àí 1,
the algorithm stops.
Else if Dr‚àíLr‚àí1(i),Lr‚àí1(i) = 0,
Set Lr(i) = Lr‚àí1(i) and œÉ
(r‚àíLr(i))
Lr(i)
= œÉ
(r‚àí1‚àíLr‚àí1(i))
Lr‚àí1(i)
.
Else (i.e. Dr‚àíLr‚àí1(i),Lr‚àí1(i) 6= 0),
68
algorithm, to the matrix M , there is at least one available candidate value. Thus at least
one @m+1‚àíLm(i),Lm(i) calculated by (4.7), where 0 ‚â§ i ‚â§ Œ≥ ‚àí 1, is available.
Supposed that, for a specified i, the @m+1‚àíLm(i),Lm(i) is calculated and it is available.
By Lemma 12, we may have an available candidate @m+1‚àíLm(i)‚àíŒ≥,Lm(i)+Œ≥=@m+1‚àíLm(i),Lm(i),
which resides at the position (m+1‚àíLm(i)‚àí Œ≥, Lm(i)+ Œ≥) of the matrixM , if (1) m+1‚àí
Lm(i)‚àíŒ≥ is a nongap at Q, and (2) there is no pair (s, u) such that √óRs (u) ‚â• m+1‚àíLm(i)‚àíŒ≥
and √óRs (u) ‚â° m+1‚àíLm(i)(mod Œ≥). In [9], Feng and Rao have proved that the total number
of correct candidates for the confronted unknown syndrome is greater than the total number
of incorrect ones. Thus with the consistent term being considered, the real value of the
confronted unknown syndrome can be determined by the majority voting scheme(MVS).
Once the value of Sm+1 or its consistent term is obtained, Algorithm I can then go one step
further until a coefficient œÉ
(‚åäm+1
2
‚åã+g‚àí1)
j for some j is found. With these data, an error-locator
polynomial can then be found.
4.2 A Systolic Array Implementation
As discussed in Section 3.2, to determine an error-locator polynomial, it is necessary to find
a set of linearly dependent columns in the extended syndrome matrix M . If the Hamming
weight wt(e) of an error pattern is not greater then the error capacity t, the rank of M is
less or equal to t. It seems that only t+ 1 columns of M should be examined. By Theorem
4, for finding an error-locator polynomial, it is necessary to know the value of each syndrome
Sr, 0 ‚â§ r ‚â§ m + g. However, the syndromes of order m + 1 to m + g are unknown. Thus
we need consider all columns in M for obtaining the exact value of the syndrome Sr with
0 ‚â§ r ‚â§ m+ g through majority voting.
70
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 






































































																	
																	
																	
																

															

															
															
														

													

												

												
											

									

								

							

				

			






































Figure 4.1: The extended syndrome matrix M of the Hermitian code.
S0 = 0 but S4 6= 0, we have √ó(0) = 4. The non-local connections can be removed by
adding transmittent variables m
(0)
oi,oj , where m
(0)
oi,j = m
(0)
oi‚àí1,oj for all oi, oj with 5 ‚â§ oi ‚â§ 17
and 4 ‚â§ oj ‚â§ 23, and m
(0)
0,oj
= m
(0)
4,oj
for all oj with 4 ‚â§ oj ‚â§ 23. Projecting out the i-axis
of the graph will produce a one-dimensional array, which corresponds to the first row of the
two-dimensional array in Figure 4.3, for the first elimination step.
Observe that at step k, where k is a nongap, we have a
(k)
i,j = a
(k‚àí1)
i,j if i < √ó(k), since all the
entries a
(k‚àí1)
i,j are zero for all i < √ó(k). Thus at step k, a graph identical to that in Figure
4.2(a), but of size ( ‚åäm+1‚åã
2
+1‚àík)√ó(m+1‚àíg‚àík), can be developed. A three-dimensional data
dependency graph is then produced by replicating and connecting subgraphs of appropriate
size in the k direction. The shape of the space is shown in Figure 4.2(b). By projecting out
the i-axis in the three-dimensional data dependency graph, a two-dimensional systolic array
is then established in Figure 4.3. The two-dimensional systolic array in Figure 4.3 is almost
72
Two kinds of processing elements, named PE cells and D cells, are required. Each D cell,
which corresponds to a blank column in M , contains only delay units. A PE cell, which
corresponds to an effective column inM , deals with the left-column replacement operations
related to the corresponding column. The functions of a PE cell will be discussed later. No-
tice that all syndromes lying on the same slanted diagonal in M , i.e. with the same order,
will be examined at the same step in the proposed one-dimensional array.
Now consider the data dependencies associated with the unknown syndrome Sr, for r =
m+ 1, ¬∑ ¬∑ ¬∑ , m+ g. Denote the first unknown syndrome at the position (r‚àíLr‚àí1(i), Lr‚àí1(i))
of M by the symbol @. We first assume that the value Sr is known and equal to @. As we
can see in Algorithm I, if the column MCLr‚àí1(i) is a partial linear combination of its previous
columns up to its first r ‚àí 1 ‚àí Lr‚àí1(i) components, the PESBM algorithm intends to zero
out the data discrepancy Dr‚àíLr‚àí1(i),Lr‚àí1(i) by the left-column operation with the calculated
value @r‚àíLr‚àí1(i),Lr‚àí1(i). If the candidate @ is available, then we have @ = @r‚àíLr‚àí1(i),Lr‚àí1(i).
The value @r‚àíLr‚àí1(i),Lr‚àí1(i) will then be passed to the majority voting scheme(MVS) for
determining the real value of the first confronted syndrome. Thus the data dependencies
of computations of an available candidate is the same as the Gaussian elimination without
column exchange. Therefore, the one-dimensional systolic array in Figure 4.3 can be utilized
for the implementation of the PESBM algorithm. Notice that by inserting the blank entries
in M and D cells in the one-dimensional array, syndromes with the same order will be
examined simultaneously by the array. Thus the available candidate values for the first
confronted syndrome will be calculated and passed to the MVS at the same time.
4.2.2 Reduction of Hardware Complexity
Since the syndrome matrix S and the extended syndrome matrix M are symmetric and
the left-column replacement algorithm is a type of restricted Gaussian elimination, the sym-
metry property described in the last chapter can be used in our implementation for deter-
mining the unknown syndromes and thus reducing hardware complexity. Moreover, as we
74
instead of examining each column where the syndrome Sr (or its consistent term) lies on,
the PESBM algorithm (Algorithm I) only inspects the column MCLr(i) for each i = 0 to Œ≥‚àí1.
Consequently, only Œ≥ discrepancies and at most Œ≥ coefficient vectors should be calculated
and updated at the each step. Thus those PE cells which will never operate at the same
time can shall all operation functions.
To explain the share of cell functions more precisely, we first give some notations. For each
i, 0 ‚â§ i ‚â§ Œ≥ ‚àí 1, define hi to be the largest integer such that i + hiŒ≥ ‚â§ t + ‚åä
g‚àí1
2
‚åã + g. Let
C(i) to be class consisting of the columns in M whose column indices modulo by Œ≥ will be
i, i.e. C(i) = {MCj |j ‚â° i(mod Œ≥)}. Observe that in Algorithm I, for each i, 0 ‚â§ i ‚â§ Œ≥ ‚àí 1,
only one column in the set C(i) will be examined at each step. Thus those PE cells , which
corresponds to the columns in the same class C(i), can shall all operation functions, such
as the calculations of discrepancies and the updates of coefficient vectors. For convenience,
for each i, we extract all operation functions from each PE corresponding to the columns in
C(i) and put them into an cell, named operation element (OE(i)). Then there are totally at
most Œ≥ OE cells and each OE cell can be reused at different steps. The functions of all PE
cells, D cells and OE cells will be further discussed in the next subsection.
4.3 Cell Functions
As described in Subsection 4.2.2, all operation functions in a PE cell are extracted to the
corresponding OE cell. Thus the only remained function in a PE cell is data transmission.
Figure 4.4(a) shows the input and output definitions of the PE cell, named PE(j), which is
the (j + 1)th element in the one-dimensional array and corresponds to the effective column
MCj in M .
As we can see in Figure 4.4(a), two vectors aj and bj , received from the left-hand side, will
be passed immediately to the corresponding OE cell below, say OE(i), where j ‚â° i(mod Œ≥),
for calculating discrepancies and updating of coefficients, and then be pipelined to the next
76
Pass aÀúj and bÀúj to the right-hand side cell.
Since a D cell corresponds to a non-effective column in M , the cell function of a D cell
becomes quite simple. The input and output definitions of a D cell are given in Figure 4.4(b).
When the cell D(j) receives the input vectors a and b in the beginning of a step, it passes
theses vectors immediately to the corresponding OE cell, say OE(i), where j ‚â° i(mod Œ≥),
and then transmit them to its righthand side cell at the end of the step.
Now consider the functions of an OE cell. The input and output definitions of the ith OE
cell, named OE(i), are presented in Figure 4.5. Recall that in Subsection 4.2.2, we define hi
to be the largest integer satisfying i + hiŒ≥ ‚â§ t + ‚åä
g‚àí1
2
‚åã + g for each i, 0 ‚â§ i ‚â§ Œ≥ ‚àí 1. Thus
the cell OE(i) receives the input vectors ci, ci+Œ≥, ¬∑ ¬∑ ¬∑ , ci+hiŒ≥ from the above one-dimensional
array in the beginning of a step and returns the modified data di,di+Œ≥, ¬∑ ¬∑ ¬∑ ,di+hiŒ≥ to the
corresponding PE cells. Observe that if i + hŒ≥ is a gap for some h, OE(i) receives ci+hŒ≥ =
{ai+hŒ≥, bi+hŒ≥} from the cell D(i + hŒ≥). For convenience, we define k(IR)i+hŒ≥ = 1 in OE(i)
and OE(i) does not need to output di+hŒ≥, if i+ hŒ≥ is a gap at Q. To deal with the unknown
syndromes, a set of signals, say @ andAV , is sent from OE(i) to the MVS below. The signal
@ represents the value of an available candidate for the first confronted syndrome (if there
is any) and the vector AV indicates which candidates in C(i) are available. If the candidate
of the first confronted syndrome in the column MCi+hŒ≥ is available, the value of AVi+hŒ≥ is set
to be one; otherwise, zero. After the exact value @MV S of the first confronted syndrome is
determined by MVS, @MV S will be returned to OE(i) from MVS for further processing.
Recall that in Subsection 4.1.3, for each i, we define ‚Ñìi to be the smallest nongap at Q such
that ‚Ñìi ‚â° i(mod Œ≥) . Let h‚Ä≤i be the integer satisfying ‚Ñìi = i+ h
‚Ä≤
iŒ≥. Then the column M
C
i+hŒ≥ is
effective if and only if h ‚â• h‚Ä≤i. As mentioned in Subsection 4.2.2, the cell OE(i), 0 ‚â§ i ‚â§ Œ≥‚àí1,
deals with all operation functions associated with all columns in the class C(i). Thus a set of
internal control signals, denoted by {{wsi+hŒ≥}h‚Ä≤i‚â§i‚â§hi, {Li+hŒ≥}h‚Ä≤i‚â§i‚â§hi, {IMi+hŒ≥}h‚Ä≤i‚â§i‚â§hi, {chki+hŒ≥}0‚â§i‚â§hi
, EnableL,EnableD,EnableœÉ, SelectD, SelectœÉ,œÉ}, are required in each OE(i).
78
EnableœÉ to enable the updating of the coefficient vector and use SelectœÉ to select the input
coefficient vector which should be used in the updating process.
The set of signals {chki+hŒ≥}0‚â§h‚â§hi marks whether a column in M has been checked.
chki+hŒ≥ = 0 means that the column M
C
i+hŒ≥ is not checked yet, while chki+hŒ≥ = 1 means
MCi+hŒ≥ has been checked. Thus the initial value of chki+hŒ≥ is one if 0 ‚â§ h ‚â§ h
‚Ä≤
i, and zero if
h‚Ä≤i < h ‚â§ hi. Notice that in Algorithm I, if a pivot is found in the column M
C
i+q‚Ä≤Œ≥ at step
r, where q‚Ä≤ satisfies i + q‚Ä≤Œ≥ = Lr‚àí1(i), a √ó is marked and the coefficient vector is modified
only on that column and a new value Lr(i) is generated by (4.4) or (4.5). Suppose that
Lr‚àí1(i) = i + h
‚Ä≤Œ≥ and Lr(i) = i + h‚ÄùŒ≥. To simplify the control circuits in our design, the
modified values of √ó, √ó‚àí1 and coefficient vectors will be sent back to all PE(i+hŒ≥)‚Äôs, where
h‚Ä≤ ‚â§ h < h‚Äù. Thus the signal IMi+hŒ≥ (Index of Modification), h
‚Ä≤
i ‚â§ h ‚â§ hi, is required
for indicating whether the modified data is necessary to be sent back to PE(i + hŒ≥). If
IMi+hŒ≥ = 1, then the modified values of √ó, √ó‚àí1 and coefficient vectors will be sent to
PE(i+ hŒ≥); otherwise, IMi+hŒ≥ = 0 and OE(i) bypass the original values to PE(i+ hŒ≥).
As mentioned in Section 3.2, the calculation of consistent terms of syndromes So‚Ñì in S (or
inM) may occur. Once a one-point AG code is constructed, the relation of consistent terms
with syndromes Soi and the locations of consistent terms in S (or inM) can be determined
by the curve and the selected basis of the vector space L(mQ). Let S ‚Ä≤o‚Ñì denote a consistent
term of So‚Ñì , then we have
S ‚Ä≤o‚Ñì = So‚Ñì +
‚Ñì‚àí1‚àë
k=1
Sok . (4.9)
From (4.9), it can be seen that the consistent term S ‚Ä≤o‚Ñì can be obtained by adding the original
syndrome So‚Ñì with some previously appeared syndromes. Suppose that the calculation of
consistent term S ‚Ä≤o‚Ñì occurs in the column M
C
i+hŒ≥ for some h. Then the corresponding previ-
ously appeared syndromes will either arrive from the the top-end of PE(i+ hŒ≥) of pipelined
through its left PE cells. Thus PE(i+ hŒ≥) passes a previously appeared syndrome, say Sok ,
for the calculation of S ‚Ä≤o‚Ñì to OE(i) at the step ok, and Sok will be registered in OE(i) for
80
Else (i.e. ain(Lr‚àí1(i)) is an effective and known syndrome),
For all h, h‚Ä≤i ‚â§ h ‚â§ hi, set
aÀúin(i+hŒ≥) =
{
check ‚àí consistent(ain(i+hŒ≥)), if wsi+hŒ≥ = 1 and E(IR)i+hŒ≥ = 1,
0, otherwise.
Set EnableD = 1.
Calculate the discrepancy D by D = œÉ ¬∑ (ai+h‚Ä≤Œ≥, aÀúin(i+h‚Ä≤Œ≥)), where ‚Äù¬∑‚Äù
means the inner product of tow vectors and (ai+h‚Ä≤Œ≥ , aÀúin(i+h‚Ä≤Œ≥)) is selected
by SelectD.
If D = 0 and IRi+h‚Ä≤Œ≥ = ‚åä
m+1
2
‚åã+ g ‚àí 1,
the algorithm stops.
Else if D = 0,
Set EnableœÉ = 0.
Go to Bypass.
Else
If √ói+h‚Ä≤Œ≥ = 1 (i.e. D can be zeroed out),
Set EnableœÉ = 1.
Update œÉ ‚Üê œÉ ‚àí D((√ó‚àí1)i+h‚Ä≤Œ≥)(œÉ√ói+h‚Ä≤Œ≥ , 0), where (√ó
‚àí1)i+h‚Ä≤Œ≥ and
œÉ√ói+h‚Ä≤Œ≥ are selected by SelectœÉ.
Go to Bypass.
Else (i.e. D can not be zeroed out),
Go to Modify.
Else (i.e. k(IR)j+h‚Ä≤Œ≥ = 0 and ain(j+h‚Ä≤Œ≥) is an unknown syndrome),
If AVi+h‚Ä≤Œ≥ = 1 (i.e. the candidate in M
C
Lr‚àí1(i)
is available),
Set EnableD = 1.
Calculate @ by @ = ‚àíœÉ¬∑(ai+h‚Ä≤Œ≥ , aÀúin(i+h‚Ä≤Œ≥)), where (ai+h‚Ä≤Œ≥ , aÀúin(i+h‚Ä≤Œ≥)) is selected
by SelectD.
Send @ to MVS ‚Üí @MV S.
82
are selected by SelectœÉ.
Go to Bypass.
Else (i.e Li+hŒ≥ = 0 for every h, hi ‚â§ h ‚â§ hi, and every effective column in the class C(i) has
a pivot),
If k(IR)j = 1 (i.e. ain(Lr‚àí1(i)) is a known syndrome),
For all h, h‚Ä≤i ‚â§ h ‚â§ hi, set
aÀúin(i+hŒ≥) =
{
check ‚àí consistent(ain(i+hŒ≥)), if E(IR)i+hŒ≥ = 1,
0, otherwise.
Set EnableD = 0 and EnableœÉ = 0.
Go to Bypass.
Else (i.e. ain(Lr‚àí1(i)) is an unknown syndrome),
Hold and wait for the output @MV S from MVS.
For all h, h‚Ä≤i ‚â§ h ‚â§ hi, set
aÀúin(i+hŒ≥) =
{
check ‚àí consistent(@MV S), if E(IR)i+hŒ≥ = 1,
0, otherwise.
Set EnableD = 0.
Go to Bypass.
Bypass:
Set EnableL = 0.
For each h, h‚Ä≤i ‚â§ h
‚Ä≤ ‚â§ hi, set √óÀúi+hŒ≥ = √ói+hŒ≥, (√óÀú
‚àí1
)i+hŒ≥ = (√ó‚àí1)i+hŒ≥ and œÉÀúi+hŒ≥ =
(œÉi+hŒ≥, 0).
Go to End.
Modify:
Set EnableL = 1.
For each h, h‚Ä≤i ‚â§ h ‚â§ hi,
84
where h‚Ä≤ is an integer such that i+ h‚Ä≤Œ≥ = Lr‚àí1(i). With (4.10), we can see that the enable
signals EnableD, EnableL and EnableœÉ are simple functions of control signals {wsi+hŒ≥},
{E(IR)i+hŒ≥}, {√ói+hŒ≥}, {AVi+hŒ≥} and {Li+hŒ≥}. Therefore, all these enable signals can be
implemented by combinational logic circuits of control signals. Moreover, to determine the
value of AVi+hŒ≥ for some h, we first notice that
AVi+hŒ≥ =
{
1, if h ‚â• h‚Ä≤ and √ói+hŒ≥ = 0,
0, otherwise.
(4.11)
Thus the signal AVi+hŒ≥ can be implemented by simple combinational logic of {chki+hŒ≥} and
{√ói+hŒ≥}. The implementation of other control signals, such as {Li+hŒ≥}, {IMi+hŒ≥} and
{chki+hŒ≥} can be achieved by a finite state machine with states {Li+hŒ≥}. We will discuss
the finite state machine in detail by an example in the later section.
Figure 4.6 and Figure 4.7 demonstrate the two main computation functions with OE(i).
Suppose that the currently used coefficient vector is œÉj . If EnableD = 1, a discrepancy D
or an available candidate value for the first confronted syndrome is calculated by using the
rightmost j ‚àí 1 finite-field multipliers in the circuit in Figure 4.6. If D is nonzero, then the
control signal EnableœÉ is examined to decide whether the currently used coefficient œÉj
needs to be updated by the circuit in Figure 4.7. Observe that when the operation of
updating occurs, then the currently used coefficient œÉj will be substituted according to
(4.4) or (4.5). That is, there exists some h‚Äù such that the content in œÉ should be replaced
by œÉ ‚àíD(√ó‚àí1)i+h‚ÄùŒ≥(œÉi+h‚ÄùŒ≥, 0). The corresponding (√ó‚àí1)i+h‚ÄùŒ≥ and (œÉi+h‚ÄùŒ≥, 0) are selected
by two multiplexers via the signal SelectœÉ. We note that the rightmost component of the
coefficient vector stored in œÉ of an OE cell is designed to be always one. Consequently,
only k ‚àí 1 finite-field multipliers are necessary for the cell OE(i) in Figure 4.6, where k is
an integer such that i+ hiŒ≥ = ok. Also note that the rightmost component of œÉ√ói+hŒ≥ , for all
h, 0 ‚â§ h ‚â§ hi, is either 0 or 1. Therefore, the corresponding multiplication with
D(√ó‚àí1)i+hŒ≥ in the operation of modifying coefficient vector can be implemented by a
multiplexer, as shown in Figure 4.7. Hence only k ‚àí 1 finite-field multipliers are necessary
for the cell OE(i) in Figure 4.7, too. Since the time for the computation of discrepancy (or
86
Table 4.1: The truth table of E(IR)j and k(IR)j for the cell PE(j), for a given Hermitian
code C‚Ñ¶(23Q)
k(IR)j
IRj E(IR)j j = 8 j = 9 j = 10 j = 12 j = 13 j = 14
0 1 1 1 1 1 1 1
1 0 1 1 1 1 1 1
2 0 1 1 1 1 1 1
3 0 1 1 1 1 1 1
4 1 1 1 1 1 1 1
5 1 1 1 1 1 1 1
6 0 1 1 1 1 1 1
7 0 1 1 1 1 1 1
8 1 1 1 1 1 1 1
9 1 1 1 1 1 1 1
10 1 1 1 1 1 1 0
11 0 1 1 1 1 0 0
12 1 1 1 1 0 0 0
13 1 1 1 1 0 0 0
14 1 1 1 0 0 0 0
15 1 1 0 0 0 0 0
16 1 0 0 0 0 0 0
17 1 0 0 0 0 0 0
Since the Hermitian code C‚Ñ¶(23Q) over GF (4
2) has Œ≥ = 4, we can divide the columns in
M into four classes: C(0) = {MC0 ,M
C
4 ,M
C
8 ,M
C
12}, C
(1) = {MC1 ,M
C
5 ,M
C
9 ,M
C
13},
C(2) = {MC2 ,M
C
6 ,M
C
10,M
C
14}, and C
(3) = {MC3 ,M
C
7 ,M
C
11}. For each i, 0 ‚â§ i ‚â§ 3, all
columns in the ith class C(i) can share the same operation functions in the cell OE(i).
Notice that the columns in C(3) are all non-effective. Therefore, OE(3) is empty. Since we
have h0 = 12 = o7, h1 = 13 = o8, and h2 = 14 = o9, thus totally 6 + 7 + 8 = 21 finite-field
multipliers and 4 finite-field inverters are required in our design.
Recall that as mentioned in Section 4.3, each cell OE(i) needs a finite state machine with
states {Li+hŒ≥}h‚Ä≤i‚â§h‚â§hi to implement the function of updating Lr(i) (if necessary) at a
specific step r. To explain the state transition table of the finite state machine in OE(i)
concisely, we introduce a set of intermediary signals {I‚Ñìi+hŒ≥}0‚â§h‚â§hi (Index of ‚Ñì√ó). The
88
Table 4.2: The transition tables of the state machine in OE cells, for a given Hermitian code
C‚Ñ¶(23Q).
L0 L4 L8 L12 I‚Ñì0 I‚Ñì4 I‚Ñì8 I‚Ñì12 LÀú0 LÀú4 LÀú8 LÀú12 Àúchk0 Àúchk4 Àúchk8 Àúchk12 IM0 IM4 IM8 IM12
1000 01‚àó‚àó 0100 1100 1000
001‚àó 0010 1110 1100
0001 0001 1111 1110
0000 0000 1111 1111
0100 001‚àó 0010 1110 0100
0001 0001 1111 0110
0000 0000 1111 0111
0010 0001 0001 1111 0010
0000 0000 1111 0011
0001 0000 0000 1111 0001
0000 0000 0000 1111 0000
(a) The transition table of the state machine in OE(0).
L5 L9 L13 I‚Ñì1 I‚Ñì5 I‚Ñì9 I‚Ñì13 LÀú5 LÀú9 LÀú13 Àúchk1 Àúchk5 Àúchk9 Àúchk13 IM5 IM9 IM13
100 001‚àó 010 1110 100
0001 001 1111 110
0000 000 1111 111
010 0001 001 1111 010
0000 000 1111 011
001 0000 000 1111 001
000 0000 000 1111 000
(b) The transition table of the state machine in OE(1).
L10L14 I‚Ñì2 I‚Ñì6 I‚Ñì10 I‚Ñì14 LÀú10 LÀú14 Àúchk2 Àúchk6 Àúchk10 Àúchk14 IM10 IM14
10 0001 01 1111 10
0000 00 1111 11
01 0000 00 1111 01
00 0000 00 1111 00
(c) The transition table of the state machine in OE(2).
90
              
     
  
     
  
 
  

  
  
  






	















 

 

 

 

 

 

  









	
Ô¨Ä



Ô¨Å
Ô¨Ç

	



Ô¨Å
Ô¨Ç
Ô¨É



Ô¨Å
Ô¨É
Ô¨É





Ô¨É
Ô¨É
Ô¨É






Ô¨É
Ô¨É
Ô¨É


 
!






  



Ô¨É
Ô¨É
Ô¨É







Ô¨É
Ô¨É
Ô¨É
"
#$
Figure 4.3: The data flows of two-dimensional array and one-dimensional array of Gaussian
elimination without column exchange.
92
     


1, ‚àíjjœÉ


1,jœÉ





2, ‚àíjjœÉ



     




	


1‚àí
‚Ä≤‚Ä≤
√óRq











i)( 1‚àí√ó
Œ≥ihi+
‚àí√ó )( 1


)
,0,...,0(
Œ≥‚Ä≥+
√ó
ihi














),0,...,0(
i√ó

Œ≥ihi+√ó

R
ihi

1,
Œ≥‚Ä≥+
√ó
R
j
ihi

2, ‚àí√ó
‚Ä≥+ Œ≥
R
j
ihi

1, ‚àí√ó
‚Ä≥+ Œ≥

Ô¨ÄÔ¨Å




)1,,...,,0,...,0(),0,...,0( 1,1, ‚àí= jjjj œÉœÉÔ¨Ç
Ô¨É


	

 
!

Ô¨Å


"
#
	


#
	



$$#

#
	
 %





Figure 4.7: The circuit for updating coefficient vectors in the cell OE(i).
151617181920212223000000 SSSSSSSSS
& & &&&&&&&&&&&&&
'(
)
*+ ,
)-
+ ,
).
+ ,
)
/+ ,
)0
+ ,
)
1+
,
2334
'(
)5
+
'(
)
6+
'(
)7
+
'(
)
8+
'(
)-
*+
'(
)-.
+
'(
)-
/+
'(
)-5
+
0S 4S 5S 8S 9S 10S 12S 13S 14S‚àÜ ‚àÜ ‚àÜ ‚àÜ ‚àÜ ‚àÜ
9 :
; : <
: = : >? : >>: >@ : >A :
B
(
CDE
B
(
CFE
B
(
C
G
E
B
(
CHE
IJ
K L
MNO
P Q
L
ONRS TU
VW
X
W
Y
Z
[
NO U
L
\
RO
W
M
]
^
U
L
ROM
L_ _L
SNU
F
G
H `
Y a b c d
Figure 4.8: The proposed implementation of the PESBM algorithm via systolic array for the
Hermitian code C‚Ñ¶(23Q).
94
fa,b = x
ayb, i.e.,
Sa,b =
n‚àí1‚àë
i=0
eifa,b(Pi). (5.1)
The order of Sa,b is defined to be the pole order of fa,b at the infinity point P‚àû, which is
equal to aq + b(q + 1). Since cHTm = 0, the syndromes with order no greater than m can be
generated from the received word, i.e., Sa,b =
‚àën‚àí1
i=0 rifa,b(Pi), and those syndromes are
called known syndromes.
An polynomial œÉ(x, y) in GF (q2)[x, y] is called an error-locator polynomial if it satisfies
œÉ(Pij ) = 0, ‚àÄj = 1, . . . , t. For an error pattern e, the set of all error locator polynomials
forms an error locator ideal. With the knowledge of known syndromes and by using the the
Feng-Rao majority voting principle, an error-locator polynomial with least pole order at
P‚àû or a Gro¬®bner basis of the error locator ideal can be calculated. Also, all syndromes of
any order can be determined by using the Feng-Rao majority voting principle.
Under the assumption t ‚â§ t‚àó(m), the error-locator polynomial with least pole order at P‚àû
can be expressed of the form œÉ(x, y) =
‚àët‚àó(m)+1
‚Ñì=1 œÉ‚Ñìfo‚Ñì , where fo‚Ñì is the ‚Ñìth elements in the
basis of L(mP‚àû). In the rest of this paper, we only consider the error-locator polynomial
œÉ(x, y) with least pole order at P‚àû and let Q1, ¬∑ ¬∑ ¬∑ , Qs be the zeros of œÉ(x, y). For
convenience, we call Q1, ¬∑ ¬∑ ¬∑ , Qs potential error points. Notice that the set of error points
{Pi1, ¬∑ ¬∑ ¬∑ , Pit} is contained in {Q1, ¬∑ ¬∑ ¬∑ , Qs} and some values ei corresponding the potential
error point Qi in the error pattern e are zeros. Thus the syndrome Sa,b in (5.1) can be
rewritten as
Sa,b =
s‚àë
i=1
eifa,b(Qi). (5.2)
For a fixed j, 1 ‚â§ j ‚â§ s, a separating function œÉj(x, y) for the potential error point
Qj = (Œ±j, Œ≤j) is a polynomial in GF (q
2)[x, y] with the property
œÉj(Qi) = 0, for all 1 ‚â§ i ‚â§ s and i 6= j,
œÉj(Qj) 6= 0.
(5.3)
96
Theorem 16. Let œÉ(x, y) =
‚àët‚àó(m)+1
‚Ñì=1 œÉ‚Ñìfo‚Ñì be an error-locator polynomial and Qi,
i = 1, . . . , s, be the potential error points on œÉ(x, y). For a fixed potential error point
Qj = (Œ±j, Œ≤j), the quotient polynomial q
(1)
j (x, y) in GF (q
2)[x, y] constructed as
q
(1)
j (x, y) =
œÉ(x, y)Hq(x, Œ≤j)‚àí œÉ(x, Œ≤j)Hq(x, y)
(x‚àí Œ±j)(y ‚àí Œ≤j)
(5.6)
has the property {
ŒΩQj(q
(1)
j ) = ŒΩQj(œÉ)‚àí 1,
ŒΩQi(q
(1)
j ) ‚â• ŒΩQi(œÉ), 1 ‚â§ i ‚â§ s, i 6= j.
(5.7)
5.2 An Algorithm for the Error-value Evaluation of
Hermitian Codes with One Error-Locator
Polynomial
Let œÉ(x, y) =
‚àët‚àó(m)+1
‚Ñì=1 œÉ‚Ñìfo‚Ñì be an error-locator polynomial with least pole order at P‚àû and
Qi, i = 1, . . . , s, be the potential error points on œÉ(x, y). Consider the potential error point
Qi and the local ring OQi(Hq). Since Qi is a zero of œÉ(x, y), œÉ(x, y) is in the maximal ideal
MQi(Hq), i.e., ŒΩQi(œÉ) > 0. Thus from (5.3), we can see that œÉj is a separating function for
the point Qj if and only if
ŒΩQi(œÉj) > 0, for all 1 ‚â§ i ‚â§ s and i 6= j,
ŒΩQj(œÉj) = 0.
(5.8)
Consider the polynomial q
(1)
j (x, y) defined in Theorem 16. Let œÉ
(1)
j (x, y) be the remainder
of q
(1)
j (x, y) divided by Hq(x, y), i.e.,
œÉ
(1)
j (x, y) = q
(1)
j (x, y)‚àíHq(x, y)q(x, y), (5.9)
for some q(x, y) ‚àà GF (q2)[x, y]. Then we have
ŒΩQi(œÉ
(1)
j ) ‚â• min{ŒΩQi(q
(1)
j (x, y)), ŒΩQi(Hq(x, y)q(x, y))}
= min{ŒΩQi(q
(1)
j (x, y), ŒΩQi(Hq(x, y)) + ŒΩQi(q(x, y))},
for all i = 1, . . . , s. Since Hq(x, y) = xq+1 ‚àí yq ‚àí y ‚â° 0 in the function field GF (q2)(x, y),
we have ŒΩQi(Hq(x, y)) =‚àû for all i = 1, . . . , s. From (5.7), we can see that
ŒΩQj(q
(1)
j (x, y)) = ŒΩQj(œÉ)‚àí 1 <‚àû,
98
(b) Calculate
q
(k+1)
j (x, y) =
œÉ
(k)
j (x, y)Hq(x, Œ≤j)‚àí œÉ
(k)
j (x, Œ≤j)Hq(x, y)
(x‚àí Œ±j)(y ‚àí Œ≤j)
and
œÉ
(k+1)
j (x, y) = q
(k+1)
j (x, y) mod (x
q+1 ‚àí yq ‚àí y)
=
‚àë
a,b œÉ
(k+1)
j,a,b x
ayb.
(c) If œÉ
(k+1)
j (Œ±j, Œ≤j) = 0, let k ‚Üê k + 1 and go to step (b).
(d) ej =
‚àë
a,b œÉ
(k+1)
j,a,b
Sa,b‚àë
a,b œÉ
(k+1)
j,a,b
Œ±aj Œ≤
b
j
.
5.3 An Efficient Hardware Architecture for the
Error-Value Evaluation of Hermitian Codes
In this section, we firstly describe the functions of the error-value solver of Hermitian codes
according to Algorithm I. Then we present the main circuits in the error-value solver by
utilizing the Horner‚Äôs loop in the decoding of Reed-Solomon codes. Finally, we propose a
division circuit for calculating œÉ
(k+1)
j (x, y) in Algorithm I via systolic arrays.
5.3.1 Functions of the Error-Value Solver
Assume that all the syndromes Sa,b involved in (5.5) are known by Feng-Rao majority
voting scheme. From Algorithm I, we can see that there are four tables needed for the data
storage in the error-value solver when computing the polynomial œÉ
(k)
j (x, y) =
‚àë
a,b œÉ
(k)
j,a,bx
ayb
in the kth iteration at the potential error position Qj . The first table, called œÉ-table, is
used to store the two-dimensional coefficients œÉ
(k)
j,a,b in the polynomial œÉ
(k)
j (x, y). Since we
need to calculate the polynomials Hq(x, Œ≤j) and œÉ
(k)
j (x, Œ≤j) in step (b) of Algorithm I, two
tables, named Hx-table and œÉx-table, are required to save the coefficients of Hq(x, Œ≤j) and
œÉ
(k)
j (x, Œ≤j). Moreover, we use an additional table, called œÜ-table, to store the temporary
coefficients.
100
Step I : ( set the initial values for a new potential error point QNP )
Load œÉa,b‚Äôs to œÉ-table ( i.e., set the initial polynomial œÉ
(0)
NP (x, y) to be œÉ(x, y)).
Calculate Hq(x, Œ≤NP ) and store the coefficients hNP,a of Hq(x, Œ≤NP ) =
‚àëq+1
a=0 hNP,ax
a
into Hx-table.
Calculate œÉ
(0)
NP (x, Œ≤NP ) and store the coefficients œÉ
(0)
NP,a of œÉ
(0)
NP (x, Œ≤NP ) =
‚àë
a œÉ
(0)
NP,ax
a
into œÉx-table.
Trigger SNI and set NI = 1.
Step II : ( the NI-th iteration when calculating the separation function of the point QNP )
(1) Calculate
œÜ
(NI)
NP (x, y) , œÉ
(NI‚àí1)
NP (x, y)Hq(x, Œ≤NP )‚àí œÉ
(NI‚àí1)
NP (x, Œ≤NP )Hq(x, y)
=
‚àë
a,b œÜ
(NI)
NP,a,bx
ayb,
(5.11)
and store the coefficients œÜ
(NI)
NP,a,b of œÜ
(NI)
NP (x, y) into œÜ-table.
(2) Calculate
q
(NI)
NP (x, y) ,
œÜ
(NI)
NP
(x,y)
(x‚àíŒ±NP )(y‚àíŒ≤NP )
(5.12)
and
œÉ
(NI)
NP (x, y) , q
(NI)
NP (x, y) mod (x
q+1 ‚àí yq ‚àí y)
=
‚àë
a,b œÉ
(NP )
NI,a,bx
ayb,
(5.13)
and update œÉ-table by the coefficients œÉ
(NI)
NP,a,b of œÉ
(NI)
NP (x, y).
(3) Calculate
œÉ
(NI)
NP (x, Œ≤NP ) (5.14)
and update œÉx-table by the coefficients œÉ
(NI)
NP,a of œÉ
(NI)
NP (x, Œ≤NP ) =
‚àë
a œÉ
(NI)
NP,ax
a.
(4) Calculate œÉ
(NI)
NP (Œ±NP , Œ≤NP ) = œÉ
(NI)
NP (x, Œ≤NP )|x=Œ±NP .
Step III: ( check whether the polynomial œÉ(NI)(x, y) is a seperating function)
If œÉ
(NI)
NP (Œ±NP , Œ≤NP ) = 0 (i.e., œÉ
(NI)
NP (x, y) is still not a separation function)
Trigger SNI and set NI = NI + 1.
Go to Step II.
102
  )(Œ≤f
Œ≤
nn
fff ,,, 10 ‚àí
)(Œ±   	
 	 			  Œ±
Figure 5.2: A Horner‚Äôs loop.
initialized as zeros and the final result is stored in the registers, too. Detailed operations
are described as follows. The value of the registers is fn when the first clock comes. And
the next incoming coefficient fn‚àí1 will update the result as (fnŒ≤ + fn‚àí1). When the third
clock comes, the result becomes to ((fnŒ≤ + fn‚àí1)Œ≤ + fn‚àí2), and so forth. Until the last
coefficient f0 enters, the computation is completed. Thus totally (n + 1) clocks are
required. As we can see, this circuit needs only one finite-field adder, one finite-field
multiplier and a buffer, which is simple. We call this circuit a Horner‚Äôs loop and the
multiplication factor Œ≤ the feedback gain.
By writing hNP,0 = ‚àí(Œ≤
q
NP + Œ≤NP ) as f(Œ≤NP ), where f(y) = ‚àíy
q ‚àí y, the Horner‚Äôs loop in
Figure 5.2 can be used to evaluate hNP,0. In our utility, the feedback gain Œ≤ and the input
coefficients f0, . . . , fn in Figure 5.2 must be replaced by Œ≤NP and 0,‚àí1, 0 . . . 0Ô∏∏ Ô∏∑Ô∏∑ Ô∏∏
q‚àí2
,‚àí1,
respectively. We can see that only one finite field multiplier and (q + 1) clocks are needed
to generate the coefficients hNP,0. Since the values of hNP,a, a = 1, . . . , q + 1, are
immediately stored into hx-table once the signal SNP is triggered, totally one finite field
multiplier and (q + 1) clocks are needed to generate the coefficients hNP,a of Hq(x, Œ≤NP ).
For convenience to describe the circuits in the following steps and derive the complexity of
our error-value solver, we need the following definition:
Definition 17. Let F (x, y) =
‚àë
a,b Fa,bx
ayb be a bi-variate polynomial in GF (q2)[x, y]. For
104
Notice that the calculation of Hq(x, Œ≤NP ) and œÉ
(0)
NP (x, Œ≤NP ) can be started at the same
time. Hence to complete Step I, totally (q + 2) multipliers and max{q + 1, b(0)NP + 1} clock
periods are needed.
Now, we consider the calculation of the polynomial œÜ
(NI)
NP (x, y) in Step II(1). Since
Hq(x, Œ≤NP ) = xq+1 + hNP,0 and œÉ
(NI‚àí1)
NP (x, Œ≤NP ) =
‚àëa(NI‚àí1)
NP
a=0 œÉ
(NI‚àí1)
NP,a x
a, we have
œÜ
(NI)
NP (x, y) , œÉ
(NI‚àí1)
NP (x, y)Hq(x, Œ≤NP )‚àí œÉ
(NI‚àí1)
NP (x, Œ≤NP )Hq(x, y)
= œÉ
(NI‚àí1)
NP (x, y)(x
q+1 + hNP,0)‚àí (
‚àëa(NI‚àí1)
a=0 œÉ
(NI‚àí1)
NP,a x
a)(xq+1 ‚àí yq ‚àí y)
= œÜ
(NI)‚Ä≤
NP (x, y) + œÜ
(NI)‚Ä≤‚Ä≤
NP (x, y),
(5.18)
where œÜ
(NI)‚Ä≤
NP (x, y) = œÉ
(NI‚àí1)
NP (x, y)(x
q+1 + hNP,0) and
œÜ
(NI)‚Ä≤‚Ä≤
NP (x, y) = ‚àí(
‚àëa(NI‚àí1)
NP
a=0 œÉ
(NI‚àí1)
NP,a x
a)(xq+1 ‚àí yq ‚àí y).
From (5.18), we can write œÜ
(NI)‚Ä≤‚Ä≤
NP (x, y) as
œÜ
(NI)‚Ä≤‚Ä≤
NP (x, y) = ‚àí(
‚àëa(NI‚àí1)
NP
a=0 œÉ
(NI‚àí1)
NP,a x
a)(xq+1 ‚àí yq ‚àí y)
= ‚àí
‚àëa(NI‚àí1)
NP
a=0 œÉ
(NI‚àí1)
NP,a x
a+q+1 + (
‚àëa(NI‚àí1)
NP
a=0 œÉ
(NI‚àí1)
NP,a x
a)y
+(
‚àëa(NI‚àí1)
NP
a=0 œÉ
(NI‚àí1)
NP,a x
a)yq
=
‚àë
a,b œÜ
(NI)‚Ä≤‚Ä≤
NP,a,bx
ayb.
Then the coefficients œÜ
(NI)‚Ä≤‚Ä≤
NP,a,b of œÜ
(NI)‚Ä≤‚Ä≤
NP (x, y) can be determined as
œÜ
(NI)‚Ä≤‚Ä≤
NP,a+q+1,0 = ‚àíœÉ
(NI‚àí1)
NP,a , 0 ‚â§ a ‚â§ a
(NI‚àí1)
NP
œÜ
(NI)‚Ä≤‚Ä≤
NP,a,1 = œÉ
(NI‚àí1)
NP,a , 0 ‚â§ a ‚â§ a
(NI‚àí1)
NP
œÜ
(NI)‚Ä≤‚Ä≤
NP,a,q = œÉ
(NI‚àí1)
NP,a , 0 ‚â§ a ‚â§ a
(NI‚àí1)
NP
œÜ
(NI)‚Ä≤‚Ä≤
NP,a,b = 0, otherwise.
(5.19)
Let œÜ[a, b] denote the (a, b)-position in the content of œÜ-table, which stores the coefficient
œÜ
(NI)
NP,a,b of œÜ
(NI)
NP (x, y). To save the complexity for calculating œÜ
(NI)
NP (x, y), we immediately
store the coefficients of œÜ
(NI)‚Ä≤‚Ä≤
NP (x, y) once the signal SNI is triggered. From (5.19), we can
see the initial values of œÜ[a, b] = œÜ
(NI)‚Ä≤‚Ä≤
NP,a,b in œÜ-table can be loaded directly from the œÉx-table .
Now, we consider the first term œÜ
(NI)‚Ä≤
NP (x, y) in (5.18). From (5.13) and (5.15), we can see
106
that the coefficient œÜ
(NI)‚Ä≤
NP,a+q+1,b is just the input œÜ
(NI‚àí1)
NP,a,b and the coefficient œÜ
(NI)‚Ä≤
NP,a,b can be
generated by multiplying the input œÉ
(NI‚àí1)
NP,a,b with hNP,0. Since
œÜ
(NI)
NP (x, y) = œÜ
(NI)‚Ä≤
NP (x, y) + œÜ
(NI)‚Ä≤‚Ä≤
NP (x, y), to obtain the coefficients œÜ
(NI)
NP,a,b (or œÜ
(NI)
NP,a+q+1,b) of
œÜ
(NI)
NP (x, y), we need to add the coefficients œÜ
(NI)‚Ä≤
NP,a,b (or œÜ
(NI)‚Ä≤
NP,a+q+1,b) with the value
œÜ[a, b] = œÜ
(NI)‚Ä≤‚Ä≤
NP,a,b (or œÜ[a+ q + 1, b] = œÜ
(NI)‚Ä≤‚Ä≤
NP,a+q+1,b) pre-stored in the œÜ-table. As we can see in
Figure 5.3, the circuit needs only one multiplier. Since 0 ‚â§ b ‚â§ b(NI‚àí1)NP , the circuit requires
(b
(NI‚àí1)
NP + 1) clocks to sequentially generate the coefficients œÜ
(NI)
NP,a,b and œÜ
(NI)
NP,a+q+1,b from
b = b
(NI‚àí1)
NP to b = 0. Since a ranges from 0 to a
(NI‚àí1)
NP , (a
(NI‚àí1)
NP + 1) copies of Figure 5.3 are
needed if we want to simultaneously generate the coefficients œÜ
(NI)
NP,a,b for all a. From (5.13),
we can see that a
(NI‚àí1)
NP ‚â§ q for all NP and NI. Thus to complete the calculation of
œÜ
(NI)
NP (x, y) in Step II(1), totally (q + 1) finite-field multipliers and (b
(NI‚àí1) + 1) clocks are
needed.
Since the computation in Step II(2) involves a division process for bivariate polynomials,
which is complicated, we leave it to the next subsection. Now, we consider the calculation
of the coefficients œÉ
(NI)
NP,a of the polynomial œÉ
(NI)
NP (x, Œ≤NP ) in Step II(3). From (5.16) and
(5.17), we can see that for a fixed a, the Horner‚Äôs loop in Figure 5.2 can be used to
calculate the coefficient œÉ
(NI)
NP,a with the feedback gain Œ≤ and the input coefficients f0, . . . , fn
replaced by Œ≤NP and œÉ
(NI)
NP,a,0, . . . , œÉ
(NI)
NP,a,b
(NI)
NP
, respectively. Since œÉ
(NI)
NP,a,b = 0 for all b > b
(NI)
NP ,
it needs at most (b
(NI)
NP + 1) clock periods to generate the coefficient œÉ
(NI)
NP,a for a given a.
Since deg œÉ
(NI)
NP (x, Œ≤NP ) ‚â§ a
(NI)
NP , (a
(NI)
NP + 1) Horner‚Äôs loops are needed if we want to
simultaneously generate all coefficients œÉ
(NI)
NP,a, 0 ‚â§ a ‚â§ a
(NI)
NP . From (5.13), we can see that
a
(NI)
NP ‚â§ q for all NP and NI. Thus at most (q + 1) Horner‚Äôs loops are needed for this step.
Notice that the calculations of œÉ
(NI)
NP,a‚Äôs will not take place at the same time for different NI,
thus the (q + 1) Horner‚Äôs loops for generating the coefficients œÉ
(NI)
NP,a in Step I(1) can be
reused in this step for efficiency. Therefore, no additional multiplier and (b
(NI)
NP + 1) clocks
are required to calculate all coefficients œÉ
(NI)
NP,a of œÉ
(NI)
NP (x, Œ≤NP ) in Step II(3).
108
1) ( ‚àí
 
ba
ba
NI
baNP S
,
,
)(
,,
œÉ
),()( NPNPNINP Œ≤Œ±œÉ
NPe
Figure 5.4: A circuit for calculating the error value eNP .
5.3.3 The Division Circuit in the Error-Value Evaluator
In this subsection, we focus on designing an architecture for calculating the polynomial
œÉ
(NI)
NP (x, y) in Step II(2) of Function I. As we can see in the last subsection, all operations
related to the polynomial œÉ
(NI‚àí1)
NP (x, y) have completed and œÉ-table contains the coefficients
œÉ
(NI‚àí1)
NP,a,b of œÉ
(NI‚àí1)
NP (x, y) in the end of Step II(1) for the NI-th iteration. Thus for
convenience to update the contents of œÉ-table we can reset œÉ-table to be zero at the
beginning of Step II(2).
To calculate the polynomial œÉ
(NI)
NP (x, y), we firstly consider the polynomial œÜ
(NI)
NP (x, y) in
(5.11). Let A
(NI)
NP and B
(NI)
NP be the (1, 0) and (0, 1)-weighted degree of œÜ
(NI)
NP (x, y)
respectively, as defined in Definition 17. Then œÜ
(NI)
NP (x, y) can be expressed as
œÜ
(NI)
NP (x, y) =
‚àëA(NI)
NP
a=0
‚àëB(NI)
NP
b=0 œÜ
(NI)
NP,a,bx
ayb
=
‚àëA(NI)
NP
+B
(NI)
NP
d=0
‚àëB(NI)
NP
b=0 œÜ
(NI)
NP,d‚àíb,bx
d‚àíbyb,
(5.21)
where œÜ
(NI)
NP,d‚àíb,b = 0 if d‚àí b < 0 or d‚àí b > A
(NI)
NP . (Note that the coefficient œÜ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
may be equal to zero.) Since deg1,0 œÉ
(NI‚àí1)
NP (x, y) = a
(NI‚àí1)
NP and deg1,0Hq(x, y) = q + 1, we
have deg œÉ
(NI‚àí1)
NP (x, Œ≤NP ) ‚â§ a
(NI‚àí1)
NP and degHq(x, Œ≤NP ) ‚â§ q + 1. Since
deg0,1 œÉ
(NI‚àí1)
NP (x, y) = b
(NI‚àí1)
NP , deg0,1Hq(x, y) = q and from (5.11), we have
A
(NI)
NP , deg1,0 œÜ
(NI)
NP (x, y) ‚â§ a
(NI‚àí1)
NP + q + 1
B
(NI)
NP , deg0,1 œÜ
(NI)
NP (x, y) ‚â§ max{b
(NI‚àí1)
NP , q}.
(5.22)
Now, we consider the quotient polynomial q
(NI)
NP (x, y) =
œÜ
(NI)
NP
(x,y)
(x‚àíŒ±NP )(y‚àíŒ≤NP )
defined in (5.13).
110
set {xayb|0 ‚â§ a ‚â§ A(NI)NP , 0 ‚â§ b ‚â§ B
(NI)
NP }, where N = (A
(NI)
NP + 1)(B
(NI)
NP + 1). Then
œÜ
(NI)
NP (x, y) can be expressed as
œÜ
(NI)
NP (x, y) = œÜ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
xA
(NI)
NP yB
(NI)
NP
+œÜ
(NI)
NP,A
(NI)
NP
‚àí1,B
(NI)
NP
xA
(NI)
NP
‚àí1yB
(NI)
NP + œÜ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
‚àí1
xA
(NI)
NP yB
(NI)
NP
‚àí1
+ ¬∑ ¬∑ ¬∑+ œÜ(NI)NP,0,0
=
‚àëN‚àí1
i=0 œÜ
(NI)
NP,iœài,
where œÜ
(NI)
NP,i , œÜ
(NI)
NP,ai,bi
if œài = x
aiybi. To calculate q
(NI)
NP (x, y), we can divide œÜ
(NI)
NP (x, y)
from the greatest term œÜ
(NI)
NP,N‚àí1œàN‚àí1 = œÜ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
xA
(NI)
NP yB
(NI)
NP to the smallest term
œÜ
(NI)
NP,0œà0 = œÜ
(NI)
NP,0,0 by the denominator polynomial (x‚àí Œ±NP )(y ‚àí Œ≤NP ) according to the
(1, 1)-revlex monomial ordering.
The detailed division procedure for calculating q
(NI)
NP (x, y) is described as follows. Let
Q(j)(x, y) and R(j)(x, y) be the quotient term and remainder polynomial at the jth step of
the division procedure. Initially, we let q
(NI)
NP (x, y) = 0, Q
(0)(x, y) = 0 and
R(0)(x, y) = œÜ
(NI)
NP (x, y). At the first step, we divide the greatest term
R
(0)
N‚àí1œàN‚àí1 = œÜ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
xA
(NI)
NP yB
(NI)
NP in R(0)(x, y) by the leading term xy of D(x, y) to
obtain the first quotient term
Q(1)(x, y) = œÜ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
xA
(NI)
NP
‚àí1yB
(NI)
NP
‚àí1 = Q
(1)
A
(NI)
NP
‚àí1,B
(NI)
NP
‚àí1
xA
(NI)
NP
‚àí1yB
(NI)
NP
‚àí1. Then we
update the quotient polynomial q
(NI)
NP (x, y) by q
(NI)
NP (x, y) +Q
(1)(x, y) and the remainder
polynomial by R(1)(x, y) , R(0)(x, y)‚àíQ(1)(x, y)D(x, y) =
‚àëN‚àí1
i=0 R
(1)
i œài with R
(1)
N‚àí1 = 0.
Notice that when calculating R(1)(x, y), except the the divided monomial œàN‚àí1, only the
coefficients of the monomials xA
(NI)
NP
‚àí1yB
(NI)
NP , xA
(NI)
NP yB
(NI)
NP
‚àí1 and xA
(NI)
NP
‚àí1yB
(NI)
NP
‚àí1, whose
(1, 1)-weights are all less than the (1, 1)-weight of œàN‚àí1 = x
A
(NI)
NP yB
(NI)
NP , in the product
Q(1)(x, y)(x‚àí Œ±NP )(y ‚àí Œ≤NP ) are subtracted from R(0)(x, y). At the jth step, the (j ‚àí 1)th
remainder polynomial R(j‚àí1)(x, y) can be written as R(j‚àí1)(x, y) =
‚àëN‚àí1
i=0 R
(j‚àí1)
i œài with
R
(j‚àí1)
i = 0, ‚àÄi ‚â• N ‚àí (j ‚àí 1). Suppose that œàN‚àíj = x
aN‚àíjybN‚àíj . We divide the term
R
(j‚àí1)
N‚àíj œàN‚àíj in R
(j‚àí1)(x, y) by the leading term xy of D(x, y) to obtain the jth quotient
term Q(j)(x, y) = Q
(j)
aN‚àíj‚àí1,bN‚àíj‚àí1
xaN‚àíj‚àí1ybN‚àíj‚àí1. Then we update the quotient polynomial
q
(NI)
NP (x, y) by q
(NI)
NP (x, y) +Q
(j)(x, y) and the remainder polynomial by
112
no greater than A
(NI)
NP +B
(NI)
NP ‚àí 1. Since
Q(1)(x, y)D(x, y) = (
‚àëB(NI)
NP
b=0 Q
(1)
d‚àíb‚àí1,b‚àí1x
d‚àíb‚àí1yb‚àí1)(xy ‚àí Œ±NP y ‚àí Œ≤NPx+ Œ±NPŒ≤NP )
=
‚àëB(NI)
NP
b=0 Q
(1)
d‚àíb‚àí1,b‚àí1(x
d‚àíbyb ‚àí Œ±NPxd‚àíb‚àí1yb ‚àí Œ≤NPxd‚àíbyb‚àí1
+Œ±NPŒ≤NPx
d‚àíb‚àí1yb‚àí1),
with d = A
(NI)
NP +B
(NI)
NP , we can see that except the divided monomials x
d‚àíbyb with
d = A
(NI)
NP +B
(NI)
NP , the product Q
(1)(x, y)D(x, y) only influences the coefficients of the
monomials with (1, 1)-weights equal to A
(NI)
NP +B
(NI)
NP ‚àí 1 or A
(NI)
NP +B
(NI)
NP ‚àí 2 when
calculating R(1)(x, y) from R(0)(x, y). At the (j + 1)th step, the jth remainder polynomial
R(j)(x, y) can be written as R(j)(x, y) =
‚àëA(NI)
NP
+B
(NI)
NP
d=0
‚àëB(NI)
NP
b=0 R
(j)
d‚àíb,bx
d‚àíbyb with R
(j)
d‚àíb,b = 0,
for all 0 ‚â§ b ‚â§ B(NI)NP and d ‚â• (A
(NI)
NP +B
(NI)
NP )‚àí (j ‚àí 1), i.e., the (1, 1)-weight of lm(R
(j)) is
no greater than A
(NI)
NP +B
(NI)
NP ‚àí j. We divide all terms R
(j)
d‚àíb,bx
d‚àíbyb, b = 0, . . . , B
(NI)
NP , in
R(j)(x, y) with d = A
(NI)
NP +B
(NI)
NP ‚àí j by the leading term xy of D(x, y) to obtain the
(j + 1)th homogeneous quotient polynomial
Q(j+1)(x, y) =
‚àëB(NI)
NP
b=0 R
(j)
d‚àíb,bx
d‚àíb‚àí1yb‚àí1
=
‚àëB(NI)
NP
b=0 Q
(j+1)
d‚àíb‚àí1,b‚àí1x
d‚àíb‚àí1yb‚àí1
(5.25)
with d = A
(NI)
NP +B
(NI)
NP ‚àí j. Then we update the quotient polynomial q
(NI)
NP (x, y) by
q
(NI)
NP (x, y) +Q
(j+1)(x, y) and calculate the (j + 1)th remainder polynomial
R(j+1)(x, y) , R(j)(x, y)‚àíQ(j+1)(x, y)D(x, y) =
‚àëA(NI)
NP
+B
(NI)
NP
d=0
‚àëB(NI)
NP
b=0 R
(j+1)
d‚àíb,bx
d‚àíbyb with
R
(j+1)
d‚àíb,b = 0 for all 0 ‚â§ b ‚â§ B
(NI)
NP and d ‚â• A
(NI)
NP +B
(NI)
NP ‚àí j, i.e., the (1, 1)-weight of
lm(R(j+1)) is no greater than A
(NI)
NP +B
(NI)
NP ‚àí (j + 1). Since
Q(j+1)(x, y)D(x, y) = (
‚àëB(NI)
NP
b=0 Q
(j+1)
d‚àíb‚àí1,b‚àí1x
d‚àíb‚àí1yb‚àí1)(xy ‚àí Œ±NPy ‚àí Œ≤NPx+ Œ±NPŒ≤NP ),
where d = A
(NI)
NP +B
(NI)
NP ‚àí j,
=
‚àëB(NI)
NP
b=0 Q
(j+1)
d‚àíb‚àí1,b‚àí1(x
d‚àíbyb ‚àí Œ±NPx
d‚àíb‚àí1yb ‚àí Œ≤NPx
d‚àíbyb‚àí1
+Œ±NPŒ≤NPx
d‚àíb‚àí1yb‚àí1),
we can see that except the divided monomials xd‚àíbyb with d = A
(NI)
NP +B
(NI)
NP ‚àí j, the
product Q(j+1)(x, y)D(x, y) only influences the coefficients of the monomials with
(1, 1)-weights A
(NI)
NP +B
(NI)
NP ‚àí j ‚àí 1 and A
(NI)
NP +B
(NI)
NP ‚àí j ‚àí 2 when calculating R
(j+1)(x, y)
from R(j)(x, y). After the (A
(NI)
NP +B
(NI)
NP + 1)-th step is completed, we have
114
the node (A
(NI)
NP , B
(NI)
NP ) on the slanted diagonal with d = A
(NI)
NP +B
(NI)
NP in the DG with
k = 1, of R(0)(x, y) by lt(D) = xy. Then the first homogeneous quotient polynomial
Q(1)(x, y) = R
(0)
A
(NI)
NP
,B
(NI)
NP
xA
(NI)
NP
‚àí1yB
(NI)
NP
‚àí1 = Q
(1)
A
(NI)
NP
‚àí1,B
(NI)
NP
‚àí1
xA
(NI)
NP
‚àí1yB
(NI)
NP
‚àí1
is generated. By propagating the products Q
(1)
A
(NI)
NP
‚àí1,B
(NI)
NP
‚àí1
Œ±NP , Q
(1)
A
(NI)
NP
‚àí1,B
(NI)
NP
‚àí1
Œ≤NP and
‚àíQ(1)
A
(NI)
NP
‚àí1,B
(NI)
NP
‚àí1
Œ±NPŒ≤NP to the neighbor nodes (A
(NI)
NP ‚àí 1, B
(NI)
NP ), (A
(NI)
NP , B
(NI)
NP ‚àí 1) and
(A
(NI)
NP ‚àí 1, B
(NI)
NP ‚àí 1) of the node (A
(NI)
NP , B
(NI)
NP ) respectively, the coefficients of the next
remainder polynomial R(1)(x, y) can be calculated. At the second iteration, we divide the
terms R
(1)
A
(NI)
NP
‚àí1,B
(NI)
NP
xA
(NI)
NP
‚àí1yB
(NI)
NP and R
(1)
A
(NI)
NP
,B
(NI)
NP
‚àí1
xA
(NI)
NP yB
(NI)
NP
‚àí1 with (1, 1)-weight
d = A
(NI)
NP +B
(NI)
NP ‚àí 1, which correspond to the nodes (A
(NI)
NP ‚àí 1, B
(NI)
NP ) and
(A
(NI)
NP , B
(NI)
NP ‚àí 1) on the slanted diagonal with d = A
(NI)
NP +B
(NI)
NP ‚àí 1 in the DG with k = 2,
of R(1)(x, y) by lt(D) = xy. Then we get the second homogeneous quotient polynomial
Q(2)(x, y) = R
(1)
A
(NI)
NP
‚àí1,B
(NI)
NP
xA
(NI)
NP
‚àí2yB
(NI)
NP
‚àí1 +R
(1)
A
(NI)
NP
,B
(NI)
NP
‚àí1
xA
(NI)
NP
‚àí1yB
(NI)
NP
‚àí2
= Q
(2)
A
(NI)
NP
‚àí2,B
(NI)
NP
‚àí1
xA
(NI)
NP
‚àí2yB
(NI)
NP
‚àí1 +Q
(2)
A
(NI)
NP
‚àí1,B
(NI)
NP
‚àí2
xA
(NI)
NP
‚àí1yB
(NI)
NP
‚àí2.
By propagating the products
Q
(2)
A
(NI)
NP
‚àí2,B
(NI)
NP
‚àí1
Œ±NP , Q
(2)
A
(NI)
NP
‚àí2,B
(NI)
NP
‚àí1
Œ≤NP ,‚àíQ
(2)
A
(NI)
NP
‚àí2,B
(NI)
NP
‚àí1
Œ±NPŒ≤NP ,
Q
(2)
A
(NI)
NP
‚àí1,B
(NI)
NP
‚àí2
Œ±NP , Q
(2)
A
(NI)
NP
‚àí1,B
(NI)
NP
‚àí2
Œ≤NP ,‚àíQ
(2)
A
(NI)
NP
‚àí1,B
(NI)
NP
‚àí2
Œ±NPŒ≤NP
to the neighbor nodes of the nodes (A
(NI)
NP ‚àí 1, B
(NI)
NP ) and (A
(NI)
NP , B
(NI)
NP ‚àí 1), the
coefficients of the next remainder polynomial R(2)(x, y) can be calculated. At the (j + 1)th
iteration, we divide all terms R
(j)
d‚àíb,bx
d‚àíbyb, b = 0, . . . , B
(NI)
NP , with (1, 1)-weight
d = A
(NI)
NP +B
(NI)
NP ‚àí j, which correspond to the nodes (d‚àí b, b), b = 0, . . . , B
(NI)
NP , in the
slanted diagonal with d = A
(NI)
NP +B
(NI)
NP ‚àí j in the DG with k = j + 1, of R
(j)(x, y) by
lt(D) = xy. Then the (j + 1)th homogeneous quotient polynomial
Q(j+1)(x, y) =
‚àëB(NI)
NP
b=0 R
(j)
d‚àíb,bx
d‚àíb‚àí1yb‚àí1 =
‚àëB(NI)
NP
b=0 Q
(j+1)
d‚àíb‚àí1,b‚àí1x
d‚àíb‚àí1yb‚àí1 with
d = A
(NI)
NP +B
(NI)
NP ‚àí j is generated. By propagating the products Q
(j+1)
d‚àíb‚àí1,b‚àí1Œ±NP ,
Q
(j+1)
d‚àíb‚àí1,b‚àí1Œ≤NP and ‚àíQ
(j+1)
d‚àíb‚àí1,b‚àí1Œ±NPŒ≤NP with d = A
(NI)
NP +B
(NI)
NP ‚àí j and b = 0, . . . , B
(NI)
NP to
the nodes on the slanted diagonals with d = A
(NI)
NP +B
(NI)
NP ‚àí j ‚àí 1 and
116
the same time, all coefficients œÜ
(NI)
NP,d‚àíb,b, b = 0, . . . , B
(NI)
NP , of œÜ
(NI)
NP (x, y) must be input into
the PE‚Äôs simultaneously. Then totally (A
(NI)
NP +B
(NI)
NP + 1) steps are required to complete
the division process. Since œÜ
(NI)
NP (x, y) can be divided by D(x, y) and lt(D) = xy with the
(1, 1)-revlex monomial ordering, all coefficients of the monomials xa, a = 0, . . . , A
(NI)
NP , or
yb, b = 0, . . . , B
(NI)
NP , in R
(A
(NI)
NP
+B
(NI)
NP
‚àía)(x, y) or R(A
(NI)
NP
+B
(NI)
NP
‚àíb)(x, y) must be zero. Thus
the leftmost PE corresponding to a = 0 in the one-dimensional array can be omitted and
the number of PEs in the one-dimensional array becomes A
(NI)
NP . Also, the number of steps
can be reduced to (A
(NI)
NP +B
(NI)
NP ‚àí 1), since only the terms lying the slanted diagonal with
d = A
(NI)
NP +B
(NI)
NP to d = 2 are required to be computed. From (5.22), we can see that for
the NI-th iteration of Step II(2) in Function I, at most (a
(NI‚àí1)
NP + q + 1) PEs and
(a
(NI‚àí1)
NP + q +max{b
(NI‚àí1)
NP , q}) steps are needed to completed the division process for
calculating q
(NI)
NP (x, y) in (5.13).
Now, we derive the cell functions of a PE cell. From (5.25), we can see that at the (j +1)th
step, the (j + 1)th homogeneous quotient polynomial Q(j+1)(x, y) is a linear combination of
monomials with the same (1, 1)-weight A
(NI)
NP +B
(NI)
NP ‚àí j ‚àí 2. Thus we have
q
(NI)
NP,d‚àíb‚àí1,b‚àí1 = Q
(j+1)
d‚àíb‚àí1,b‚àí1 = R
(j)
d‚àíb,b, (5.26)
where d = A
(NI)
NP +B
(NI)
NP ‚àí j, b = 1, . . . , B
(NI)
NP and 1 ‚â§ d‚àí b ‚â§ A
(NI)
NP . From Figure 5.6 and
(5.26), the coefficients q
(NI)
NP,a,b in the quotient polynomial q
(NI)
NP (x, y) =
‚àë
a,b q
(NI)
NP,a,bx
ayb can
be determined recursively as follows:
For d = A
(NI)
NP +B
(NI)
NP down to d = 2
(the (j + 1)th iteration, where j = A
(NI)
NP +B
(NI)
NP ‚àí d)
118
a PE cell is then derived as follows:
qout = œÜin +R + C,
CÀú = D1[qout]√ó Œ≤NP ,
RÀú = (D1[qout]‚àíD1[D1[qout]√ó Œ≤NP ])√ó Œ±NP
= (D1[qout]‚àíD1[CÀú])√ó Œ±NP ,
(5.29)
where CÀú and RÀú are the updated variables for C and R after one step.
 
 
NPŒ±
R~
outq
inœÜ
R
NPŒ≤
C~
][1 outqD
Figure 5.8: The circuits inside a PE cell.
Figure 5.8 demonstrates the circuit inside a PE cell. From Figure 5.8, we can see that there
are two finite-field multipliers in a PE cell. Since the leftmost PE cell with a = 1 needs not
to pass any signal to its left-hand side, only the circuits inside the dash box in Figure 5.8
are necessary for the first PE cell. Since there are A
(NI)
NP PEs, totally (2(A
(NI)
NP ‚àí 1) + 1)
multipliers are needed for calculating q
(NI)
NP (x, y). From (5.22), we can see that for the
NI-th iteration in Step II of Function I, at most (2a
(NI‚àí1)
NP + 2q + 1) multipliers are
necessary. Since each step in the division process spends only one clock cycle,
(a
(NI‚àí1)
NP + q +max{b
(NI‚àí1)
NP , q}) clock cycles are required to completed the division process.
Since a
(NI‚àí1)
NP ‚â§ q by (5.13), the numbers of multipliers and consumed clock cycles can be
further upper bounded by (4q + 1) and (2q +max{b(NI‚àí1)NP , q}) respectively when
calculating q
(NI)
NP (x, y) in (5.13).
Finally, we consider the calculation of œÉ
(NI)
NP (x, y) in Step II(2). From (5.13) and (5.24),
120
(a, b)th position of œÉ-table. Thus to calculate œÉ
(NI)
NP (x, y) from q
(NI)
NP (x, y), no additional
finite-field multipliers and clock cycles are needed. Therefore, totally (4q + 1) multipliers
and (2q +max{b(NI‚àí1)NP , q}) clock cycles are required to finish Step II(2).
Note that to save the time complexity, the calculation of q
(NI)
NP (x, y) in Step II(2) does not
need to wait until the computations of œÜ
(NI)
NP (x, y) in Step II(1) are completed. Actually,
the calculation in Step II(2) can be started after one clock of the first results œÜ
(NI)
NP,a,b
(NI‚àí1)
NP
,
0 ‚â§ a ‚â§ 2q + 1, coming out from the circuit shown in Figure 5.3. Consider the coefficient
œÜ
(NI)
NP,a,b, which is generated from the circuit in Figure 5.3 at the (b
(NI‚àí1)
NP ‚àí b+ 1)th clock
and will be input to the one-dimensional systolic array at the
((a
(NI‚àí1)
NP + q + 1) + (max{b
(NI‚àí1)
NP , q})‚àí (a+ b) + 1) th clock. Since
(a
(NI‚àí1)
NP + q + 1) + (max{b
(NI‚àí1)
NP , q})‚àí (a+ b) + 1 ‚â• b
(NI‚àí1)
NP ‚àí b+ 1, the coefficient œÜ
(NI)
NP,a,b
can be registered in the œÜ-table once it is generated and then input to the one-dimensional
systolic array after ((a
(NI‚àí1)
NP + q+1)+ (max{b
(NI‚àí1)
NP , q})‚àí (a+ b)+ 1)‚àí (b
(NI‚àí1)
NP ‚àí b+1)+1
clocks. Thus the number of consumed clock cycles to complete Step II(1) and Step II(2)
is upper bound by (2q +max{b(NI‚àí1)NP , q}+ 1).
5.4 Complexity Analysis
Table 5.1: Hardware and time complexities of each step in Function I.
number of multipliers number of consumed clocks
Step I q+2 max{q + 1, b(0)NP + 1}
Step II(1) q+1
2q +max{b(NI‚àí1)NP , q}+ 1Step II(2) 4q+1
Step II(3) 0 b
(NI)
NP + 1
Step II(4) 0 a
(NI)
NP + 1
Step III(1) 0 b
(ŒΩQNP (œÉ))
NP + 1
Step III(2) 1 0
Since multiplication is the most complicated operation in a finite field when considering the
hardware circuitry, we express the hardware complexity in terms of the number of
122
b
(0)
NP = b
(0) ‚â§ ‚åä
ot‚àó(m)
q+1
‚åã for all NP . Now, we consider the upper bounds of a(NI)NP and b
(NI)
NP .
From (5.13), we have
a
(NI)
NP , deg1,0(œÉ
(NI)
NP )
= deg1,0(q
(NI)
NP mod (x
q+1 ‚àí yq ‚àí y))
‚â§ q,
(5.35)
for any NP and NI. The following lemma describes an upper bound of b
(NI)
NP :
Lemma 20. Consider the potential error point QNP on an error-locator polynomial
œÉ(x, y) =
‚àët‚àó(m)+1
‚Ñì=1 œÉ‚Ñìfo‚Ñì . Let œÉ
(NI)
NP (x, y) be the polynomial defined in (5.13) and b
(NI)
NP be
the (0, 1)-weighted degree of œÉ
(NI)
NP (x, y). Then for any NI ‚â• 1, we have
b
(NI)
NP ‚â§ bÀú
(0) +NI(q ‚àí 1), (5.36)
where bÀú(0) , max{‚åä
ot‚àó(m)
q+1
‚åã, q}.
Proof. We prove this lemma by induction on NI. Consider the case NI = 1. From (5.22)
and (5.23), we have
deg1,0 q
(1)
NP (x, y) ‚â§ A
(1)
NP ‚àí 1 ‚â§ a
(0) + q
deg0,1 q
(1)
NP (x, y) ‚â§ B
(1)
NP ‚àí 1 ‚â§ max{b
(0)
NP , q} ‚àí 1 ‚â§ max{‚åä
ot‚àó(m)
q+1
‚åã, q} ‚àí 1 = bÀú(0) ‚àí 1.
Then q
(1)
NP (x, y) can be written as
q
(1)
NP (x, y) =
a(0)+q‚àë
a=0
bÀú(0)‚àí1‚àë
b=0
q
(1)
NP,a,bx
ayb.
From (5.13), we have
œÉ
(1)
NP (x, y) , q
(1)
NP (x, y)mod (x
q+1 ‚àí yq ‚àí y).
Then
œÉ
(1)
NP (x, y) =
‚àëa(0)+q
a=0
‚àëbÀú(0)‚àí1
b=0 q
(1)
NP,a,bx
ayb mod (xq+1 ‚àí yq ‚àí y)
= (
‚àëq
a=0
‚àëbÀú(0)‚àí1
b=0 q
(1)
NP,a,bx
ayb +
‚àëa(0)+q
a=q+1
‚àëbÀú(0)‚àí1
b=0 q
(1)
NP,a,bx
ayb) mod (xq+1 ‚àí yq ‚àí y)
= (
‚àëq
a=0
‚àëbÀú(0)‚àí1
b=0 q
(1)
NP,a,bx
ayb +
‚àëa(0)‚àí1
a=0
‚àëbÀú(0)‚àí1
b=0 q
(1)
NP,a+q+1,bx
a+q+1yb) mod (xq+1 ‚àí yq ‚àí y)
=
‚àëq
a=0
‚àëbÀú(0)‚àí1
b=0 q
(1)
NP,a,bx
ayb +
‚àëa(0)‚àí1
a=0
‚àëbÀú(0)‚àí1
b=0 q
(1)
NP,a+q+1,bx
a(yq + y)yb
=
‚àëq
a=0
‚àëbÀú(0)‚àí1
b=0 q
(1)
NP,a,bx
ayb +
‚àëa(0)‚àí1
a=0
‚àëbÀú(0)‚àí1
b=0 q
(1)
NP,a+q+1,bx
a(yq+b + y1+b)
=
‚àëq
a=0
‚àëbÀú(0)+(q‚àí1)
b=0 (q
(1)
NP,a,b + q
(1)
NP,a+q+1,b‚àíq + q
(1)
NP,a+q+1,b‚àí1)x
ayb.
124
From (5.35) and (5.36), tNP can be further upper bounded by
tNP ‚â§ bÀú(0) + 1
+
‚àëŒΩQNP (œÉ)
NI=1 ((2q +max{bÀú
(0) + (NI ‚àí 1)(q ‚àí 1), q}+ 1) + (bÀú(0) +NI(q ‚àí 1) + 1) + (q + 1))
+(bÀú(0) + ŒΩQNP (œÉ)(q ‚àí 1) + 1)
= bÀú(0) + 1 +
‚àëŒΩQNP (œÉ)
NI=1 (2q + 2bÀú
(0) + 2NI(q ‚àí 1) + 4) + (bÀú(0) + ŒΩQNP (œÉ)(q ‚àí 1) + 1)
= bÀú(0) + 1 + ((q ‚àí 1)ŒΩQNP (œÉ)
2 + (3q + 3 + 2bÀú(0))ŒΩQNP (œÉ)) + (bÀú
(0) + ŒΩQNP (œÉ)(q ‚àí 1) + 1)
= (q ‚àí 1)ŒΩQNP (œÉ)
2 + (2bÀú(0) + 4q + 2)ŒΩQNP (œÉ) + 2bÀú
(0) + 2.
(5.37)
Since QNP is a simple point on the Hermitian curve Hq(x, y) = xq+1 ‚àí yq ‚àí y, we have
ŒΩQNP (œÉ) = I(QNP ,Hq ‚à© œÉ),
where I(QNP ,Hq ‚à© œÉ) is the intersection number of Hq(x, y) and œÉ(x, y) at QNP [7]. Thus
(5.37) can be further written as
tNP ‚â§ (q ‚àí 1)I(QNP ,Hq ‚à© œÉ)2 + (2bÀú(0) + 4q + 2)I(QNP ,Hq ‚à© œÉ) + 2bÀú(0) + 2. (5.38)
Suppose QNP , NP = 1, . . . , s are all potential error points on the error-locator polynomial
œÉ(x, y). Let Œ¶(m) be the total number of consumed clocks in our error-value solver for the
Hermitian code Cm over GF (q
2). From (5.38), Œ¶(m) can be expressed as
Œ¶(m) =
‚àës
NP=1 tNP
‚â§
‚àës
NP=1((q ‚àí 1)I(QNP ,Hq ‚à© œÉ)
2 + (2bÀú(0) + 4q + 2)I(QNP ,Hq ‚à© œÉ) + 2bÀú(0) + 2)
= (q ‚àí 1)
‚àës
NP=1 I(QNP ,Hq ‚à© œÉ)
2 + (2bÀú(0) + 4q + 2)
‚àës
NP=1 I(QNP ,Hq ‚à© œÉ)
+2(bÀú(0) + 1)s.
(5.39)
To derive a more explicit upper bound of Œ¶(m), we need the well-known Bezout‚Äôs Theorem
in the following:
Theorem 21. (Bezout‚Äôs Theorem) Let F1 and F2 be projective plane curves of degrees m1
and m2 respectively. Assume that F1 and F2 have no common component. Let P
‚àó is a
point in the projective plane P2 and I(P ‚àó, F1 ‚à© F2) is the intersection number of F1 and F2
at P . Then ‚àë
P ‚àó
I(P ‚àó, F1 ‚à© F2) = m1m2.
126
Equation (5.43) is true since‚àës
NP=1 I(QNP ,Hq ‚à© œÉ)
2 ‚â§ (
‚àës
NP=1 I(QNP ,Hq ‚à© œÉ))
2
‚â§ ((q + 1)Œ∑)2.
Moreover, since I(QNP ,Hq ‚à© œÉ) ‚â• 1 for all NP = 1, . . . , s, and from (5.42), we have
s ‚â§ (q + 1)Œ∑. (5.44)
From Lemma 22 and (5.44), the total number Œ¶(m) of consumed clocks in (5.39) can be
further upper bounded as
Œ¶(m) ‚â§ (q ‚àí 1)(q + 1)2Œ∑2 + (2bÀú(0) + 4q + 2)(q + 1)Œ∑ + 2(bÀú(0) + 1)(q + 1)Œ∑. (5.45)
Take the Hermitian code C21 over GF (4
2) for example. Since the the designed
error-correcting capacity t‚àó(21) for the Hermitian code C21 is equal to 5, the error-locator
polynomial with least pole order at P‚àû can be expressed as
œÉ(x, y) = œÉ1 + œÉ2x+ œÉ3y + œÉ4x
2 + œÉ5xy + œÉ6y
2. Then we have Œ∑ = 2 and
bÀú(0) = max{‚åäo6
5
‚åã, 4} = max{2, 4} = 4. Thus from (5.45), we have Œ¶(21) ‚â§ 660.
128
are interesting and important in coding theory. All the results in Section 2.1, 2.2 and 2.3
are given in [5] and the readers familiar with this topic can skip them.
6.1.1 Places and Discrete Valuation Rings
As discussed in [7, Chapter 7]), any projective curve X (singular or nonsingular) has a
nonsingular (smooth) model XÀú of X and K(XÀú) ‚àº= K(X), where K(X) and K(XÀú) are
function fields of X and XÀú respectively. This relation is called a birationally equivalence
between XÀú and X. For a projective curve X and K is algebraically closed, there is a
one-to-one correspondence between the places of X and the discrete valuation rings of
K(X), see [7]. From the viewpoint of function fields, the reasonable definition of places is
put in this section. From now on, we concentrate on the function fields K(X) over K for
some projective curve X and the special case that K (not necessary algebraically closed) is
algebraically closed in K(X).
Definition 23. An (algebraic) function field F over K (F/K) is a field extension of K
such that for some x ‚àà F\K which is transcendental over K, [F : K(x)] is finite. 
Definition 24. A valuation ring O of a function field F/K is a subring of F satisfying
1. K ( O ( F .
2. For any x ‚àà F , x ‚àà O or x‚àí1 ‚àà O.

Definition 25. Let R be a Noetherian (every ideal of R is finitely generated) domain
which is not a field. If R has only one maximal ideal and the maximal ideal is generated by
one element, then R is called a discrete valuation ring. 
The definition of places in a function field is based on the following theorem.
130
Proof. Given any x 6= 0 in F . First assume that x ‚àà OP . Suppose that for some x ‚àà OP , x
is not of the form utm, where u is a unit in OP and m ‚àà N‚à™ {0}. Then consider the infinite
sequence {xi} defined as x1 = x and for i > 1, xi ‚àà OP satisfies xi‚àí1 = xit. Then we have
an infinite ascending chain of proper ideals of OP as (x1) ( (x2) ( (x3) ( ¬∑ ¬∑ ¬∑ which
contradicts to the fact that OP is Noetherian. So any x ‚àà OP has the form. For x /‚àà OP ,
since x‚àí1 ‚àà OP , x‚àí1 = wts for some unit w ‚àà OP and s ‚àà N ‚à™ {0}. Hence x = w‚àí1t‚àís. If
x = u1t
m1 = u2t
m2 , where ui are units. Assume that m2 ‚â• m1. Then u
‚àí1
2 u1 = t
m2‚àím1 . So
m1 = m2 and u1 = u2. This shows that this representation is unique.
Check that vP (¬∑) is independent of the choice of t. If t0 is another prime element for P . Let
t = u1t
k1
0 and t0 = u2t
k2 , where u1, u2 are units and k1, k2 > 0. Then t = (u1u
k1
2 )t
k1k2
implies that k1 = k2 = 1. So t0 = vt for some unit v.
Next show that vP (¬∑) is a discrete valuation. Obviously, vP (¬∑) satisfies the conditions
(1),(2),(4) in Definition 28. Let xi = uit
mi (i = 1, 2), where ui are units and mi ‚àà Z,
assume that m1 ‚â• m2. Then x1 + x2 = u2tm2(u
‚àí1
2 u1t
m1‚àím2 + 1). Since
u‚àí12 u1t
m1‚àím2 + 1 ‚àà OP , vP (u
‚àí1
2 u1t
m1‚àím2 + 1) ‚â• 0. Hence vP (x1 + x2) ‚â• m2. This shows
that vP (¬∑) satisfies condition (3).
To check condition (5), it only needs to show that K ‚à© P = {0}. If x 6= 0 and x ‚àà K ‚à© P ,
then x‚àí1 ‚àà K but x‚àí1 /‚àà OP which contradicts to the fact that K ‚äÜ OP . 
Definition 30. Assume F/K is a function field and x ‚àà F , P ‚àà PF . If vP (x) > 0, then we
say that x has a zero at P . If vP (x) < 0, then we say that x has a pole at P . 
Given a place P , consider the quotient field FP := OP/P , since K ‚à© P = {0}, K can be
embedded into FP . Moreover, FP is a finite extension of K ( [5, Proposition I.1.14]).
The degree of P is defined to be [FP : K] and denoted as deg P . A place of degree 1 is
called a rational place. For P ‚àà PF and x ‚àà OP , the coset of x in OP module P is denoted
by x(P ). If x 6‚àà OP , x(P ) is defined to be ‚àû. So for any x ‚àà F , we can consider x as a
132
Lemma 31. For any x ‚àà F , deg(x) = 0. So deg(x)0 = deg(x)‚àû, where
(x)0 :=
‚àë
P‚ààPF : vP (x)>0
vP (x)P,
(x)‚àû :=
‚àë
P‚ààPF : vP (x)<0
‚àívP (x)P.
Moreover, deg(x)0 = [F : K(x)] if x /‚àà K. 
Definition 32. For x ‚àà F , the order of x (ord(x)) is defined to be ord(x) = deg(x)0. 
By an easy inductive proof, we can get that dimD ‚â§ degD + 1 and degD ‚àí dimD ‚â§ c for
some constant c (independent of D). Define the genus of F (g(F )) to be
g(F ) := max{degD ‚àí dimD + 1 | D ‚àà DF}.
It is obvious that g(F ) ‚â• 0. The next is the Riemann‚Äôs theorem [5, Theorem I.4.17].
Theorem 33. (Riemann‚Äôs Theorem) Let F/K be a function field of genus g.
(a) For any divisor D ‚àà DF ,
dimD ‚â• degD + 1‚àí g.
(b) There is an integer c, depending on F/K such that
dimD = degD + 1‚àí g.
whenever degD ‚â• c.

Weil Differentials
In fact, Weil differentials and differentials are equivalent (for differentials, see [7, Chapter
8]). In the viewpoint of Weil differentials, any differential can be regarded as a K-linear
map from the set of adeles to the function field. It is easier to prove the Riemann-Roch
theorem by using Weil differentials than differentials.
134
Definition 38. The divisor (œâ) of a Weil differential œâ 6= 0 is the uniquely determined
divisor of F/K satisfying
1. œâ vanishes on AF ((œâ))+ F .
2. If œâ vanishes on AF (D) + F then D ‚â§ (œâ).
A divisor W is called a canonical divisor of F/K if W = (œâ) for some œâ ‚àà ‚Ñ¶F . 
In the Riemann‚Äôs Theorem, we have the inequality dimD ‚â• degD + 1‚àí g for D ‚àà DF . Let
i(D) = dimD ‚àí degD ‚àí 1 + g, the index of speciality of D. It is clear that i(D) ‚â• 0 and
i(D) = 0 if degD is sufficiently large by Riemann‚Äôs theorem. The Riemann-Roch
theorem [5, Theorem I.5.15] gives the exact value of i(D).
Theorem 39. (Riemann-Roch Theorem) Let W be a canonical divisor, then for any
D ‚àà DF ,
dimD = degD + 1‚àí g + dim(W ‚àíD),
where g is the genus of F/K. 
6.1.3 Algebraic Geometric Codes
Geometric Goppa (AG) codes were first proposed in [2]. They are a generalization of the
classical Goppa codes [48]. In [11], there is an equivalence definition of AG codes, and from
this viewpoint, AG codes can be considered as a generalization of Reed-Solomon codes.
Here we just introduce the later definition, for the first, it needs some knowledge about the
residues of Weil differentials.
Definition 40. Let F/Fq be a function field, D = P1 + P2 + ¬∑ ¬∑ ¬∑+ Pn, where P1, P2, . . . , Pn
are rational places in PF . If G is a divisor of F and suppD ‚à© suppG = ‚àÖ, then define the
geometric Goppa Code CL(D,G) as
CL(D,G) := {(x(P1), . . . , x(Pn)) | x ‚àà L(G)} ‚äÜ F
n
q .
136
Consider the special case G = rQ and D = P1 + ¬∑ ¬∑ ¬∑+ Pn for some distinct rational places
P1, P2, . . . , Pn, Q in PF and some positive integer r > 0. The geometric Goppa code
Cr := CL(D, rQ)
is determined by the space L(rQ). If r > 0 and L(rQ)\L((r ‚àí 1)Q) = ‚àÖ, then the integer r
is called a gap at Q, otherwise r is called a nongap at Q. There are g gaps at
Q [5, Theorem I.6.7], where g is the genus of F/Fq. The set of nongaps at Q forms a
numerical subsemigroup of N and is called the Weierstrass semigroup with respect to Q,
denoted as SQ.
Definition 42. A set {x1, . . . , xl} ‚àà F (F being a function field) is called a Weierstrass
generating set with respect to a place Q if (xi)‚àû = viQ for vi distinct and {v1, . . . , vl}
generates the Weierstrass semigroup SQ with respect to Q. In particular, if V = {v1, . . . , vl}
is a minimal generating set for SQ (this means that any subset of V can‚Äôt generate SQ),
then {x1, . . . , xl} is called a reduced Weierstrass generating set (with respect to Q). 
6.1.4 Long Codes
In the consideration for the construction of long block codes, the asymptotic problem is
interesting. In this section, we state one of the asymptotic problem, for the others, see [49].
AG codes play an important role in this asymptotic problem in coding theory.
Asymptotic Problem
For an [n, k, d] code C, the parameters R(C) = k/n and Œ¥(C) = d/n are important. For long
code construction, we want to know the behavior of the parameters Œ¥(C) and R(C) when
the length n of the code becomes large. This is one of the asymptotic problems. Fix q (a
power of a prime), consider the set Vq containing all the point (Œ¥, R) such that there exists
a code C over Fq (the finite field of q elements) with the parameters (Œ¥(C), R(C)) = (Œ¥, R).
138
Proof. Consider a sequence of function fields Fi/Fq of genus gi and with Ni rational places
such that (such a sequence exists by the definition of A(q)) gi ‚Üí‚àû as i‚Üí‚àû and
lim
i‚Üí‚àû
Ni/gi = A(q).
For 0 ‚â§ Œ¥ ‚â§ 1‚àí A(q)‚àí1, choose a sequence ri > 0 such that
lim
i‚Üí‚àû
ri/Ni = 1‚àí Œ¥ and ri < Ni ‚àí 1.
For each i, consider the code Ci = CL(Di, riQi), where Qi is an arbitrarily chosen rational
place of Fi/Fq and Di is the formal sum of all rational places of Fi/Fq except for Qi. Then
deg(riQi ‚àíDi) < 0 and so dim(riQi ‚àíDi) = 0. The parameters ki and di then satisfies
ki = dim(riQi)‚àí dim(riQi ‚àíDi) = dim(riQi) ‚â• ri + 1‚àí g
by the theorem of Riemann-Roch and
di ‚â• Ni ‚àí 1‚àí deg(riQi) = Ni ‚àí ri ‚àí 1.
Then
R(Ci) ‚â•
ri + 1
Ni
‚àí
gi
Ni
and Œ¥(Ci) ‚â• 1‚àí
ri ‚àí 1
Ni
.
Assume that R(Ci)‚Üí R and Œ¥(Ci)‚Üí Œ¥‚Ä≤ (otherwise choose a convergent subsequence). Thus
R ‚â• 1‚àí Œ¥ ‚àíA(q)‚àí1 and Œ¥‚Ä≤ ‚â• Œ¥. Since the function Œ±q is decreasing,
Œ±q(Œ¥) ‚â• Œ±q(Œ¥
‚Ä≤) ‚â• 1‚àí Œ¥ ‚àíA(q)‚àí1.

The Tsfasman-VlaÀádut-Zink Bound
The Drinfeld-VlaÀádut Bound says that A(q) ‚â§ q1/2 ‚àí 1 ( [5, Theorem V.3.6]). Moreover, if
q is a square, A(q) = q1/2 ‚àí 1 [4]. Apply Theorem 44, we get the Tsfasman-VlaÀádut-Zink
bound for a square q as
Œ±q(Œ¥) ‚â• 1‚àí Œ¥ ‚àí
1
q1/2 ‚àí 1
for 0 ‚â§ Œ¥ ‚â§ 1‚àí
1
q1/2 ‚àí 1
.
140
Note that the limit exists and so this definition is well-defined [34]. If Œª(F) > 0, then F is
called to be asymptotically good. F is called asymptotically optimal if Œª(F) = q1/2 ‚àí 1.
We say that F is asymptotically bad if Œª(F) = 0.
6.2.1 Finite Extensions of Function Fields
In this section we consider the finite extensions F ‚Ä≤/K of function fields F/K, this means
that F ‚Ä≤ is a finite dimensional field extension of the field F .
Ramification index and difference exponent
Let P be a place in PF . A place P ‚Ä≤ ‚àà PF ‚Ä≤ is called an extension of P and denoted by P ‚Ä≤|P
if P ‚äÇ P ‚Ä≤. If t is a prime element for P , define e(P ‚Ä≤|P ) := vP ‚Ä≤(t), the ramification index of
P ‚Ä≤ over P . By the definition of e(P ‚Ä≤|P ), vP ‚Ä≤(x) = e(P
‚Ä≤|P ) ¬∑ vP (x) for any x ‚àà F . The
extension P ‚Ä≤|P is called ramified if e(P ‚Ä≤|P ) > 1 and unramified if e(P ‚Ä≤|P ) = 1. In fact,
e(P ‚Ä≤|P ) ‚â§ [F ‚Ä≤ : F ] [5, Corollary III.1.12]. P ‚Ä≤|P is called totally ramified if
e(P ‚Ä≤|P ) = [F ‚Ä≤ : F ]. Let O‚Ä≤P := {x ‚àà F
‚Ä≤ | x is integral over OP}, the integral closure of OP
in F ‚Ä≤. Consider the set
CP := {x ‚àà F
‚Ä≤ | TrF ‚Ä≤/F (x ¬∑ O
‚Ä≤
P ) ‚äÜ OP},
where TrF ‚Ä≤/F is the trace map of the field extension F
‚Ä≤/F [51, pp. 289]. Then CP is an
O‚Ä≤P -module and CP = t ¬∑ O‚Ä≤P for some t ‚àà F ‚Ä≤ [5, Proposition III.4.2]. Define the different
exponent of P ‚Ä≤ over P by
d(P ‚Ä≤|P ) := ‚àívP ‚Ä≤(t).
By [5, Proposition III.4.2], d(P ‚Ä≤|P ) is well-defined and d(P ‚Ä≤|P ) ‚â• 0. Moreover,
d(P ‚Ä≤|P ) = 0 for almost all P ‚àà PF and P
‚Ä≤|P . So we can define the divisor
Diff(F ‚Ä≤/F ) :=
‚àë
P‚ààPF
‚àë
P ‚Ä≤|P
d(P ‚Ä≤|P ) ¬∑ P ‚Ä≤,
142
polynomial a(T ) ‚àà K[T ], the formal derivative a‚Ä≤(T ) = a0, so a(T ) is separable if and only
if a0 6= 0.
Theorem 46. Let F/K be a function field with characteristic p > 0 and a(T ) ‚àà K[T ] is a
separable additive polynomial of degree pn. Assume that all the roots of a(T ) = 0 lie in K.
Consider the field extension F ‚Ä≤ = F (x) with a(x) = u for some u ‚àà F . If for some place
Q ‚àà PF , vQ(u) = ‚àím < 0 and gcd(m, p) = 1, then
(a) Q is totally ramified in F ‚Ä≤/F and the field extension F ‚Ä≤/F is Galois with
[F ‚Ä≤ : F ] = pn. The Galois group of F ‚Ä≤/F is isomorphic to (Z/pZ)n. If Q‚Ä≤|Q is the
extension of Q in F ‚Ä≤, then
d(Q‚Ä≤|Q) = (pn ‚àí 1)(m+ 1).
(b) If P ‚àà PF and vP (u) ‚â• 0, then dF ‚Ä≤/F (P ) = 0 and hence P is unramified in F ‚Ä≤/F .
Proof. (a) Assume Q‚Ä≤|Q is an extension of Q in F ‚Ä≤, then a(x) = u implies that vQ‚Ä≤(x) < 0
and
pnvQ‚Ä≤(x) = e(Q
‚Ä≤|Q)vQ(u) = e(Q
‚Ä≤|Q)(‚àím).
Then e(Q‚Ä≤|Q) = pn and vQ‚Ä≤(x) = ‚àím since gcd(p,m) = 1 and e(Q‚Ä≤|Q) ‚â§ pn. Therefore,
[F ‚Ä≤ : F ] = pn and so a(T )‚àí u is irreducible over K. It is then obvious that F ‚Ä≤/F is a
splitting (by the assumption that all the roots of a(T ) = 0 lie in K) and separable
extension and so is Galois.
Next claim that AutFF
‚Ä≤ ‚àº= (Z/pZ)n, where AutFF ‚Ä≤ is the Galois group for F ‚Ä≤ over F . Let
Œì = {Œ± ‚àà F | a(Œ±) = 0}, note that Œì ‚äÇ K by hypothesis. For 0 6= Œ± ‚àà Œì, l ¬∑ Œ± ‚àà Œì for any
l ‚àà N, so Œ±, 2Œ±, . . . , (p‚àí 1)Œ± are distinct in Œì since p is prime and Œ± 6= 0. If iŒ± = jŒ≤ for
Œ±, Œ≤ ‚àà Œì and 1 ‚â§ i, j ‚â§ p‚àí 1, then choose s, t such that is+ pt = 1, then
Œ± = (is+ pt)Œ± = isŒ± = jsŒ≤.
So there exists Œ±1, . . . , Œ±m in Œì such that each cyclic group Gi generated by Œ±i satisfies
Gi ‚à©Gj = {0} whenever j 6= i and Œì = ‚äïGi, i.e., m = n and Œì ‚àº= (Z/pZ)n. It is obvious
144
6.3 The Tower FH
For q > 0 a power of a prime, consider the tower of function fields FH = {Hi | i ‚â• 1}
defined as follows:
H1 = Fq2(x1), where x1 is transcendental over Fq2 .
For i ‚â• 2, Hi = Hi‚àí1(xi), where x
q
i + xi = x
q+1
i‚àí1 .
(6.1)
In this section, we will show that this tower FH is asymptotically bad. However, the
construction of the AG codes corresponding to these function fields is simple since it is easy
to find a Weierstrass generating set.
Theorem 47. Let FH = {Hi | i ‚â• 1} be defined as in above and n ‚â• 2.
(a) If P‚àû ‚àà PH1 is the simple pole of x1, then it is totally ramified in Hn/H1 and (also
denote the extension of P‚àû in PHn as P‚àû) for all 1 ‚â§ i ‚â§ n,
vP‚àû(xi) = ‚àíq
n‚àíi(q + 1)i‚àí1. (6.2)
(b) Hn is Galois over H1 such that [Hn : H1] = q
n and the Galois group of Hi/Hi‚àí1 for
each 2 ‚â§ i ‚â§ n is isomorphic to Z/pZ each of which is given by xi 7‚àí‚Üí xi + Œ≤ for some
Œ≤ ‚àà Fq2 with Œ≤
q + Œ≤ = 0.
(c) P‚àû is the only one place with nonzero different exponent and the only one place
ramified in Hn/H1. The different exponent of P‚àû in Hn/H1 is
dHn/H1(P‚àû) = (q ‚àí 1)
n‚àí1‚àë
i=1
qn‚àí1‚àíi(q + 1)i + qn‚àí1 ‚àí 1.
Proof.
(a) Let eHi/Hj (P‚àû) be the ramification index of P‚àû in Hi/Hj. When n = 2,
q ¬∑ vP‚àû(x2) = (q + 1)(‚àí1) ¬∑ eH2/H1(P‚àû).
146
Hn
qn‚àí2 qn‚àí3(q + 1) qn‚àíi(q + 1)i‚àí2 qn‚àíi+1(q + 1)i‚àí1
Fq2(x1, x2) Fq2(x2, x3)¬∑ ¬∑ ¬∑ Fq2(xi‚àí1, xi) Fq2(xi, xi+1)
q q + 1 q q + 1 q q + 1 q
Fq2(x1) Fq2(x2) Fq2(x3) Fq2(xi‚àí1) Fq2(xi)
By Theorem 46, the only place ramified in H2/H1 is P‚àû. And for 2 < i ‚â§ n, the
place ramified (and has nonzero difference exponent) in Hi/Hi‚àí1 is a pole of xi. But
‚àívP‚àû(xi) = q
n‚àíi(q + 1)i‚àí1 = ord(xi), so there is only one pole P‚àû of xi. Hence the
only one place ramified (has nonzero different exponent) in Hn/H1 is P‚àû which has
degree 1.
Let di = dHi/H1(P‚àû). For i ‚â• 3,
di = eHi/Hi‚àí1(P‚àû) ¬∑ dHi‚àí1/H1(P‚àû) + dHi/Hi‚àí1(P‚àû)
by [5, Corollary III.4.11]. By Theorem 46, d2(P‚àû) = (q ‚àí 1)(q + 2) and
dHi/Hi‚àí1(P‚àû) = (q + 1)
i‚àí1. So the assertion follows immediately as follows:
dn = q ¬∑ dn‚àí1 + (q ‚àí 1)((q + 1)
n‚àí1 + 1)
= qn‚àí2d2 + q
n‚àí3(q ‚àí 1)((q + 1)2 + 1) + ¬∑ ¬∑ ¬∑+ (q ‚àí 1)((q + 1)n‚àí1 + 1)
= qn‚àí2(q ‚àí 1)((q + 1) + 1) + (q ‚àí 1)
n‚àí1‚àë
i=2
qn‚àí1‚àíi((q + 1)i + 1).
= (q ‚àí 1)
n‚àí1‚àë
i=1
qn‚àí1‚àíi((q + 1)i + 1).
= (q ‚àí 1)
n‚àí1‚àë
i=1
qn‚àí1‚àíi(q + 1)i + (q ‚àí 1)
n‚àí1‚àë
i=1
qn‚àí1‚àíi.
= (q ‚àí 1)
n‚àí1‚àë
i=1
qn‚àí1‚àíi(q + 1)i + qn‚àí1 ‚àí 1.

148
The number of rational points in the projective curve Xn defined in (6.1) is q
n+1 + 1, this
follows by counting the solution (x0 : x1 : . . . : xn) ‚àà P n(Fq2) of the homogeneous equationsÔ£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
xq2x0 + x2x
q
0 = x
q+1
1
xq3x0 + x3x
q
0 = x
q+1
2
...
xqnx0 + xnx
q
0 = x
q+1
n‚àí1
.
Lemma 50. The projective curve Xn ‚äÇ P n(Fq2) defined in (6.1) has q
n+1 + 1 rational
points. 
Naturally, we then have a question: Is N(Hn) equal to the number of rational points at Xn,
i.e., N(Hn) = q
n+1 + 1? If so, then FH is asymptotically bad since
Œª(FH) ‚â§ lim
n‚Üí‚àû
qn+1 + 1
1
2
(n‚àí 1)(q ‚àí 1)qn‚àí1
=
q2
q ‚àí 1
lim
n‚Üí‚àû
2
n‚àí 1
= 0.
Theorem 51. N(Hn) = q
n+1 + 1.
Proof. The place P‚àû is totally ramified (by Theorem 46). Claim that all the other
rational places in H1 split completely in Hn/H1. Prove this by induction on n:
n = 2, consider the equation ‚àè
Œ≤‚àà‚Ñ¶0
(x2 ‚àí Œ≤) = x
q+1
1 .
If P0 ‚àà PH1 is the zero of x1, let P
‚Ä≤ ‚àà PH2 be an extension of P0, then P
‚Ä≤|P0 is unramified
by Theorem 46 and ‚àë
Œ≤‚àà‚Ñ¶0
vP ‚Ä≤(x2 ‚àí Œ≤) = (q + 1).
Hence the place P ‚Ä≤ ‚à© Fq2(x2) is a zero of
‚àè
Œ≤‚àà‚Ñ¶0
(x2 ‚àí Œ≤) and so is the zero of x2 ‚àí Œ≤0 for
some Œ≤0 ‚àà ‚Ñ¶0. Since each x2 ‚àí Œ≤ has no common zero in Fq2(x2),
q + 1 = vP ‚Ä≤(x2 ‚àí Œ≤0).
150
we get the decompositions ‚àè
Œ≤‚àà‚Ñ¶0
(xn ‚àí Œ≤) = x
q+1
n‚àí1,‚àè
Œ≤‚àà‚Ñ¶i
(xn ‚àí Œ≤) =
‚àè
Œ±‚ààŒìi
(xn‚àí1 ‚àí Œ±).
Since any rational place in Hn‚àí1 except for P‚àû is a zero of xn‚àí1 ‚àí Œ± for some Œ± ‚àà Fq2, the
same argument shows that they splits completely in Hn/Hn‚àí1. This completes the proof. 
6.3.2 The Weierstrass semigroup
Fix n, the order of each xi is q
n‚àíi(q + 1)i‚àí1 and P‚àû is the only pole of xi. When we
consider the Weierstrass semigroup S
(n)
P‚àû
with respect to P‚àû,
qn‚àí1, qn‚àí2(q + 1), . . . , (q + 1)n‚àí1 are nongaps and forms a subsemigroup S ‚Ä≤ of S
(n)
P‚àû
. The
sequence (a1, . . . , an) := (q
n‚àí1, qn‚àí2(q + 1), . . . , (q + 1)n‚àí1) is in fact a telescopic sequence,
the semigroup S ‚Ä≤ is therefore a telescopic semigroup [53]. The genus g(S ‚Ä≤) of
S ‚Ä≤ [53, Lemma 6.5] satisfies
2g(S ‚Ä≤)‚àí 1 = ‚àía1 + (q ‚àí 1)
n‚àë
i=2
ai
= ‚àíqn‚àí1 + (q ‚àí 1)
n‚àë
i=2
qn‚àíi(q + 1)i‚àí1
= ‚àíqn‚àí1 + (q ‚àí 1)
n‚àí1‚àë
i=1
qn‚àíi‚àí1(q + 1)i
= 2g(Hn)‚àí 1.
So g(S ‚Ä≤) = g(Hn) and hence S
‚Ä≤ = S
(n)
P‚àû
.
6.3.3 An explicit construction
Fix a function field F/Fq and a rational place Q. Let {œÜ1, . . . , œÜl} ‚äÇ F be a Weierstrass
generating set with respect to Q, then it is easy to find a basis for each corresponding AG
codes CL(D, rQ) (D = P1 + . . .+ PN is the divisor of the summation of all rational places
152
0.05 0.055 0.06 0.065 0.07 0.075 0.08 0.085 0.09 0.095 0.1
10‚àí8
10‚àí7
10‚àí6
10‚àí5
10‚àí4
10‚àí3
10‚àí2
10‚àí1
Symbol error probability
B
lo
ck
 e
rro
r r
at
e
RS code
H‚àícode 
H3‚àícode
Figure 6.1: Block Error Rate versus Symbol Error Probability.
Choose r = 187, consider the (256, 128, d‚Ä≤) code CL(D, rP‚àû) over F16, where d
‚Ä≤ ‚â• 69.
Compare this code with the (16, 8, 9) extended Reed-Solomon code and the (64, 32, 27)
Hermitian code [54]. They all have rate 1/2, and as seen in Figure 6.1, the (256, 128, d‚Ä≤)
code CL(D, rP‚àû) is better than the others two when the channel symbol error rate is no
more than 0.07.
154
algorithm is simple and efficien. The computation complexity of the algorithm is in the
order of O(Œ≥n2), which is the same as that of the Ko¬®tter‚Äôs algorithm, where n is the code
length. Yet the PESBM algorithm is superior, comparing with the Ko¬®tter‚Äôs algorithm, in
the following aspects:
1. With the early stopped property, the PESBM algorithm can save both processing
time and computation complexity.
2. With storing both nonzero discrepancy as well as the corresponding coefficient vector,
the proposed algorithm prevents from the additional multiplicative operations for the
normalization of the saved coefficient vector. The saved coefficient vector needs to be
normalized only when it is being used to update the currently used coefficient vector.
3. An accurate method of counting the available candidates is developed in the
algorithm. Based on the point of view from the Feng-Rao algorithm, the method to
count the total number of the available candidates is not correct in that of the
Ko¬®tter‚Äôs algorithm.
In this project, we also developed an efficient implementation of the PESBM algorithm via
systolic array by adopting the Œ≥-shift properties on the extended syndrome matrix M . The
time complexity of the proposed design is m+ g + 1 by using (t+ ‚åäg‚àí1
2
‚åã+ 1) PE cells, g D
cells and Œ≥ OE cells. Totally, at most Œ≥ finite-field inverters,
Œ≥(2t+2‚åä g‚àí1
2
‚åã‚àíŒ≥+1)
2
finite field
multipliers and Œ≥ finite state machines are required in the proposed architecture.
Therefore, the proposed design achieves a huge reduction on the hardware complexity by
only adding simple control circuits.
In this project, we have proposed an algorithm of error-value evaluation of Hermitian codes
by modifying the algorithm proposed in [31], when only one error-locator polynomial is
given instead of an ideal of error-locator polynomials. We also developed a hardware
architecture of the proposed algorithm and implemented via systolic array.
156
(a.2) If m is in the range (i‚àí 1)(q2 ‚àí 1) ‚â§ m < (i‚àí 1)(q2 ‚àí 1) + q2 + (q ‚àí 2)(q + 1),
then we have 0 < ki < ni and the set of roots of the leading monomial gii(t) of gi is
GF (q2)‚àó\ {Œ±‚àí(r+s(q+1)) ‚àà GF (q2)‚àó : 0 ‚â§ r ‚â§ q, 0 ‚â§ s ‚â§ q ‚àí 2,
rq + s(q + 1) + (i‚àí 1)(q2 ‚àí 1) ‚â§ m},
where GF (q2)‚àó is the multiplicative group GF (q2) \ {0}.
(a.3) If m < (i‚àí 1)(q2 ‚àí 1), then we have ki = 0.
(b) Let i = q + 1.
(b.1) If m ‚â• q(q2 ‚àí 1) + (q ‚àí 2)(q + 1), then kq+1 = nq+1.
(b.2) If m is in the range q(q2 ‚àí 1) ‚â§ m < q(q2 ‚àí 1) + (q ‚àí 2)(q + 1), then we have
0 < kq+1 < nq+1 and the set of roots of the leading monomial gq+1q+1(t) of gq+1 is
GF (q)‚àó \ {Œ±‚àís(q+1) ‚àà GF (q)‚àó : 0 ‚â§ s ‚â§ q ‚àí 2, s(q + 1) + q(q2 ‚àí 1) ‚â§ m},
where GF (q)‚àó is the multiplicative group GF (q) \ {0}.
(b.3) If m < q(q2 ‚àí 1), then we have kq+1 = 0.
Proof. Since a root of the gii(t) corresponds to a marked box in the ith row of the root
diagram, items (a.1), (a.2) and (b.1) follow immediately from Theorem 3.3(2), Theorem 3.4
and Theorem 3.3(3) in [12] respectively. By substituting q + 1, (
‚àèq
k=1Mk(y))y
s,
Pq+1,j = (0, Œ±
Œªq+1+j(q+1)) and vq+1(t) =
‚àëq‚àí2
j=0(Œ±
s(q+1)t)j for i, (
‚àèi‚àí1
k=1Mk(y))x
rys,
Pi,j = (Œ±
j, Œ±Œªi+j(q+1)) and vi(t) =
‚àëq2‚àí2
j=0 (Œ±
r+s(q+1)t)j respectively and following the
derivation in the proof of Theorem 3.4 in [12], the result of item (b.2) can be obtained.
(Here, we have changed the notations m, ‚Ñìm+1, hm+1(t), ‚Ñìi and hi(t) used in the proof of
Theorem 3.4 in [12] to our notations q, Œªq+1, vq+1(t), Œªi and vi(t).) Thus only items (a.3)
and (b.3) are left to be proved.
We first prove item (b.3). Since the marked boxes in the root diagram for Cm form a
subset of the marked box in the root diagram for Cm‚Ä≤ for any m
‚Ä≤ < m, it suffices to show
that the (q+ 1)th row of the root diagram for Cm is full if m = q(q
2‚àí 1)‚àí 1. Now consider
158
Thus the (q + 1)th row of the the root diagram for Cm is full.
We finally prove item (a.3) by induction. Since the marked boxes in the root diagram for
Cm form a subset of the marked box in the root diagram for Cm‚Ä≤ for any m
‚Ä≤ < m, it suffices
to show that for each 1 ‚â§ i ‚â§ q, if m = (i‚àí 1)(q2 ‚àí 1)‚àí 1, then the ith row of the root
diagram for Cm is full. We fist consider i = q and let m = (q ‚àí 1)(q2 ‚àí 1)‚àí 1. As discussed
in the proof of item (b.3), we can see that the (q + 2)th rows of the two root diagrams for
Cm and Cm+1 are full and the jth rows of the two root diagrams are empty for all
1 ‚â§ j ‚â§ q ‚àí 2. Note that (q + 1)th rows in the root diagrams for Cm and Cm+1 are both
full from (b.3). By following the same argument as in the proof of item (b.3), we can show
that the empty boxes in the (q‚àí 1)th row of the root diagram for Cm are the same as those
in the (q ‚àí 1)th row of the root diagram for Cm+1. Thus the root diagrams for Cm and
Cm+1 differ only in the qth row. Since the qth row of the root diagram for Cm+1 contains
only one empty box and k(m+ 1) = k(m) + 1 again by (1.5), the qth row of the root
diagram for Cm must be full. With similar argument, we can show that if the ith row of
the root diagram for Cm, m = (i‚àí 1)(q2 ‚àí 1)‚àí 1, is full, then the (i‚àí 1)th row of the root
diagram for Cm‚Ä≤ , m
‚Ä≤ = (i‚àí 2)(q2 ‚àí 1)‚àí 1, is also full. Hence by induction, for each
1 ‚â§ i ‚â§ q, if m = (i‚àí 1)(q2 ‚àí 1)‚àí 1, then the ith row of the root diagram for Cm is full. 2
Now we proceed to prove Theorem 2.
Proof of Theorem 2
We first proof that k1 ‚â• k2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• kq. Let i(m) be the smallest positive integer such that
m < (i(m)‚àí 1)(q2 ‚àí 1) + q2 + (q ‚àí 2)(q + 1). Consider the following cases:
‚Ä¢ i(m) > q : By Proposition 52 (a.1), we have kj = nj for all j ‚â§ q. Thus k1 = ¬∑ ¬∑ ¬∑ = kq.
‚Ä¢ i(m) = q : Again by Proposition 52 (a.1), we have kj = nj for all j ‚â§ q ‚àí 1. Thus
k1 = ¬∑ ¬∑ ¬∑ = kq‚àí1 > kq by Proposition 52 (a.2).
160
‚Ä¢ m < q(q2 ‚àí 1) : From Proposition 52 (b.3), we have kq+1 = 0 ‚â§ kq.
‚Ä¢ q(q2 ‚àí 1) ‚â§ m < (q ‚àí 1)(q2 ‚àí 1) + q2 + (q ‚àí 2)(q + 1) : In this case, we have
0 < kq < nq, 0 < kq+1 < nq+1, and
kq = |{(r, s)|0 ‚â§ r ‚â§ q, 0 ‚â§ s ‚â§ q ‚àí 2, rq + s(q + 1) + (q ‚àí 1)(q
2 ‚àí 1) ‚â§ m}|
kq+1 = |{s|0 ‚â§ s ‚â§ q ‚àí 2, s(q + 1) + q(q
2 ‚àí 1) ‚â§ m}|
from Proposition 52 (a.2) and (b.2). Thus kq ‚â• kq+1.
To finish our proof, consider kq+1 and kq+2. Since nq+2 = 1, we have kq+2 ‚â§ 1. If
kq+1 < kq+2, then we have kq+2 = 1 and kq+1 = 0. Since kq+2 = 1 implies that (0, 0, . . . , 0, 1)
is a codeword of Cm, the minimum distance dmin(m) of Cm must be equal to 1. However,
kq+1 = 0 implies that m < q(q
2 ‚àí 1) by Proposition 52 (b.3). From (1.6), the minimum
distance satisfies
dmin(m) ‚â• q
3 ‚àím > q,
which is a contradiction. Hence we conclude that k1 ‚â• k2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• kq ‚â• kq+1 ‚â• kq+2. 2
.2 Proof of Lemma 3
With (3.2) and by induction, the kth column ak, 1 ‚â§ k ‚â§ n, of the original matrix A is a
linear combination of the first k columns a‚Ä≤v, 1 ‚â§ v ‚â§ k, of the left-reduced matrix A
‚Ä≤,
ak = Œ≥k‚àí1a
‚Ä≤
1 + ¬∑ ¬∑ ¬∑+ Œ≥1a
‚Ä≤
k‚àí1 + a
‚Ä≤
k (1)
where Œ≥v, 1 ‚â§ v ‚â§ k ‚àí 1, are constants and dependent on the column index k.
Now assume that the left-reduced matrix A‚Ä≤ of A has a pivot at the (i, j)th position. Then
as stated in (3.3), column aj of A is a partial linear combination of its previous columns
up to the (i‚àí 1)th entry. Suppose that column aj of A is also a partial linear combination
of its previous columns up to the ith entry, i.e. there exist Œ≤1, . . . , Œ≤i‚àí1 such that
au,j = ‚àíŒ≤j‚àí1au,1 ‚àí ¬∑ ¬∑ ¬∑ ‚àí Œ≤1au,j‚àí1, ‚àÄ 1 ‚â§ u ‚â§ i. (2)
162
Bibliography
[1] V. D. Goppa, ‚ÄúCodes associated with divisors,‚Äù Probl. Peredachi Inform., vol. 13,
no. 1, pp. 33‚Äì39, 1977.
[2] ‚Äî‚Äî, ‚ÄúCodes on algebraic curves,‚Äù Dokl. Akad. Nauk SSSR, vol. 24, pp. 170‚Äì172, 1981.
[3] ‚Äî‚Äî, ‚ÄúAlgebraic-geometric codes,‚Äù Izv. Akad. Nauk SSSR, vol. 21, pp. 75‚Äì91, 1983.
[4] M. A. Tsfasman, S. G. VlaÀádut, and T. Zink, ‚ÄúModular curves, Shimura curves, and
Goppa codes better than Varshamove-Gilbert bound,‚Äù Math. Nachr., vol. 104, pp.
13‚Äì28, 1982.
[5] H. Stichtenoth, Algebraic Function Fields and Codes. New York: Springer-Verlag,
1993.
[6] C.-C. Lu, H.-S. Wang, and J.-P. Chen, ‚ÄúEfficient architectures for syndrome
generation and error location search in the decoding of hermitian codes,‚Äù IEEE Trans.
Communications, vol. 52, no. 2, pp. 176‚Äì179, Feb. 2004.
[7] W. Fulton, Algebraic Curves: An Intorduction to Algebraic Geometry. Rewood City,
CA: Addson-Wesley, 1989.
[8] K. Saints and C. Heegard, ‚ÄúAlgebraic-geometric codes and multidimensional cyclic
codes : A unified theory and algorithms for decoding using gro¬®bner bases,‚Äù IEEE
Trans. Inform. Theory, vol. 41, pp. 1733‚Äì1751, Nov. 1995.
164
[19] I. M. Duursma93, ‚ÄúMajority coset decoding,‚Äù IEEE Trans. Inform. Theory, vol. 39,
pp. 1067‚Äì1070, May 1993.
[20] G. L. Feng, V. K. Wei, T. R. N. Rao, and K. K. Tzeng, ‚ÄúSimplified understanding and
effect decoding of a class of algebraic-geometric codes,‚Äù IEEE Trans. Inform. Theory,
vol. 40, pp. 981‚Äì1002, July 1994.
[21] S. Sakata, ‚ÄúFinding a minimal set of linear recurring relations capable of generating a
giving finite two-dimensional array,‚Äù I. Symbolic Comput., vol. 5, pp. 321‚Äì337, May
1988.
[22] J. Justensen, K. J. Larsen, H. E. Jensen, and T. H√∏holdt, ‚ÄúFast decoding of codes from
algebraic plane curves,‚Äù IEEE Trans. Inform. Theory, vol. 38, pp. 111‚Äì119, Jan. 1992.
[23] S. Sakata, J. Justesen, Y. Madelung, H. E. Jensen, and T. H√∏holdt, ‚ÄúA fast decoding
method of AG codes from Miura-Kamiya curves cab up to half the Feng-Rao bound,‚Äù
Finite Fields and Their Applications, vol. 1, pp. 83‚Äì101, 1995.
[24] C.-W. Liu and C.-C. Lu, ‚ÄúA view of gaussian elimination applied to early stopped
Berlekamp-Massey algorithm,‚Äù IEEE Trans. Communications, vol. 55, no. 6, pp.
1131‚Äì1143, June 2007.
[25] M. Kurihara and S. Sakata, ‚ÄúA fast parallel decoding algorithm for general one-point
AG codes with a systolic array architecture,‚Äù Proceedings of IEEEE ISIT‚Äô95, p. 99.
[26] S. Sakata and M. Kurihara, ‚ÄúA systolic array architecture for implementation a fast
parallel decoding algoritm of one-point AG codes,‚Äù Proceedings of ISIT‚Äô97, p. 378.
[27] M. E. O‚ÄôSullivan, ‚ÄúVlsi architecture for a decoder for Hermitian codes,‚Äù Proceedings of
ISIT‚Äô97, p. 376.
[28] C.-W. Liu, K.-T. Huang, and C.-C. Lu, ‚ÄúA systolic array implementation of the
Fang-Rao algorithm,‚Äù IEEE Trans. Computers, vol. 48, no. 7, pp. 690‚Äì706, July 1999.
166
[40] A. Skorobogatov and S. VlaÀádut, ‚ÄúOn the deocding of algebraic-geometric codes,‚Äù
IEEE Trans. Inform. Theory, vol. 36, pp. 1051‚Äì1060, Sept. 1990.
[41] D. A. Leonard, ‚ÄúError-locator ideals for algebraic-goemetric codes,‚Äù IEEE Trans.
Inform. Theory, vol. 41, pp. 819‚Äì824, May 1995.
[42] S. Sakata, H. E. Jensen, and T. H√∏holdt, ‚ÄúGeneralized Berlekamp-Massey decoding of
algebraic-geometric codes up to half the Feng-Rao bound,‚Äù IEEE Trans. Inform.
Theory, vol. 41, pp. 1762‚Äì1768, Nov. 1995.
[43] P. Q. B. Hochet and Y. Robert, ‚ÄúSystolic gaussian elimination over gf(p) with partial
pivoting,‚Äù IEEE Trans. Computers, vol. 38, no. 9, pp. 1321‚Äì1324, Sep. 1989.
[44] S.-Y. Kung, VLSI Array Processors. NJ: Prentice Hall, 1988.
[45] P. S. Tseng, A Systolic Array Parallelizing Compiler. Boston, MA: Kluwer Academic,
1990.
[46] M. F. Atiyah and I. G. Macdonald, Introduction to Commutative Algebra. Reading,
MA: Addison-Wesley, 1969.
[47] H. Matsumura, Commutative Algebra, 2nd ed. Reading, MA: Benjamin/Cummings,
1992.
[48] F. J. MacWilliams and N. J. A. Sloane, The Theory of Error-Correcting codes. New
York: North-Holland, 1978.
[49] M. A. Tsfasman and S. G. VlaÀádut, Algebraic-Geometric Codes. Amsterdam,
Netherlands: Kluwer Academic, 1991.
[50] J. H. van Lint, Introduction to Coding Theory, 3rd ed. Berlin, Germany:
Springer-Verlag, 1999.
[51] T. W. Hungerford, Algebra. New York: Springer-Verlag, 1974.
168
Ô®àÊîøÈô¢ÂúãÂÆ∂ÁßëÂ≠∏ÂßîÂì°ÊúÉË£úÂä©ÂúãÂÖßÂ∞àÂÆ∂Â≠∏ËÄÖÂá∫Â∏≠ÂúãÈöõÂ≠∏Ë°ìÊúÉË≠∞Â†±Âëä 
                                                            2007 Ô¶é 7 Êúà 27 Êó• 
Â†±Âëä‰∫∫ÂßìÂêç  Ô¶ÄÂø†Ê¥• 
 
ÊúçÂãôÊ©üÊßã
ÂèäËÅ∑Á®± 
 
ÂúãÔß∑Ê∏ÖËèØÂ§ßÂ≠∏ÈõªÊ©üÂ∑•Á®ãÁ≥ªÊïôÊéà 
 
 ÊúÉË≠∞ÊôÇÈñì 
     Âú∞Èªû 
2007 Ô¶é 6 Êúà 24 Êó•Ëá≥ 2007 Ô¶é 6 Êúà 29 Êó• 
Nice, France 
ÊúÉË≠∞ 
ÂêçÁ®± 
 (‰∏≠Êñá) ‰∫åÔºêÔºê‰∏ÉÔ¶é IEEE ÂúãÈöõË®äÊÅØÔß§Ô•ÅÊúÉË≠∞ 
 (Ëã±Êñá) 2007 IEEE International Symposium on Information Theory 
ÁôºË°® 
Ô•ÅÊñá 
È°åÁõÆ 
 (‰∏≠Êñá) ‰ª•Âª£Áæ©ÂÖãÔ§èÂÖßÂÖãÁ©çÂª∫ÊßãÁöÑÁ¢º 
 (Ëã±Êñá) On Codes Constructed by Generalized Kronecker Product 
 
Ë°® Y04 
