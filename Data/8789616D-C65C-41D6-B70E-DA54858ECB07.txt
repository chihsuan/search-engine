å¯ä¾›æ¨å»£ä¹‹ç ”ç™¼æˆæœè³‡æ–™è¡¨ 
â–¡ å¯ç”³è«‹å°ˆï§  â–‰ å¯æŠ€è¡“ç§»è½‰                                      æ—¥æœŸï¼š98ï¦10æœˆ28æ—¥ 
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•« 
è¨ˆç•«åç¨±ï¼šç§»å‹•å¹³è‡ºä¹‹å‰ç»æ€§è¦–è¨Šç›£æ§æŠ€è¡“ 
è¨ˆç•«ä¸»æŒäººï¼šé»ƒé›…è»’       
è¨ˆç•«ç·¨è™Ÿï¼šNSC 97-2221-E-216-040             
å­¸é–€ï¦´åŸŸï¼šè³‡è¨Š 
æŠ€è¡“/å‰µä½œåç¨± å¼·å¥æ€§äººè‡‰è¾¨ï§¼ 
ç™¼æ˜äºº/å‰µä½œäºº é»ƒé›…è»’ 
æŠ€è¡“ï¥¯æ˜ 
z ä¸­æ–‡ï¼šæœ¬æŠ€è¡“å¯¦ç¾äºŒç¨®å…·é«˜é‘‘åˆ¥ï¨çš„äººè‡‰è¾¨ï§¼æ–¹æ³•(CMSM å’Œ
GDA)ï¼Œä¸¦å°‡ä»–å€‘æœ‰æ•ˆçš„çµ„åˆï¼Œä»¥å¾—åˆ°ä¸€å¥—å…·å¼·éµæ€§çš„äººè‡‰è¾¨ï§¼
ç³»çµ±ã€‚CMSM (Constrained Mutual Subspace Methodï¼Œé™åˆ¶æ€§å­ç©º
é–“)ä½¿ç”¨å¤šå¼µå½±åƒæ‰€å½¢æˆçš„å­ç©ºé–“ä½œç‚ºè¾¨ï§¼çš„ä¾æ“šï¼Œå¯è¡¨ç¤ºä½¿ç”¨
è€…äººè‡‰ç‰¹æœ‰çš„è®ŠåŒ–å½¢æ…‹ã€‚GDA (Generalized Discriminant Analysis)
ä½¿ç”¨æ ¸å‡½ï¥©ä¹‹éç·šæ€§å€åˆ¥åˆ†æï¼Œå°‡è³‡ï¦¾æ˜ å°„è‡³é«˜ç¶­ï¨ç©ºé–“ä½¿å…¶ç›¡
ï¥¾ç·šæ€§å¯åˆ†å‰²ã€‚é€™äºŒç¨®æ–¹æ³•ä½¿ç”¨ï¥§åŒç¨®ï§çš„ç‰¹å¾µå’Œæ¯”å°æ©Ÿåˆ¶ï¼Œå› 
æ­¤ä»–å€‘çš„æ¯”å°çµæœå…·æœ‰é«˜ï¨çš„äº’è£œæ€§ã€‚é‡å°é€šç”¨çš„ Banca äººè‡‰è³‡
ï¦¾åº«ï¼Œæœ¬ç ”ç©¶çš„è¾¨ï§¼çµæœåœ¨å®¹å¿ 10%çš„éŒ¯èª¤æ¥å—ï¥¡(False Accept 
Rate)æ¢ä»¶ä¸‹ï¼Œæ­£ç¢ºçš„è¾¨ï§¼ï¥¡(Recognition Rate)å¯é«˜é” 98%ã€‚ 
è‹±æ–‡ï¼šThis paper presents a robust face recognition method which two 
highly discriminating algorithms (CMSM and GDA) to recognize 
human faces. CMSM (Constraint Mutual Subspace Method) constructs 
a class subspace for each person and makes the relation between class
subspaces by projecting them onto a generalized difference subspace 
so that the canonical angles between subspaces are enlarged to 
approach to the orthogonal relation. GDA (Generalized Discriminant 
Analysis) adopts kernel function operator to make it easy to extend and 
generalize the classical Linear Discriminant Analysis to a non linear 
one. Both CMSM and GDA are effective to recognize human faces, 
however, CMSM constructs a subspace from several face images and 
GDA needs only one face image to perform recognition. Obviously, 
these two methods inherently have different properties and abilities of 
recognition so that we combine them together. Experimental results 
show that the proposed method can achieve good recognition
accuracy. 
é™„ä»¶äºŒ 
Face Recognition Based on Complementary Matching of Single Image and 
Sequential Images 
 
 
Yea-Shuan Huang  and  Wei-Cheng Liu 
Computer Science & Information Engineering, Chung Hua University 
No.707, Sec. WuFu Rd., Hsinchu, Taiwan, 300, R.O.C. 
 
 
Abstract 
 
This paper presents a robust face recognition method 
which two highly discriminating algorithms (CMSM and 
GDA) to recognize human faces. CMSM (Constraint 
Mutual Subspace Method) constructs a class subspace 
for each person and makes the relation between class 
subspaces by projecting them onto a generalized 
difference subspace so that the canonical angles between 
subspaces are enlarged to approach to the orthogonal 
relation. GDA (Generalized Discriminant Analysis) 
adopts kernel function operator to make it easy to extend 
and generalize the classical Linear Discriminant 
Analysis to a non linear one. Both CMSM and GDA are 
effective to recognize human faces, however, CMSM 
constructs a subspace from several face images and GDA 
needs only one face image to perform recognition. 
Obviously, these two methods inherently have different 
properties and abilities of recognition so that we 
combine them together. Experimental results show that 
the proposed method can achieve good recognition 
accuracy. 
 
1. Introduction 
Biometric identification technology is a very popular 
research field in the recent years. Various methods have 
been proposed that use different kinds of biometric data. 
Among them, face recognition consistently obtains a 
great expectation since it is contact-free and is user 
friendly. Therefore, a lot of research efforts have been 
devoted to this field, and many face recognition 
approaches based on a variety of machine learning 
theorem have been developed already. For example, 
subspace methods such as PCA [1] and LDA [2] are 
commonly used which project high dimensional features 
to low dimensional features and not only faster but also 
better recognition can be achieved. In general, LDA has 
better recognition ability than PCA which is based on an 
eigenvalue resolution and gives an exact solution of the 
maximum of the inertia. But even LDA fails for a 
nonlinear problem. A Generalized Discriminant Analysis 
(GDA) is developed to overcome this difficulty by 
mapping the input space into a high dimensional feature 
space with linear properties so that it can solve the 
problem in a LDA classical way. 
Basically, the feature derived from a single image 
denotes the location of this image in a high dimensional 
feature space. In the feature space, the locations 
corresponding to two similar images will be in general 
close to each other, and the locations of two very distinct 
images then will be quite separated apart. Therefore, 
recognition based on a single image mainly measures the 
distance (or similarity) of the features between the input 
pattern and the reference patterns. However, the feature 
derived from a set of sequential images of the same 
person can present the unique variation model of this 
person. Therefore, recognition based on sequential 
images indeed compares the specific variation pattern of 
the unknown input subject and that of each individual 
class. The two kinds of recognition seem to be 
complementary in nature. With this understanding, it will 
be very useful if both kinds of methods are combined 
together. In this paper, GDA (Generalized Discriminant 
Analysis) and CMSM (Constraint Mutual Subspace 
Method) are used together to recognize faces not only 
because they both have high recognition abilities, but 
also they probably are complementary to each other since 
GDA takes a single-image matching strategy and CMSM 
takes a sequential-image matching strategy. 
This paper is organized as follows. Section 2 describes 
two recognition models; the first is GDA and the second 
is CMSM. A linear mechanism is also proposed to 
integrate their recognition results. Section 3 presents the 
experiment results on the famous Banca face database, 
and the final conclusion is drawn in Section 4. 
 
2. Face identification method 
In this section, we describe our face recognition 
framework which integrates a single-image matching 
module and a sequence-image matching module. The 
single-image matching module uses a GDA algorithm to 
reduce feature dimension first, and then adopts a nearest 
distance classification to recognize the input pattern; the 
sequence-image matching module uses a CMSM metric 
which projects each individual subspace including the 
input and the reference subspaces onto a common 
difference subspace and their canonical angles are used 
space F. Through the kernel method, GDA in general has 
much better discrimination ability than LDA. 
The transformed feature ğ‘¦  now becomes ğ‘¦ = ğ‘£ğ‘‡ğ‘¥ 
where ğ‘¥  is a sample feature vector. For recognition, a 
nearest distance classification metric is applied. Let  
ğ¼1 , â‹¯ , ğ¼ğ‘š  denote the feature vectors of ğ‘š input samples,  
ğ¼1
ğºğ·ğ´ , â€¦ , ğ¼ğ‘š
ğºğ·ğ´  are their GDA transformed vectors, and 
ğ‘…ğ‘˜ ,ğ‘
ğºğ·ğ´   ğ‘ = 1, â€¦ , ğ‘›  be the GDA transformed feature 
vector of the ğ‘th sample of the ğ‘˜th enrolled person. Then, 
the distance of the m input samples and the n reference 
data of the kth person becomes 
ğ·ğ‘–ğ‘ ğ‘¡(ğ‘˜) = min
ğ‘=1,â‹¯,ğ‘š
 min
q=1,â‹¯,n
ğ‘‘ ğ¼ğ‘
ğºğ·ğ´ , ğ‘…ğ‘˜ ,ğ‘
ğºğ·ğ´  
 
2.2. Constrain mutual subspace method 
2.2.1. Concept of canonical angle 
In linear algebra, the similarity between two 
subspaces is calculated by the angle between them. 
Suppose  ğ‘…1, â€¦ , ğ‘…ğ‘Ÿ  is a set of r reference patterns, 
 ğ¼1 , â€¦ , ğ¼ğ‘   is a set of s input patterns, and each pattern is 
represented by an f-dimensional feature vector. With 
PCA, an rno-dimensional reference subspace Î© can be 
constructed from  ğ‘…1, â€¦ , ğ‘…ğ‘Ÿ , and an sno-dimensional 
input subspace Î› can be constructed from  ğ¼1 , â€¦ , ğ¼ğ‘   
respectively. Therefore, Î© is an  rno Ã— f  matrix and Î› is 
an  sno Ã— f  matrix. In general, the relations of r, s, rno and 
sno are chosen to be rnoâ‰¦r, snoâ‰¦s and rnoâ‰¦sno. We can 
further obtain rno canonical angles   ğœƒ1, â€¦ , ğœƒğ‘Ÿğ‘›ğ‘œ   between 
subspace Î© and subspace Î› by the following equations: 
CXC ï¬ï€½  
ï€¨ ï€© ï€¨ ï€©ï€¨ ï€©ïƒ¥ ï€½ ïƒ—ïƒ—ï€½ï€½ no
r
k jkkiijij
xxX
1
  , ï¹ï¦ï¦ï¹
 
where 
iï¹  and iï¦  denote respectively the i-th f-
dimensional orthonormal basis vector of subspace Î© and 
Î›, ğœ† is an eigenvalue of X and C is the eigenvectors of X, 
and X is an rno Ã— rno matrix. The value ğ‘ğ‘œğ‘ 
2 ğœƒğ‘–  of the i-th 
smallest canonical angle equals to the i-th largest 
eigenvalue of Î›. The largest eigenvalue (i.e. ğ‘ğ‘œğ‘ 2 ğœƒ1) is 
taken to denote the similarity between subspace Î© and Î›. 
 
2.2.2. Generation of constrained subspace 
In CMSM, it is essentially important to generate a 
proper constrained subspace C which contains the 
effective matching components but eliminating the 
unnecessary ones. By projecting the input subspace and 
reference subspaces to a constrained subspace, it could 
extract discriminating features for recognizing pattern 
classes. 
Suppose there are in total Np reference subspaces. To 
generate a constrained subspace, we compute the 
projection matrix Î©k of the k-th reference subspace using 
ï€¨ ï€©
Tr
j
k
j
k
jk
no
P ïƒ¥ ï€½ï€½ 1 ï¹ï¹  
where rno is the number of eigenvectors of a reference 
subspace, k
jï¹  is the j-th orthonormal basis vector of the 
k-th reference subspace, and each Pk is a ğ‘“ Ã— ğ‘“ matrix. 
Then, we calculate the eigenvectors of the summation 
matrix ğ‘† =  ğ‘ƒ1 + ğ‘ƒ2 + â‹¯ + ğ‘ƒğ‘ğ‘ , that is ğ‘†ğ´ = ğœ†ğ´, where ğœ† 
and A denote the eigenvalues and the eigenvectors of S 
respectively. Finally, the t eigenvectors [A1,â€¦,At] 
corresponding to the t smallest eigenvalues are selected 
to construct the constrained subspace CS (that is  
CS=[A1,â€¦,At]tÃ—f). For a more detailed description of 
CMSM, please see [4]. 
 
2.2.3. Matching on constrained subspace 
Suppose there are in total L recognition classes. Î  
denotes the input subspace derived from the input 
sequence samples, and Î¤k (1 â‰¦ k â‰¦ L) denotes the 
subspace derived from the training sequence samples of 
class k. Five steps need to be performed for pattern 
matching as follows: 
1. Project each Î¤k onto CS and generate an rnoÃ— t 
projection matrix Pk; 
2. Normalize each Pk, and with a Gram-Schmidt 
algorithm derive a reference subspace Î©k with 
basis {ğœ“1
ğ‘˜ , â‹¯ , ğœ“ğ‘¡_ğ‘›ğ‘œ
ğ‘˜ }; 
3. Project Î  onto CS and generate an snoÃ— t 
projection matrix Q; 
4. Normalize Q, and with a Gram-Schmidt algorithm 
derive the input subspace Î› with basis 
{ğœ™1, â‹¯ , ğœ™ğ‘¡_ğ‘›ğ‘œ }; 
5. Compute the similarity ğ‘†ğ‘–ğ‘š(ğ‘˜) between Î› and Î©i 
by using the canonical angle computation as 
ğ‘†ğ‘–ğ‘š(ğ‘˜) =    ğœ“ğ‘–
ğ‘˜ , ğœ™ğ‘—  
2
ğ‘Ÿğ‘›ğ‘œ
ğ‘—
ğ‘ ğ‘›ğ‘œ
ğ‘–
 
 
2.3. Combination scheme 
Obviously, ğ·ğ‘–ğ‘ ğ‘¡(ğ‘˜)  is a distance measurement and  
ğ‘†ğ‘–ğ‘š(ğ‘˜)  is a similarity measurement, they have totally 
different interpretation, and both small  ğ·ğ‘–ğ‘ ğ‘¡(ğ‘˜)  and 
large ğ‘†ğ‘–ğ‘š(ğ‘˜)  denote that the input patterns and the 
reference data of person k are similar to each other. In 
order to combine the matching scores of GDA and 
CMSM, the integrated value of similarity is calculated as 
ïƒ·
ïƒ¸
ïƒ¶
ïƒ§
ïƒ¨
ïƒ¦
ï€­ï‚´ï€«ï‚´ï€½
ï³
ï¡ï·ï·
)(
)()( 21
kDist
kSimksimilarity  
where ğœ”1  and ğœ”2  are the combining weights of the two 
matching scores, and ğ›¼  and ğœ  are two normalized 
parameter. All the parameters are decided by experiments. 
 
3. Experimental results 
We used the famous Banca face database to evaluate 
the performance of the proposed recognition method. The 
Banca database contains 52 individuals and each 
individual has 12 image sequences that were taken in 
different time, at different locations and by difference 
cameras. Each image sequence consists of 10 face images 
 
 2.1. The shape model 
 
Supposed there are n facial feature points and each one 
is located at obvious face contour. The position of these 
n points can be arranged into a shape vector X, that is 
 
ğ‘‹ = [ ğ‘¥1 , ğ‘¦1 , ğ‘¥2 , ğ‘¦2 , â€¦ , ğ‘¥ğ‘˜ , ğ‘¦ğ‘˜ , â€¦ , ğ‘¥ğ‘› , ğ‘¦ğ‘›  ]
ğ‘‡  
 
where ğ‘¥ğ‘˜  and ğ‘¦ğ‘˜  are the X coordinate and Y coordinate 
of the k
th
 point respectively. 
All the training shapes should be aligned first 
because we want to obtain the statistic variation of 
shapes instead of the variation of locations. The ASM 
alignment procedure is an iterative process to align 
multiple face shapes which can be summarized as 
follow: 
1. All training sample are normalized according to 
two eyes positions. 
2. Rotate, scale and translate each shape to align with 
the first shape in the training set. 
3. Calculate the mean shape from the aligned shapes. 
4. Normalize the mean shape. 
5. Realign every shape with the normalized mean 
shape. 
6. If not convergence, return to step 3. 
 
When finishing the alignment procedure, by using 
the Principal Component Analysis (PCA) operation 
eigenvectors corresponding to shape variations can be 
generated. Therefore, a shape model can be represented 
as: 
ğ‘¥ = ğ‘‹ + ğ‘ƒğ‘ 
 
where ğ‘‹  is the mean shape, ğ‘ƒ = [ğ›·1  ğ›·2 â€¦  ğ›·ğ‘¡] is the 
eigenvectors corresponding to the t largest eigenvalues 
and ğ‘  is the shape parameter which is the projection 
coefficiency that X projects onto P. Figure 1 shows the 
face models of the first three eigenvectors with varying 
ğ‘ğ‘–  value. Obviously, ğ‘ğ‘–  defines shape variation. In 
general, the larger ğ‘ğ‘–  is, the more deviation the face 
shape will be. Usually, ğ‘ğ‘–  is constrained within the 
range of  Â± 3 ğœ†ğ‘–  , so that a constructed face shape will 
not degenerate too much. 
 
 
Fig. 1: The variation of the first three parameters of the 
face model, the horizontal represent the variation value 
of shape parameter, and the vertical corresponds to the 
face models derived from different eigenvectors. 
 
2.2. The feature model 
 
In general, we suppose a landmark is located on the 
strong edge. According to the normal direction of 
landmark, we can get m pixels on both sides (Fig. 2) of 
this landmark and each pixel has a gray-level value. So 
there are in total 2m+1 gray-level values which form a 
gray-level profile represented as 
ğ‘”ğ‘– =  ğ‘”ğ‘–0, ğ‘”ğ‘–1 , â€¦ , ğ‘”ğ‘–(2ğ‘š) , where i is the landmark index. 
In order to capture the frequency information, the 
profile first derivative ğ‘‘ğ‘”ğ‘—  is calculated as  
 
ğ‘‘ğ‘”ğ‘– =  ğ‘”ğ‘–1 âˆ’ ğ‘”ğ‘–0 , ğ‘”ğ‘–2 âˆ’ ğ‘”ğ‘–1 , â€¦ , ğ‘”ğ‘– 2ğ‘š âˆ’ğ‘”ğ‘–(2ğ‘šâˆ’1) . 
 
In order to lessen the effect of varying image lighting 
and contrast, the profile is normalized as  
 
ğ‘¦ğ‘– =
ğ‘‘ğ‘” ğ‘–
 |ğ‘‘ğ‘” ğ‘–ğ‘˜ |
2ğ‘šâˆ’1
ğ‘˜=0
   where ğ‘‘ğ‘”ğ‘–ğ‘˜ = ğ‘”ğ‘– ğ‘˜+1 âˆ’ ğ‘”ğ‘–ğ‘˜ . 
 
The feature vector ğ‘¦ğ‘–  is called grayscale profile. 
 
 
Fig. 2: The selected feature points for constructing the 
grayscale profile. 
 
2.3. The ASM algorithm 
 
The ASM searching algorithm uses an iteration 
process to find the best landmarks which can be 
summarized as follow: 
 
1. Initial the shape parameters ğ‘  to zero (the mean 
shape). 
2. Generate the shape model point using the ğ‘¥ = ğ‘‹ +
ğ‘ƒğ‘. 
3. Find the best landmark ğ‘§  by using the feature 
model. 
4. Calculate the parameters bâ€²  by the following 
equation           
 ğ‘â€² = ğ‘ƒğ‘‡(ğ‘§ âˆ’ ğ‘‹ ). 
5. Restrict parameter bâ€²  to be within Â± 3 ğœ†ğ‘– . 
If |ğ‘â€² âˆ’ ğ‘|  is less than the threshold value, then the 
matching process is completed; else ğ‘ = ğ‘â€² , then return 
to step 2. 
 
considerably. In fact, different landmarks may have 
different search range for their best performance. With 
this consideration, landmarks are divided into five 
clusters according to their locations and structures. The 
five clusters are the eye cluster, the eyebrow cluster, the 
nose cluster, the mouth cluster and the facial contour 
cluster. Because the shape variations of the five clusters 
are different, we can utilize the shape variation 
information to determine more appropriate searching 
ranges so that better landmark matching can be achieved. 
Therefore, we calculate the standard deviation ğ‘†ğ‘–  of the 
i-th landmark and then the largest standard deviation ğ‘†ğ‘  
of each cluster will be used to derive the search range of 
this cluster. It can be represented as 
 
ğ‘†ğ‘ = ğ‘šğ‘ğ‘¥
ğ‘–âˆˆğ‘
(ğ‘†ğ‘–)   
 
where b is 0 to 4 which corresponds to one specific 
cluster. Finally, in order to obtain better results, we take 
twice the length of ğ‘†ğ‘  to indicate the profile search 
range. 
 
However, some search ranges of the upper eyelid 
landmarks are overlapped with those of the lower eyelid 
landmarks. This situation will result in the possibility to 
mismatch them, i.e. an upper eyelid landmark is 
mismatched to a lower eyelid landmark or vice versa. In 
order to avoid this problem, the search range of each 
eyelid landmark must be constrained. According to the 
geometry of eye-lid landmarks, the search range of each 
upper eyelid landmark should not be lower than the 
middle line between the inner eye corner and the outer 
eye corner. Similarly, we can define the constraint for 
the lower eyelid landmark. Fig. 6 illustrates the search 
range restrictions of the upper and the lower eyelid 
landmarks. 
 
 
Fig. 6: Example of the restrictions of the eyelid. 
 
The same issue also happens in the mouth 
landmarks, but we cannot use the same constraints to 
restrict their search ranges. This is because people has 
various expressions and the lower lip may be higher 
than the connection line of the two mouth corners. 
Therefore, we intend to use a sorting method to reduce 
the impact of this issue. Firstly, the mouth landmarks 
are clustered into three groups: G1, G2 and G3. Fig. 7 
shows the clustering results. In the G1 group, when the 
best matching points of all 4 landmarks have been 
determined, we can reference the heights of the 4 
matching points to order the four landmarks. The order 
should be the same as the one defined in Fig. 7. The 
other two clusters G2 and G3 use the same method. 
Accordingly, the wrong mismatch in mouth will also be 
decreased. 
 
 
Fig. 7: The clustering results. 
 
Finally, the Mahalanobis distance is used to 
measure distance between patterns which is computed 
as  
 
ğ‘“ ğ‘Œ =   ğ‘Œ âˆ’ ğ‘Œğ‘–  ğ‘‡ğ¶ğ‘–
âˆ’1(ğ‘Œ âˆ’ ğ‘Œğ‘– ) 
 
where ğ¶ğ‘–  is the covariance matrix of landmark i. 
 
        We use an Adaboost algorithm to construct a 
detector for each landmark of the corner group. The 
Adaboost algorithm has been extensively used for object 
detection and it often has an outstanding performance. 
The search range of a corner landmark is ğ‘™ Ã— ğ‘™ centered 
by the corresponding landmark location of the 
reconstructed shape model. If there are multiple 
detected points among the search range, the point being 
closest to the corresponding landmark of the 
reconstructed shape model is taken as the best result. 
This strategy can avoid abnormal deviation so that it 
still has a chance to obtain a good matching location in 
the next iteration. 
 
4. EXPERIMENTAL RESULTS 
 
We use the well known BioID face database as the 
training database, which contains 1508 face images. In 
order to increase the training samples, the mirror images 
will be used, too. In total, there are 3016 face images 
used in the training stage. The Cohn Kanade database 
which contains 2132 face images is used for testing. Fig. 
8 show some samples of both databases. 67 landmark 
points are manually labeled for all the images of the two 
databases. 
 
  
(a)                                       (b) 
Fig. 8: (a)Examples of the BIOID database, 
           (b) Examples of the Cohn Kanade database. 
 
In order to evaluate the accuracy of our proposed 
method, the error rate E is defined as 
REFERENCES 
 
[1] ZHANG Baizhen and RUAN Qiuqi, "Facial feature 
extraction using improved deformable templates", The 
8th International Conference on Signal Process 
Nolumn4, page : digital object identifier 
10.1109/ICOSP.2006.345927. 
[2] T. F. Coots, C. Taylor, D. Cooper, and J. Graham, 
Active shape models â€“ their training and application, 
Computer Vision and Image Understanding, 61(1):38-
59, January 1995. 
[3] T. F. Cootes and C. J. Taylor, Statistical models of 
appearance for computer vision, Tech. Report, Oct 2001, 
University of Manchester, 
http://www.isbe.man.ac.uk/~bim/, Oct 2001. 
[4]  Kwok-Wai Wan, Kin-Man Lam, Kit-Chong Ng, "An 
accurate active shape model for facial feature 
extraction", Pattern Recognition Letters , Volume 26 , 
Issue 15,  November 2005 
[5]  T. F. Cootes, G. J. Edwards, and C. J. Taylor, Active 
Appearance Models, in Proc. European Conference on 
Computer Vision 1998. (H. Burkhardt and B. Neumann 
Ed.s). Vol. 2, pp. 484~498, Springer, 1998. 
[6]  T.F. Cootes, G.J. Edwards, C.J. Taylor, Active 
appearance models, IEEE Transactions on Pattern 
Analysis and Machine Intelligence 23 (2001) 681â€“685. 
[7] Fei Zuo, Peter H.N. de With, Fast facial feature 
extraction using a deformable shape model with Haar-
wavelet based local texture attributes, Proceedings of 
IEEE Conference on ICIP, pp. 1425-1428, 2004. 
[8] Chunhua Du, Qianq Wu, Jie Yang, Zhenq Wu, SVM 
based ASM for facial landmarks location, 8th IEEE 
international Conference on Computer and Information 
Technology, 2008. CIT 2008. 
[9] Feng Jiao, Stan Li, Heung-Yeung Shum, Dale 
Schuurmans, Face Alignment Using Statistical Models 
and Wavelet Feature, Proceedings of IEEE Conference 
on CVPR, pp.321-327,2003. 
 
è¡¨ Y04 
sectionsï¼Œé¡Œç›®ç‚º 
ï‚² Interaction & virtual reality 
ï‚² Motion & multiview 
ï‚² Visual surveillance 
ï‚² Feature extraction & pattern recognition 
ï‚² Human sensing 
ï‚² Industrial applications 
ï‚² Geographic information systems 
ï‚² Machine vision for transportation 
 
 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
 
ï¬ MVA(Machine Vision Applications)æœƒè­°é™¤äº†é‡è¦–é›»è…¦è¦–è¦ºæ–°æŠ€è¡“çš„ç ”ç™¼ä»¥å¤–ï¼Œä¹Ÿ
éå¸¸é‡è¦–æ‡‰ç”¨çš„é–‹ç™¼ï¼Œä¾‹å¦‚è·¯æ¨™çš„è¾¨è­˜ã€è…³å‹çš„æ¸¬é‡ã€æ°´è³ªçš„ä¼°æ¸¬å’Œå¸ƒæ–™çš„æª¢æŸ¥
ç­‰ã€‚é€™æ¨£æ€§è³ªçš„ç ”è¨æœƒé —é©åˆå­¸æ ¡è€å¸«çš„åƒèˆ‡ï¼Œä¸ä½†å¯ä»¥äº¤æ›ç ”ç™¼çš„å¿ƒå¾—ï¼Œé‚„å¯
ä»¥çœ‹åˆ°å¤šæ–¹é¢çš„æ‡‰ç”¨ï¼Œåˆºæ¿€è€å¸«é€²è¡Œç”¢å­¸åˆä½œè¨ˆç•«çš„å‹•æ©Ÿã€‚ 
ï¬ æ­¤æ¬¡æ‰€é ’ç™¼çš„ 5 ç¯‡éå»åå¹´é‡å¤§è²¢ç»è«–æ–‡çä¸­å¤§éƒ¨åˆ†å¾—çè€…å‡æ˜¯æ—¥æœ¬äººï¼Œé€™å€‹ç¾
è±¡é›–ç„¶ä¾†è‡ªæ—¥æœ¬äººå°å®¶å­æ°£çš„ç‰¹è³ªå¤–(ä¸é¡˜å°‡çå¸„å‡åˆ†é…)ï¼Œä¹Ÿé¡¯ç¤ºæˆ‘å€‘ç ”ç©¶çš„å“
è³ªéœ€å¤§åŠ›çš„åŠ å¼·ï¼Œå¦å‰‡å°åœ‹éš›ç¤¾æœƒç„¡æ³•é€ æˆå¯¦éš›çš„å¹«åŠ©ã€‚ 
 
ä¸‰ã€å»ºè­° 
 
ï¬ æœ¬æ ¡è€å¸«å¤šåƒèˆ‡åœ‹éš›æ€§æœƒè­°ï¼Œé™¤äº†ä»‹ç´¹ç ”ç©¶æˆæœï¼Œå¢åŠ å­¸æ ¡çš„çŸ¥ååº¦ä»¥å¤–ï¼Œä¹Ÿèƒ½
å¿«é€Ÿæ“´å±•è¦–é‡ï¼Œå»ºç«‹åˆä½œç®¡é“ï¼Œå°æœªä¾†ç ”ç©¶å’Œæ•™å­¸æœ‰å¾ˆå¤§çš„å¹«åŠ©ã€‚ 
ï¬ é¼“å‹µå­¸æ ¡çš„ç ”ç©¶ç”Ÿåƒèˆ‡é€™ç¨®ç ”ç©¶èˆ‡æ‡‰ç”¨çµåˆçš„åœ‹éš›æ€§æœƒè­°ï¼Œè®“ä»–å€‘æ›´äº†è§£ç ”ç©¶çš„
å¯¦ç”¨åƒ¹å€¼ï¼Œä»¥æ¿€ç™¼å­¸ç¿’å’Œç ”ç©¶çš„ç†±èª ã€‚ 
 
å››ã€æ”œå›è³‡æ–™åç¨±åŠå…§å®¹   
 
æœƒè­°è«–æ–‡é›†ä¸€æœ¬å’Œå…‰ç¢Ÿç‰‡ä¸€ç‰‡ 
 
 
è¡¨ Y04 
ï‚² (9/12) The state of the art of 3D video technologies â€“ accurate 3D shape and 
motion reconstruction, high fidelity visualization, and efficient coding for 3D 
video, by Prof. Takashi Matsuyama, Kyoto university; 
ï‚² (9/13) Data compression by data hiding, by Prof. Hyoung Joong Kim, Korea 
university; 
ï‚² (9/14) Multimodal information fusion in the virtual environment and its 
applications in produce design, by Prof. Jianrong Tan, Zhejiang university.. 
ï‚· æ­¤æœƒè­°ç¸½å…±åŒ…å« 39 sectionsï¼Œå…¶ä¸­æˆ‘åƒåŠ çš„ sections æœ‰ 
ï‚² Multimedia Signal Processing for Intelligent Applications 
ï‚² Intelligent Surveillance and Pattern Recognition 
ï‚² Advances in Biometrics(I) 
ï‚² Advances in Biometrics(II) 
ï‚² Intelligent Image and Signal Processing 
ï‚² Behavior Analysis and Abnormal Event Detection 
ï‚² Statistical Image Processing and Application 
ï‚² Application of Intelligent Computing to Signal and Image Processing 
 
 
å…­ã€èˆ‡æœƒå¿ƒå¾— 
 
ï¬ International Conference on Intelligent Information Hiding and 
Multimedia Signal Processing (IIHMSP2009)æœƒè­°åŒ…å«å»£æ³›çš„ç ”ç©¶è­°é¡Œï¼Œéƒ½
æ˜¯é›»è…¦è¦–è¦ºé ˜åŸŸè¿‘å¹´ä¾†é‡è¦çš„ç ”ç©¶é ˜åŸŸï¼Œè—‰ç”±èˆ‡å…¶ä»–å­¸è€…çš„äº¤è«‡ï¼Œå¯ä»¥æ“´å±•ç ”ç©¶
è€…è¦–é‡ï¼Œåˆºæ¿€è€å¸«é€²è¡Œç”¢å­¸åˆä½œè¨ˆç•«çš„å‹•æ©Ÿï¼Œé —é©åˆå­¸æ ¡è€å¸«çš„åƒèˆ‡ã€‚ 
ï¬ ç”±æ–¼å¤§é˜ªå¸‚ç«‹å¤§å­¸åè­½æ•™æˆ Hiromitsu Hama æœ‰æ„é¡˜ä¾†å°åƒè¨ªï¼Œä»Šå¾Œå°‡ç¹¼çºŒè¯çµ¡ï¼Œ
ä¿ƒæˆæ­¤äº‹ï¼Œæˆ–è¨±æœ‰åŠ©æ–¼å­¸æ ¡åœ¨åœ‹éš›åŒ–å’Œåœ‹éš›åˆä½œç­‰äº‹å‹™çš„æ¨å»£æœ‰å¹«åŠ©ã€‚ 
 
ä¸ƒã€å»ºè­° 
 
ï¬ æœ¬æ ¡è€å¸«å¤šåƒèˆ‡åœ‹éš›æ€§æœƒè­°ï¼Œé™¤äº†ä»‹ç´¹ç ”ç©¶æˆæœï¼Œå¢åŠ å­¸æ ¡çš„çŸ¥ååº¦ä»¥å¤–ï¼Œä¹Ÿèƒ½
å¿«é€Ÿæ“´å±•è¦–é‡ï¼Œå»ºç«‹åˆä½œç®¡é“ï¼Œå°æœªä¾†ç ”ç©¶å’Œæ•™å­¸æœ‰å¾ˆå¤§çš„å¹«åŠ©ã€‚ 
ï¬ é¼“å‹µå­¸æ ¡çš„ç ”ç©¶ç”Ÿåƒèˆ‡é€™ç¨®ç ”ç©¶èˆ‡æ‡‰ç”¨çµåˆçš„åœ‹éš›æ€§æœƒè­°ï¼Œè®“ä»–å€‘æ›´äº†è§£ç ”ç©¶çš„
