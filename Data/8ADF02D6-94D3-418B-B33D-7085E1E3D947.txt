1 
 
ä¸€ã€ ä¸­ã€è‹±æ–‡æ‘˜è¦åŠé—œéµè©ž (keywords) 
 
è‹±æ–‡æ‘˜è¦: All of our research results for the third year (2009/8/1~2010/7/31) are highly innovative cutting 
edge research contributions on advanced blind source separation (BSS) methods for both biomedical and 
hyperspectral image analysis and multiuser communications, including separation criterion design, algorithm 
development, and identifiability analysis. For biomedical and hyperspectral image analysis where the sources 
are usually statistically correlated, based on convex analysis and optimization theory, we have developed 
effective BSS algorithms without involving any source statistical assumption, and also devised their robust 
extensions to account for the presence of noise in the observations. For multiuser communications, we have 
developed a Khatri-Rao (KR) subspace approach by utilizing the local second-order statistics (SOSs) of 
quasi-stationary signals for estimation of direction of arrival (DOA) (applicable to the underdetermined 
systems) and blind channel identification. These research results partly have been published by IEEE Trans. 
Signal Processing and a book chapter (Cambridge University Press) or have been submitted to IEEE 
Transactions on Geoscience and Remote Sensing â€“ Special Issue on Spectral Unmixing of Remotely Sensed 
Data (under review), and partly have been presented at or submitted to major flagship international 
conferences.    
 
ä¸­æ–‡æ‘˜è¦: æˆ‘å€‘åœ¨å‰çž»æ€§ç›²è”½è¨Šè™Ÿæºåˆ†ï§ªæ–¼ç”Ÿç‰©é†«å­¸å½±åƒåˆ†æžå’Œå¤šç”¨æˆ¶é€šè¨Šçš„æ‰€æœ‰ç ”ç©¶æˆæžœçš†æ˜¯
é«˜å‰µæ–°ä¸”å‰æ²¿çš„ï¼Œå…¶ä¸­åŒ…å«ï¦ºåˆ†ï§ªæº–å‰‡è¨­è¨ˆã€æ¼”ç®—æ³•ç ”ç™¼èˆ‡é‘‘åˆ¥åˆ†æž(identifiability analysis)ã€‚é‡å°è¨Šè™Ÿ
æºé€šå¸¸æ˜¯çµ±è¨ˆä¸Šç›¸é—œçš„ç”Ÿç‰©é†«å­¸å’Œè¶…å…‰è­œå½±åƒåˆ†æžæ–¹é¢ï¼ŒåŸºæ–¼å‡¸åˆ†æžå’Œæœ€ä½³åŒ–ï§¤ï¥ï¼Œæˆ‘å€‘å·²é–‹ç™¼ï¦ºï¥§
éœ€ä»»ä½•çµ±è¨ˆçš„å‡è¨­ä¸”æœ‰æ•ˆçš„ç›²è”½è¨Šè™Ÿæºåˆ†ï§ªæ¼”ç®—æ³•ï¼Œä¸¦å°‡æ¼”ç®—æ³•å»¶ä¼¸ä»¥è€ƒï¥¾å°æ–¼é›œè¨Šçš„ç©©å¥æ€§ã€‚åœ¨å¤š
ç”¨æˆ¶é€šä¿¡æ–¹é¢ï¼Œæˆ‘å€‘å·²é–‹ç™¼å‡ºä¸€ç¨® Khatri-Rao çš„å­ç©ºé–“æ–¹æ³•ï¼Œï§ç”¨æº–å¹³ç©©è¨Šè™Ÿ(quasi-stationary signal)
çš„å€åŸŸäºŒéšŽçµ±è¨ˆï¥¾(second order statistics, SOSs)ï¤­ä¼°è¨ˆåˆ°é”æ–¹å‘(direction of arrival, DOA)(å¯æ‡‰ç”¨æ–¼æ¬ 
å®šç³»çµ±(underdetermined systems))å’Œç›²è”½é€šé“è¾¨ï§¼ã€‚é€™äº›ç ”ç©¶æˆæžœéƒ¨åˆ†å·²ç™¼è¡¨æ–¼ IEEE Trans. Signal 
Processing å’Œå°ˆæ›¸ç« ç¯€(åŠæ©‹å¤§å­¸å‡ºç‰ˆç¤¾)ï¼Œæˆ–å·²æŠ•ç¨¿æ–¼ IEEE Transactions on Geoscience and Remote 
Sensing â€“ Special Issue on Spectral Unmixing of Remotely Sensed Data å¯©æŸ¥ä¸­ï¼Œéƒ¨åˆ†æˆæžœå·²ç™¼è¡¨æ–¼æˆ–å·²æŠ•
ç¨¿æ–¼ä¸»è¦æ——è‰¦æ€§åœ‹éš›æœƒè­°ã€‚ 
 
é—œéµè©ž: Blind source separation, convex analysis and optimization, biomedical image analysis, 
hyperspectral image analysis, quasi-stationary signals 
3 
 
analysis, the BSS methods developed for hyperspectral image analysis can also be applied to biomedical 
image analysis. Figure 2 illustrates conceptually that the hyperspectral sensor receives the electromagnetic 
(EM) scattering patterns reflected from the Earth surface, and that hyperspectral unmixing decomposes the 
measured image cube into the spectra of materials (or endmember signatures, i.e., spectral signatures of water, 
tree and land) and the associated mixing proportions (or abundance maps, i.e., distributions of different 
materials (water, tree and land)). The extracted endmember signatures and abundance maps can be used to 
identify what the materials are and how they are distributed in the scanned surface. 
 
100 200 300 400 500 600 700
0.04
0.045
0.05
0.055
0.06
0.065
0.07
time (sec)
 
 
Figure 1. BSS in analysis for a sequence of 19 DCE-MRI images, extracting the information of the distributions of 
tumor tissue, normal tissue and plasma input, together with their time activity curves (fast flow, slow flow and plasma 
input).  
 
   
 
 
 
 
 
 
 
 
 
 
Figure 2. Illustration of BSS in hyperspectral unmixing.  
 
5 
 
programming. In addition, we have also applied the chance constrained reformulation to alternating volume 
maximization (AVMAX) algorithm [8] (see Research Results in the First Two Years (2007/8/1~2009/7/31) at 
the end of this section) (based on the volume maximization of the simplex constructed by the extracted 
endmembers) for endmember extraction to mitigate the effect of noise, and the resultant robust AVMAX 
algorithm is implemented by second-order-cone programming (see #6 below). Furthermore, we have provided 
a convex optimization perspective to simplex volume maximization problem. Specifically, we have proved 
the endmember identifiability of the simplex volume maximization criterion, proposed two novel algorithms 
including their convergence analysis, and developed another robust extension of AVMAX using worst-case 
optimization (see #7 below). Since the amount of hyperspectral data may be quite large in practical 
applications, we proposed two effective and computationally efficient endmember extraction algorithms 
called simplex estimation by projection (SIMPLE-Pro) algorithm and p-norm based pure pixel identification 
(TRI-P) algorithm along with their endmember identifiability (see # 8 below).  
For direction-of-arrival (DOA) estimation of quasi-stationary signals in microphone array systems, we 
have developed a Khatri-Rao (KR) subspace approach by utilizing the local second-order statistics (SOSs) of 
quasi-stationary signals. It has been proven that the DOA identifiability still holds even when the number of 
sensors is about half of the number of sources, and the KR formulation provides a simple yet effective way of 
eliminating the unknown spatial noise covariance from the signal SOSs (see #9 below). Moreover, we have 
considered a more challenging case of blind channel identification where the channel matrix does not exhibit 
array response structure. We have proposed a channel column identification method using alternating 
projections (AP), and proved that in the absence of statistical modeling errors (due to finite-sample effects), 
the AP method (with prewhitening) almost surely converges to a true channel column in one iteration. A 
method that uses AP to effectively identify all the channel columns is also developed (see #10 below).  
Except for the paper publications in 2010, one of my former Ph.D. students, Dr. Tsung-Han Chan (who 
finished his Ph.D. July 2009), has been dedicated to development of BSS algorithms based on convex analysis 
and optimization to solve the "mixed pixel problem" in biomedical images and hyperspectral images. His 
Ph.D. dissertation (see # 11 below), entitled â€œConvex Analysis Based Non-negative Blind Source Separation 
for Biomedical and Hyperspectral Image Analysisâ€ has been awarded 2010 ï¦Žä¸­è¯æ±ºç­–ç§‘å­¸å­¸æœƒæš¨å°ç£ä½œ
æ¥­ç ”ç©¶å­¸æœƒåšå£«çµ„ï¥æ–‡ç«¶è³½å„ªå‹Â  andÂ  ä¸­è¯æ°‘åœ‹å½±åƒè™•ï§¤èˆ‡åœ–å½¢ï§¼åˆ¥å­¸æœƒç¬¬ä¸‰å±†åšå£«æœ€ä½³ï¥æ–‡çŽ-ä½³ä½œ.Â 
He is currently a post doctor of my research lab (WCSP lab), and continues focusing his research on BSS 
algorithms for biomedical image analysis and hyperspectral image analysis.  
Summary: Our research in developing advanced BSS methods has led to 4 journal papers including 2 
published in IEEE Transactions on Signal Processing and 2 submitted to IEEE Transactions on Geoscience 
and Remote Sensing â€“ Special Issue on Spectral Unmixing of Remotely Sensed Data, 5 international 
conference papers, including 2 published in IEEE ICASSP-2010 and IEEE WHISPERS-2010, respectively, 
and 3 submitted to IEEE ICASSP-2011, 1 Ph.D. dissertation that has received two best Ph.D. dissertation 
7 
 
spindle fibers can be clearly observed, and these results exhibit a good agreement with biological expectation.  
 
 
Figure 3. Fluorescence microscopy images: (a) the measured newt lung cell images, (b) the ROI images of (a), and (c) 
the unmixed images of intermediate filaments (top), chromosomes (middle), and spindle fibers (bottom) obtained by the 
proposed nLCA-IVM.  
 
#2. (Book chapter) W.-K. Ma, T.-H. Chan, C.-Y. Chi, and Y. Wang, â€œConvex analysis for non-negative blind 
source separation with application in imaging,â€ in Chapter 7, Convex Optimization in Signal Processing and 
Communications, Editors: D. P. Palomar and Y. C. Eldar, UK: Cambridge University Press, 2010.   
 
Abstract: In recent years, there has been a growing interest in blind separation of non-negative sources, as 
simply non-negative blind source separation (nBSS). Potential applications of nBSS include biomedical 
imaging, multi/hyper-spectral imaging, and analytical chemistry. In this chapter, we describe a rather new 
endeavor of nBSS, where convex geometry is utilized to analyze the nBSS problem. Called convex analysis 
of mixtures of non-negative sources (CAMNS), the framework described here makes use of a very special 
assumption called local dominance, which is a reasonable assumption for source signals exhibiting sparsity or 
high contrast. Under the local dominant and some general nBSS assumptions, we show that the source signals 
can be perfectly identified by finding the extreme points of an observation-constructed polyhedral set. Two 
methods for practically locating the extreme points are also derived. One is analysis-based with some 
appealing theoretical guarantees, while the other is heuristic in comparison but is intuitively expected to 
provide better robustness against model mismatches. Both are based on linear programming and thus can be 
effectively implemented. Simulation results on several data sets are presented to demonstrate the efficacy of 
9 
 
programming (SQP) solvers in an alternating fashion. Monte Carlo simulations are presented to demonstrate 
the efficacy of the proposed RMVES algorithm over several existing benchmark hyperspectral unmixing 
methods, including the original MVES algorithm.  
 
#5 (Journal paper) A. Ambikapathi, T.-H. Chan, W.-K.Ma, and C.-Y. Chi, â€œA chance constrained robust 
minimum volume enclosing simplex algorithm for hyperspectral unmixing,â€ submitted to IEEE Trans. 
Geoscience and Remote Sensing â€“ Special Issue on Spectral Unmixing of Remotely Sensed Data, Sept. 2010.  
 
Abstract: Effective unmixing of hyperspectral data cube under a noisy scenario has been a challenging 
research problem in remote sensing arena. A branch of existing hyperspectral unmixing algorithms is based 
on Craigâ€™s criterion, which states that the vertices of the minimum volume simplex enclosing the 
hyperspectral data should yield high fidelity estimates of the endmember signatures associated with the data 
cloud. Recently, we have developed a minimum volume enclosing simplex (MVES) algorithm based on 
Craigâ€™s criterion and validated that the MVES algorithm is very useful to unmix highly mixed hyperspectral 
data. However, the presence of noise in the observations expands the actual data cloud and as a consequence 
the endmember estimates obtained by applying the MVES algorithm to the noisy data may no longer be in 
close proximity to the true endmember signatures. In this work, we propose a robust minimum volume 
enclosing simplex (RMVES) algorithm that accounts for the noise effects in the observations by employing 
chance constraints. These chance constraints in turn control the volume of the resulting simplex. Under the 
Gaussian noise assumption, the chance constrained minimum volume enclosing simplex problem can be 
formulated into a deterministic non-linear program. The problem can then be conveniently handled by 
alternating optimization, in which each subproblem involved is handled by using sequential quadratic 
programming solvers. The proposed RMVES is compared with several existing benchmark algorithms 
including its predecessor, the MVES algorithm. Monte-Carlo simulations and real hyperspectral data 
experiments are presented to demonstrate the efficacy of the proposed RMVES algorithm. 
 
Figure 4 shows some experimental results obtained by the RMVES method for real hyperspectral images 
(taken from http://aviris.jpl.nasa.gov/html/aviris.freedata.html) collected by airbone visible/infrared imaging 
spectrometer (AVIRIS) flight over the Cuprite mining site, Nevada, in 1997. The results show a high 
agreement with the reported ground truth.  
11 
 
existing pure pixel based hyperspectral unmixing methods, including its predecessor, AVMAX algorithm.  
#7 (Journal paper) T.-H. Chan, W.-K.Ma, A. Ambikapathi, and C.-Y. Chi, â€œA simplex volume maximization 
framework for hyperspectral endmember extraction,â€ submitted to IEEE Trans. Geoscience and Remote 
Sensing â€“ Special Issue on Spectral Unmixing of Remotely Sensed Data, Sept. 2010.  
 
Abstract: In hyperspectral remote sensing, the spectra of materials (or endmember signatures) enable 
identification of the materials that make up a scanned scene of interest. However, the limited spatial resolution 
of the sensors usually makes each pixel spectrum a mixture of endmember signatures. The procedure of 
recovering the endmember signatures from the hyperspectral data is called endmember extraction. In the late 
90's, Winter proposed an endmember extraction belief, which states that the ground truth endmembers may be 
found by locating a collection of pixel vectors whose simplex volume is the maximum. Winter's belief has 
stimulated much attention, resulting in many different variations of Winter-based pixel search algorithms, 
widely known as N-FINDR, being proposed. In this work, we take on a continuous optimization perspective 
to revisit Winter's belief, where the aim is to provide an alternative framework of formulating and 
understanding Winter's belief in a systematic manner. We first prove that fundamentally, the existence of pure 
pixels is not only sufficient for the Winter problem to perfectly identify the ground truth endmembers, but 
also necessary. Then, under the umbrella of the Winter problem, we derive two endmember extraction 
methods using two different optimization methodologies. One is based on alternating optimization. The 
outcome turns out to be an N-FINDR variant, but the proposed formulation enables us to pin down some of its 
convergence characteristics. Another is based on successive optimization; coincidently it is found to be 
strikingly similar to the vertex component analysis (VCA) algorithm. The endmember identifiability of the 
two proposed methods is also proven. Furthermore, we propose a robust worst-case generalization of the 
Winter problem for accounting for the perturbed pixel effects in the noisy scenario. An algorithm combining 
alternating optimization and projected subgradients is devised to deal with the problem. Some simulation 
results are presented to demonstrate the efficacy of the proposed methods.  
 
#8 (Conference paper) A. Ambikapathi, T.-H. Chan, C.-Y. Chi and K. Keizer, â€œTwo effective and 
computationally efficient pure-pixel based algorithms for hyperspectral endmember extraction,â€ submitted to 
IEEE-ICASSP, Prague, Czech Republic, May 22-27, 2011.  
 
Abstract: Endmember extraction is of prime importance in the process of hyperspectral unmixing so as to 
study the mineral composition of a landscape from its hyperspectral observations. Though, a whole bunch of 
pure-pixel based endmember extraction algorithms exists, the quest for a reliable, repeatable, and 
computationally efficient endmember extraction algorithm still prevails. In this work, we propose two 
pure-pixel based endmember extraction algorithms called simplex estimation by projection (SIMPLE-Pro) 
13 
 
handling those per-channel-column problems. Interestingly, we prove that under an ideal model condition, the 
proposed AP algorithm converges to a true channel column almost surely, and it does so within one iteration. 
Our simulation results show that the proposed method terminates in 3-4 iterations on average when noise is 
present, and that it achieves satisfactory channel estimation performance as compared to some other existing 
BCI-QSS methods. 
 
#11. (Ph.D. dissertation) Tsung-Han Chan, â€œConvex analysis Based Non-negative Blind Source Separation 
for Biomedical and Hyperspectral Image Analysis,â€ Ph.D. dissertation, National Tsing Hua University, 
Taiwan, 2009. (This dissertation has been awarded 2010 ï¦Žä¸­è¯æ±ºç­–ç§‘å­¸å­¸æœƒæš¨å°ç£ä½œæ¥­ç ”ç©¶å­¸æœƒåšå£«çµ„
ï¥æ–‡ç«¶è³½å„ªå‹Â  andÂ  ä¸­è¯æ°‘åœ‹å½±åƒè™•ï§¤èˆ‡åœ–å½¢ï§¼åˆ¥å­¸æœƒç¬¬ä¸‰å±†åšå£«æœ€ä½³ï¥æ–‡çŽ-ä½³ä½œ) 
 
Abstract: This dissertation deals with the topic of non-negative blind source separation (nBSS), a 
widely-applicable technique in many real-world applications, such as multichannel biomedical image analysis 
and hyperspectral image analysis. Fundamentally, unlike the skills involved in relevant existing frameworks, 
such as non-negative extension of independent component analysis (ICA) and non-negative matrix 
factorization (NMF), we exploit convex geometry to develop two nBSS frameworks without any source 
statistical independence/uncorrelatedness assumption. The first framework called convex analysis of mixtures 
of non-negative sources (CAMNS) makes use of an insightful and practical model assumption (called source 
local dominance) to connect nBSS and convex geometry. It leads to a deterministic, convex analysis based 
nBSS criterion that boils down nBSS problem to the problem of finding all the extreme points of an 
observation-constructed polyhedral set (or an extreme point enumeration problem). We derive two linear 
programming based methods for efficiently locating the extreme points. One is analytically based and 
provides some appealing theoretical guarantees, while the other is heuristic but provides better robustness 
when model assumptions are not perfectly satisfied. Simulation results for several data sets are presented to 
demonstrate the efficacy of the CAMNS-based methods over several existing benchmark nBSS methods. In 
addition, experimental results with real biomedical images are presented to evaluate the high practical 
applicability of CAMNS. 
    In hyperspectral remote sensing, unmixing a data cube into the spectral signatures (or endmenbers) and 
their corresponding mixing proportion (or abundance fractions) plays a crucial role in analyzing the 
mineralogical composition of a solid surface. Such an unmixing problem nature has a lot in common with 
nBSS problem. The second framework describes a new convex analysis and optimization perspective to 
hyperspectral unmixing. By the notion of convex analysis, we formulate two optimization problems for 
hyperspectral unmixing, which have intuitive ideas (or beliefs without any rigorous analysis and proof) that 
â€œthe endmembers are determined by vertices of the maximum volume simplex within all the observed pixelsâ€ 
proposed by Winter in late 1990, and that â€œthe endmembers are determined by the vertices of a minimum 
15 
 
 
Figure 5. Experimental results obtained by CAMNS using 150 dynamic florescent images of a mouse provided by 
Cambridge Research & Instrumentation (CRi).  
 
 
Research Results in the First Two Years (2007/8/1-2009/7/31)  
---------------------------------- 
[1] X. Chen, Chong-Yung Chi, Chon-Wa Wang, S. Zhou and Y. Yao, â€œNon-Cancellation multistage 
Kurtosis maximization with prewhitening for blind source separation,â€™â€™ 41th Asilomar Conference on 
Signals, Systems and Computers, Monterey, CA, Nov. 4-7, 2007.  
[2] L. Chen, Yue Wang, Chong-Yung Chi, Z. Szabo, P. Choyke, â€œSeparating composite signals in 
multi-probe dynamic biomedical imaging,â€™â€™ 41th Asilomar Conference on Signals, Systems and 
Computers, Monterey, CA, Nov. 4-7, 2007.  
[3] Tsung-Han Chan, Wing-Kin Ma, Chong-Yung Chi, Yue Wang, â€œBlind separation of non-negative 
sources by convex analysis: Effective method using linear programming,â€™â€™ IEEE ICASSP-2008, Las 
Vegas, Nevada, March 30-April 4, 2008.  
[4] L. Chen, T.-H. Chan, P. L Choyke, C.-Y. Chi, G. Wang and Y. Wang, â€œConvex analysis and 
separation of composite signals in DCE-MRI,â€™â€™ IEEE International Symposium on Biomedical 
Imaging, Paris, France, May 14-17, 2008. 
[5] T.-H. Chan, L. Chen, P. L. Choyke, Chong-Yung Chi, and Y. Wang, â€œConvex analysis for separation 
of functional patterns in DCE-MRI: A longitudinal study to antiangiogenic therapy,â€™â€™ in Proc. 2008 
IEEE Workshop on Machine Learning for Signal Processing (MLSP'08), CancÃºn, Mexico, Oct. 16-19, 
17 
 
åœ‹ç§‘æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•«æˆæžœå ±å‘Šè‡ªè©•è¡¨ 
 
è«‹å°±ç ”ç©¶å…§å®¹èˆ‡åŽŸè¨ˆç•«ç›¸ç¬¦ç¨‹ï¨ã€é”æˆé æœŸç›®æ¨™æƒ…æ³ã€ç ”ç©¶æˆæžœä¹‹å­¸è¡“æˆ–æ‡‰ç”¨åƒ¹
å€¼ï¼ˆç°¡è¦æ•˜è¿°æˆæžœæ‰€ä»£è¡¨ä¹‹æ„ç¾©ã€åƒ¹å€¼ã€å½±éŸ¿æˆ–é€²ä¸€æ­¥ç™¼å±•ä¹‹å¯èƒ½æ€§ï¼‰ã€æ˜¯å¦é©
åˆåœ¨å­¸è¡“æœŸåˆŠç™¼è¡¨æˆ–ç”³è«‹å°ˆï§ã€ä¸»è¦ç™¼ç¾æˆ–å…¶ä»–æœ‰é—œåƒ¹å€¼ç­‰ï¼Œä½œä¸€ç¶œåˆè©•ä¼°ã€‚
1. è«‹å°±ç ”ç©¶å…§å®¹èˆ‡åŽŸè¨ˆç•«ç›¸ç¬¦ç¨‹ï¨ã€é”æˆé æœŸç›®æ¨™æƒ…æ³ä½œä¸€ç¶œåˆè©•ä¼° 
â– é”æˆç›®æ¨™ 
â–¡ æœªé”æˆç›®æ¨™ï¼ˆè«‹ï¥¯æ˜Žï¼Œä»¥ 100 å­—ç‚ºé™ï¼‰ 
â–¡ å¯¦é©—å¤±æ•— 
â–¡ å› æ•…å¯¦é©—ä¸­æ–· 
â–¡ å…¶ä»–åŽŸå›   ï¥¯æ˜Žï¼š  
2. ç ”ç©¶æˆæžœåœ¨å­¸è¡“æœŸåˆŠç™¼è¡¨æˆ–ç”³è«‹å°ˆï§ç­‰æƒ…å½¢ï¼š 
ï¥æ–‡ï¼šâ– å·²ç™¼è¡¨ â–¡æœªç™¼è¡¨ä¹‹æ–‡ç¨¿ â– æ’°å¯«ä¸­ â–¡ç„¡ 
å°ˆï§ï¼šâ–¡å·²ç²å¾— â–¡ç”³è«‹ä¸­ â– ç„¡ 
æŠ€è½‰ï¼šâ–¡å·²æŠ€è½‰ â–¡æ´½è«‡ä¸­ â– ç„¡ 
å…¶ä»–ï¼šï¼ˆä»¥ 100 å­—ç‚ºé™ï¼‰ 
 More high-quality research works are yet to be published in top-tier international 
journals and conferences.  
 
3. è«‹ä¾å­¸è¡“æˆå°±ã€æŠ€è¡“å‰µæ–°ã€ç¤¾æœƒå½±éŸ¿ç­‰æ–¹é¢ï¼Œè©•ä¼°ç ”ç©¶æˆæžœä¹‹å­¸è¡“æˆ–æ‡‰ç”¨åƒ¹
å€¼ï¼ˆç°¡è¦æ•˜è¿°æˆæžœæ‰€ä»£è¡¨ä¹‹æ„ç¾©ã€åƒ¹å€¼ã€å½±éŸ¿æˆ–é€²ä¸€æ­¥ç™¼å±•ä¹‹å¯èƒ½æ€§ï¼‰ï¼ˆä»¥
500 å­—ç‚ºé™ï¼‰ 
 
åœ¨éŽåŽ»ä¸‰ï¦ŽæœŸåœ‹ç§‘æœƒè¨ˆç•«è³‡æºä¹‹æœ‰æ•ˆæŒ¹æ³¨ä¸‹ï¼Œæˆ‘å€‘å¾—ä»¥æŠ•å…¥ï¤å¤šäººï¦Šæ–¼ç”Ÿ
ç‰©é†«å­¸å½±åƒåŠé€šè¨Šæ–¹é¢çš„ç ”ç©¶ï¼Œä¸¦ç¶“ç”±åœ‹éš›åˆä½œç ”ç©¶åœ˜éšŠå…±åŒçš„åŠªï¦Šï¼Œä½¿
å¾—ç ”ç©¶é †ï§æŽ¨å±•ï¼Œé«˜å‰µæ–°çš„ç ”ç©¶æˆæžœä¹Ÿï§“çºŒç™¼è¡¨æ–¼ä¸€ï§Šçš„åœ‹éš›æœƒè­°ã€åœ‹éš›
æœŸåˆŠåŠå°ç§‘å­¸ç ”ç©¶çš„å…·é«”è²¢ç»ï¼Œå°æå‡æœ¬åœ‹çš„åœ‹éš›è²è­½æœ‰ç›¸ç•¶å¤§çš„å¹«åŠ©ã€‚
å±•æœ›æœªï¤­ï¼Œæˆ‘å€‘å°‡æŒçºŒç ”ç™¼å‰çž»æ€§è¨Šè™Ÿæºåˆ†ï§ªæ¼”ç®—æ³•ï¼Œç”¨æ–¼åˆ†æžç”Ÿç‰©é†«å­¸
å½±åƒèˆ‡è¶…å…‰è­œå½±åƒï¼Œä¸¦ä½¿ç”¨å¤§ï¥¾çœŸå¯¦ç”Ÿç‰©é†«å­¸å½±åƒï¥©æ“šåŠè¶…å…‰è­œå½±åƒï¥©æ“š
é€²ï¨ˆæ¼”ç®—æ³•ä¹‹æ”¹ï¥¼åŠé©—è­‰ï¼ŒæœŸè¨±æœªï¤­æ­¤ç ”ç©¶æˆæžœï¥§åƒ…å¯ä¿ƒé€²é†«è—¥ç™¼å±•åŠè‡ª
ç„¶ç’°å¢ƒä¹‹ä¿è­·ï¼Œï¤èƒ½æ”¹å–„äººï§ä¹‹ï¨›ç¥‰ã€‚ 
 
 
[19]. Possible circumstances under which the NMF yields
unique decomposition can also be found in [20].
The nBSS based on the pure-source sample assumption
(where a pure-source sample means a data point that is fully
dominated by only one source) has been investigated for
separation of dependent sources in biomedical imaging [21],
[22], [23] and remote sensing applications [24], [25], [26]. In
[27], a neural network algorithm, searching for the edges of
data scatter plot, exploits the existence of pure-source
samples to iteratively update the unmixing matrix such that
the sum of all the off-diagonal entries of the angular
proximity matrix (analogous to the cross-correlation matrix)
of the extracted edges is minimum. A recently reported
nBSS method, convex analysis of mixtures of nonnegative
sources (CAMNS) [22], based on the pure-source sample
assumption, has been theoretically proven to achieve perfect
separation by searching for all the extreme points of an
observation-constructed polyhedral set. In remote sensing,
many spectral unmixing algorithms [24], [25], [26], also
based on the pure-source sample assumption, try to search
for the purest observed pixels from the spectral data set and
then identify the mixing matrix. The use of such spectral
unmixing methods is usually followed by an inversion
process, such as nonnegative least-squares method, to
recover the sources. Similar ideas for finding the mixing
matrix can be found in our recently reported nBSS method,
namely nLCA [23], which estimates the mixing matrix by an
algebraic edge search algorithm. Herein, we renamed the
nLCA reported in [23] nLCA-edge search (nLCA-ES) so as to
distinguish it from the one to be presented in this paper.
In this paper, we propose a joint correlation function of
multiple signals for the design of the demixing matrix. The
joint correlation function of the observations mixed by
nonnegative combinations of the sources would be higher
than that of the sources. Based on this idea and the
nonnegativity constraint of the sources, a novel nLCA is
proposed for which the nBSS problem is formulated into a
problem of minimizing the joint correlation function of all
the demixed sources under the nonnegativity constraint. A
closed-form solution for the case of two sources with two
observations is provided. For the general case of multiple
sources, the proposed nLCA can be fulfilled by an iterative
volume maximization algorithm (nLCA-IVM) using linear
program (LP), which finds the optimal demixing matrix by
maximizing the volume of a solid region formed by the
demixed source vectors regardless of whether the pure-
source sample assumption is valid or not. However, the
source identifiability of the nLCA can be proven under the
existence of pure-source samples. In contrast to other
spectral unmixing methods, the proposed nLCA-IVM,
which is never a pure-source sample criterion-based nBSS
algorithm, is able to separate all the (dependent or
independent) sources from the given mixtures without
involving any inversion process. Comparative experimental
results using synthetic data and real biomedical data are
presented to demonstrate the superior performance of
nLCA-IVM over several existing benchmark methods.
Section 2 presents the nBSS problem formulation and
the model assumptions. In Section 3, the novel nLCA is
presented followed by the closed-form solution of the
optimum demixing matrix for the two-source case, the
nLCA-IVM algorithm, and the source identifiability of the
nLCA under the existence of pure-source samples. For
real-world applications, where the number of mixtures is
larger than the number of sources, Section 4 presents noise
reduction and rank reduction prior to the nBSS processing
using the nLCA-IVM algorithm. In Section 5, simulation
results are presented with synthetic mixed human face
images, infrared spectral signals, and dual-energy X-ray
images to evaluate the performance of the proposed
nLCA-IVM algorithm. Then a real data analysis of
fluorescence microscopy imaging and that of dynamic
contrast-enhanced magnetic resonance imaging (DCE-MRI)
using the proposed nLCA-IVM algorithm are presented to
show its effectiveness. Finally, some conclusions are drawn
in Section 6.
2 PROBLEM FORMULATION
For ease of later use, let us define the following notations:
IR; IRN; IRMN Set of real numbers, N-dimensional
vectors, M N matrices
IRÃ¾; IRNÃ¾ ; IR
MN
Ã¾ Set of nonnegative real numbers,
N-dimensional vectors,
M N matrices
1N N  1 vector with all the entries equal
to unity
IN N N identity matrix
ei Unit vector of proper dimension with the
ith entry equal to unity
0 Zero vector of proper dimension
k  k Euclidean norm of a vector or Frobenius
norm of a matrix
 Componentwise inequality
diagf1; . . . ; Ng N N diagonal matrix with diagonal
entries 1; . . . ; N
Consider the following M N linear mixing system:
xÂ½n Â¼ AsÂ½n; n Â¼ 1; 2; . . . ; L; Ã°1Ãž
where xÂ½n Â¼ Ã°x1Â½n; x2Â½n; . . . ; xM Â½nÃžT is the nth data point of
the given M observations (or mixtures), A Â¼ Â½aijMN 2
IRMN is an unknown mixing matrix, sÂ½n Â¼ Ã°s1Â½n; s2Â½n; . . . ;
sN Â½nÃžT is the nth source point consisting of N sources, and
L > maxfM;Ng is the data length (i.e., number of pixels in
each observation and each source).
Our goal herein is to design a real demixing matrixW Â¼
Â½wijNM such that
yÂ½n Â¼WxÂ½n Â¼ Ã°WAÃžsÂ½n Â¼ PsÂ½n; Ã°2Ãž
where yÂ½n Â¼ Ã°y1Â½n; y2Â½n; . . . ; yN Â½nÃžT is the extracted (un-
mixed or demixed) source point and P Â¼WA 2 IRNN is a
permutation matrix, meaning that the extracted source
point yÂ½n is equivalent to the true source point sÂ½n up to a
permutation.
For ease of the ensuing derivations, we let
xi Â¼ Ã°xiÂ½1; xiÂ½2; . . . ; xiÂ½LÃžT ; Ã°ith observationÃž
where i Â¼ 1; 2; . . . ;M and
si Â¼ Ã°siÂ½1; siÂ½2; . . . ; siÂ½LÃžT ; Ã°ith sourceÃž;
yi Â¼ Ã°yiÂ½1; yiÂ½2; . . . ; yiÂ½LÃžT ; Ã°ith extracted sourceÃž;
876 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 5, MAY 2010
Authorized licensed use limited to: National Tsing Hua University. Downloaded on March 25,2010 at 01:50:03 EDT from IEEE Xplore.  Restrictions apply. 
sources to form an effective criterion for the design of the
demixing matrix. Instead, we propose a joint correlation
function of fx1; x2; . . . ; xMg, where M  2, defined as
corrÃ°x1; x2; . . . ; xMÃž Â¼4 1
V Ã°solfx1; x2; . . . ; xMgÃž ; Ã°8Ãž
where
V Ã°solfx1; x2; . . . ; xMgÃž Â¼
Z
solfx1;x2;...;xMg
d; Ã°9Ãž
is the volume of solfx1; x2; . . . ; xMg and
R
solfx1;x2;...;xMg d is
the multiple integral over solfx1; x2; . . . ; xMg. The proposed
joint correlation function of multiple signals also reflects the
same behavior as the conventional pairwise correlation
coefficient. Specifically, as shown in Fig. 1b, the larger the
value of corrÃ°x1; x2Ãž, the smaller the area of solfx1; x2g. Since
the distance from the origin to the hyperplane passing
through x1 and x2 (i.e., affÃ°x1; x2Ãž) is fixed, the smaller the
area of solfx1; x2g, the smaller the value of Ã°x1; x2Ãž. Hence,
it can be inferred that the larger the value of corrÃ°x1; x2Ãž, the
larger the value of %Ã°x1; x2Ãž.
Based on Theorem 1 and (9), we have V Ã°solfx1;
x2; . . . ; xMgÃž	 V Ã°solfs1; s2; . . . ; sNgÃž, which implies the
following corollary:
Corollary 1. Suppose that (A2) and (A4) hold. Then,
corrÃ°s1; s2; . . . ; sNÃž 	 corrÃ°x1; x2; . . . ; xMÃž:
Note that corrÃ°x1; x2; . . . ; xMÃž 	 corrÃ°y1; y2; . . . ; yNÃž if the
demixing matrix satisfies (A2) (i.e., W 2 IRNMÃ¾ ) and (A4)
(i.e.,W1M Â¼ 1N ) byCorollary 1.However,W 62 IRNMÃ¾ in the
demixingmatrixdesign tobepresentedbelow is a realN M
matrix such that corrÃ°y1; y2; . . . ; yNÃž 	 corrÃ°x1; x2; . . . ; xMÃž. If
W 2 IRNM can be properly designed, corrÃ°y1; y2; . . . ; yNÃž Â¼
corrÃ°s1; s2; . . . ; sNÃž can be achieved.
Corollary 1 is profound, which motivates the idea of
designing the demixing matrix W 2 IRNM by minimizing
the joint correlation function of the N extracted sources
y1; y2; . . . ; yN , that is,
min
W2IRNM
corrfy1; y2; . . . ; yNg; Ã°10Ãž
subject to (s.t.)
yi Â¼
XM
jÂ¼1
wijxj  0; i Â¼ 1; 2; . . . ; N; Ã°due to Ã°A1ÃžÃž; Ã°11Ãž
W1M Â¼WA1N Â¼ P1N Â¼ 1N: Ã°due to Ã°A4ÃžÃž: Ã°12Ãž
From (8), the nBSS problem (10) subject to the two
constraints given by (11) and (12) can be reformulated as
max
W2IRNM
V Ã°solfy1; y2; . . . ; yNgÃž
s:t: W1M Â¼ 1N; yi Â¼ MjÂ¼1wijxj  0; 8i;
Ã°13Ãž
which is a nonlinear and nonconvex optimization problem,
implying that finding a closed-form solution of (13) for any
M and N is almost formidable.
Next, let us present how to solve (13) for the case of
M Â¼ N Â¼ 2 and the case of M Â¼ N  2, respectively,
followed by the associated source identifiability.
3.1 Case of M Â¼ N Â¼ 2: Closed-Form Solution
Although problem (13) is a nonconvex problem, fortunately
we can find a closed-form solution for the case of
M Â¼ N Â¼ 2, as stated in the following proposition:
Proposition 1. Under (A1) to (A4), problem (13) for M Â¼
N Â¼ 2 has the closed-form solution W?, where
w?11 Â¼ maxn
x2Â½n
x1Â½n  x2Â½n
 x1Â½n > x2Â½n; 8n
 
; Ã°14aÃž
w?21 Â¼ minn
x2Â½n
x1Â½n  x2Â½n
 x1Â½n < x2Â½n; 8n
 
; Ã°14bÃž
w?12 Â¼ 1 w?11; w?22 Â¼ 1 w?21: Ã°14cÃž
Proof. By (11), (12), and M Â¼ N Â¼ 2,
yi Â¼ wi1x1 Ã¾ wi2x2 Â¼ wi1Ã°x1  x2Ãž Ã¾ x2; i Â¼ 1; 2: Ã°15Ãž
Thus, maximizing V Ã°solfy1; y2gÃž (i.e., the shaded area in
Fig. 2) is equivalent to increasing jw11j and jw21j, where
w11 	 0 and w21  0. Therefore, problem (13) can be
decomposed into the following two subproblems:
w?11 Â¼ minw w
s:t: w 	 0; wÃ°x1  x2Ãž Ã¾ x2  0;
Ã°16aÃž
w?21 Â¼ maxw w
s:t: w  0; wÃ°x1  x2Ãž Ã¾ x2  0:
Ã°16bÃž
Note that the inequality wÃ°x1  x2Ãž Ã¾ x2  0 is equiva-
lent to wÃ°x1Â½n  x2Â½nÃž Ã¾ x2Â½n  0 for n Â¼ 1; 2; . . . ; L.
Therefore, the constraintwÃ°x1  x2Ãž Ã¾ x2  0 itself implies
w  x2Â½n=Ã°x1Â½n  x2Â½nÃž; for x1Â½n > x2Â½n; Ã°17Ãž
w 	 x2Â½n=Ã°x1Â½n  x2Â½nÃž; for x1Â½n < x2Â½n; Ã°18Ãž
which further lead to
max
n
x2Â½n
x1Â½n  x2Â½n
 x1Â½n > x2Â½n
 
	 w Ã°19Ãž
878 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 5, MAY 2010
Fig. 2. Geometric illustration of the observations x1 and x2 and the
unmixed signals y1 and y2 for M Â¼ N Â¼ 2.
Authorized licensed use limited to: National Tsing Hua University. Downloaded on March 25,2010 at 01:50:03 EDT from IEEE Xplore.  Restrictions apply. 
where xÂ½l1; . . . ;xÂ½lr are the extreme points of the convex
hull and r 	 L is the total number of the extreme points.
Then, by (31) and (30), it follows that
F Â¼

w 2 IRN j wT1N Â¼ 1;
Xr
iÂ¼1
iw
TxÂ½li  0;
Xr
iÂ¼1
i Â¼ 1; i  0; 8i

;
Ã°32Ãž
Â¼ fw 2 IRN j wT1N Â¼ 1;
wTxÂ½li  0; i Â¼ 1; . . . ; rg;
Ã°33Ãž
where xÂ½l1; . . . ;xÂ½lr can be identified by using the quickhull
algorithm [34], a well-known extreme point search method.
Suppose that r extreme points, or equivalently resultant r
inequality constraints in (33), are found, the computational
complexity of the nLCA-IVM then becomes OÃ°N2rÃž
instead of OÃ°N2LÃž, where  is the number of iterations
involved. Due to the use of the quickhull algorithm, the
computational efficiency improvement of the proposed
nLCA-IVM can be measured by the constraint reduction
ratio, defined as
 Â¼ L r
L
: Ã°34Ãž
Surely, the larger the value of , the less the computational
complexity in solving (24).
Some BSS methods, nLCA-ES [23], CAMNS [22], nICA
[13], FastICA [12], NMF [14], quasi-Newton NMF (QNMF)
[18], and sparse NMF (sNMF) [17], can be easily verified to
have complexities (for the case ofM Â¼ N) given by OÃ°N3LÃž,
OÃ°Ã°N  1Ãž2LÃž, OÃ°N2LÃž, OÃ°N3LÃž, OÃ°N2LÃž, and OÃ°N2LÃž,
respectively. The complexity order comparison for the
above-mentioned BSS methods is listed in Table 2.
Obviously, the proposed nLCA-IVM has lower complexity
order than nICA, FastICA, NMF, QNMF, and sNMF.
A geometrical illustration of how the proposed
nLCA-IVM method works for the case of M Â¼ N Â¼ 3 is
shown in Fig. 3. For each row vector update of W, the
unmixed source, say y1, is moved to the boundary of the
feasible set, while fixing y2 and y3, such that V Ã°solfy1; y2; y3gÃž
increases maximally (since the LP always yields the optimal
y1 on the boundary of the feasible set). By repeating the
procedure, the three sources will be literally identified.
Similarly, for the M Â¼ N Â¼ 2 case as shown in Fig. 2, the
nLCA-IVM algorithm involves only two row vector updates,
implying that the global optimal solution for W can be
obtained in only one iteration (in addition to one iteration for
the convergence check). Thus, we can conclude that the
proposed nLCA-IVM algorithm is able to provide the
globally optimal solution for M Â¼ N Â¼ 2 as given in
Proposition 1 above, but no guarantee of the global
optimality forM Â¼ N > 2.
3.3 Source Identifiability of the nLCA
We address here the source identifiability problem of nLCA
concerning certain conditions (assumptions) under which
the N extracted sources are identical to all of the N true
sources. In many biomedical imaging [8], [21] and remote
sensing applications [24], [29], the sources meet the
assumption that pure-source samples exist:
. (A5) There exists at least one index set fl1; l2; . . . ; lNg
such that sÂ½li Â¼ siÂ½liei, i Â¼ 1; 2; . . . ; N .
Note that the corresponding data points xÂ½n for n 2
fl1; l2; . . . ; lNg become
xÂ½li Â¼ aisiÂ½li; Ã°35Ãž
where ai is the ith column vector of A. Namely, the data
point xÂ½li is only contributed from source si. For investigat-
ing the source identifiability of the nLCA, the following
lemma is needed:
Lemma 1. Suppose that (A2), (A3), and (A4) hold and M Â¼ N .
Then, jdetÃ°AÃžj 	 1, and the equality holds if and only ifA is a
permutation matrix.
The proof of Lemma 1 is given in Appendix B. Under
the existence of pure-source samples, the source identifia-
bility of the proposed nLCA is established in the
following theorem:
Theorem 2. (Source identifiability of nLCA). Suppose that
(A1)-(A5) hold. Then,
W?A Â¼ P; Ã°36Ãž
whereW? is the optimal solution of problem (24) and P is an
N N permutation matrix.
Proof. Substituting W? into the constraints of problem (24)
yields
W?1N Â¼ 1N; Ã°37aÃž
W?xÂ½n  0; n Â¼ 1; 2; . . . ; L: Ã°37bÃž
880 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 5, MAY 2010
TABLE 2
Complexity Order Comparison of Various BSS Methods
In the table, N denotes the number of sources, L denotes the data length,  denotes the number of iterations, and r 	 L denotes the number of
extreme points of the convex hull (31).
Fig. 3. Geometric interpretation of nLCA-IVM for M Â¼ N Â¼ 3, where y1
is updated with y2 and y3 fixed, such that V Ã°solfy1; y2; y3gÃž increases
maximally.
Authorized licensed use limited to: National Tsing Hua University. Downloaded on March 25,2010 at 01:50:03 EDT from IEEE Xplore.  Restrictions apply. 
iterations set to 2,000. The other two algorithms, nLCA-ES
and CAMNS, themselves are not iterative algorithms
(without involving convergence). For sNMF [17], the
regulation parameter controlling the sparseness constraint
was set to 0.6. Other parameter settings for QNMF are as
follows: At each iteration, the source matrix S is updated
using the fixed-point algorithm with regulation parameter
 Â¼ 0e	
, where 0 Â¼ 20, 	 Â¼ 0:02, and 
 denotes the
iteration number, and the mixing matrixA is updated using
the quasi-Newton algorithm [18]. On the other hand, in the
last two experiments, the convergence tolerance was still set
to  Â¼ 105 for the proposed nLCA-IVM.
Let qÃ°siÃž Â¼ Ã°1TLsi=LÃž1L and let S^ Â¼ Ã°s^1; s^2; . . . ; s^NÃž de-
note the extracted source matrix. The cross-correlation
coefficient  between S^ and S, defined as
 Â¼ 1
N
max
2N
ci2f1;1g
XN
iÂ¼1
Ã°si  qÃ°siÃžÃžT Ã°cis^i  ciqÃ°s^iÃžÃž
ksi  qÃ°siÃžk  ks^i  qÃ°s^iÃžk
;
(0 	  	 1) is used as the measure for performance evalua-
tion and comparison of the algorithms under test, where
N Â¼ f Â¼ Ã°1; . . . ; NÃž j i 2 f1; . . . ; Ng; i 6Â¼ j; 8i 6Â¼ jg i s
the set of all the permutations of f1; 2; . . . ; Ng and ci is the
sign (or the polarity) ambiguity between the extracted
source s^i and the true source si. The above performance
measure itself is an optimization problem to find the optimal
 and ci such that  is maximum, and it can be efficiently
solved by Kuhn-Munkres algorithm [37]. Note that the
larger the value of , the better the performance of the
associated algorithm under evaluation.
For the computational complexity comparison of the
proposed nLCA-IVM (including the use of the quickhull
algorithm) and the other seven algorithms, the computation
time (seconds) of each BSS algorithm (implemented with
Matlab 7.0) when executed using a Toshiba Satellite A100
laptop personal computer (equipped with Intel Centrino
Duo Processor T2050 CPU 1.6 GHz, 1 GB memory and
Microsoft windows XP home edition version 2002 with
service pack 2) was measured as the corresponding
computational complexity index.
5.1 Experiment 1: Human Face Image Separation
In this experiment, two test scenarios were considered. In the
first scenario, six 128 128 human face images (i.e.,
L Â¼ 16;384) (shown in Fig. 4a) taken from [38] were used as
the source signals to generate six noise-free observations (i.e.,
M Â¼ 6) and 20 noisy observations (i.e., M Â¼ 20) with the
same signal-to-noise ratio (SNRi Â¼ 25 dB, i Â¼ 1; 2; . . . ; 20)
where SNRi Â¼ kxi  ik2=Ã°L2i Ãž and 2i is the variance of
independent and identically distributed zero-meanGaussian
noise components in i. In the second scenario, the same
simulation was performed except that only the first four
human face images in Fig. 4a were used. For the noise-free
case, apart from the mixtures of the original sources, we also
considered the case by artificially adding a set of pure-source
samples fsÂ½li Â¼ siÂ½liei; i Â¼ 1; 2; . . . ; Ng to the original
sources in order to verify the source identifiability of the
nLCA. For the noisy case, we applied the various BSS
methods to process the noise-reduced observations, i.e., xi,
i Â¼ 1; 2; . . . ; N , given by (46).
The values of averaged , denoted by ave, forN Â¼ 6 in this
experiment are listed in Table 3. It is to bementioned that the
original six sources do not have pure-source samples. From
the table, one can observe that, for both the noise-free and
noisy cases, the proposed nLCA-IVM (with maximum ave)
outperforms all the other algorithms. For the case that pure-
source samples were artificially added in the original six
sources, nLCA-IVM, nLCA-ES, and CAMNS achieve perfect
separation (with ave Â¼ 1), and hence, outperform all of the
other algorithms. These results indicate that the proposed
nLCA-IVM is less sensitive to the existence of pure-source
samples than nLCA-ES and CAMNS. In addition, it obtains
the global optimum solution for each realization when six
pure-source samples are artificially made existent in the
original sources, thus justifying the source identifiability of
thenLCA as stated inTheorem2 (despite noguarantee for the
global optimum solution when N > 2). One typical realiza-
tion among the 50 independent runs for the noiseless case is
shown in Fig. 4, where all of the separated images have been
properly ordered for ease of illustrative comparison among
all the BSS algorithms under test. One can see in Fig. 4 that the
separated images obtained by the nLCA-IVM match the
882 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 5, MAY 2010
Fig. 4. Human face images in the absence of noise: (a) the sources,
(b) the observations, and the extracted sources obtained by
(c) nLCA-IVM, (d) nLCA-ES, (e) CAMNS, (f) nICA, (g) FastICA,
(h) NMF, (i) QNMF, and (j) sNMF.
Authorized licensed use limited to: National Tsing Hua University. Downloaded on March 25,2010 at 01:50:03 EDT from IEEE Xplore.  Restrictions apply. 
properly scaled and ordered for ease of illustrative
comparison among all the BSS algorithms under test. One
can see in this figure that the separated source spectra
obtained from nLCA-IVM; nLCA-ES, and CAMNS match
the original source spectra much better than those obtained
by the other algorithms.
5.3 Experiment 3: Dual-Energy X-Ray Image
Decomposition
Accurate detection of lung nodules (the early sign of lung
cancers) using dual-energy chest X-ray imaging is an
important diagnostic task. However, the presence of ribs
or clavicles overlapped with soft tissue presents significant
challenge to detect subtle nodules. Effective separation of
bone and soft tissues in dual-energy chest X-ray imaging is
highly desirable. In this experiment, we applied the
nLCA-IVM to process two mixed images of soft and bone
tissues (L Â¼ 26;896) taken from [39] as shown in Fig. 6a. The
values of ave of all the BSS algorithms under test are also
given in Table 5. It is to be mentioned that two pure-source
samples exist in the original two sources. It can be seen
from the table that nLCA-IVM, nLCA-ES, and CAMNS all
perform perfectly (i.e., ave Â¼ 1), justifying the source
identifiability of nLCA (as stated in Theorem 2) and the
global optimum solution achieved by the proposed
nLCA-IVM (see Proposition 1). On the other hand, the
value of Tave (0.46 second) associated with the proposed
nLCA-IVM is larger than that (0.06 second) associated with
884 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 5, MAY 2010
Fig. 5. Infrared spectra of acetone (left plot), dichloromethane (middle
plot), and toluene (right plot): (a) the sources, (b) the observations and
the extracted sources obtained by (c) nLCA-IVM, (d) nLCA-ES,
(e) CAMNS, (f) nICA, (g) FastICA, (h) NMF, (i) QNMF, and (j) sNMF.
Fig. 6. Dual energy X-ray images: (a) the sources, (b) the observations
and the extracted sources obtained by (c) nLCA-IVM, (d) nLCA-ES,
(e) CAMNS, (f) nICA, (g) FastICA, (h) NMF, (i) QNMF, and (j) sNMF.
TABLE 5
Averaged Cross-Correlation Coefficients (ave) between Sources and Their Estimates, Averaged Computation Time (Tave),
Averaged Number of Iterations (ave) of Various BSS Methods over 50 Monte Carlo Runs in Experiments 2 and 3
Authorized licensed use limited to: National Tsing Hua University. Downloaded on March 25,2010 at 01:50:03 EDT from IEEE Xplore.  Restrictions apply. 
as well as ai1 > 0, ai2 > 0, and ai1 Ã¾ ai2 â€™ 1 for all i, which
are consistent with (A2) and (A4).
6 CONCLUSIONS
We have presented a new nLCA method for the blind
separation of nonnegative and potentially correlated sources
with a given set of mixed observations. The optimum
unmixing matrix is obtained by minimizing the joint
correlation of all the separated source signals subject to the
nonnegativity constraint. A closed-form solution for the
unmixing matrix of the two-source case was provided. The
nLCA method is implemented by an iterative volume
maximization algorithm (i.e., nLCA-IVM) that uses compu-
tationally efficient LP solvers. We also proved the source
identifiability of the nLCA method under the existence of
pure-source samples (i.e., (A5)), and furthermore, it never
relies on (A5) and thus exhibits good performance even
when (A5) is not satisfied perfectly. Comparative experi-
mental results using simulated data were presented to
demonstrate the superior performance and computational
efficiency of the nLCA-IVM over several existing benchmark
methods. Evaluation of the proposed nLCA-IVM using real
biomedical data was also presented, which provides some
insights into the underlying biomedical relevant character-
istics and physiologies. Due to a variety of potential
biomedical imaging applications, nontrivial modifications
of this algorithm may be needed for specific applications,
and they are left as future research along with other
applications, where nBSS is needed.
APPENDIX A
PROOF OF THEOREM 1
1. By (3), (A2), and (A4), for any y 2 convfx1; . . . ; xMg,
y Â¼
XM
iÂ¼1
ixi;

MiÂ¼1i Â¼ 1; i  0

Â¼
XM
iÂ¼1
i
XN
jÂ¼1
aijsj; Ã°by Ã°3ÃžÃž;
Ã°47Ãž
where aij  0 and NjÂ¼1aij Â¼ 1. By letting j Â¼ MiÂ¼1iaij, (47)
becomes
y Â¼
XN
jÂ¼1
jsj 2 convfs1; s2; . . . ; sNg Ã°48Ãž
because j  0 and NjÂ¼1j Â¼ 1. Thus, we can conclude that
convfx1; x2; . . . ; xMg
 convfs1; s2; . . . ; sNg.
2. Again, by (3), (A2), and (A4), for any y 2 solfx1; x2; . . . ;
xMg, y given by (47) still holds except for MiÂ¼1i 	 1; i  0,
and so
y Â¼
XN
jÂ¼1
jsj 2 solfs1; s2; . . . ; sNg Ã°49Ãž
because j  0 and NjÂ¼1j 	 1. Thus, solfx1; x2; . . . ;
xMg 
 solfs1; s2; . . . ; sNg.
APPENDIX B
PROOF OF LEMMA 1
By Hadamardâ€™s inequality [36],
jdetÃ°AÃžj 	
YN
iÂ¼1
XN
jÂ¼1
jaijj2
 !1=2
; Ã°50Ãž
and the equality holds if and only if all the rows of A are
orthogonal. By the inequality (see [42, Theorem 2.33]),
XN
jÂ¼1
b2j
 !1=2
	
XN
jÂ¼1
jbjj; Ã°51Ãž
and the equality holds if and only if b Â¼ Ã°b1; b2; . . . ; bNÃžT Â¼
blel for some l and bl 6Â¼ 0. By (A2)-(A4), (50), and (51),we have
jdetÃ°AÃžj 	
YN
iÂ¼1
XN
jÂ¼1
a2ij
 !1=2
	
YN
iÂ¼1
XN
jÂ¼1
aij
 !
Â¼ 1; Ã°52Ãž
where the first equality holds if and only if all the rows of
A Â¼ Ã°a1; . . . ; aNÃžT are orthogonal, and the second equality
holds if and only if ai Â¼ el (since
PN
jÂ¼1 aij Â¼ 1Ãž for some l.
Thus, it can be easily inferred that jdetÃ°AÃžj Â¼ 1 if and only if
A is a permutation matrix.
ACKNOWLEDGMENTS
The authors would like to sincerely thank Dr. Wing-Kin Ma
and Mr. A. ArulMurugan for their valuable advice,
suggestions, and discussions during the preparation of the
886 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 5, MAY 2010
Fig. 8. DCE-MRI images where M Â¼ 19: (a) the measured breast MR
images, (b) the ROI images of (a), (c) the outputs of PCA, (d) the
extracted permeability images of fast perfusion (top) and slow perfusion
(bottom), and (e) the associated TACs of fast perfusion (dashed line)
and slow perfusion (solid line) obtained by the proposed nLCA-IVM.
Authorized licensed use limited to: National Tsing Hua University. Downloaded on March 25,2010 at 01:50:03 EDT from IEEE Xplore.  Restrictions apply. 
Chong-Yung Chi received the PhD degree in
electrical engineering from the University of
Southern California in 1983. From 1983 to
1988, he was with the Jet Propulsion Laboratory,
Pasadena, California. He has been a professor
in the Department of Electrical Engineering
since 1989 and the Institute of Communications
Engineering (ICE) since 1999 (also the Chair-
man of ICE for 2002-2005) at National Tsing
Hua University, Hsinchu, Taiwan. Currently, he
is an associate editor for the IEEE Signal Processing Letters and the
IEEE Transactions on Circuits and Systems I, and a member of the
IEEE Signal Processing Committee on Signal Processing Theory and
Methods. He coauthored a technical book, Blind Equalization and
System Identification (Springer, 2006) and has published more than
150 technical papers. His research interests include signal processing
for wireless communications, convex analysis and optimization for blind
source separation, biomedical imaging, and hyperspectral imaging. He
is a senior member of the IEEE.
Tsung-Han Chan received the BS degree
from the Department of Electrical Engineering,
Yuan Ze University, Taoyuan, Taiwan, in 2004
and the PhD degree from the Institute of
Communications Engineering, National Tsing
Hua University (NTHU), Hsinchu, Taiwan, in
2009. Currently, he is a postdoctoral research
fellow with the Institute of Communications
Engineering, NTHU. In 2008, he was a visiting
doctoral graduate research assistant with
Virginia Polytechnic Institute and State University, Arlington. His
research interests are in signal processing, convex optimization, and
pattern analysis, with a recent emphasis on dynamic medical imaging
and remote sensing applications.
Yue Wang received the BS and MS degrees in
electrical and computer engineering from
Shanghai Jiao Tong University in 1984 and
1987, respectively, and the PhD degree in
electrical engineering from the University of
Maryland Graduate School in 1995. In 1996,
he was a postdoctoral fellow at Georgetown
University School of Medicine. From 1996 to
2003, he was an assistant and later an associate
professor of electrical engineering at the Catho-
lic University of America. In 2003, he joined Virginia Polytechnic Institute
and State University and is currently the endowed Grant A. Dove
Professor of electrical and computer engineering. He became an elected
fellow of the American Institute for Medical and Biological Engineering
(AIMBE) and was named an ISI highly cited researcher by Thomson
Scientific in 2004. His research interests focus on statistical pattern
recognition, machine learning, signal and image processing, with
applications to computational bioinformatics and biomedical imaging.
. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.
888 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 32, NO. 5, MAY 2010
Authorized licensed use limited to: National Tsing Hua University. Downloaded on March 25,2010 at 01:50:03 EDT from IEEE Xplore.  Restrictions apply. 
ii
List of contributors
Wing-Kin Ma
Wing-Kin Ma received the B.Eng. (with First Class Honors) in electrical and
electronic engineering from the University of Portsmouth, Portsmouth, U.K., in
1995, and the M.Phil. and Ph.D. degrees, both in electronic engineering, from
the Chinese University of Hong Kong (CUHK), Hong Kong, in 1997 and 2001,
respectively.
He is currently an Assistant Professor in the Department of Electronic Engineer-
ing, CUHK. He was with the Institute of Communications Engineering, National
Tsing Hua University, Taiwan, also as an Assistant Professor, from 2005 to 2007.
He is still holding an adjunct position there. Prior to becoming a faculty, he held
various research positions at McMaster University, Canada, CUHK, Hong Kong,
and the University of Melbourne, Australia. His research interests are in signal
processing and communications, with a recent emphasis on MIMO techniques
and convex optimization.
Dr. Maâ€™s Ph.D. dissertation was commended to be â€œof very high quality and well
deserved honorary mentioningâ€ by the Faculty of Engineering, CUHK, in 2001.
He is currently an Associate Editor of the IEEE Transactions on Signal
Processing.
Tsung-Han Chan
Tsung-Han Chan received the B.S. degree from the Department of Electrical
Engineering, Yuan Ze University, Taoyuan, Taiwan, R.O.C., in 2004. He is cur-
rently working towards the Ph.D. degree in the Institute of Communications
Engineering, National Tsing Hua University, Hsinchu, Taiwan, R.O.C. In 2008,
he was a visiting Doctoral Graduate Research Assistant at Virginia Polytech-
nic Institute and State University, Arlington. His research interests are in signal
processing, convex optimization, and pattern analysis with a recent emphasis on
dynamic medical imaging and remote sensing applications.
Chong-Yung Chi
Chong-Yung Chi received the Ph.D. degree in Electrical Engineering from the
University of Southern California in 1983. From 1983 to 1988, he was with the Jet
Propulsion Laboratory, Pasadena, California. He has been a Professor with the
Department of Electrical Engineering since 1989 and the Institute of Communi-
iv
vi
2
4 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
BSS methods are â€˜blindâ€™ in the sense that the mixing process is not known,
at least not explicitly. But what is universally true for all BSS frameworks is
that we make certain presumptions on the source characteristics (and some-
times on the mixing characteristics as well), and then exploit such characteris-
tics during the blind separation process. For instance, independent component
analysis (ICA) [1, 2], a major and very representative BSS framework on which
many BSS methods are based, assumes that the sources are mutually uncorre-
lated/independent random processes possibly with non-Gaussian distributions.
There are many other possibilities one can consider; for example, using quasi-
stationarity [3, 4] (speech signals are quasi-stationary), and using boundness of
the source magnitudes [5, 6, 7] (suitable for digital signals). In choosing a right
BSS method for a particular application, it is important to examine whether the
underlying assumptions of the BSS method are a good match to the application.
For instance, statistical independence is a reasonable assumption in applications
such as speech signal separation, but it may be violated in certain imaging sce-
narios such as hyperspectral imaging [8].
This book chapter focuses on non-negative blind source separation (nBSS), in
which the source signals are assumed to take on non-negative values. Naturally,
images are non-negative signals. Potential applications of nBSS include biomed-
ical imaging [9], hyperspectral imaging [10], and analytical chemistry [11]. In
biomedical imaging, for instance, there are realistic, meaningful problems where
nBSS may serve as a powerful image analysis tool for practitioners. Such exam-
ples will be brieï¬‚y described in this book chapter.
In nBSS, how to cleverly utilize source non-negativity to achieve clean sepa-
ration has been an intriguing subject that has received much attention recently.
Presently available nBSS methods may be classiï¬ed into two groups. One group
is similar to ICA: Assume that the sources are mutually uncorrelated or indepen-
dent, but with non-negative source distributions. Methods falling in this class
include non-negative ICA (nICA) [12], stochastic non-negative ICA (SNICA)
[13], and Bayesian positive source separation (BPSS) [14]. In particular, in nICA
the blind separation criterion can theoretically guarantee perfect separation of
sources [15], under an additional assumption where the source distributions are
non-vanishing around zero (this is called the well-grounded condition).
Another group of nBSS methods does not rely on statistical assumptions.
Roughly speaking, these methods explicitly exploit source non-negativity or even
mixing matrix non-negativity, with an attempt to achieve some kind of least
square ï¬tting criterion. Methods falling in this group are generally known as (or
may be vaguely recognized as) non-negative matrix factorization (NMF) [16, 17].
An advantage with NMF is that it does not operate on the premise of mutual
uncorrelatedness/independence as in the ï¬rst group of nBSS methods. NMF is a
nonconvex constrained optimization problem. A popular way of handling NMF
is to apply gradient descent [17], but it is known to be suboptimal and slowly
convergent. A projected quasi-Newton method has been incorporated in NMF
to speed up its convergence [18]. Alternatively, alternating least squares (ALS)
6 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
(a) Vector space of the signals.
Solid: true sources; dashed: observations.
(b) From the observations we can construct
a polyhedral set, in which the true source 
vectors must lie.
(d) Implemention of CAMNS:
computationally estimate the 
extreme points; e.g., by LP.
(c) Key result of CAMNS:
the true source vectors are at the 
extreme points of the polyhedral set. 
Figure 1.1: An intuitive illustration of how CAMNS operates.
1.2 Problem Statement
Our notations are standard, following that in convex optimization for signal
processing:
R, RN , RMÃ—N Set of real numbers, N -vectors, M Ã—N matrices
R+, RN+ , R
MÃ—N
+ Set of non-negative real numbers, N -vectors, M Ã—N matri-
ces
1 All one vector
IN N Ã—N identity matrix
ei Unit vector of proper dimension with the ith entry equal to
1
 Componentwise inequality
â€– Â· â€– Euclidean norm
N (Î¼,Î£) Gaussian distribution with mean Î¼ and covariance Î£
diag{x1, . . . , xN} Diagonal matrix with diagonal entries x1, . . . , xN
det(X) Determinant of a square matrix X
8 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
=
Time
X
Time
SourcesObservations
Plasma input
Fast flow
Slow flow
Region of interest
extraction
DCE-MRI of breast tumor
Figure 1.2: The BSS problem in DCE-MRI applications.
the weighted summation of spatially-distributed vascular permeability associ-
ated with diï¬€erent perfusion rates. The BSS methods can be applied to com-
putationally estimate the time activity curves (mixing matrix) and underlying
compartment vascular permeability (sources) within the tumor site.
Example 1.2: Dynamic Fluorescent Imaging (DFI)
DFI exploits highly speciï¬c and biocompatible ï¬‚uorescent contrast agents to
interrogate small animals for drug development and disease research [29]. The
technique generates a time series of images acquired after injection of an inert
dye, where the dyeâ€™s diï¬€erential biodistribution dynamics allow precise delin-
eation and identiï¬cation of major organs. However, spatial resolution and quan-
titation at depth is not one of the strengths of planar optical approaches, due
mainly to the malign eï¬€ects of light scatter and absorption.
The DFI data acquired in a mouse study, for instance, is shown in Figure
1.3, where each DFI image (observation) is delineated as a linear mixture of the
anatomical maps associated with diï¬€erent organs. The BSS methods can be used
10 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
and a slow perfusion source images [9] can be higher than 95%. For high contrast
images such as human portraits, we found that (A2) would also be an appropriate
assumption.
We make two more assumptions
(A3) The mixing matrix has unit row sum; i.e., for all i = 1, . . . ,M ,
Nâˆ‘
j=1
aij = 1. (1.3)
(A4) M â‰¥ N and A is of full column rank.
Assumption (A4) is rather standard in BSS. Assumption (A3) is essential to
CAMNS, but can be relaxed through a model reformulation [28]. Moreover, in
MRI (e.g., in Example 1.1), (A3) is automatically satisï¬ed due to the so-called
partial volume eï¬€ect [28]. The following example shows how we can relax (A3):
Example 1.3: Suppose that the model in (1.2) does not satisfy (A3). For simplic-
ity of exposition of the idea, assume non-negative mixing; i.e., aij â‰¥ 0 for all (i, j)
(extension to aij âˆˆ R is possible). Under such circumstances, the observations are
all non-negative and we can assume that
xTi 1 = 0
for all i = 1, . . . ,M . Likewise, we can assume sTj 1 = 0 for all j. The idea is to
enforce (A3) by normalizing the observation vectors:
xÂ¯i 
xi
xTi 1
=
Nâˆ‘
j=1
(
aijs
T
j 1
xTi 1
)(
sj
sTj 1
)
. (1.4)
By letting aÂ¯ij = aijsTj 1/x
T
i 1 and sÂ¯j = sj/s
T
j 1, we obtain a model xÂ¯i =âˆ‘N
j=1 aÂ¯ij sÂ¯j which is in the same form as the original signal model in (1.2). It is
easy to show that the new mixing matrix, denoted by AÂ¯, has unit row sum [or
(A3)].
It should also be noted that the model reformulation above does not damage
the rank of the mixing matrix. Speciï¬cally, if the original mixing matrix A
satisï¬es (A4), then the new mixing matrix AÂ¯ also satisï¬es (A4). To show this,
we notice that the relationship of A and AÂ¯ can be expressed as
AÂ¯ = Dâˆ’11 AD2 (1.5)
where D1 = diag(xT1 1, ...,x
T
M1) and D2 = diag(s
T
1 1, ..., s
T
N1). Since D1 and D2
are of full rank, we have rank(AÂ¯) =rank(A).
Throughout this chapter, we will assume that (A1)- (A4) are satisï¬ed unless
speciï¬ed.
12 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
The number P in (1.7) is called the aï¬ƒne dimension, which characterizes the
eï¬€ective dimension of the aï¬ƒne hull. The aï¬ƒne dimension must satisfy P â‰¤
N âˆ’ 1. Moreover,
Property 1.1. If {s1, . . . , sN} is an aï¬ƒnely independent set (which means that
{s1 âˆ’ sN , . . . , sNâˆ’1 âˆ’ sN} is linearly independent), then the aï¬ƒne dimension is
maximal; i.e., P = N âˆ’ 1.
1.3.2 Convex Hull
Given a set of vectors {s1, . . . , sN} âŠ‚ RL, the convex hull is deï¬ned as
conv{s1, . . . , sN} =
{
x =
Nâˆ‘
i=1
Î¸isi
âˆ£âˆ£âˆ£âˆ£ Î¸ âˆˆ RN+ ,
Nâˆ‘
i=1
Î¸i = 1
}
. (1.8)
A convex hull would be a line segment for N = 2, a triangle for N = 3. This is
illustrated in Figure 1.5.
s1
s1
s2s2
s3
00
N = 2 N = 3
conv{s1, s2} conv{s1, s2, s3}
Figure 1.5: Examples of convex hulls for N = 2 and N = 3.
An important concept related to convex hull is that of extreme points, also
known as vertices. From a geometric perspective, extreme points are the â€˜corner
pointsâ€™ of the convex hull. A point x âˆˆ conv{s1, . . . , sN} is said to be an extreme
point of conv{s1, . . . , sN} if x can never be a convex combination of s1, . . . , sN
in a non-trivial manner; i.e.,
x =
Nâˆ‘
i=1
Î¸isi (1.9)
for all Î¸ âˆˆ RN+ ,
âˆ‘N
i=1 Î¸i = 1, and Î¸ = ei for any i. Some basic properties about
extreme points are as follows:
Property 1.2. The set of extreme points of conv{s1, . . . , sN} must be either
the full set or a subset of {s1, . . . , sN}.
14 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
An illustration is shown in Figure 1.6(a) to pictorially demonstrate the aï¬ƒne
hull equivalence in Lemma 1.1. Since Lemma 1.1 represents an essential part to
CAMNS, here we provide the proof to illustrate its idea.
s1
s2
x1
x2
x3
e1
e2
e3 aï¬€{s1, s2} = aï¬€{x1,x2,x3}
Figure 1.6: A geometric illustration of the aï¬ƒne hull equivalence in Lemma 1.1,
for the special case of N = 2, M = 3, and L = 3.
Proof of Lemma 1.1. Any x âˆˆ aï¬€{x1, ...,xM} can be represented by
x =
Mâˆ‘
i=1
Î¸ixi, (1.12)
where Î¸ âˆˆ RM , Î¸T1 = 1. Substituting (1.2) into (1.12), we get
x =
Nâˆ‘
j=1
Î²jsj , (1.13)
where Î²j =
âˆ‘M
i=1 Î¸iaij for j = 1, ..., N , or equivalently
Î² = AT Î¸. (1.14)
Since A has unit row sum [(A3)], we have
Î²T1 = Î¸T (A1) = Î¸T1 = 1. (1.15)
This implies that Î²T1 = 1, and as a result it follows from (1.13) that x âˆˆ
aï¬€{s1, ..., sN}.
On the other hand, any x âˆˆ aï¬€{s1, ..., sN} can be represented by (1.13) for
Î²T1 = 1. Since A has full column rank [(A4)], there always exist a Î¸ such that
(1.14) holds. Substituting (1.14) into (1.13) yields (1.12). Since (1.15) implies
that Î¸T1 = 1, we conclude that x âˆˆ aï¬€{x1, ...,xM}.
Thus, by constructing the observation aï¬ƒne hull, the source aï¬ƒne hull
will be obtained. Using the linear equality representation of an aï¬ƒne hull,
16 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
Remember that we are dealing with non-negative sources. Hence, any source
vector si must lie in
S  aï¬€{s1, . . . , sN} âˆ© RL+
= A(C,d) âˆ©RL+
= {x | x = CÎ± + d, x  0, Î± âˆˆ RNâˆ’1}. (1.22)
Note that we have knowledge of S only through (1.22), a polyhedral set repre-
sentation. The following lemma plays an important role:
Lemma 1.3. The polyhedral set S is identical to the source convex hull; that is,
S = conv{s1, . . . , sN}. (1.23)
Following the illustration in Figure 1.6, in Figure 1.7 we geometrically demon-
strate the equivalence of S and conv{s1, . . . , sN}. This surprising result is due
mainly to the local dominance, and we include its proof here considering its
importance.
Proof of Lemma 1.3. Assume that z âˆˆ aï¬€{s1, ..., sN} âˆ© RL+:
z =
Nâˆ‘
i=1
Î¸isi  0, 1T Î¸ = 1.
From (A2), it follows that z[i] = Î¸isi[i] â‰¥ 0, âˆ€i. Since si[i] > 0, we must have
Î¸i â‰¥ 0, âˆ€i. Therefore, z lies in conv{s1, ..., sN}. On the other hand, assume that
z âˆˆ conv{s1, ..., sN}, i.e.,
z =
Nâˆ‘
i=1
Î¸isi, 1T Î¸ = 1, Î¸  0
implying that z âˆˆ aï¬€{s1, ..., sN}. From (A1), we have si  0 âˆ€i and subsequently
z  0. This completes the proof for (1.23).
Furthermore, we can deduce from Lemma 1.2 and Property 1.3 that
Lemma 1.4. The set of extreme points of conv{s1, . . . , sN} is {s1, . . . , sN}.
Summarizing all the results above, we establish an nBSS criterion as follows:
18 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
special cases. It will also shed light into the volume maximization heuristics
considered in the later part of this chapter.
Consider the pre-image of the observation-constructed polyhedral set S, under
the mapping s = CÎ± + d:
F = {Î± âˆˆ RNâˆ’1 âˆ£âˆ£ CÎ± + d  0 }
=
{
Î± âˆˆ RNâˆ’1 âˆ£âˆ£ cTnÎ± + dn â‰¥ 0, n = 1, . . . , L } (1.25)
where cTn is the nth row of C. There is a direct correspondence between the
extreme points of S and F [26]:
Lemma 1.5. The polyhedral set F in (1.25) is equivalent to a simplex
F = conv{Î±1, . . . ,Î±N} (1.26)
where each Î±i âˆˆ RNâˆ’1 satisï¬es
CÎ±i + d = si. (1.27)
The proof of Lemma 1.5 is given in Appendix 1.9.3. Since the set of extreme
points of a simplex conv{Î±1, . . . ,Î±N} is {Î±1, . . . ,Î±N} (Property 1.4), we have
the following alternative nBSS criterion:
Criterion 1.2 (Alternative form of Criterion 1.1). Use the aï¬ƒne set ï¬tting
solution in Proposition 1.1 to compute (C,d). Then, ï¬nd all the extreme points
of the simplex
F = {Î± âˆˆ RNâˆ’1 âˆ£âˆ£ CÎ± + d  0 } (1.28)
and denote the obtained set of extreme points by {Î±Ë†1, . . . , Î±Ë†N}. Output
sË†i = CÎ±Ë†i + d, i = 1, . . . , N (1.29)
as the set of estimated source vectors.
It follows directly from Theorem 1.1 and Lemma 1.5 that
Theorem 1.2. The solution to Criterion 1.2 is uniquely given by the set of true
source vectors {s1, . . . , sN}, under the premises in (A1) to (A4).
In [26], we have used Criterion 1.2 to develop simple nBSS algorithms for
the cases of two and three sources. In the following we provide the solution
for the two-source case and demonstrate its eï¬€ectiveness using synthetic X-ray
observations:
20 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
1.5 Systematic Linear Programming Method for CAMNS
This section, as well as the next section are dedicated to the practical imple-
mentations of CAMNS. In this section, we propose an approach that uses linear
programs (LPs) to systematically fulï¬l the CAMNS criterion, speciï¬cally Crite-
rion 1.1. An appealing characteristic of this CAMNS-LP method is that Crite-
rion 1.1 does not appear to be related to convex optimization at ï¬rst look, and
yet it can be exactly solved by CAMNS-LP as long as the problem assumptions
(A1)-(A4) are valid.
Our problem as speciï¬ed in Criterion 1.1 is to ï¬nd all the extreme points of the
polyhedral set S in (1.22). In the optimization literature this problem is known
as vertex enumeration; see [33, 34, 35] and the references therein. The available
extreme-point ï¬nding methods are sophisticated, requiring no assumption on
the extreme points. However, the complexity of those methods would increase
exponentially with the number of inequalities L (note that L is also the data
length in our problem, which is often large in practice). The notable diï¬€erence of
the development here is that we exploit the characteristic that the extreme points
s1, ..., sN are linearly independent (Lemma 1.2). By doing so we will establish
an extreme-point ï¬nding method (for CAMNS) whose complexity is polynomial
in L.
Our approach is to identify one extreme point at one time. Consider the fol-
lowing linear minimization problem:
p = min
s
rT s
subject to (s.t.) s âˆˆ S
(1.35)
for some arbitrarily chosen direction r âˆˆ RL, where p denotes the optimal objec-
tive value of (1.35). By the polyhedral representation of S in (1.22), problem
(1.35) can be equivalently represented by an LP
p = min
Î±
rT (CÎ± + d)
s.t. CÎ± + d  0
(1.36)
which can be solved by readily available algorithms such as the polynomial-
time interior-point methods [36, 37]. Problem (1.36) is the problem we solve in
practice, but (1.35) leads to important implications to extreme-point search.
A fundamental result in LP theory is that rT s, the objective function of (1.35),
attains the minimum at a point of the boundary of S. To provide more insights,
some geometric illustrations are given in Figure 1.9. We can see that the solution
of (1.35) may be uniquely given by one of the extreme points si [Figure 1.9(a)],
or it may be any point on a face [Figure 1.9(b)]. The latter case poses a trouble to
our task of identifying si, but it is arguably not a usual situation. For instance,
in the illustration in Figure 1.9(b), r must be normal to s2 âˆ’ s3 which may be
unlikely to happen for a randomly picked r. With this intuition in mind, we can
prove the following lemma:
22 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
that (1.36) and (1.37) should both lead to new extreme points. Interestingly, we
found theoretically that such an expectation is not true, but close. It can be
shown that (see Appendix 1.9.5)
Lemma 1.7. Suppose that r = Bw, where B âˆˆ RLÃ—L is given by (1.39) and w
is randomly generated following a distribution N (0, IL). Then, with probability
1, at least one of the optimal solutions of (1.36) and (1.37) is a new extreme
point; i.e., si for some i âˆˆ {l + 1, ..., N}. The certiï¬cate of ï¬nding new extreme
points is indicated by |p| = 0 for the case of (1.36), and |q| = 0 for (1.37).
By repeating the above described procedures, we can identify all the extreme
points s1, ..., sN . The resultant CAMNS-LP method is summarized in Algorithm
1.1.
Algorithm 1.1. CAMNS-LP
Given an aï¬ƒne set characterization 2-tuple (C,d).
Step 1. Set l = 0, and B = IL.
Step 2. Randomly generate a vector w âˆ¼ N (0, IL), and set r := Bw.
Step 3. Solve the LPs
p = min
Î±:CÎ±+d0
rT (CÎ± + d)
q = max
Î±:CÎ±+d0
rT (CÎ± + d)
and obtain their optimal solutions, denoted by Î±1 and Î±

2, respectively.
Step 4. If l = 0
SË† = [ CÎ±1 + d, CÎ±2 + d ]
else
If |p| = 0 then SË† := [ SË† CÎ±1 + d ].
If |q| = 0 then SË† := [ SË† CÎ±2 + d ].
Step 5. Update l to be the number of columns of SË†.
Step 6. Apply QR decomposition
SË† = Q1R1,
where Q1 âˆˆ RLÃ—l and R1 âˆˆ RlÃ—l. Update B := IL âˆ’Q1QT1 .
Step 7. Repeat Step 2 to Step 6 until l = N .
The CAMNS-LP method in Algorithm 1.1 is not only systematically straight-
forward to apply, it is also eï¬ƒcient due to the maturity of convex optimiza-
tion algorithms. Using a primal-dual interior-point method, each LP problem [or
the problem in (1.35) or (1.37)] can be solved with a worst-case complexity of
O(L0.5(L(N âˆ’ 1) + (N âˆ’ 1)3))  O(L1.5(N âˆ’ 1)) for L N [37]. Since the algo-
24 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
where
Î”(Î²1, . . . ,Î²N ) =
[
Î²1 Â· Â· Â· Î²N
1 Â· Â· Â· 1
]
âˆˆ RNÃ—N . (1.42)
Suppose that {Î²1, . . . ,Î²N} âŠ‚ F . As illustrated in the picture in Fig-
ure 1.10, the volume of conv{Î²1, . . . ,Î²N} should be no greater than that
of F = conv{Î±1, . . . ,Î±N}. Hence, by ï¬nding {Î²1, . . . ,Î²N} âŠ‚ F such that the
respective simplex volume is maximized, we would expect that {Î²1, . . . ,Î²N}
is exactly {Î±1, . . . ,Î±N}, the ground truth we are seeking. This leads to the
following variation of the CAMNS criterion:
F
Î²1 Î²2
Î²3
Î±1
Î±2
Î±3
conv{Î²1,Î²2,Î²3}
Figure 1.10: A geometric illustration for {Î²1, . . . ,Î²N} âŠ‚ F for N = 3.
Criterion 1.3 (Volume maximization alternative to Criterion 1.2). Use the
aï¬ƒne set ï¬tting solution in Proposition 1.1 to compute (C,d). Then, solve the
volume maximization problem
{Î±Ë†1, . . . , Î±Ë†N} = arg max
Î²1,...,Î²N
V (Î²1, . . . ,Î²N )
s.t. {Î²1, . . . ,Î²N} âŠ‚ F ,
(1.43)
Output
sË†i = CÎ±Ë†i + d, i = 1, . . . , N (1.44)
as the set of estimated source vectors.
Like Criteria 1.1 and 1.2, Criterion 1.3 can be shown to provide the same
perfect separation result
Theorem 1.3. The globally optimal solution of (1.43) is uniquely given by
Î±1, . . . ,Î±N , under the premises of (A1)-(A4).
26 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
The objective function in (1.47) is still nonconvex, but (1.47) can be solved in a
globally optimal manner by breaking it into two LPs:
p = max
Î²jâˆˆRNâˆ’1
bTj Î²j + (âˆ’1)N+jdet(BNj)
s.t. CÎ²j + d  0.
(1.48)
q = min
Î²jâˆˆRNâˆ’1
bTj Î²j + (âˆ’1)N+jdet(BNj)
s.t. CÎ²j + d  0.
(1.49)
The optimal solution of (1.47), denoted by Î±Ë†j , is the optimal solution of (1.48) if
|p| > |q|, and the optimal solution of (1.49) if |q| > |p|. This partial maximiza-
tion is conducted alternately (i.e., j := (j modulo N) + 1) until some stopping
rule is satisï¬ed.
The CAMNS alternating volume maximization heuristics, or simply CAMNS-
AVM, is summarized in Algorithm 1.2.
28 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
on a challenging scenario reminiscent of ghosting eï¬€ects in photography. Sec-
tion 1.7.3 considers a problem in which the sources are faces of ï¬ve diï¬€erent
persons. Section 1.7.4 uses Monte Carlo simulation to evaluate the performance
of CAMNS-based algorithms under noisy condition. For performance compari-
son, we also test three existing nBSS algorithms, namely non-negative matrix
factorization (NMF) [16], non-negative independent component analysis (nICA)
[12], and Ergodanâ€™s algorithm (a BSS method that exploits magnitude bounds
of the sources) [6].
The performance measure used in this book chapter is described as follows. Let
S = [s1, . . . , sN ] be the true multi-source signal matrix, and SË† = [sË†1, . . . , sË†N ] be
the multi-source output of a BSS algorithm. It is well known that a BSS algorithm
is inherently subject to permutation and scaling ambiguities. We propose a sum
square error (SSE) measure for S and SË† [40, 41], given as follows:
e(S, SË†) = min
Ï€âˆˆÎ N
Nâˆ‘
i=1
âˆ¥âˆ¥âˆ¥âˆ¥si âˆ’ â€–siâ€–â€–sË†Ï€iâ€– sË†Ï€i
âˆ¥âˆ¥âˆ¥âˆ¥
2
(1.50)
where Ï€ = (Ï€1, . . . , Ï€N ), and Î N = {Ï€ âˆˆ RN | Ï€i âˆˆ {1, 2, . . . , N}, Ï€i = Ï€j for i =
j} is the set of all permutations of {1, 2, ..., N}. The optimization of (1.50) is to
adjust the permutation Ï€ such that the best match between true and estimated
signals is yielded, while the factor â€–siâ€–/â€–sË†Ï€iâ€– is to ï¬x the scaling ambiguity.
Problem (1.50) is the optimal assignment problem which can be eï¬ƒciently solved
by Hungarian algorithm 1 [42].
1.7.1 Example of 3-Source Case: Cell Separation
In this example three 125Ã— 125 cell images, displayed in Figure 1.12(a), were
taken as the source images. Each image is represented by a source vector si âˆˆ RL,
by scanning the image vertically from top left to bottom right (thereby L =
1252 = 15625). For the three source images, we found that the local dominance
assumption is not perfectly satisï¬ed. To shed some light into this, we propose a
measure called the local dominance proximity factor (LDPF) of the ith source,
deï¬ned as follows:
Îºi = max
n=1,...,L
si[n]âˆ‘
j =i sj [n]
. (1.51)
When Îºi =âˆž, we have the ith source satisfying the local dominance assumption
perfectly. The values of Îºiâ€™s in this example are shown in Table 1.1, where we
see that the LDPFs of the three sources are strong but not inï¬nite.
1 A Matlab implementation is available at http://si.utia.cas.cz/Tichavsky.html.
30 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
(a) (b) (c)
(d) (e) (f) (g)
Figure 1.12: Cell separation: (a) The sources, (b) the observations, and the
extracted sources obtained by (c) CAMNS-AVM method, (d) CAMNS-LP
method, (e) NMF, (f) nICA and (g) Erdoganâ€™s algorithm.
32 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
(a)
(b)
(c)
(d)
(e)
(f)
(g)
Figure 1.13: Ghosting reduction: (a) The sources, (b) the observations, and
the extracted sources obtained by (c) CAMNS-AVM method, (d) CAMNS-LP
method, (e) NMF, (f) nICA and (g) Erdoganâ€™s algorithm.
34 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
(e) (f) (g)
Figure 1.14: Human face separation (continued): The extracted sources obtained
by (e) NMF, (f) nICA and (g) Erdoganâ€™s algorithm.
1.8 Summary and Discussion
In this book chapter, we have shown how convex analysis provides a new avenue
to approaching non-negative blind source separation. Using convex geometry
concepts such as aï¬ƒne hull and convex hull, an analysis was carried out to
show that under some appropriate assumptions nBSS can be boiled down to
a problem of ï¬nding extreme points of a polyhedral set. We have also shown
how this extreme point ï¬nding problem can be solved by convex optimization,
speciï¬cally by using LPs to systematically ï¬nd all the extreme points.
The key success of this new nBSS framework is based on a deterministic signal
assumption called local dominance. Local dominance is a good model assump-
tion for sparse or high-contrast images, but it may not be perfectly satisï¬ed
36 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
sometimes. We have developed an alternative to the systematic LP method that
is expected to yield better robustness against violation of local dominance. The
idea is to solve a volume maximization problem. Despite the fact that the pro-
posed algorithm uses heuristics to handle volume maximization (which is non-
convex), simulation results match with our intuitive expectation that volume
maximization (done by a heuristics) can exhibit better resistance against the
model mismatch.
We have carried out a number of simulations using diï¬€erent sets of image
data, and have demonstrated that the proposed convex analysis based nBSS
methods are promising, both by visual inspection and by the sum-square-error
separation performance measure. Other methods such as nICA and NMF were
also compared to demonstrate the eï¬€ectiveness of the proposed methods.
38 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
1.9.3 Proof of Lemma 1.5
Equation (1.25) can also be expressed as
F = { Î± âˆˆ RNâˆ’1 | CÎ± + d âˆˆ conv{s1, ..., sN} } .
Thus, every Î± âˆˆ F satisï¬es
CÎ± + d =
Nâˆ‘
i=1
Î¸isi (1.61)
for some Î¸  0, Î¸T1 = 1. Since C has full column rank, (1.61) can be re-
expressed as
Î± =
Nâˆ‘
i=1
Î¸iÎ±i, (1.62)
where Î±i = (CTC)âˆ’1CT (si âˆ’ d) (or CÎ±i + d = si). Equation (1.62) implies
that F = conv{Î±1, ...,Î±N}.
We now show that F = conv{Î±1, ...,Î±N} is a simplex by contradiction. Sup-
pose that {Î±1, ...,Î±N} are not aï¬ƒnely independent. This means that for some
Î³1, . . . , Î³Nâˆ’1,
âˆ‘Nâˆ’1
i=1 Î³i = 1, Î±N =
âˆ‘Nâˆ’1
i=1 Î³iÎ±i can be satisï¬ed. One then has
sN = CÎ±N + d =
âˆ‘Nâˆ’1
i=1 Î³isi, which is a contradiction to the property that
{s1, ..., sN} is linearly independent.
1.9.4 Proof of Lemma 1.6
Any point in S = conv{s1, ..., sN} can be equivalently represented by s =âˆ‘N
i=1 Î¸isi, where Î¸  0 and Î¸T1 = 1. Applying this result to (1.35), problem
(1.35) can be reformulated as
min
Î¸âˆˆRN
âˆ‘N
i=1 Î¸iÏi
s.t. Î¸T1 = 1, Î¸  0,
(1.63)
where Ïi = rT si. We assume without loss of generality that Ï1 < Ï2 â‰¤ Â· Â· Â· â‰¤ ÏN .
If Ï1 < Ï2 < Â· Â· Â· < ÏN , then it is easy to verify that the optimal solution to (1.63)
is uniquely given by Î¸ = e1. In its counterpart in (1.35), this translates into
s = s1. But when Ï1 = Ï2 = Â· Â· Â· = ÏP and ÏP < ÏP+1 â‰¤ Â· Â· Â· â‰¤ ÏN for some P ,
the solution of (1.63) is not unique. In essence, the latter case can be shown to
have a solution set
Î˜ = {Î¸ | Î¸T1 = 1, Î¸  0, Î¸P+1 = ... = Î¸N = 0}. (1.64)
We now prove that the non-unique solution case happens with probability
zero. Suppose that Ïi = Ïj for some i = j, which means that
(si âˆ’ sj)T r = 0. (1.65)
40 Chapter 1. Convex Analysis for Non-negative Blind Source Separation with Application in Imaging
1.9.6 Proof of Theorem 1.3
In problem (1.43), the constraints Î²i âˆˆ F = conv{Î±1, . . . ,Î±N} imply that
Î²i =
Nâˆ‘
j=1
Î¸ijÎ±j (1.71)
where
âˆ‘N
j=1 Î¸ij = 1 and Î¸ij â‰¥ 0 for i = 1, . . . , N . Hence we can write
Î”(Î²1, . . . ,Î²N ) = Î”(Î±1, . . . ,Î±N )Î˜T , (1.72)
where Î˜ = [Î¸ij ] âˆˆ RNÃ—N+ and Î˜1 = 1. For such a structured Î˜ it was shown
that (Lemma 1 in [43])
|det (Î˜)| â‰¤ 1 (1.73)
and that |det (Î˜)| = 1 if and only if Î˜ is a permutation matrix. It follows from
(1.41), (1.72) and (1.73) that
V (Î²1, . . . ,Î²N ) =|det(Î”(Î±1, . . . ,Î±N )Î˜T )|/(N âˆ’ 1)!
= |det(Î”(Î±1, . . . ,Î±N ))||det(Î˜)|/(N âˆ’ 1)!
â‰¤ V (Î±1, . . . ,Î±N ) (1.74)
and that the equality holds if and only if Î˜ is a permutation matrix, which
implies {Î²1, . . . ,Î²N} = {Î±1, . . . ,Î±N}. Hence we conclude that V (Î²1, . . . ,Î²N ) is
maximized if and only if {Î²1, . . . ,Î²N} = {Î±1, . . . ,Î±N}.
Acknowledgments
This work was supported in part by the National Science Council (R.O.C.) under
Grants NSC 96-2628-E-007-003-MY3, by the U.S. National Institutes of Health
under Grants EB000830 and CA109872, and by a grant from the Research Grant
Council of Hong Kong (General Research Fund, Project 2150599).
42 References
[14] S. Moussaoui, D. Brie, A. Mohammad-Djafari, and C. Carteret, â€œSeparation
of non-negative mixture of non-negative sources using a Bayesian approach
and MCMC sampling,â€ IEEE Trans. Signal Process., vol. 54, no. 11, pp.
4133â€“4145, Nov. 2006.
[15] M. D. Plumbley, â€œConditions for nonnegative independent component
analysis,â€ IEEE Signal Processing Letters, vol. 9, no. 6, pp. 177â€“180, 2002.
[16] D. D. Lee and H. S. Seung, â€œLearning the parts of objects by non-negative
matrix factorization,â€ Nature, vol. 401, pp. 788â€“791, Oct. 1999.
[17] â€”â€”, â€œAlgorithms for non-negative matrix factorization,â€ in NIPS. MIT
Press, 2001, pp. 556â€“562.
[18] R. Zdunek and A. Cichocki, â€œNonnegative matrix factorization with con-
strained second-order optimization,â€ Signal Processing, vol. 87, no. 8, pp.
1904â€“1916, 2007.
[19] C. Lawson and R. J. Hanson, Solving Least-Squares Problems. New Jersey:
Prentice-Hall, 1974.
[20] R. Tauler and B. Kowalski, â€œMultivariate curve resolution applied to spec-
tral data from multiple runs of an industrial process,â€ Anal. Chem., vol. 65,
pp. 2040â€“2047, 1993.
[21] A. Zymnis, S.-J. Kim, J. Skaf, M. Parente, and S. Boyd, â€œHyperspectral
image unmixing via alternating projected subgradients,â€ in 41st Asilomar
Conference on Signals, Systems, and Computers, Paciï¬c Grove, CA, Nov.
4-7, 2007.
[22] C.-J. Lin, â€œProjected gradient methods for non-negative matrix factoriza-
tion,â€ Neural Computation, vol. 19, no. 10, pp. 2756â€“2779, 2007.
[23] H. Laurberg, M. G. Christensen, M. D. Plumbley, L. K. Hansen, and S. H.
Jensen, â€œTheorems on positive data: On the uniqueness of NMF,â€ Compu-
tational Intelligence and Neuroscience, p. ID764206, 2008.
[24] P. Hoyer, â€œNonnegative sparse coding,â€ in IEEE Workshop on Neural Net-
works for Signal Processing, Martigny, Switzerland, Sept. 4-6, 2002, pp.
557â€“565.
[25] H. Kim and H. Park, â€œSparse non-negative matrix factorizations via alter-
nating non-negativity-constrained least squares for microarray data analy-
sis,â€ Bioinformatics, vol. 23, no. 12, pp. 1495â€“1502, 2007.
[26] T.-H. Chan, W.-K. Ma, C.-Y. Chi, and Y. Wang, â€œA convex analysis frame-
work for blind separation of non-negative sources,â€ IEEE Trans. Signal
Processing, vol. 56, no. 10, pp. 5120â€“5134, Oct. 2008.
[27] F.-Y. Wang, Y. Wang, T.-H. Chan, and C.-Y. Chi, â€œBlind separation of
multichannel biomedical image patterns by non-negative least-correlated
component analysis,â€ in Lecture Notes in Bioinformatics (Proc. PRIBâ€™06),
Springer-Verlag, vol. 4146, Berlin, Dec. 9-14, 2006, pp. 151â€“162.
[28] F.-Y. Wang, C.-Y. Chi, T.-H. Chan, and Y. Wang, â€œBlind separation
of positive dependent sources by non-negative least-correlated component
analysis,â€ in IEEE International Workshop on Machine Learning for Signal
Index
aï¬ƒne hull, 11â€“14
alternating optimization, 23, 25â€“27
blind source separation (BSS), 3â€“5, 7â€“10, 13,
17â€“19
blind source separation criterion, 16â€“18
convex analysis, 5, 11, 13
convex hull, 12, 13, 16
imaging, 3, 4, 7â€“9
linear program (LP), 20â€“23, 26
non-negativity, 4, 5, 9
polyhedral set, 5, 16â€“18, 20, 23, 25
simplex, 13, 18, 19, 23, 25
44
The goal of CAMNS-AVM is to estimate the sources s1, . . . , sN
from the given observations x1, . . . ,xM , assuming prior knowledge
of N . Moreover, CAMNS-AVM assumes that
(A1) For all j = 1, . . . , N , sj  0.
(A2) For each i âˆˆ {1, . . . , N}, there exists an (unknown) index `i
such that si[`i] > 0 and sj [`i] = 0, âˆ€j 6= i.
(A3) For all i = 1, . . . ,M ,âˆ‘Nj=1 aij = 1.
(A4) M â‰¥ N and A , [aij ]MÃ—N is of full column rank.
Next, we will briefly review the CAMNS criterion [1, 8] and the
CAMNS-AVM [1].
2.1. CAMNS Criterion
We first define the polyhedral set S as follows:
S = { x | x = CÎ±+ d  0, Î± âˆˆ RNâˆ’1} (2)
where C and d are given by
d =
1
M
Mâˆ‘
i=1
xi, C = [ q1(UU
T ), . . . , qNâˆ’1(UU
T ) ] (3)
in which U = [ x1 âˆ’ d, . . . ,xM âˆ’ d ] âˆˆ RLÃ—M and qi(UUT )
denotes the unit-norm eigenvector associated with the ith principal
eigenvalue of UUT . Under assumptions (A1)-(A4), it has been
shown in [8] that S is equal to the source convex hull; i.e.,
S = conv{s1, . . . , sN} ,
{
x =
Nâˆ‘
i=1
Î¸isi
âˆ£âˆ£âˆ£âˆ£ Î¸  0, 1TLÎ¸ = 1
}
,
where Î¸ = [Î¸1, . . . , Î¸L]T , and that the extreme points of S are ex-
actly the true sources s1, . . . , sN . Then, consider the pre-image of
S under the affine mapping x = CÎ±+ d:
F =
{
Î± âˆˆ RNâˆ’1
âˆ£âˆ£CÎ±+ d  0 } (4a)
=
{
Î± âˆˆ RNâˆ’1
âˆ£âˆ£ cTnÎ±+ dn â‰¥ 0, n = 1, . . . , L }, (4b)
where cTn is the nth row vector of C and dn is the nth element of d.
It has been shown that there is a one-to-one correspondence between
extreme points of S and F , and that F is a simplex [1, 8], as stated
in CAMNS criterion:
CAMNS criterion [1]: Find all the extreme points of the polyhe-
dral set F given by (4a) and denote the obtained extreme points
by {Î±1, . . . ,Î±N}. Output
si = CÎ±i + d, i = 1, . . . , N, (5)
as the estimated sources.
2.2. Alternating Volume Maximization for CAMNS
To fulfill the CAMNS criterion, we consider the following simplex
volume maximization problem [1]:
{Î²?1 , . . . ,Î²
?
N} = arg max
Î²1,...,Î²NâˆˆF
vol(Î²1, . . . ,Î²N ) (6)
= arg max
Î²1,...,Î²NâˆˆF
|det (âˆ†(Î²1, . . . ,Î²N ))| , (7)
where vol(Î²1, . . . ,Î²N) is the volume of conv{Î²1, . . . ,Î²N} [10]
and
âˆ†(Î²1, . . . ,Î²N) =
[
Î²1 Â· Â· Â· Î²N
1 Â· Â· Â· 1
]
âˆˆ RNÃ—N . (8)
F
c
T
L
Î± + dL â‰¥ 0
c
T
2
Î± + d2 â‰¥ 0
c
T
1
Î± + d1 â‰¥ 0
Î±1
Î±2
Î±3
Î²1
Î²2
Î²3
Fig. 1. A geometrical illustration of F and Problem (6) for N = 3.
A pictorial illustration of (7) for N = 3 is given in Figure 1. One
would expect that as |det (âˆ†(Î²1, . . . ,Î²N))| reaches the maximum,
{Î²?1 , . . . ,Î²
?
N} = {Î±1, . . . ,Î±N}. This expectation has been theo-
retically proven in [1] under assumptions (A1)-(A4).
We handle Problem (7) by alternating optimization. Consider
the cofactor expansion for det(âˆ†(Î²1, . . . ,Î²N)) along the jth col-
umn for any j âˆˆ {1, . . . , N}:
det(âˆ†(Î²1, . . . ,Î²N)) = b
T
j Î²j + (âˆ’1)
N+jdet(BNj) (9)
where bj = [(âˆ’1)i+jdet(Bij)]Nâˆ’1i=1 and Bij is the submatrix of
âˆ†(Î²1, . . . ,Î²N) with ith row and jth column removed. The partial
maximization with respect to Î²j with all other Î²is fixed is expressed
as follows:
max
Î²jâˆˆF
|bTj Î²j + (âˆ’1)
N+jdet(BNj)|. (10)
Such a partial maximization problem can be globally solved by the
following two LPs:
p? = max
Î²jâˆˆF
b
T
j Î²j + (âˆ’1)
N+jdet(BNj), (11a)
q? = min
Î²jâˆˆF
b
T
j Î²j + (âˆ’1)
N+jdet(BNj). (11b)
The optimal solution of (10) is chosen as the optimal solution of
(11a) if |p?| > |q?|, and is that of (11b) if |q?| > |p?|. Prob-
lem (11) can be implemented by available LP solvers, such as Se-
DuMi [11] and CVX [12]. The partial maximization problem (11)
is solved cyclically (i.e., j := (j modulo N ) +1) until some prede-
fined stopping rule is satisfied. Then, the sources can be estimated
by sË†j = CÎ²Ë†j+d, where {Î²Ë†1, . . . , Î²Ë†N} is the outcome of the above
described alternating volume maximization process. The CAMNS-
AVM algorithm is summarized in Table 1.
3. FAST CAMNS-AVM ALGORITHM
In this section, we propose three complexity reduction methods for
CAMNS-AVM (respectively in each subsection) so that its compu-
tational efficiency can be significantly improved.
3.1. Volume maximization problem equivalence
Problem (7) can be simplified as stated in the following lemma:
Lemma 1. Consider the following problem:
max
Î²1,...,Î²NâˆˆF
det(âˆ†(Î²1, . . . ,Î²N)). (12)
The optimal solution of (12) is also optimal to (7).
The proof of Lemma 1 is given in Section 6.1. By the cofactor ex-
pansion for det(âˆ†(Î²1, . . . ,Î²N)) given by (9), we can obtain the
jth partial maximization problem for (12) given by (11a) rather than
the two LPs given by both (11a) and (11b) in the original CAMNS-
AVM. Next, we will present how to reduce the computational com-
plexity in solving the LP in (11a).
thetically generated 7 mixtures from 7 human face images (M =
N = 7 and L = 76800). A sum square error (SSE) between sË†i and
si is used as the performance measure [1]. In addition, the compu-
tation time T (in secs) of the method (implemented in Mathworks
Matlab R2008a) running in a desktop computer equipped with Core
2 Duo CPU 2.33GHz, 4GB memory is used as our computational
complexity measure.
The average SSE and computation time T are shown in Table 4.
In Case A, the original CAMNS-AVM (in Table 1) is used. In Case
B, we consider the CAMNS-AVM with Steps 3 and 4 (in Table 1)
replaced by solving (11a) only. Case C is similar to Case B except
for F (in (11a)) replaced by (13). Finally, Case D is for the fast
CAMNS-AVM (given in Table 3). One can see that the average SSE
are the same for all the cases, and the computational efficiency im-
proves from Case A to Case D. In particular, the computation time T
in Case B is almost twice less than that in Case A. The computation
time T in Case C is significantly smaller than that in Case B, because
the redundant inequality constraints in F have been effectively re-
moved by Quickhull [9]. Furthermore, by using the customized LP
and the warm start mechanism, the computation time T in Case D is
less than that in Case C around 3 seconds. As a result, the computa-
tion time of the proposed fast CAMNS-AVM (Case D) is more than
ten times less than that of the original CAMNS-AVM (Case A).
Table 4. The average SSE and T for performance and complexity
comparison of the original CAMNS-AVM (Case A) with different
complexity reduction methods (Cases B, C, and D), where Case D is
the proposed fast CAMNS-AVM algorithm.
no. of constraints SSE (dB) T (secs)
Case A L =76800 27.19 394.27
Case B L =76800 27.19 224.94
Case C r =2186 27.19 41.02
Case D r =2186 27.19 38.08
In summary, the proposed fast CAMNS-AVM algorithm ob-
tained by the three computation reduction methods (as stated in
Lemma 1, Lemma 2, and the customized LP solver) has signifi-
cantly lower computational complexity than the original CAMNS-
AVM without any performance loss. The presented simulation re-
sults have shown that the computational efficiency of the proposed
fast CAMNS-AVM algorithm is much higher (one-order) than that
of the original CAMNS-AVM.
5. APPENDIX
6.1. Proof of Lemma 1. Since det(G) = âˆ’det(H) if H results
from G by interchanging any two column vectors, an optimal solu-
tion of (7) where det(âˆ†(Î²?1 , . . . ,Î²?N)) â‰¥ 0 always exists. Hence,
Lemma 1 is proved. 
6.2. Proof of Lemma 2. Since xi  0, âˆ€i in image applications and
the data where xi[n] = 0 can be removed without loss of generality,
we have d  0 by (3). Then, an equivalent form of (4b) is derived
as follows:
F =
{
Î± âˆˆ RNâˆ’1
âˆ£âˆ£ vTnÎ± â‰¥ âˆ’1, n = 1, . . . , L } (16)
=
{
Î± âˆˆ RNâˆ’1
âˆ£âˆ£ vTÎ± â‰¥ âˆ’1,v âˆˆ conv{v1, . . . ,vL}}, (17)
where vn = cn/dn. The equivalence of (16) and (17) is proved
in Section 6.3 below. Moreover, any convex set can be rep-
resented by its extreme points [13], i.e., conv{v1, . . . ,vL} =
conv
{
vl1 , . . . ,vlr
}
, where vl1 , . . . ,vlr are the extreme points of
conv
{
v1, . . . ,vL
}
. Then, we have
F =
{
Î± âˆˆ RNâˆ’1
âˆ£âˆ£ vTÎ± â‰¥ âˆ’1,v âˆˆ conv{vl1 , . . . ,vlr}}
=
{
Î± âˆˆ RNâˆ’1
âˆ£âˆ£ vnÎ± â‰¥ âˆ’1, n = l1, . . . , lr }. (18)
Hence, (13) directly follows from (18). 
6.3. Proof of the equivalence between (16) and (17). Let
F1 =
{
Î± âˆˆ RNâˆ’1
âˆ£âˆ£ vTnÎ± â‰¥ âˆ’1, n = 1, . . . , L },
F2 =
{
Î± âˆˆ RNâˆ’1
âˆ£âˆ£ vTÎ± â‰¥ âˆ’1,v âˆˆ conv{v1, . . . ,vL}}.
We first prove that F1 âŠ† F2. For any vector x âˆˆ F1 (i.e.,
v
T
nx â‰¥ âˆ’1, n = 1, . . . , L), it follows that Î¸nvTnx â‰¥ âˆ’Î¸n, n =
1, . . . , L for any Î¸n â‰¥ 0 and
âˆ‘L
n=1
Î¸n = 1. Then, we have
(
âˆ‘L
n=1 Î¸nvn)
T
x â‰¥ âˆ’1. By letting v =
âˆ‘L
n=1 Î¸nvn, it follows
that vTx â‰¥ âˆ’1, v âˆˆ conv
{
v1, . . . ,vL
}
, implying x âˆˆ F2.
Next, we prove F2 âŠ† F1. For any vector x âˆˆ F2, we haveâˆ‘L
n=1
Î¸nv
T
nx â‰¥ âˆ’1 for all Î¸n â‰¥ 0 and
âˆ‘L
n=1
Î¸n = 1. This
directly implies vTnx â‰¥ âˆ’1 by choosing Î¸n = 1 and Î¸i = 0 for any
n âˆˆ {1, . . . , L} and all i 6= n. Hence, x âˆˆ F1 is true. 
6. REFERENCES
[1] W.-K. Ma, T.-H. Chan, C.-Y. Chi, and Y. Wang, â€œConvex anal-
ysis for non-negative blind source separation with application
in imaging,â€ in Chapter 7, Convex Optimization in Signal Pro-
cessing and Communications, Editors: D. P. Palomar and Y. C.
Eldar, UK: Cambridge University Press, 2010.
[2] Y. Wang, J. Xuan, R. Srikanchana, and P. L. Choyke, â€œModeling
and reconstruction of mixed functional and molecular patterns,â€
Intl. J. of Biomed. Imag., vol. 2006, pp. 1â€“9, 2006.
[3] N. Keshava and J. Mustard, â€œSpectral unmixing,â€ IEEE Signal
Process. Mag., vol. 19, no. 1, pp. 44â€“57, Jan. 2002.
[4] M. D. Plumbley, â€œAlgorithms for non-negative independent
component analysis,â€ IEEE Trans. Neural Netw., vol. 14, no.
3, pp. 534â€“543, 2003.
[5] S. Moussaoui, D. Brie, A. Mohammad-Djafari, and C. Carteret,
â€œSeparation of non-negative mixture of non-negative sources us-
ing a Bayesian approach and MCMC sampling,â€ IEEE Trans.
Signal Process., vol. 54, no. 11, pp. 4133â€“4145, Nov. 2006.
[6] D. Lee and H. S. Seung, â€œLearning the parts of objects by non-
negative matrix factorization,â€ Nature, vol. 401, pp. 788â€“791,
Oct. 1999.
[7] P. O. Hoyer, â€œNon-negative matrix factorization with sparseness
constraints,â€ J. of Mach. Learn. Research, vol. 5, pp. 1457â€“
1469, 2004.
[8] T.-H. Chan, W.-K. Ma, C.-Y. Chi, and Y. Wang, â€œA convex anal-
ysis framework for blind separation of non-negative sources,â€
IEEE Trans. Signal Process., vol. 56, no. 10, pp. 5120â€“5134,
Oct. 2008.
[9] C. B. Barber, D. P. Dobkin and H. Huhdanpaa, â€œThe quickhull
algorithm for convex hulls,â€ ACM Trans. Math. Software, vol.
22, 1996.
[10] G. Strang, Linear Algebra and Its Application, CA: Thomson,
4th edition, 2006.
[11] J. F. Sturm, â€œUsing SeDuMi 1.02, a MATLAB toolbox for
optimization over symmetric cones,â€ Optimization Methods and
Software, vol. 11-12, pp. 625â€“653, 1999.
[12] M. Grant and S. Boyd, â€œCVX: Matlab software for disci-
plined convex programming, version 1.21,â€ http://cvxr.
com/cvx, Sept. 2010.
[13] S. Boyd and L. Vandenberghe, Convex Optimization, Cam-
bridge Univ. Press, 2004.
the endmember signature matrix whose ith column vector ai is the
ith endmember signature, s[n] = [ s1[n], . . . , sN [n] ]T âˆˆ RN is the
nth abundance vector comprising N fractional abundances, w[n] =
[ w1[n], . . . , wM [n] ]
T is the zero-mean white Gaussian noise vector
(i.e.,N (0, Ïƒ2IM ), where Ïƒ2 is the noise variance) and L is the total
number of observed pixel vectors.
The goal of hyperspectral unmixing is to estimate the endmem-
ber signature matrix A and the abundances s[1], . . . , s[L] from the
noisy observed pixels y[1], . . . , y[L], assuming that the number of
endmembers N is known a priori.
We consider the following general assumptions that are applica-
ble to HU algorithms :
(A1) (Non-negativity condition) si[n] â‰¥ 0 âˆ€ i, n.
(A2) (Full additivity condition)   Ni=1 si[n] = 1 âˆ€ n.
(A3) min{L,M} â‰¥ N and A is of full column rank.
(A1) and (A2) are valid assumptions in hyperspectral imaging
because the abundances are fractional proportions [1â€“4]. In addition,
the number of pixels and that of spectral bands involved are larger
than the number of endmembers and each endmember has its unique
signature, which justifies (A3).
Before getting into the core of development, we need to describe
a basic, essential concept in MVES, namely convex hull and simplex
[9]. Given a set of vectors {a1, . . . , aN} âŠ‚ RM , the convex hull of
{a1, . . . , aN} is defined as
conv{a1, . . . , aN} =  x =
N

i=1
Î¸iai 



1
T
NÎ¸ = 1, Î¸  0  , (3)
where Î¸ = [ Î¸1, . . . , Î¸N ]T âˆˆ RN . In addition, a convex
hull conv{a1, . . . , aN} is called a simplex if M = N âˆ’ 1 and
a1, . . . , aN are affinely independent.
3. REVIEW OF MVES PROBLEM FORMULATION
The noise-free signal model given by (2) is considered here for re-
viewing the formulation and theory behind MVES problem [8]. Like
many other HU algorithms [1], we begin with dimension reduction
of the observed pixels by projecting them onto an observed-pixel-
constructed affine set [8, 10], as given in the following lemma:
Lemma 1. (Dimension reduction by affine set fitting [8]) Under
(A2) and (A3), the dimension-reduced pixel vector xËœ[n] can be
obtained by an affine transformation of x[n]:
xËœ[n] = CT (x[n]âˆ’ d) âˆˆ RNâˆ’1, (4)
where (C,d) is the affine set fitting solution given by
d =
1
L
L

n=1
x[n], (5)
C = [ q1(UU
T ), q2(UU
T ), . . . , qNâˆ’1(UU
T ) ], (6)
whereU = [ x[1]âˆ’d, . . . ,x[L]âˆ’d ] âˆˆ RMÃ—L, and qi(R) denotes
the eigenvector associated with the ith principal eigenvalue of R.
An important remark related to Lemma 1 is as follows:
(R1) The affine set fitting solution of the noisy observed pixels
(CË†, dË†) (obtained from (5) and (6) by replacing x[n]withy[n])
gives a best approximation in the least-squares sense [10], and
it asymptotically approaches the true (C,d) for large L.
Since   Nj=1 sj [n] = 1 [(A2)], it follows by substituting the
noise-free signal model (2) into (4) that
xËœ[n] =
N

j=1
sj [n]Î±j , (7)
where Î±j = CT (aj âˆ’ d) âˆˆ RNâˆ’1 is the jth dimension-reduced
endmember signature. Moreover, due to si[n] â‰¥ 0 [(A1)], it has
been proven in [8] that
xËœ[n] âˆˆ conv{Î±1, . . . ,Î±N} âŠ‚ RNâˆ’1, âˆ€n (8)
and conv{Î±1, . . . ,Î±N} is a simplex.
Based on Craigâ€™s belief [4], the unmixing problem of finding a
minimum volume simplex enclosing all the dimension-reduced pix-
els can now be written as the following optimization problem [8]:
min
Î²1,...,Î²NâˆˆR
Nâˆ’1
V (Î²1, . . . ,Î²N)
s.t. xËœ[n] âˆˆ conv{Î²1, . . . ,Î²N}, âˆ€ n,
(9)
where V (Î²1, . . . ,Î²N) is the volume of the simplex conv{Î²1, . . . ,
Î²N} given by [11]
V (Î²1, . . . ,Î²N) =
|det(B)|
(N âˆ’ 1)! (10)
and B = [ Î²1 âˆ’ Î²N , . . . ,Î²Nâˆ’1 âˆ’ Î²N ] âˆˆ R(Nâˆ’1)Ã—(Nâˆ’1) . In
addition, by (3), we rewrite the constraint of (9) in terms of B as
xËœ[n] = Î²N + BÎ¸n, (11)
where Î¸n  0 and 1TNâˆ’1Î¸n â‰¤ 1. Hence, problem (9) can be equiv-
alently written as
min
BâˆˆR(Nâˆ’1)Ã—(Nâˆ’1),
Î²N ,Î¸1,...,Î¸LâˆˆR
Nâˆ’1
|det(B)|
s.t. Î¸n  0, 1TNâˆ’1Î¸n â‰¤ 1,
xËœ[n] = Î²N + BÎ¸n, âˆ€ n.
(12)
By letting Î¸n = HxËœ[n] âˆ’ g for all n where H = Bâˆ’1 and g =
Bâˆ’1Î²N , one can eliminate the variables Î¸n for all n in (12) and
come up with
max
HâˆˆR(Nâˆ’1)Ã—(Nâˆ’1), gâˆˆRNâˆ’1
|det(H)|
s.t. 1TNâˆ’1(HxËœ[n]âˆ’ g) â‰¤ 1,
HxËœ[n] âˆ’ g  0, âˆ€ n.
(13)
Next, we will show how the MVES problem in (13) can be reformu-
lated for the noisy scenario.
4. ROBUST MVES PROBLEM AND ALGORITHM
Let us consider the noisy signal model given by (1). From (1), (4)
and (R1), we get
yËœ[n]  CË†T (y[n]âˆ’ dË†) âˆ¼= xËœ[n] + CË†Tw[n], (14)
where CË†Tw[n] is a random vector following N (0, Ïƒ2INâˆ’1) (since
CË†T is a semi-unitary matrix). Following the footsteps of the MVES
problem formulation, the robust MVES problem can be expressed
by replacing xËœ[n] by yËœ[n]âˆ’ CË†Tw[n] in (13):
max
H, g
|det(H)|
s.t. 1TNâˆ’1(HyËœ[n]âˆ’HCË†Tw[n]âˆ’ g) â‰¤ 1,
HyËœ[n]âˆ’HCË†Tw[n]âˆ’ g  0, âˆ€ n,
(15)
where 1TNâˆ’1HCË†
Tw[n]  z[n] and HCË†Tw[n]  u[n]
are a random variable and a random vector with distribu-
tionsN (0, Ïƒ21TNâˆ’1HHT1Nâˆ’1) andN (0, Ïƒ2HHT ), respectively.
Since the constraints in (15) involve randomness, they can be man-
ifested as chance constraints so as to mitigate the noise effects in
1203
Table 1. Average Ï†en and Ï†ab (degrees) over the various unmixing methods for different purity levels (Ï) and SNRs.
Methods Ï
Ï†en Ï†ab
SNR (dB) SNR (dB)
20 25 30 35 40 20 25 30 35 40
N-FINDR-FCLS
0.7 5.45 5.31 5.24 5.11 5.16 22.54 21.86 21.63 19.76 19.82
0.85 2.65 2.67 2.66 2.65 2.61 9.60 8.37 8.03 7.93 7.77
1 1.15 0.58 0.33 0.18 0.10 6.14 3.59 2.13 1.24 0.72
MVSA
0.7 5.95 4.03 2.67 2.12 1.40 20.80 14.56 7.88 4.81 3.14
0.85 5.99 3.75 2.61 2.07 1.27 19.65 12.12 7.17 4.16 2.34
1 6.12 3.96 2.71 2.14 1.33 18.93 11.55 6.68 3.85 2.15
MVES
0.7 5.17 3.26 2.43 1.73 1.01 16.66 10.58 6.51 3.81 2.17
0.85 5.28 3.59 2.65 1.85 1.11 16.88 10.98 7.20 4.26 2.38
1 6.67 4.37 3.35 2.50 1.55 19.81 13.09 9.58 6.81 4.50
RMVES
0.7 1.69 1.09 0.76 0.46 0.43 9.21 5.37 3.21 1.98 1.32
0.85 1.90 1.30 1.01 0.53 0.44 8.34 5.90 3.48 2.03 1.38
1 2.89 2.27 2.05 1.69 1.40 9.75 6.27 5.31 3.32 2.85
for all n = 1, . . . , L. To ensure (A1), the negative values of  s[n] are
made zero. The above procedure is referred to as RMVES algorithm.
The proposed RMVES algorithm uses the well known VCA [3]
for the initialization of (24) [6]. The optimization problem in (24) is
then solved by sequential quadratic programming (SQP). Moreover,
in each iteration of the RMVES algorithm (which involves N âˆ’ 1
row updates of H), we use the solution of the previous iteration as
the initialization for the current iteration.
5. SIMULATIONS AND CONCLUSIONS
In this section, the efficacy of the proposed RMVES algorithm is
demonstrated using 50 Monte Carlo runs for various purity lev-
els and SNRs. In each run, 1000 noise-free observed pixel vec-
tors were synthetically generated following the signal model in (2),
where 6 endmembers (i.e., Alunite, Buddingtonite, Calcite, Copi-
apite, Kaolinite, and Muscovite) with 417 bands are selected from
USGS library [12], and the abundance vectors s[n] were generated
following Dirichlet distribution D(s[n], Î¼) with Î¼ = 1
N
1N [3], for
different purity levels Ï = 0.7, 0.85, 1 [8]. The noisy data were
obtained by adding independent and identically distributed (i.i.d.)
zero-mean Gaussian noise to the noise-free data for different SNRs,
where SNR =  Ln=1 â€–x[n]â€–22/Ïƒ2ML. To maintain non-negativity
of the noisy observed pixels, we artificially set the negative values of
the noisy pixels to zero. For performance comparison, we also tested
three existing algorithms, N-FINDR-FCLS [2,13] (where FCLS [13]
was used to find the associated abundances for the endmember esti-
mates of N-FINDR), MVSA [6], and the original MVES [8].
The root-mean-square (rms) spectral angle between the true one
and estimated one (which has been widely used in HU [1, 3, 8]) is
used as the performance index. We here denote rms spectral angle
between endmembers and their estimates as Ï†en, and that between
abundance maps and their estimates as Ï†ab. By our extensive nu-
merical experience we found that Î· (which depends on Ï and SNR)
should be less than 0.5 (this can also be justified from Figure 1).
The average Ï†en and Ï†ab of the unmixing algorithms over SNR
= 20, 25, ..., 40 dB and Ï = 0.7, 0.85, 1 are shown in Table 1,
where each bold-faced number denotes the minimum rms spectral
angle associated with a specific pair of (Ï,SNR) over all the algo-
rithms. Table 1 shows that the proposed RMVES algorithm yields
the best performance for Ï = 0.7 and 0.85. For all the values of
Ï, the RMVES algorithm is better than its predecessor, the MVES
algorithm. In addition, to investigate the problem natures of (22)
and (24), we simply performed Monte Carlo simulations by directly
solving (22) using SQP for Ï = 0.7 and SNR = 20 dB. The average
Ï†en and Ï†ab were 9.04 and 24.73 degrees, respectively, which are
apparently much worse than those (1.69 and 9.21) of solving (24) in
an alternating fashion (RMVES algorithm). This may be attributed
to local optimality issues associated with the highly non-convex na-
ture of (22).
In conclusion, we have presented a robust hyperspectral unmix-
ing method, namely the RMVES algorithm which can effectively
unmix the highly mixed and noisy data. The RMVES algorithm ap-
plies chance constraints to accommodate the noise effects, and can
be suitably implemented using SQP solvers in an alternating fashion.
Simulation results showed that the RMVES algorithm outperforms
some existing benchmark algorithms and its predecessor MVES al-
gorithm. The application of RMVES algorithm to real hyperspectral
data would be our future direction.
6. REFERENCES
[1] N. Keshava and J. Mustard, â€œSpectral unmixing,â€ IEEE Signal Process.
Mag., vol. 19, no. 1, pp. 44-57, Jan. 2002.
[2] M. E. Winter, â€œN-findr: An algorithm for fast autonomous spectral
end-member determination in hyperspectral data,â€ in Proc. SPIE Conf.
Imaging Spectrometry, Pasadena, CA, Oct. 1999, pp. 266-275.
[3] J. M. P. Nascimento and J. M. B. Dias, â€œVertex component analysis:
A fast algorithm to unmix hyperspectral data,â€ IEEE Trans. Geosci.
Remote Sens., vol. 43, no. 4, pp. 898-910, Apr. 2005.
[4] M. D. Craig, â€œMinimum-volume transforms for remotely sensed data,â€
IEEE Trans. Geosci. Remote Sens., vol. 32, no. 3, pp. 542-552, 1994.
[5] N. Dobigeon, S. Moussaoui, M. Coulon, J.-Y. Toumeret, and
A. O. Hero, â€œJoint Bayesian endmember extraction and linear un-
mixing for hyperspectral imagery,â€ IEEE Trans. Signal Processing,
vol. 57, no. 11, pp. 4355-4368, Nov. 2009.
[6] J. Li, and J. M. B. Dias, â€œMinimum volume simplex analysis: A fast
algorithm to unmix hyperspectral data,â€ in Proc. IEEE IGARSS, vol.
4, Boston, MA, Aug. 8-12, 2008, pp. 2369-2371.
[7] J. M. B. Dias, â€œA variable splitting augmented Lagrangian approach
to linear spectral unmixing,â€ in Proc. IEEE WHISPERS, Grenoble,
France, Aug. 26-28, 2009.
[8] T.-H. Chan, C.-Y. Chi, Y.-M. Huang, and W.-K. Ma, â€œA convex analy-
sis based minimum-volume enclosing simplex algorithm for hyper-
spectral unmixing,â€ IEEE Trans. Signal Processing, vol. 57, no. 11,
pp. 4418-4432, Nov. 2009.
[9] S. Boyd and L. Vandenberghe, Convex Optimization, UK: Cambridge
Univ. Press, 2004.
[10] T.-H. Chan, W.-K. Ma, C.-Y. Chi, and Y. Wang, â€œA convex analysis
framework for blind separation of non-negative sources,â€ IEEE Trans.
Signal Processing, vol. 56, no. 10, pp. 5120-5134, Oct. 2008.
[11] G. Strang, Linear Algebra and Its Applications, CA: Thomson, 4th
edition, 2006.
[12] Tech. Rep., Available online: http://speclab.cr.usgs.gov/
cuprite.html.
[13] D. Heinz and C.-I. Chang, â€œFully constrained least squares linear
mixture analysis for material quantification in hyperspectral imagery,â€
IEEE Trans. Geosci. Remote Sens., vol. 39, no. 3, pp. 529-545, 2001.
1205
2I. INTRODUCTION
Hyperspectral Imaging (HI) is a crucial technique to identify the materials and their composition
in an area by exploiting the spectral diversity of the observed hyperspectral data. Areas in which HI is
employed are diverse and they include mineral identification [1], space object identification [2], analytical
chemistry [3], retinal analysis [4], and many others. In this work we focus on HI for mineral identification
and quantification, wherein a hyperspectral sensor (usually located on an aircraft or satellite) records
the electromagnetic scattering patterns of materials present in an area, over hundreds of spectral bands
that range from visible to near-infrared wavelength region. The limited spatial resolution of the sensor
used for hyperspectral imaging demands an effective hyperspectral unmixing (HU) scheme to extract
the underlying endmember signatures (or simply endmembers) and the associated abundance maps (or
abundance fractions) distributed over a scene of interest. The endmember signature corresponds to the
reflection pattern of a mineral in different wavelengths and the abundance fraction is the fractional
distribution of a mineral over the given scene. The design of HU algorithms often (but not always) involves
dimension reduction as the preprocessing step. The dimension reduction algorithms are intended to reduce
the complexity of the unmixing algorithm that will be used in the sequel and they also aid in mitigating
the noise effects in the data cloud. Conventional dimension reduction algorithms for hyperspectral data
typically include principal component analysis (PCA) [5] and maximum noise fraction (MNF) [6]. A
detailed survey of various dimension reduction algorithms can be found in [7]. However, for these
algorithms (and also for the unmixing algorithms) the number of endmembers must be known a priori. A
Neyman-Pearson detection theory-based eigen thresholding method namely, virtual dimensionality (VD)
[8] is generally used for the estimation of number of endmembers from the hyperspectral observations.
Other algorithms that can be used to estimate the number of endmembers include Akaike information
criterion (AIC) [9] and minimum description length (MDL) [10]. Hyperspectral subspace identification
(HySiMe) [11] is a recently proposed dimension reduction algorithm, that can also estimate the number
of endmembers.
Conventional HU algorithms are based on linear mixing model (to be explained later) and can be
classified into two main categories. Algorithms in the first category are based on the existence of pure
pixels (pixels that are fully contributed by a single endmember) in the given hyperspectral observations,
and those in the second category are for mixed pixels without requiring the existence of pure pixels.
Figure 1 illustrates the notion of pure pixels and mixed pixels in a hyperspectral imaging scenario. The
red pixel corresponds to a mixed pixel (land, vegetation and water) and the blue pixel corresponds to a
October 22, 2010 DRAFT
4It should be mentioned that most non-pure pixel (mixed pixel) based algorithms are more computa-
tionally complicated than the pure pixel based algorithms, and are sensitive to initialization. On the other
hand, the presence of pure pixels cannot be guaranteed for hyperspectral data that are acquired under
poor spatial resolutions. Hence there exists a tradeoff between the two classes of algorithms, in terms
of computational tractability and accuracy. Nevertheless, the performance of the HU algorithms (both
pure-pixel based and non-pure-pixel based) degrades when the observations are noisy.
In recent years, several concurrent works on linear unmixing of noisy observations have also been
reported. Those works include joint Bayesian algorithm (JBA) [29] and simplex identification via split
augmented Lagrangian (SISAL) [30]. The JBA [29] is a statistical HU algorithm and assumes conju-
gate prior distributions for the abundances and endmember parameters. The endmember signatures are
estimated by using a hierarchical Bayesian model, which generates the posterior distributions of both
the abundances and the endmember parameters. The SISAL algorithm [30] is based on Craigâ€™s criterion,
and it employs variable splitting and augmented Lagrangian approach to estimate the minimum volume
simplex. Further, it uses soft constraints to mitigate the effects of the outlier pixels.
In this work, we propose a robust minimum volume enclosing simplex (RMVES) algorithm for a
general case, where the pure-pixel assumption is no longer a necessity and the uniform/non-uniform
additive Gaussian noise is present in the observations. We begin with a linear mixing model and suitably
modify an existing dimension reduction method, called affine set fitting [25], so as to account for the
presence of noise. A toy example shown in Figure 2 demonstrates how Craigâ€™s criterion may yield
inaccurate endmember estimates for the noisy observations (shown as dots).
Craig's simplex
  True simplex
Noisy observed pixels
Fig. 2. Illustration of Craigâ€™s simplex for noisy observations, where number of endmembers is equal to 3.
Clearly, the random noise expands the data cloud, and hence the volume of the simplex (vertices
October 22, 2010 DRAFT
6used in HU [1], [13], [17], [18], [32], [33]. We consider a scenario in which a hyperspectral sensor
measures solar electromagnetic radiation from N distinct substances. Owing to low spatial resolution,
each observed pixel vector represents a mixture of multiple distinct substances. Hence, each pixel vector
of the hyperspectral images measured over M spectral bands can be represented by the following MÃ—N
linear mixing model
y[n] = x[n] +w[n], (1)
x[n] = As[n] =
Nâˆ‘
i=1
si[n]ai, âˆ€ n = 1, . . . , L. (2)
In (1), y[n] = [ y1[n], . . . , yM [n] ]T represents the nth observed pixel vector comprising M spectral bands,
x[n] = [ x1[n], . . . , xM [n] ]
T corresponds to its noise-free counterpart, and w[n] = [ w1[n], . . . , wM [n] ]T
is the noise vector. In (2), A = [ a1, . . . ,aN ] âˆˆ RMÃ—N denotes the endmember signature matrix with
the ith column vector ai being the ith endmember signature, s[n] = [ s1[n], . . . , sN [n] ]T âˆˆ RN is the
nth abundance vector comprising N fractional abundances, and L is the total number of observed pixels.
The noise vector w[n] considered in the signal model (1) is zero-mean, uniform/non-uniform additive
Gaussian noise vector i.e., N (0,D), where D = diag(Ïƒ2i ) âˆˆ RMÃ—M is a diagonal matrix and Ïƒ2i denotes
the noise variance in the ith spectral band. If Ïƒ2i = Ïƒ2j , âˆ€ i 6= j, then it is called uniform Gaussian noise,
else it is called non-uniform Gaussian noise. The additive Gaussian noise is a reasonable assumption and
is widely used in designing HU algorithms [1], [17], [29].
Assuming prior knowledge about the number of endmembers N , we aim to estimate the endmember
signature matrix A and the abundances s[1], . . . , s[L] from the noisy pixels y[1], . . . ,y[L], under the
following general assumptions [18], [32]:
(A1) Intensities of all the abundance vectors are non-negative, i.e., si[n] â‰¥ 0, for all i and n.
(A2) Abundance fractions are proportionally distributed in each observed pixel, i.e.,âˆ‘Ni=1 si[n] = 1, âˆ€ n.
(A3) min{L,M} â‰¥ N and the endmembers are linearly independent, i.e., A is of full column rank.
In the ensuing development, we employ two convex analysis concepts, namely, affine hull and convex
hull [31]. While the affine hull is employed for dimension reduction, the convex hull is used to infer the
geometry of the observations. For ease of later use, they are defined below:
â€¢ The affine hull of {a1, . . . ,aN} âŠ‚ RM is defined as
aff{a1, . . . ,aN} =
{
x =
Nâˆ‘
i=1
Î¸iai
âˆ£âˆ£âˆ£âˆ£ 1TNÎ¸ = 1, Î¸ âˆˆ RN
}
, (3)
October 22, 2010 DRAFT
8and qi(R) denotes the unit-norm eigenvector associated with the ith principal eigenvalue of the matrix
R.
It should be mentioned that under (A2) and (A3), the above dimension reduction procedure is a lossless
transformation in the absence of noise, and the dimension of the data is reduced from M to N âˆ’ 1. An
illustration of dimension reduction via the affine set fitting procedure is given in Figure 3, where N = 3.
0
Dimension Reduction
a1
a2
a3
c1
c2
d
x[n]
R
M
R
Nâˆ’1
xËœ[n]
Î±1
Î±2
Î±3
xËœ[n] = CT (x[n]âˆ’ d)
aff{a1, . . . , aN} = aff{x[1], . . . ,x[L]} = A(C,d)
C = [c1 c2], d =
1
L
Lâˆ‘
n=1
x[n].
Fig. 3. Illustration of dimension reduction by affine set fitting for N=3.
Since
âˆ‘N
j=1 sj [n] = 1 [(A2)], it follows by substituting the noise-free signal model (2) into (6) that
xËœ[n] =
Nâˆ‘
j=1
sj[n]Î±j , âˆ€ n = 1, . . . , L, (10)
where
Î±j = C
T (aj âˆ’ d) âˆˆ R
Nâˆ’1 (11)
is the jth dimension-reduced endmember signature. Moreover, due to si[n] â‰¥ 0 [(A1)], it can be seen
that
xËœ[n] âˆˆ conv{Î±1, . . . ,Î±N} âŠ‚ R
Nâˆ’1, âˆ€ n (12)
and conv{Î±1, . . . ,Î±N} is a simplest simplex. This can be regarded as the outcome of the fact that the
affine transformation of a simplex is also a simplex, as one can infer from (2) that for all n, x[n] âˆˆ
conv{a1, . . . ,aN}, and conv{a1, . . . ,aN} âŠ‚ RM is itself a simplex [by (A3)].
However, when the data are corrupted by noise, the observations can no longer lie in a single specific
affine set. Accordingly, for the noisy scenario, the approximate affine set fitting parameters (CÌ‚ and dÌ‚)
can be obtained as explained below.
October 22, 2010 DRAFT
10
IV. BRIEF REVIEW OF MVES ALGORITHM
In this section, we briefly review the MVES formulation and algorithm [25]. The MVES algorithm is
based on Craigâ€™s criterion [20] and it aims to solve the following optimization problem [18] [25]:
min
Î²1,...,Î²NâˆˆRNâˆ’1
V (Î²1, . . . ,Î²N )
s.t. xËœ[n] âˆˆ conv{Î²1, . . . ,Î²N}, âˆ€ n.
(20)
Here, xËœ[n] is the dimension-reduced noise-free observation, Î²1, . . . ,Î²N correspond to the dimension
reduced endmember signatures, and V (Î²1, . . . ,Î²N ) is the volume of the simplex conv{Î²1, . . . , Î²N},
given by [35]
V (Î²1, . . . ,Î²N ) =
|det(B)|
(N âˆ’ 1)!
, (21)
where (N âˆ’ 1)! represents factorial of N âˆ’ 1 and
B = [ Î²1 âˆ’ Î²N , . . . ,Î²Nâˆ’1 âˆ’ Î²N ] âˆˆ R
(Nâˆ’1)Ã—(Nâˆ’1). (22)
By (5) and by (22), the constraints of (20) can be expressed as
xËœ[n] =
Nâˆ‘
i=1
si[n]Î²i = Bs
â€²[n] + Î²N , âˆ€ n, (23)
where
sâ€²[n] =[s1[n], . . . , sNâˆ’1[n]]
T âˆˆ RNâˆ’1+ , (24a)
sN [n] =1âˆ’ 1
T
Nâˆ’1s
â€²[n] â‰¥ 0, (24b)
are the abundance fractions of the nth pixel in the observations. Note that (24a) and (24b) jointly enforce
the non-negativity and full-additivity constraints on the abundances. Equation (23) can be re-written as
sâ€²[n] = Bâˆ’1(xËœ[n]âˆ’ Î²N ) = HxËœ[n]âˆ’ g, âˆ€ n, (25)
where
H =Bâˆ’1, (26a)
g =Bâˆ’1Î²N . (26b)
Substituting (25) into (24) gives rise to the following equivalent constraints
si[n] =h
T
i xËœ[n]âˆ’ gi â‰¥ 0, âˆ€ i = 1, . . . , N âˆ’ 1, n = 1, . . . , L, (27a)
sN [n] =1âˆ’ 1
T
Nâˆ’1(HxËœ[n]âˆ’ g) â‰¥ 0, âˆ€ n = 1, . . . , L, (27b)
October 22, 2010 DRAFT
12
where Pr(Â·) represents probability, ui[n] , hTi CÌ‚Tw[n] is a random variable with the distribution
N (0,hTi CÌ‚
TDCÌ‚hi), i = 1, . . . , Nâˆ’1, z[n] , 1
T
Nâˆ’1HCÌ‚
Tw[n] is a random variable with the distribution
N (0, Ïƒ21TNâˆ’1HCÌ‚
TDCÌ‚HT1Nâˆ’1), and Î· âˆˆ (0, 1) is a design parameter.
Qualitative discussion on Î· value: First and foremost, we notice that the random variables ui[n], i =
1, . . . , N âˆ’1, and z[n] are zero-mean Gaussian random variables. Next, we study the nature of the lower
bound on the abundance fractions, si[n], i = 1, . . . , N, for various ranges of Î·. To begin with, let us
consider a general chance constraint, Pr(a â‰¤ b) â‰¥ Î·, where a is a zero-mean Gaussian random variable
and b âˆˆ R is a constant. The probability density function f(a) of the random variable a is shown in
Figure 4, where the shaded region corresponds to Pr(a â‰¤ k) = Î·, for a fixed Î· value in a given range,
and k is the corresponding lower bound of b such that Pr(a â‰¤ b) â‰¥ Î· always holds. As one can observe,
for Î· < 0.5, Î· = 0.5, and Î· > 0.5, k will be non-positive, non-negative, and positive, respectively.
Applying these inferences to (30b) and (30c) of (30), we have the following observations:
â€¢ The case of Î· > 0.5: From (30b), the lower bound on the abundance fractions si[n] = hTi yËœ[n] âˆ’
gi, i = 1, . . . , N âˆ’ 1, must be positive. Further, from (27b), we have âˆ’sN [n] = 1TNâˆ’1HyËœ[n] âˆ’
1TNâˆ’1g âˆ’ 1 (where the full additivity constraint is implicit here), by which (30c) can be written as
Pr(âˆ’z[n] â‰¤ sN [n]) â‰¥ Î·. Thus, for Î· > 0.5, it could be readily inferred that the lower bound on
sN [n] must be positive. These constraints on the abundances of problem (30) will result in a simplex
that will enclose all the data points. A larger Î· value (Î· âˆˆ (0.5, 1)) will make the lower bound on
the abundances more positive, and will result in a larger estimated simplex than the one estimated
under the constraints of (28) (i.e., [(A1)] and [(A2)]).
â€¢ The case of Î· = 0.5: By similar arguments, one can conclude from (30b) and (30c) that si[n] i =
1, . . . , N âˆ’ 1 must be non-negative and âˆ’sN [n] must be non-positive, respectively. This along with
the full-additivity constraint (as implicit in (30c)) is exactly the constraints of (28) as dictated by
Craigâ€™s criterion. Hence, when Î· = 0.5, the simplex estimated by (30) will exactly be the same as
the simplex estimated by (28).
â€¢ The case of Î· < 0.5: In this case, the abundance fractions si[n], i = 1, . . . , N (refer to (30b) and
(30c)) are allowed to take negative values (the negative lower bound on the abundances depends on
Î· value), which makes the simplex to leave some observations outside the estimated simplex. Thus
the estimated simplex will be smaller than (in fact, a subset of) the ones estimated for Î· â‰¥ 0.5, and
therefore is expected to be close to the true simplex.
The chance constraints in (30) can be further simplified by normalizing the random variables involved [31].
October 22, 2010 DRAFT
14
the simplex obtained by the MVES algorithm [25]. Let
F(Î·) = {(H,g) | Î¦âˆ’1(Î·)
âˆš
Qi â‰¤ h
T
i yËœ[n]âˆ’ gi, âˆ€ i, n, (32)
1TNâˆ’1HyËœ[n]âˆ’ 1
T
Nâˆ’1g âˆ’ 1 â‰¤ Î¦
âˆ’1(1âˆ’ Î·)
âˆš
1TNâˆ’1Q1Nâˆ’1, âˆ€ n},
which is exactly the constraint set of (31). As Î¦âˆ’1(Î·) and Î¦âˆ’1(1 âˆ’ Î·) are monotone increasing and
decreasing functions of Î·, respectively, one can infer that F(Î·1) âŠ† F(Î·2), âˆ€ Î·1 â‰¥ Î·2. Further, it could
be inferred that, when Î· = 0.5, the constraint set F(0.5) of (31) is the same as that of (28). Thus, the
optimal |det(H)| will be larger for smaller Î·. Also note from (26a) that an increased value of |det(H)|
corresponds to a decreased value of |det(B)| = 1/|det(H)| (i.e., a smaller simplex), which is what we
wish to achieve, so that the estimated simplex will be close to the true simplex. Thus, we can further
conclude that the apt range of Î· should be between 0 and 0.5. Finally, it is worthwhile to mention
that when Î· = 0, |det(H)| becomes unbounded as F(0) = R(Nâˆ’1)Ã—(Nâˆ’1) Ã— RNâˆ’1 and hence |det(B)|
becomes zero, and for the other extreme case of Î· = 1, problem (31) becomes infeasible (as could be
inferred from (32)), since F(1) = Ï† (empty set).
Figure 5 illustrates a scatter plot (for N = 3) of the dimension-reduced noisy observations and the
simplex, conv{Î±1,Î±2,Î±3} obtained by the RMVES algorithm (to be presented below) for different
values of Î·. As elaborately discussed above, one can see from Figure 5 that when Î· < 0.5, the solution
of RMVES problem approaches the true simplex (as some of the outlier pixels or possibly noisy pixels
are left outside the simplex). When Î· = 0.5 the minimum volume simplex tightly encloses all the
observations, and when Î· > 0.5, the simplex expands, but still encloses all the observations. Note that
for Î· â‰¥ 0.5, the estimated simplex is away from the true simplex.
While the feasible set of (31) could be convex or non-convex (depending on Î·), the objective function
of (31) is always non-convex. Inspired by our previous works [25] [34], we utilize the alternating
optimization methodology, by virtue of which we form subproblems that are hopefully â€œless non-convexâ€
than (31). We consider the cofactor expansion for det(H) along the ith row of H:
det(H) =
Nâˆ’1âˆ‘
j=1
(âˆ’1)i+jhijdet(Hij), (33)
where Hij is the submatrix of H with the ith row and jth column removed. One can observe from (33)
that det(H) is linear in each hTi , which enables us to update hTi and gi while fixing the other rows of
H and the other entries of g. Then the partial maximization of (31) with respect to hTi and gi can be
October 22, 2010 DRAFT
16
solution of the maximization problem (35a) if |p?| > |q?|, and that of the minimization problem (35b)
if |q?| > |p?|. This row-wise maximization is conducted cyclically (i.e., i := (i modulo (N âˆ’ 1)) + 1
via each row update of H). We define one iteration as one full update of the matrix H and the vector
g. At each iteration, if the relative change in |det(H)| between the current and the previous iterations
exceeds a given threshold, then we continue with the next updating iteration, else the updated H and g
are the obtained optimum estimates. It should be noted that though the sub-problems (35a) and (35b) are
non-convex, the objective functions are now linear (convex). From our simulation experience, we found
that this strategy substantially aids in mitigating the local optimality issue associated with the problem
(31) (this will be discussed in Section VI-C).
Suppose that a solution (H?, g?) is obtained by the above mentioned alternating maximization method.
From (11), (26a), (26b), the endmember signatures can then be recovered by aÌ‚i = CÎ±Ì‚i + d for i =
1, . . . , N , where
Î±Ì‚N = (H
?)âˆ’1g?, (36)
[Î±Ì‚1, ..., Î±Ì‚Nâˆ’1] = Î±Ì‚N1
T
Nâˆ’1 + (H
?)âˆ’1. (37)
The abundance estimates are obtained by replacing xËœ[n] with yËœ[n] in (27). That is,
sÌ‚[n] = [ sâ€²[n]T 1âˆ’ 1TNâˆ’1s
â€²[n] ]T+,
= [ (H?yËœ[n]âˆ’ g?)T 1âˆ’ 1TNâˆ’1(H
?yËœ[n]âˆ’ g?) ]T+ (38)
for all n = 1, . . . , L and [Â·]+ denotes element-wise non-negative projection, so as to ensure the non-
negativity of the abundance fractions (since the simplex estimated by RMVES will no longer enclose
all the dimension-reduced data completely). The above illustrated procedure is collectively termed as the
RMVES algorithm.
The proposed RMVES algorithm uses the well known VCA [17] for the initialization of (34) [32]. The
endmember estimates obtained by VCA are first expanded until all the dimension-reduced data are well
within the simplex formed by the expanded endmember estimates. The expanded endmember estimates
are then used to get the initial estimates of H (by (26a)) and g (by (26b)). The pseudo-code of the
expanded VCA, that is used for RMVES initialization [24] is given in Table I. The pseudo-code of the
proposed RMVES algorithm is given in Table II.
VI. SIMULATIONS
In this section, the efficacy of the proposed RMVES algorithm is demonstrated. The results shown in
this section are averaged over 50 Monte Carlo runs for each considered scenario. To show the robustness
October 22, 2010 DRAFT
18
TABLE II
PSEUDO-CODE FOR RMVES ALGORITHM
Given The noisy observed data y[n] for n = 1, . . . , L, the number of endmembers N , a
design parameter Î·, and a convergence tolerance Îµ > 0.
Step 1. Estimate the noise covariance matrix D using multiple regression analysis [11].
Step 2. Dimension reduction: Obtain the dimension-reduced pixels: yËœ[n] = CÌ‚T (y[n] âˆ’ dÌ‚)
for all n, with the affine set fitting parameters (CÌ‚, dÌ‚) given by
dÌ‚ =
1
L
Lâˆ‘
n=1
y[n],
CÌ‚ = [ q1(UyU
T
y âˆ’ LD), q2(UyU
T
y âˆ’ LD), . . . , qNâˆ’1(UyU
T
y âˆ’ LD) ]
where Uy = [ y[1] âˆ’ dÌ‚, . . . ,y[L] âˆ’ dÌ‚ ] âˆˆ RMÃ—L and qi(R) denotes the unit-norm
eigenvector associated with the ith principal eigenvalue of the matrix R.
Step 3. Initialization: Let H = H(0) and g = g(0), where H(0) and g(0) are given by the
expanded VCA (as explained in Table I). Set i := 1 and % := | det(H)|.
Step 4. Define Q = HCÌ‚TDCÌ‚HT and let Hij âˆˆ R(Nâˆ’1)Ã—(Nâˆ’1) denote a submatrix of H
with the ith row and jth column removed. Then, handle the following problems
p? = max
h
T
i
,gi
Nâˆ’1âˆ‘
j=1
(âˆ’1)i+jhijdet(Hij)
s.t. Î¦âˆ’1(Î·)
âˆš
Qi â‰¤ h
T
i yËœ[n]âˆ’ gi, âˆ€ n,
1
T
Nâˆ’1HyËœ[n]âˆ’ 1
T
Nâˆ’1gâˆ’ 1 â‰¤ Î¦
âˆ’1(1âˆ’ Î·)
âˆš
1TNâˆ’1Q1Nâˆ’1,
q? = min
h
T
i
,gi
Nâˆ’1âˆ‘
j=1
(âˆ’1)i+jhijdet(Hij)
s.t. Î¦âˆ’1(Î·)
âˆš
Qi â‰¤ h
T
i yËœ[n]âˆ’ gi, âˆ€ n,
1
T
Nâˆ’1HyËœ[n]âˆ’ 1
T
Nâˆ’1gâˆ’ 1 â‰¤ Î¦
âˆ’1(1âˆ’ Î·)
âˆš
1TNâˆ’1Q1Nâˆ’1,
using SQP and obtain their solutions, denoted by (hÂ¯Ti , gÂ¯i) and (hTi , gi), respectively.
Step 5. If |p?| > |q?|, update (hTi , gi) := (hÂ¯Ti , gÂ¯i), otherwise (hTi , gi) := (hTi , gi).
Step 6. If (i modulo (N âˆ’ 1)) 6= 0, then i := i+ 1, and go to Step 4,
else
If |max{|p?|, |q?|} âˆ’ %|/% < Îµ, then H? = H and g? = g.
Otherwise, set % := max{|p?|, |q?|}, i := 1, and go to Step 4.
Step 7. Calculate the dimension reduced endmember estimates: Î±Ì‚N = (H?)âˆ’1g? and
[Î±Ì‚1, ..., Î±Ì‚Nâˆ’1] = Î±Ì‚N1
T
Nâˆ’1 + (H
?)âˆ’1.
Step 8. Calculate the actual endmember estimates: aÌ‚i = CÌ‚Î±Ì‚i + dÌ‚ for i = 1, ..., N .
Step 9. Estimate the abundance vectors
sÌ‚[n] = [ (H?yËœ[n]âˆ’ g?)T 1âˆ’ 1TNâˆ’1(H
?
yËœ[n]âˆ’ g?) ]T+, âˆ€ n.
October 22, 2010 DRAFT
20
parameter Ïn of the nth abundance vector s[n] is defined as Ïn = â€–s[n]â€– (the Euclidean norm of that
abundance vector [25]), which indicates the quantitative dominance of an endmember ai in the observed
pixel vector x[n] =
âˆ‘N
i=1 si[n]ai. It should be clear that the purity of the observed pixel x[n] is higher
for larger Ïn. In the simulations, we define a parameter Ï, which means that the Ïn for all the abundance
vectors lies between Ïâˆ’ 0.1 and Ï. In each simulation scenario a pool of 10, 000 pixels were generated
randomly by following the Dirichlet distribution and then 1000 vectors (L = 1000) with Ïn âˆˆ [Ïâˆ’0.1, Ï]
are taken from the generated pool, for a given purity parameter Ï. The purity levels considered in our
simulations are Ï = 0.6, 0.8, 1. Choosing a best Î· value for a given data set is cumbersome, as the apt
Î· value is always data dependent. Therefore, here we discuss how to set the Î· value for the RMVES
algorithm. As previously inferred, the value of Î· (which depends on Ï and SNR) should be less than
0.5, and from our extensive simulation experience we found that Î· should lie in the range of 0.01 to
0.0001. Therefore, in each simulation and real data experiment (see Section VII), Î· value for the RMVES
algorithm is set to 0.001, and since (34) is non-convex, for each given data (under a given scenario),
the RMVES algorithm is executed 10 times with 10 different expanded VCA initializations (since each
time VCA yields different endmember estimates for noisy observations), and the endmember signature
and abundance fractions associated with the largest |det(H)| is chosen as the optimal endmember and
abundance estimates for the data under consideration. Such a technique has been applied before in
handling non-convex problems, e.g., [38].
A. Uniform Gaussian noise case
For uniform Gaussian noise case, the noisy data were obtained by adding independent and identically
distributed (i.i.d.) zero-mean Gaussian noise vector to the noise-free data for different SNRs, where
SNR =
âˆ‘L
n=1 â€–x[n]â€–
2
2/(Ïƒ
2ML). To maintain non-negativity of the observed pixels, we artificially set
the negative values of the noisy pixels to zero. Table IV shows the results (Ï†en and Ï†ab) of all the
algorithms under test for observations corrupted by uniform Gaussian noise for SNRs between 15 dB
and 40 dB, and Ï = 0.6, 0.8, 1. For each scenario, the best (minimum Ï†en and Ï†ab) out of all the algorithms
is highlighted as a bold-faced number. As one can infer from Table IV the proposed RMVES algorithm
yields the best performance for highly mixed case (Ï = 0.6) when SNRâ‰¥ 20 dB. For moderately mixed
case (Ï = 0.8) RMVES is the best when SNRâ‰¥ 30 dB and for other SNRs ICE is better. As expected,
when pure pixels exist in the data (Ï = 1), though SGA is better for SNR ranging from 20 dB to 30
dB, the performance of the other pure pixel based methods such as N-FINDR and VCA are comparable
with each other, and in few other scenarios ICE and APS fares well. However, for all the values of Ï,
October 22, 2010 DRAFT
22
15 20 25 30 35 40
0
2
4
6
8
10
12
14
SNR
Av
er
ea
ge
 Ï† e
n
 
 
RMVES (with one VCA initialization)
RMVES (with ten VCA initializations)
Fig. 6. Graph showing the average endmember spectral angles obtained by the RMVES algorithm for Ï = 0.6 with one VCA
initialization and with ten VCA initializations, and choosing the spectral angle associated with the maximum |det(H)|.
VII. REAL DATA EXPERIMENTS
In this section, we demonstrate the performance of the proposed RMVES algorithm using the AVIRIS
data taken over the Cuprite Niveda site [39], as a good library of endmember signature is available [36],
[37] and thus can be used for performance evaluation. The AVIRIS data is challenging because of two
reasons. One reason is that the spatial resolution is not so high (about 17 m per pixel) and the other reason
is that the true total number of endmembers and the associated minerals are yet to be accurately identified.
While, initially it was concluded that there are about 13 minerals (endmembers) in the site, the number
kept increasing and later it was reported that there are around 70 mineral compounds (endmembers) [40]
present in the site and the number is still expected to rise. In our experiment we considered a 200Ã— 200
sub-image of the hyperspectral data, with 224 spectral bands. The bands 1 âˆ’ 2, 104 âˆ’ 113, 148 âˆ’ 167,
and 221âˆ’ 224 are less significant (due to strong noise or dense water-vapor content) and were removed.
A total of 188 bands is therefore considered. Estimation of the number of endmembers present in a
given scene of interest is an important issue, because both the dimension reduction algorithm and the
HU algorithm are in need of this number. To begin with, the eigenvalue distribution (signal energies
distribution) of the data covariance matrix tells that the number of sources according to the principal
eigenvalues is approximately 4 (N = 4). Obviously, it is an under estimate of N , as the ground truth
[36] reports more than 4 endmembers in the selected region of interest (ROI). Applying HySiMe [11] to
the cropped data set yields N = 18. On the other hand, for a very similar region of interest, it is reported
in [17] and [25] that Virtual Dimensionality (VD) [8] estimates the number of endmembers as 14. Also
October 22, 2010 DRAFT
24
0.5 1 1.5 2 2.5 0.5 1 1.5 2 2.5 0.5 1 1.5 2 2.5 0.5 1 1.5 2 2.5
Wavelength (um) Wavelength (um) Wavelength (um) Wavelength (um)
Library Spectra RMVES Spectra MVES Spectra VCA Spectra
Smectite
Pyrope
Paragonite
Nontronite 2
Nontronite 1
Muscovite
Montmorillonite 2
Montmorillonite 1
Kaolinite 2
Kaolinite 1
Goethite
Dumortierite
Desert varnish 2
Desert varnish 1
Chalcedony
Buddingtonite
Alunite 2
Alunite 1
Andradite 2 
Andradite 1 
Smectite
Nontronite 1
Muscovite
Montmorillonite 2
Montmorillonite 1
Kaolinite 1
Goethite
Dumortierite
Desert varnish 1
Chalcedony
Buddingtonite
Alunite 1
Andradite 2 
Andradite 1 
Nontronite 1
Muscovite
Montmorillonite 2
Kaolinite 2
Kaolinite 1
Goethite
Dumortierite
Desert varnish 2
Desert varnish 1
Chalcedony
Buddingtonite
Alunite 2
Alunite 1
Andradite 1 
Pyrope
Paragonite
Nontronite 2
Nontronite 1
Muscovite
Montmorillonite 1
Kaolinite 2
Dumortierite
Dumortierite
Desert varnish 1
Chalcedony
Buddingtonite
Alunite 1
Andradite 2 
Fig. 7. Endmember signatures taken from library, and the ones estimated by RMVES, MVES and VCA.
VIII. CONCLUSION
In this work, we have presented a robust HU algorithm, namely RMVES (as shown in Table II), for
effective unmixing of mixed hyperspectral data corrupted by uniform or non-uniform Gaussian noise. The
dimension reduction via affine set fitting procedure has been suitably modified for noisy hyperspectral
observations. The randomness caused by noise has been dealt by incorporating chance constraints in
the unmixing problem formulation with a design parameter Î·. A detailed analysis on the role of Î· has
been presented, and it was concluded that Î· must be less than 0.5, which along with the objective
function results in a non-convex optimization problem. To minimize the effect of non-convexity of
the objective function, an alternating optimization concept has been utilized. The partial maximization
problems involved are handled by using sequential quadratic programming (SQP) solvers. Finally, Monte-
Carlo simulations and real data experiments presented in VI and Section VII, respectively, demonstrate
October 22, 2010 DRAFT
26
REFERENCES
[1] N. Keshava and J. Mustard, â€œSpectral unmixing,â€ IEEE Signal Process. Mag., vol. 19, no. 1, pp. 44-57, Jan. 2002.
[2] V. P. Pauca, J. Piper, and R. J. Plemmons, â€œNonnegative matrix factorization for spectral data analysis,â€ Elsevier Journal
of Linear Algebra and Its Applications, vol. 416, no. 1, pp. 29-47, July 2006.
[3] M. B. Lopes, J. C. Wolff, J. B. Dias, M. Figueiredo, â€œNIR hyperspectral unmixing based on a minimum volume criterion
for fast and accurate chemical characterisation of counterfeit tablets,â€ Analytical Chemistry, vol. 82, no. 4, pp. 1462-1469,
2010.
[4] W. R. Johnson, D. W. Wilson, W. Fink, â€œSnapshot hyperspectral imaging in opthomology,â€ Journal of Biomedical Optics,
published online on Feb. 2007.
[5] M. O. Smith and P. E. Johnson and J. B. Adams, â€œQuantitative determination of mineral types and abundances from
reflectance spectra using principal component analysis,â€ Journal Geophys. Res., vol. 90, no. 2, pp. C797-C804, Feb. 1985.
[6] A. A. Green, â€œA transformation for ordering multispectral data in terms of image quality with implications for noise
removal,â€ IEEE Trans. Geosci. Remote Sens., vol. 32, no. 1, pp. 65-74, May 1988.
[7] I. K. Fodor, â€œA survey of dimension reduction techniques,â€ Techical Report, Lawrence Livermore National Laboratory,
May 2002.
[8] C.-I. Chang and Q. Du, â€œEstimation of number of spectrally distinct signal sources in hyperspectral imagery,â€ IEEE Trans.
Geosci. Remote Sens., vol. 42, no. 3, pp. 608-619, Mar. 2004.
[9] H. Akaike, â€œA new look at the statistical model identification,â€ IEEE Trans. Automat. Control, vol. AC-19, no. 6, pp.
716-723, Dec. 1974.
[10] J. Rissanen, â€œModeling by shortest data description,â€ Automatica, vol. 14, no. 5, pp. 465-471, Sep. 1978.
[11] J. M. B. Dias and J. M. P. Nascimento, â€œHyperspectral subspace identification,â€ IEEE Trans. Geosci. Remote Sens., vol.
46, no. 8, pp. 2435-2445, Aug. 2008.
[12] J. W. Boardman, F. A. Kruse, and R. O. Green, â€œMapping target signatures via partial unmixing of AVIRIS data,â€ in Proc.
Summ. JPL Airborne Earth Sci. Workshop, Pasadena, CA, Dec. 9-14, 1995, pp. 23-26.
[13] M. E. Winter, â€œN-findr: An algorithm for fast autonomous spectral end-member determination in hyperspectral data,â€ in
Proc. SPIE Conf. Imaging Spectrometry, Pasadena, CA, Oct. 1999, pp. 266-275.
[14] A. Ifarraguerri and C.-I. Chang, â€œMultispectral and hyperspectral image analysis with convex cones,â€ IEEE Trans. Geosci.
Remote Sens., vol. 37, no. 2, pp. 756-770, Mar. 1999.
[15] C. I. Chang, C. C. Wu, W. M. Liu, and Y. C. Ouyang, â€œA new growing method for simplex-based endmember extraction
algorithm,â€ IEEE Trans. Geosci. Remote Sens., vol. 44, no. 10, pp. 2804-2819, Oct. 2006.
[16] C. I. Chang, C. C. Wu, C. S. Lo, and M. L. Chang, â€œReal-time simplex growing algorithms for hyperspectral endmember
extraction,â€ IEEE Trans. Geosci. Remote Sens., vol. 48, no. 4, pp. 1834-1850, Apr. 2010.
[17] J. M. P. Nascimento and J. M. B. Dias, â€œVertex component analysis: A fast algorithm to unmix hyperspectral data,â€ IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 4, pp. 898-910, Apr. 2005.
[18] T.-H. Chan, C.-Y. Chi, W.-K. Ma, A. Ambikapathi â€œHyperspectral unmixing from a convex analysis and optimization
perspective,â€ in Proc. First IEEE WHISPERS, Grenoble, France, Aug. 26-28, 2009.
[19] D. Heinz and C.-I. Chang, â€œFully constrained least squares linear mixture analysis for material quantification in hyperspectral
imagery,â€ IEEE Trans. Geosci. Remote Sens., vol. 39, no. 3, pp. 529-545, Mar. 2001.
[20] M. D. Craig, â€œMinimum-volume transforms for remotely sensed data,â€ IEEE Trans. Geosci. Remote Sens., vol. 32, no. 3,
pp. 542-552, May 1994.
October 22, 2010 DRAFT
28
TABLE IV
AVERAGE Ï†en AND Ï†ab (DEGREES) OVER THE VARIOUS UNMIXING METHODS FOR DIFFERENT PURITY LEVELS (Ï) AND
SNRS- UNIFORM GAUSSIAN NOISE CASE.
Methods Ï
Ï†en Ï†ab
SNR (dB) SNR (dB)
15 20 25 30 35 40 15 20 25 30 35 40
N-FINDR
0.6 9.32 8.96 8.87 8.89 8.88 8.76 50.44 48.39 45.78 44.83 44.45 44.16
0.8 6.42 5.17 4.64 4.80 4.58 4.56 34.09 18.74 15.74 15.52 15.21 14.92
1 4.55 2.17 1.18 0.64 0.40 0.20 16.60 7.78 4.81 3.05 1.95 1.22
SGA
0.6 9.06 8.68 8.77 8.40 8.59 8.53 51.59 49.38 47.57 48.08 47.23 48.70
0.8 6.28 4.65 4.13 3.83 3.72 3.75 36.87 23.54 19.15 17.77 17.04 17.11
1 4.14 1.83 0.96 0.55 0.32 0.19 22.57 8.21 4.71 3.00 1.87 1.26
VCA
0.6 9.25 8.77 8.03 8.06 7.95 8.16 52.03 47.88 45.59 44.88 45.46 46.44
0.8 6.83 5.25 4.98 4.45 3.96 4.18 40.41 25.97 26.26 22.25 18.70 18.08
1 4.93 2.13 3.82 1.02 0.40 0.22 26.63 9.75 16.41 4.40 2.06 1.28
ICE
0.6 8.15 7.96 7.89 7.62 8.06 7.99 39.34 33.15 29.52 29.72 27.83 29.27
0.8 4.54 3.21 3.17 3.13 3.02 3.09 24.08 16.16 12.79 13.10 11.19 11.56
1 3.41 1.83 1.53 1.30 1.25 1.22 16.91 10.66 8.59 7.55 7.22 7.16
APS
0.6 10.70 9.03 9.24 9.80 10.50 10.99 44.70 37.72 35.14 31.75 30.13 28.41
0.8 8.57 4.64 6.95 6.34 5.62 5.13 34.75 20.53 20.34 17.85 14.58 12.49
1 6.95 2.03 5.27 1.40 0.28 0.14 24.30 9.06 11.81 4.36 1.74 1.04
MVC-NMF
0.6 11.69 10.73 10.85 9.81 10.19 10.20 45.27 36.69 37.29 33.91 35.22 33.02
0.8 10.07 5.04 6.80 4.74 3.90 3.41 33.74 17.03 22.96 14.20 11.77 9.26
1 8.68 5.05 5.94 1.40 0.50 0.15 25.14 9.63 12.09 3.50 1.81 1.01
MVSA
0.6 14.81 13.49 10.88 7.34 5.28 3.40 44.98 36.55 24.29 14.84 10.60 5.85
0.8 14.83 12.83 10.37 8.12 4.94 3.45 39.82 27.87 19.75 13.09 7.99 4.78
1 15.20 12.61 10.74 8.68 5.38 3.56 31.78 22.78 16.32 10.88 6.81 4.10
SISAL
0.6 13.61 11.56 8.90 6.43 4.32 3.33 40.77 29.96 19.25 13.46 8.34 6.42
0.8 13.79 11.55 10.06 7.22 4.61 3.17 36.72 25.82 18.51 12.09 7.41 4.42
1 14.09 11.81 10.46 7.88 4.89 3.32 29.29 21.15 15.15 10.14 6.35 3.91
MVES
0.6 14.75 11.89 9.81 7.11 4.98 3.45 42.46 32.41 21.77 14.76 9.32 5.71
0.8 14.77 12.73 10.52 7.93 5.15 3.62 45.22 32.93 22.76 15.10 9.38 5.78
1 15.71 13.51 11.00 8.71 6.36 4.59 44.90 34.52 26.24 20.07 13.73 10.30
RMVES
0.6 9.05 5.67 3.05 2.27 1.56 1.10 42.73 28.01 14.86 8.50 4.86 2.79
0.8 8.38 5.03 3.55 2.51 2.07 1.34 37.03 20.15 12.07 7.10 4.38 2.94
1 8.56 7.56 6.45 6.20 5.31 4.83 31.79 21.16 16.88 14.27 11.08 9.35
October 22, 2010 DRAFT
30
TABLE VI
AVERAGE Ï†en AND Ï†ab (DEGREES) WHILE DIRECTLY APPLYING SQP TO (31), FOR DIFFERENT Ï AND SNRS- UNIFORM
GAUSSIAN NOISE CASE.
Method Ï
Ï†en Ï†ab
SNR (dB) SNR (dB)
15 20 25 30 35 40 15 20 25 30 35 40
Applying SQP to (31)
0.6 21.01 21.11 21.00 20.25 20.44 21.08 49.38 46.99 45.96 44.07 42.63 43.72
0.8 20.56 19.91 19.07 18.63 17.40 16.31 55.85 52.61 49.34 47.46 43.66 40.64
1 20.78 19.57 17.84 15.54 15.53 15.48 62.33 56.61 51.10 45.92 45.69 45.58
TABLE VII
MEAN-REMOVED SPECTRAL ANGLES Ï† (DEGREES) BETWEEN LIBRARY SPECTRA AND ENDMEMBERS ESTIMATED BY
RMVES, MVES, AND VCA.
RMVES MVES VCA
Andradite#1 9.36 25.61 -
Andradite#2 24.52 - 18.49
Alunite#1 15.92 21.85 17.74
Alunite#2 - 17.72 -
Buddingtonite 23.54 22.98 27.25
Chalcedony 27.74 38.25 31.9
Desert Varnish#1 20.99 18.64 12.12
Desert Varnish#2 - 43.04 -
Dumortierite 20.77 29.32 31.95 (32.01)
Goethite 17.71 19.05 -
Kaolinite#1 27.25 26.50 -
Kaolinite#2 - 21.09 32.49
Montmorillonite#1 22.99 - 18.06
Montmorillonite#2 24.34 26.00 -
Muscovite 39.63 44.19 32.7
Nontronite#1 22.95 28.83 24.66
Nontronite#2 - - 21.51
Paragonite - - 35.91
Pyrope - - 25.59
Smectite 22.53 - -
Average Ï† 22.87 27.11 25.73
October 22, 2010 DRAFT
endmembersN , we aim to estimate the endmember signature matrix
A and the abundances s[1], . . . , s[L] from the given noisy pixels
y[1], . . . ,y[L], under the following general assumptions [5, 6]:
(A1) (Non-negativity condition) si[n] â‰¥ 0 âˆ€ i, n.
(A2) (Full additivity condition)
âˆ‘N
i=1 si[n] = 1 âˆ€ n.
(A3) min{L,M} â‰¥ N andA is of full column rank.
(A4) There exists an index set {1, 2, . . . , N}, such that x[i] =
ai for i = 1, . . . , N (i.e., the pure-pixel assumption).
For ease of later use, the convex hull [7] of the vectors a1, . . . ,aN âˆˆ
R
M is defined as
conv{a1, . . . ,aN} =
{
x =
Nâˆ‘
i=1
Î¸iai
âˆ£âˆ£âˆ£âˆ£ 1TNÎ¸ = 1, Î¸  0
}
, (3)
where Î¸ = [Î¸1, . . . , Î¸N ]T . A convex hull, conv{a1, . . . ,aN} is
called an (N âˆ’ 1)-dimensional simplex in RM if {a1, . . . ,aN} âŠ‚
R
M is affinely independent.
3. REVIEW OF AVMAX ALGORITHM
As in many other HU algorithms, we begin with the dimension re-
duction of the observations. In our work, we employ the affine set
fitting procedure in [5] to perform dimension reduction. To begin
with, we start with the noise-free signal model, given by (2). The
affine set fitting procedure is summarized as follows:
Lemma 1. (Dimension reduction by affine set fitting [5, 6]) Under
(A2) and (A3), a dimension-reduced pixel vector xËœ[n] can be ob-
tained by an affine transformation of x[n]:
xËœ[n] = CT (x[n]âˆ’ d) âˆˆ RNâˆ’1, (4)
where (C,d) is the affine set fitting solution given by
d =
1
L
Lâˆ‘
n=1
x[n], (5)
C = [ q1(UU
T ), q2(UU
T ), . . . , qNâˆ’1(UU
T ) ], (6)
where U = [ x[1] âˆ’ d, . . . ,x[L] âˆ’ d ] âˆˆ RMÃ—L, and qi(R) de-
notes the orthonormal eigenvector associated with the ith principal
eigenvalue ofR.
The affine set fitting solution (CË†, dË†) for noisy observations is ob-
tained by replacing x[n] in (5) and (6) with y[n]. In the noisy case,
(CË†, dË†) serves as a best least-squares approximation to true (C,d)
and the former asymptotically approaches the latter for large L. By
(2), (4) and under (A1)-(A3), it has been proved in [8] that
xËœ[n] âˆˆ conv{Î³1, . . . ,Î³N}, (7)
where Î³j = CT (aj âˆ’ d) âˆ€j = 1, . . . , N correspond to dimension
reduced endmembers.
Now, the main problem is how we estimate Î³1, . . . ,Î³N from
xËœ[1], . . . , xËœ[L]. Winter [3] proposed a belief that under (A4) the ver-
tices of the maximum volume simplex inside the data cloud (obser-
vations) yield high fidelity estimates of the endmember signatures.
Based on that, the unmixing problem [5] can be written as:
max
Î½1,...,Î½NâˆˆR
Nâˆ’1
V (Î½1, . . . ,Î½N )
s.t. Î½i âˆˆ conv{xËœ[1], . . . , xËœ[L]}, âˆ€ i,
(8)
where V (Î½1, . . . ,Î½N ) is the volume of the (N âˆ’ 1)-dimensional
simplex conv{Î½1, . . . , Î½N} in RNâˆ’1 and is given by [9],
V (Î½1, . . . ,Î½N ) =
|det (Î”(Î½1, . . . ,Î½N ))|
(N âˆ’ 1)!
, (9)
where
Î”(Î½1, . . . ,Î½N ) =
[
Î½1 Â· Â· Â· Î½N
1 Â· Â· Â· 1
]
.
By letting XËœ = [ xËœ[1], . . . , xËœ[L] ] âˆˆ R(Nâˆ’1)Ã—L and by (3), problem
(8) can be expressed as
max
Î½iâˆˆR
Nâˆ’1
Î¸1,...,Î¸NâˆˆR
L
|det(Î”(Î½1, . . . ,Î½N ))|
s.t. Î½i = XËœÎ¸i, Î¸i  0, 1
T
LÎ¸i = 1 âˆ€ i.
(10)
Though the constraints of (10) are convex, the non-convexity of the
objective function makes the problem difficult to solve. The prob-
lem may be handled in a convenient manner by the idea of cofactor
expansion and alternating optimization. The cofactor expansion of
the objective function in (10) along the jth column is given by
det(Î”(Î½1, . . . ,Î½N )) = b
T
j Î½j + (âˆ’1)
N+jdet(VNj), (11)
where bj = [(âˆ’1)i+jdet(Vij)]Nâˆ’1i=1 âˆˆ R
Nâˆ’1 and the term Vij âˆˆ
R
(Nâˆ’1)Ã—(Nâˆ’1) is a submatrix of Î”(Î½1, . . . , Î½N) with the ith row
and jth column removed. We then consider the partial maximization
of (10) with respect to Î½j and Î¸j , while fixing Î½i and Î¸i for all i = j.
The problem (10) then becomes
max
Î½jâˆˆR
Nâˆ’1,Î¸jâˆˆR
L
âˆ£âˆ£âˆ£ bTj Î½j + (âˆ’1)N+jdet(VNj)
âˆ£âˆ£âˆ£
s.t. Î½j = XËœÎ¸j , Î¸j  0, 1
T
LÎ¸j = 1.
(12)
The partial maximization problem (12) can be decomposed into the
following two linear programs:
p = max
Î½jâˆˆR
Nâˆ’1,Î¸jâˆˆR
L
b
T
j Î½j + (âˆ’1)
N+jdet(VNj)
s.t. Î½j = XËœÎ¸j , Î¸j  0, 1
T
LÎ¸j = 1,
(13)
q = min
Î½jâˆˆR
Nâˆ’1,Î¸jâˆˆR
L
b
T
j Î½j + (âˆ’1)
N+jdet(VNj)
s.t. Î½j = XËœÎ¸j , Î¸j  0, 1
T
LÎ¸j = 1.
(14)
The optimal solution of (12) is that of (13) if |p| > |q|, and that
of (14) if |q| > |p|. This procedure of alternating optimization
is performed for all the N columns (one iteration) and the relative
change in the volume of the updated Î”(Î½1, . . . ,Î½N ) is compared
with a given threshold. If it exceeds the threshold, we continue with
the next updating iteration, else we conclude that the current updated
Î½js are optimum. Once the optimal solution of (10), denoted by
Î½1 , . . . ,Î½

N is obtained, the endmember estimates can be recovered
by using, aË†i = CÎ½Ë†i + d (by virtue of (4)) for all i. Next, we aim to
make AVMAX more robust against noise effects.
4. ROBUST AVMAX FORMULATION AND ALGORITHM
In this section, we first do some reformulation to (13) and (14) so that
chance constraints can be incorporated into the unmixing problem.
Then, we move on to develop a robust version of AVMAX.
4.1. Restructuring the AVMAX algorithm
Now, let B = diag(sign(bj)) and G = âˆ’B. Then, we can have
GG = BB = INâˆ’1, bTj B = |bj |T and bTj G = âˆ’|bj |T . The
subproblems (13) and (14) can then be equivalently written as:
p = max
Î½jâˆˆR
Nâˆ’1,Î¸jâˆˆR
L
b
T
j BBÎ½j + (âˆ’1)
N+jdet(VNj)
s.t. BÎ½j = BXËœÎ¸j , Î¸j  0, 1
T
LÎ¸j = 1,
(15)
Table 1. Average Ï†en and Ï†ab (degrees) over the various unmixing methods for different purity levels (Ï) and SNRs.
Methods Ï
Ï†en Ï†ab
SNR (dB) SNR (dB)
20 25 30 35 40 20 25 30 35 40
N-FINDR
0.7 5.45 5.31 5.24 5.11 5.16 22.54 21.86 21.63 19.76 19.82
0.85 2.65 2.67 2.66 2.65 2.61 9.60 8.37 8.03 7.93 7.77
1 1.15 0.58 0.33 0.18 0.10 6.14 3.59 2.13 1.24 0.72
VCA
0.7 5.77 5.56 5.64 5.56 5.50 31.57 29.97 29.71 28.54 28.38
0.85 2.79 2.70 2.67 2.71 2.61 10.83 9.45 9.00 8.89 8.82
1 1.12 0.61 0.32 0.18 0.11 6.00 3.45 2.05 1.23 0.76
AVMAX
0.7 5.50 5.36 5.39 5.13 5.10 24.60 21.94 20.95 18.77 16.48
0.85 2.77 2.64 2.65 2.69 2.65 9.15 7.96 7.10 6.70 6.48
1 1.14 0.61 0.33 0.18 0.10 6.39 3.66 2.13 1.22 0.70
0.7 4.87 4.87 4.88 4.83 4.90 18.95 18.15 18.13 17.83 17.94
RAVMAX 0.85 2.54 2.48 2.56 2.52 2.51 8.56 7.68 7.44 7.39 7.34
(0.9 < Î· < 1) 1 0.79 0.43 0.24 0.14 0.08 4.34 2.60 1.56 0.98 0.59
In all these algorithms, FCLS [10] is used to get the abundance
maps. The performance of the algorithms under test is evaluated
by performing 50 Monte Carlo runs for various purity levels (Ï)
and SNRs [8]. The simulation settings are L = 1000 (number of
pixels), N = 6 (number of endmembers) and M = 417 (number
of observations). In each run, 1000 noise-free observed pixel vec-
tors were synthetically generated following the signal model in (2),
and the 6 endmembers (i.e., Alunite, Buddingtonite, Calcite, Copi-
apite, Kaolinite, and Muscovite) with 417 bands are selected from
USGS library [11], and the abundance vectors s[n] were generated
following Dirichlet distribution D(s[n],Î¼) with Î¼ = 1N/N [4],
for purity levels Ï = 0.7, 0.85, 1. The synthetic data for differ-
ent SNRs were obtained by adding independent and identically dis-
tributed zero-mean Gaussian noise to the noise-free data generated,
as per (1), and the SNR is defined as
âˆ‘L
n=1 â€–x[n]â€–
2
2/Ïƒ
2ML. In
our simulations, the noise covariance matrix is estimated from the
observations, using the procedure elaborated in HySime [12]. The
performance index employed is the root-mean-square (rms) spectral
angle between the true one and estimated one [1,4,8]. The rms spec-
tral angles between endmembers and their estimates are denoted as
Ï†en, and those between abundance maps and their estimates are de-
noted as Ï†ab. The average Ï†en and Ï†ab of the unmixing algorithms
over SNR = 20, 25, ..., 40 dB and Ï = 0.7, 0.85, 1 are shown in
Table 1, where each bold-faced number denotes the minimum rms
spectral angle associated with a specific pair of (Ï,SNR) over all
the algorithms. One can readily infer from Table 1 that the proposed
RAVMAX algorithm generally yields the best performance for all
the values of Ï and SNRs.
6. CONCLUSION
To account for the noise effects in an HU framework, we have pre-
sented a robust HU algorithm, i.e., RAVMAX. Here, we reformu-
lated the original AVMAX problem with deterministic constraints
into the one with chance constraints. The RAVMAX problem can
be efficiently solved by using available second-order cone program
solvers. The simulation results demonstrate the superior perfor-
mance of RAVMAX over some existing benchmark HU algorithms
including the original AVMAX. The performance of RAVMAXwith
real hyperspectral data is currently under investigation.
7. APPENDIX
Proof of Lemma 2: Firstly, it is trivial to show that the objective
function of (19) is equivalent to that of (13) as Î±j = BÎ½j , bTj B =
|bj |
T and BB = I. Next, consider the subproblem (19) (after ig-
noring the constant term in the objective function) and let S denote
the constraint set of (19). Then we have
max
Î±jâˆˆS
|bj |
T
Î±j = |bj |
T
kj ,
where kj = [maxSi [Î±j ]i]
Nâˆ’1
i=1 in which Si = {[Î±j ]i â‰¤ [BXËœÎ¸j ]i},
implying that an optimal solution, denoted by (Î±j ,Î¸j ) will make
the equality in Î±j  BXËœÎ¸j hold (i.e., the constraint will be ac-
tive). In other words, the optimal solution (Î±j , Î¸j ) belongs to the
set {(Î±j ,Î¸j) | Î±j = BXËœÎ¸j ,Î¸j  0, 1TLÎ¸j = 1}, which is equiv-
alent to the constraint set of (13). Hence we can conclude that the
subproblems (19) and (13) are equivalent. By a similar argument the
equivalence of (20) and (14) can be proved.
8. REFERENCES
[1] N. Keshava and J. Mustard, â€œSpectral unmixing,â€ IEEE Signal Process.
Mag., vol. 19, no. 1, pp. 44-57, Jan. 2002.
[2] J. W. Boardman, F. A. Kruse, and R. O. Green, â€œMapping target sig-
natures via partial unmixing of AVRIS data,â€ in Proc. Summ. JPL Air-
borne Earth Sci. Workshop, Pasadena, CA, Dec. 9-14, 1995, pp. 23-26.
[3] M. E. Winter, â€œN-findr: An algorithm for fast autonomous spectral
end-member determination in hyperspectral data,â€ in Proc. SPIE Conf.
Imaging Spectrometry, Pasadena, CA, Oct. 1999, pp. 266-275.
[4] J. M. P. Nascimento and J. M. B. Dias, â€œVertex component analysis:
A fast algorithm to unmix hyperspectral data,â€ IEEE Trans. Geosci.
Remote Sens., vol. 43, no. 4, pp. 898-910, Apr. 2005.
[5] T. H. Chan, W.-K.Ma, C.-Y. Chi and A. Ambikapathi, â€œHyperspec-
tral unmixing from a convex analysis and optimization perspective,â€ in
Proc. First IEEE WHISPERS, Grenoble, France, August 26-28, 2009.
[6] A. Ambikapathi, T.-H. Chan, W.-K.Ma and C.-Y. Chi, â€œA robust mini-
mum volume enclosing simplex algorithm for hyperspectral unmixing,
in Proc. IEEE ICASSP, Dallas, Mar. 14-19, 2010, pp. 1202-1205.
[7] S. Boyd and L. Vandenberghe, Convex Optimization, UK: Cambridge
Univ. Press, 2004.
[8] T.-H. Chan, C.-Y. Chi, Y.-M. Huang, and W.-K. Ma, â€œA convex anal-
ysis based minimum-volume enclosing simplex algorithm for hyper-
spectral unmixing,â€ IEEE Trans. Signal Processing, vol. 57, no. 11,
pp. 4418-4432, Nov. 2009.
[9] G. Strang, Linear Algebra and Its Applications, CA: Thomson, 4th
edition, 2006.
[10] D. Heinz and C.-I. Chang, â€œFully constrained least squares linear
mixture analysis for material quantification in hyperspectral imagery,â€
IEEE Trans. Geosci. Remote Sens., vol. 39, no. 3, pp. 529-545, Mar.
2001.
[11] Tech. Rep., Available online: http://speclab.cr.usgs.gov/
cuprite.html.
[12] J. M. B. Dias and J. M. P. Nascimento, â€œHyperspectral subspace iden-
tification,â€ IEEE Trans. Geosci. Remote Sens., vol. 46, no. 8, pp. 2435-
2445, Aug. 2008.
2I. INTRODUCTION
Hyperspectral imaging devices with high spectral resolution utilize more than a hundred contiguous
spectral bands to produce a set of remotely sensed images, thereby facilitating identification of composi-
tion of disparate materials over the observed scene [1], [2]. Hyperspectral imaging techniques have been
applied to various fields, including space object detection and planet exploration in space [3], [4], as well
as environmental monitoring and military surveillance on the Earth [5], [6]. In these hyperspectral images,
each pixel physically represents a surface area and could cover more than one materials (depending
on the spatial resolution of the sensor), and hence each observed pixel spectrum usually comprises
multiple spectra of materials (or endmember signatures). How the endmember signatures can be accurately
recovered from the measured data, or namely the endmember extraction problem, has been a subject of
numerous investigations during the past decade [7], [8].
A major branch of algorithms for endmember extraction is based on Craigâ€™s belief [9], which states that
the vertices of a minimum-volume simplex enclosing all the observed pixel vectors may serve as reliable
estimates of the endmembers. Algorithms that are based on Craigâ€™s belief, either explicitly or implicitly,
include iterated constrained endmembers (ICE) [10], minimum volume constrained non-negative matrix
factorization (MVC-NMF) [11], minimum volume simplex analysis (MVSA) [12], and minimum-volume
enclosing simplex (MVES) [13], to name a few. In [13], we have shown that Craigâ€™s belief yields the true
endmembers (or shortly the endmember identifiability) when there exist pure pixels (i.e., pixels composed
of a single endmember). Moreover, by empirical experience, these Craigâ€™s belief based methods work
well even when the pure pixel assumption is violated to a certain extent. However, they can be expensive
to implement computationally, due mainly to the complexity of Craigâ€™s problem.
Another branch of endmember extraction algorithms, which are generally simpler to implement, as-
sumes the existence of pure pixels and attempts to search for those pure pixels as endmember estimates.
The pure pixel assumption generally holds for cases where the remote sensing platforms (or aircrafts) fly
at a low altitude or perform small-area surveillance [14]. In the late 90â€™s, Winter proposed an endmember
extraction belief, which states that the volume of a simplex formed by the pure pixels is the maximum.
In that work Winter also developed an algorithm called N-finder (N-FINDR) [15], [16], which has
nowadays been widely used for real hyperspectral data analysis. N-FINDR examines individual pixels and
recalculates the simplex volume until it reaches the maximum. Winterâ€™s idea has stimulated much interest,
and in recent years we have seen many different variants of N-FINDR, in which various pixel search
strategies are employed for efficient implementations of N-FINDR: [17]â€“[19] considered several types of
October 22, 2010 DRAFT
4violated.
â€¢ We propose an alternating optimization-based algorithm for the Winter problem, called alternating
volume maximization (AVMAX). This algorithm can converge to a stationary point of the Winter
problem, by a result in alternating optimization. Its endmember identifiability is also proven.
â€¢ We propose a successive optimization-based algorithm for the Winter problem, called successive
volume maximization (SVMAX). The development there utilizes a special decomposition of the
simplex volume function (to be presented in Lemma 4), which will be derived in this paper. The
endmember identifiability of SVMAX is also proven.
â€¢ We formulate a robust worst-case Winter problem for accounting for the noise effects. The newly
formulated problem is in form of max-min optimization, and it is more difficult to handle than
the original Winter problem. An algorithm incorporating alternating optimization and projected
subgradients is proposed to tackle the robust Winter problem.
It is worthwhile to mention the relationship of this work and the other existing studies. Our AVMAX
method turns out to be very similar to NSS-N-FINDR [20] (an N-FINDR variant) algorithmically.
However, our work also touches on convergence issues. As another interesting coincidence, our SVMAX
method is strikingly similar to VCA. In fact, in a previous work [18], the possible connection of
VCA and Winterâ€™s belief has been alluded to. Here, in order to consolidate the similarity claim, not
only the successive optimization concept is needed, but we also need to derive a special determinant
decomposition lemma (Lemma 4). Furthermore, we should mention concurrent works on dealing with
noise in endmember extraction: In the joint Bayesian algorithm (JBA) [26], a Bayesian estimation
framework accounting for the presence of noise explicitly was employed; in simplex identification by
split augmented Lagrangian (SISAL) [27], soft constraints are utilized to mitigate outlier pixel effects
caused by noise; in robust MVES (RMVES) [28] we apply chance constraints on the original MVES
constraints. Our worst-case optimization for handling the noise effects is nonetheless different from the
existing works in terms of the criterion (or belief).
B. Organization of the Paper and Notations
This paper is organized as follows. In Section II, we present the problem statement of endmember
extraction and its convex geometry. Section III presents the problem formulation for Winterâ€™s belief.
Sections IV and V present the AVMAX and SVMAX algorithms, respectively. In Section VI, we present
the worst-case problem formulation for Winterâ€™s belief and the associated robust algorithm. Section VII
presents some simulation results to demonstrate the efficacy of the proposed algorithms. Finally, some
October 22, 2010 DRAFT
6B. Convex Geometry Representation
In our previous works [13], [33], [34] (also [35], [36] for earlier attempts by others), we have developed
a convex geometry framework for analyzing hyperspectral mixing models, using the notion of convex
analysis [37]. Here, we follow this previous framework to formulate the problem herein.
In convex analysis, an affine hull of a set of vectors, say {a1, . . . ,aN}, is defined as
aff{a1, . . . ,aN} =
{
x =
Nâˆ‘
i=1
Î¸iai
âˆ£âˆ£âˆ£âˆ£ Î¸i âˆˆ R, i = 1, . . . , N,
Nâˆ‘
i=1
Î¸i = 1
}
. (2)
The immediate implication to the hyperspectral mixing model (1) is that every observed pixel vector
x[n] lies in the endmember affine hull aff{a1, . . . ,aN}, due to (A2). The affine hull aff{a1, . . . ,aN}
can always be represented by
aff{a1, . . . ,aN} =
{
x = CÎ±+ d
âˆ£âˆ£ Î± âˆˆ RP} (3)
for some (C,d) âˆˆ RMÃ—P Ã— RM and rank(C) = P , where P is the affine dimension. Under (A3), the
affine dimension P is N âˆ’ 1. We have shown [33], [34] that (C,d) in (3) can be identified from the
observed pixel vectors {x[1], . . . ,x[L]}, as stated in the following lemma:
Lemma 1. (Affine set fitting [33], [34]) Under (A2) and (A3), the affine set fitting parameter (C,d)
in (3) can be obtained from {x[1], . . . ,x[L]} by
d =
1
L
Lâˆ‘
n=1
x[n], (4)
C = [ q1(HH
T ), q2(HH
T ), . . . , qNâˆ’1(HH
T ) ], (5)
where H = [ x[1]âˆ’d, . . . ,x[L]âˆ’d ] âˆˆ RMÃ—L, and qi(R) denotes the unit-norm eigenvector associated
with the ith principal eigenvalue of R.
Affine set fitting in Lemma 1 provides a way of dimension reduction. Since x[n] âˆˆ {x = CÎ±+d âˆ£âˆ£ Î± âˆˆ
R
Nâˆ’1
}
and C is semi-unitary, one can affinely transform each observed pixel vector x[n] by
xËœ[n] = CT (x[n]âˆ’ d) âˆˆ RNâˆ’1. (6)
Substituting (1) into (6), we get
xËœ[n] =
Nâˆ‘
i=1
si[n]Î±i, (7)
where
Î±i = C
T (ai âˆ’ d) âˆˆ RNâˆ’1, i = 1, . . . , N. (8)
October 22, 2010 DRAFT
8conv{xËœ[1], . . . , xËœ[L]}
Î±1
Î±2
Î±3
Î½1
Î½2
Î½3 conv{Î½1,Î½2,Î½3}
xËœ[n]
Fig. 2. An illustration of signal geometry of Winterâ€™s belief for N = 3.
III. FORMULATION OF WINTERâ€™S ENDMEMBER EXTRACTION BELIEF
In 1999, Winter [15] proposed an endmember extraction belief that has now attracted much interest. In
that work, it was believed that the ground truth endmembers can be located by finding a collection of pixel
vectors whose simplex volume is the largest. Following the hyperspectral signal geometry derivations in
the last section, we herein formulate Winterâ€™s belief in form of continuous optimization. As will be seen,
this endeavor will provide many implications. Our continuous optimization formulation of Winterâ€™s belief
is as follows:
max
Î½1,...,Î½NâˆˆRNâˆ’1
vol(Î½1, . . . ,Î½N )
s.t. Î½i âˆˆ conv{xËœ[1], . . . , xËœ[L]}, i = 1, . . . , N.
(13)
Problem (13), which we will name the Winter problem in the sequel, is to find an N -tuple (Î½1, . . . , Î½N )
from the pixel-constructed convex hull such that the associated simplex volume is maximized. A picture
is used to illustrate this in Figure 2. It should be noted that in Winterâ€™s original work, each endmember
estimate Î½j is restricted to be any vector in {xËœ[1], . . . , xËœ[L]}.
With the Winter criterion formulated in (13), the first question we wish to pin down is endmember
identifiability: Under the noiseless model, is the Winter problem (13) able to perfectly identify the ground
truth {Î±1, . . . ,Î±N}, and, if yes, what are the underlying conditions? This is addressed in the following
theorem:
Theorem 1. (Endmember identifiability for Winterâ€™s belief) Suppose that noise is absent, and that
(A1) âˆ’ (A3) hold. Then, the optimal solution of (13), denoted by {Î½?1 , . . . ,Î½?N}, is uniquely given by
{Î±1, . . . ,Î±N} if and only if (A4) is true.
October 22, 2010 DRAFT
10
ALTERNATING OPTIMIZATION FOR PROBLEM (14)
given a starting point (Î½Ë†1, . . . , Î½Ë†N ).
repeat the following alternating cycle:
for j = 1, . . . , N
solve the jth partial maximization problem
max
Î½jâˆˆFj
f(Î½Ë†1, . . . , Î½Ë†jâˆ’1,Î½j , Î½Ë†j+1, . . . , Î½Ë†N )
and update Î½Ë†j as a solution of the above problem.
end
until a stopping rule is satisfied.
output (Î½Ë†1, . . . , Î½Ë†N ) as an approximate solution to (14).
There is a basic convergence result for alternating optimization [39]: Suppose that f is continuously
differentiable, that each Fi is a closed convex set, and that each partial maximization problem has a
unique solution. Then, (Î½Ë†1, . . . , Î½Ë†N ) converges to a stationary point of Problem (14) for the number of
alternating cycles approaching infinity.
B. Alternating Optimization for the Winter Problem
Now, let us consider using alternating optimization to handle the Winter problem (13). By (9) and
(10), we can explicitly express Problem (13) as
max
Î½1,...,Î½NâˆˆRNâˆ’1
|det(âˆ†(Î½1, . . . ,Î½N ))|
s.t. Î½i âˆˆ F , i = 1, . . . , N,
(15)
where
F = {Î½ âˆˆ RNâˆ’1 | Î½ = XËœÎ¸, Î¸  0, 1TLÎ¸ = 1} (16)
is the convex hull of xËœ[1], . . . , xËœ[L], and XËœ = [ xËœ[1], . . . , xËœ[L] ]. Notice that F is closed convex. Another
property of Problem (15) is that the optimal solutions of (15) are order invariant; i.e., if (Î½?1 , . . . ,Î½?N )
is an optimal solution of (15), then (Î½?pi1 , . . . ,Î½?piN ), for any permutation index set {pi1, . . . , piN}, is also
optimal to (15). Moreover, by the basic matrix result that det(E) = âˆ’det(B) if B results from E by
interchanging any two columns, an optimal solution of (15) in which det(âˆ†(Î½?1 , . . . ,Î½?N )) â‰¥ 0 always
October 22, 2010 DRAFT
12
TABLE I
THE AVMAX ALGORITHM
Given a convergence tolerance Îµ > 0, the dimension reduced data {xËœ[n]}Ln=1, and
the number of endmembers N .
Step 1. initialize (Î½Ë†1, . . . , Î½Ë†N ) by randomly selecting each Î½Ë†i from {xËœ[n]}Ln=1.
Step 2. set j := 1, % := det(âˆ†(Î½Ë†1, . . . , Î½Ë†N)), and Nitr = 0.
Step 3. calculate bj = [(âˆ’1)i+jdet(V ij)]Nâˆ’1i=1 , where Vij is a submatrix of
âˆ†(Î½Ë†1, . . . , Î½Ë†N ) with the ith row and jth column removed.
Step 4. update Î½Ë†j := xËœ[`] for any ` = argn=1,...,Lmax bTj xËœ[n].
Step 5. if (j modulo N) 6= 0,
then j := j + 1 and go to Step 3,
else
update Nitr := Nitr + 1, and compute %Â¯ = det(âˆ†(Î½Ë†1, . . . , Î½Ë†N )).
if |%Â¯âˆ’ %|/% > Îµ, then set % := %Â¯, j := 1, and go to Step 3.
Step 6. output (Î½Ë†1, . . . , Î½Ë†N ) as an approximate solution to (15).
Remark 1. AVMAX turns out to have much in common with the N-FINDR algorithms [15], [18],
[20], in that both AVMAX and N-FINDR attempt to maximize the simplex volume by employing some
forms of one-at-a-time pixel search. AVMAX is particularly similar to one specific form of N-FINDR,
namely NSS-N-FINDR [20], where the pseudo code of AVMAX in Table I becomes that of NSS-N-
FINDR if we replace Step 4 by Î½Ë†j = xËœ[`] for any ` = argmaxn=1,...,L |bTj xËœ[n] + (âˆ’1)N+jdet(VNj)|
and restrict the total number of alternating cycles Nitr equal to one in Step 5.
Remark 2. While AVMAX turns out to be a pixel search method, it is a consequence of alternating
optimization for a continuously differentiable problem with a closed convex feasible set. By the
convergence result of alternating optimization [39] mentioned in the previous subsection, we come to
an interesting conclusion that the AVMAX method converges to a stationary point of Winterâ€™s problem
(17) under the mild assumption of unique solution for each partial maximization problem (18).
In the previous section, we have proven the endmember identifiability of the Winter optimization
problem. However, it does not imply that an algorithm derived from the Winter problem can always
achieve that identifiability result, since the Winter problem may have local minima; in other words,
endmember identifiability of a Winter-based algorithm and that of the Winter problem can be two different
issues. For AVMAX, we show that the endmember identifiability is the same as that promised by the
problem itself (in Theorem 1).
October 22, 2010 DRAFT
14
where F = {w âˆˆ RN | w = [Î½T 1]T , Î½ âˆˆ F}, with F having been defined in (16). The set F can be
alternatively represented by
F = {w âˆˆ RN | w = XÎ¸, Î¸  0, 1TLÎ¸ = 1}, (27)
where xÂ¯[n] = [xËœ[n]T 1]T and X = [xÂ¯[1], . . . , xÂ¯[L]].
We can decompose the objective function of (26) in the successive form in (25). To do so, we derive
the following general matrix lemma:
Lemma 4. Let Y = [y1, . . . ,yN ] âˆˆ RMÃ—N . It holds true thatâˆš
det(YTY) = â€–y1â€–2
âˆ¥âˆ¥âˆ¥PâŠ¥Y1:1y2âˆ¥âˆ¥âˆ¥2 Â· Â· Â·
âˆ¥âˆ¥âˆ¥PâŠ¥Y1:(Nâˆ’1)yNâˆ¥âˆ¥âˆ¥2 , (28)
where Y1:j = [y1, . . . ,yj ], and PâŠ¥Y1:j = IM âˆ’ Y1:j(YT1:jY1:j)â€ YT1:j is the orthogonal complement
projector of Y1:j .
Proof: The proof of Lemma 4 is given in Appendix D.
Since |det(Y)| =
âˆš
det(YTY) for square Y, Lemma 4 can be used to decompose the objective
function of Problem (26). Subsequently, we have the following successive optimization process for the
Winter problem:
wË†1 =arg max
w1âˆˆF
â€–w1â€–2 , (29a)
wË†j =arg max
wjâˆˆF
âˆ¥âˆ¥âˆ¥PâŠ¥
WË†1:(jâˆ’1)
wj
âˆ¥âˆ¥âˆ¥
2
, j = 2, . . . , N. (29b)
The successive maximizations above are nonconvex. Fortunately, we show that they can be easily solved:
Lemma 5. The partial maximizer in (29a) has an analytical solution wË†1 = xÂ¯[`] for any ` âˆˆ arg
maxn=1,...,L â€–xÂ¯[n]â€–2. Moreover, the partial maximizer in (29b) has an analytical solution wË†j = xÂ¯[`] for
any ` âˆˆ arg maxn=1,...,L â€–PâŠ¥
WË†1:(jâˆ’1)
xÂ¯[n]â€–2.
Proof: The proof of Lemma 5 is given in Appendix E.
We name the successive optimization procedure developed above the successive volume maximization
(SVMAX) method. Table II provides a summary of the SVMAX method in pseudo-code form. An
important remark is as follows:
Remark 3. As a surprising coincidence, SVMAX has striking similarities to the VCA algorithm [23].
The notably similar part lies in the result in Lemma 5: If we replace the index selection in Lemma 5 by
` âˆˆ argmaxn=1,...,L |rTj xÂ¯[n]|, where rj = PâŠ¥WË†1:(jâˆ’1)Î¾/â€–P
âŠ¥
WË†1:(jâˆ’1)
Î¾â€– with Î¾ being randomly generated,
October 22, 2010 DRAFT
16
conv{xËœ[1], . . . , xËœ[L]}
Î±1
Î±2
Î±3
Î½1
Î½2
Î½3
xËœ[n]
(a)
conv{xËœ[1], . . . , xËœ[L]}
Î±1
Î±2
Î±3
Î½1
Î½2
Î½3
r
r
r
xËœ[n]
(b)
Fig. 3. An illustration of data geometry in the presence of noise for N = 3: (a) The solution of original Winterâ€™s belief, and
(b) the solution of the worst-case Winterâ€™s belief.
3(a) to illustrate such a possible instance. Our idea of mitigating such effects is to keep the endmember
estimates Î½1, . . . ,Î½N away from the boundary of the pixel-constructed convex hull conv{xËœ[1], . . . , xËœ[L]}
by some distance, thereby attempting to bring (Î½1, . . . ,Î½N ) closer to the ground truth (Î±1, . . . ,Î±N ). A
picture illustrating this idea is shown in Figure 3(b); in the figure the circles located at the corners of
conv{xËœ[1], . . . , xËœ[L]} represent the maximum backoff regions of Î½1, . . . ,Î½N .
We formulate the above proposed belief as an optimization problem, given as follows:
max
viâˆˆRNâˆ’1,
i=1,...,N
ï£±ï£²
ï£³ minâ€–uiâ€–â‰¤r,
i=1,...,N
vol(v1 âˆ’ u1, . . . ,vN âˆ’ uN )
ï£¼ï£½
ï£¾
s.t. vi âˆˆ conv{xËœ[1], . . . , xËœ[L]}, i = 1, . . . , N,
(30)
where each ui âˆˆ RNâˆ’1, i = 1, . . . , N is an error vector that lies in a norm ball {u âˆˆ RNâˆ’1 | â€–uâ€–2 â‰¤ r},
with the given radius r. By letting (v?1, . . . ,v?N ,u?1, . . . ,u?N ) denote the outer-inner solution of (30), the
robust Winter endmember estimates are Î½?i = v?i âˆ’ u?i , i = 1, . . . , N .
Physically speaking, our formulated problem in (30) takes on a worst-case robust strategy. The newly
incorporated vectors u1, . . . ,uN are used to enforce the backoffs of Î½1, . . . ,Î½N in a worst-case simplex
volume sense, while the radius r represents how much backoff of Î½1, . . . ,Î½N is allowed. In other words,
the value r is also a sort of measure that quantifies the desired robustness against noise. As a rule of
thumb, one should increase r when the noise variance, or the magnitude of the noise perturbations,
increases.
October 22, 2010 DRAFT
18
Ï•(Î¸j , Î˜Ë†j) is concave w.r.t. Î¸j [37] due to the pointwise infimum property1. However, Problem (35) is still
difficult to handle due to the fact that Ï•(Î¸j , Î˜Ë†j) does not have a closed form and is non-differentiable.
Next, we will concentrate on dealing with Problem (35).
2) Projected subgradient method for (35): In the optimization literature, there are methods available
for solving non-differentiable convex problems, and those include the subgradient methods, cutting plane
methods, and ellipsoid methods [39]. Here, we employ the projected subgradient method to deal with (35),
due to its relative implementation simplicity. The projected subgradient method for (35) is as follows.
The basic idea of the projected subgradient method is to generate a sequence of points according to
the following iteration
Î¸
(k+1)
j =
{
Î¸
(k)
j âˆ’ Î³kg(k)
}
S
(36)
where g(k) is a subgradient of âˆ’Ï•(Î¸j , Î˜Ë†j) at Î¸(k)j , Î³k is the step size, k is the current iteration number,
and {x}S = argminÎ¸âˆˆS â€–xâˆ’Î¸â€–2 denotes the projection of x onto S . The projected subgradient method
keeps track of the best solution found; i.e., at each iteration we update
Ï•
(k+1)
best = max
{
Ï•
(k)
best, Ï•(Î¸
(k+1)
j , Î˜Ë†j)
}
(37)
and update Î¸Ë†j = Î¸(k+1)j if Ï•
(k+1)
best = Ï•(Î¸
(k+1)
j , Î˜Ë†j). As a key property, the projected subgradient method
can converge to the optimal objective value for certain kinds of step size sequences; e.g., the diminishing
step size sequence Î³k = Î³/
âˆš
k for some Î³ > 0 [39].
The projection onto the simplex {x}S can be efficiently implemented by a waterfilling-type algorithm
[40]. The subgradients for âˆ’Ï•(Î¸j , Î˜Ë†j) can be computed as follows. By Danskinâ€™s theorem [39], a
subgradient of âˆ’Ï•(Î¸j , Î˜Ë†j) at Î¸j is given by
g = âˆ’XËœTbj(U?), (38)
where U? = [u?1, . . . ,u?N ] is the optimal solution of the inner problem of (33) given by
(u?1, . . . ,u
?
N ) = arg min
â€–uiâ€–2â‰¤r,
i=1,...,N
det(âˆ†(XËœÎ¸Ë†1 âˆ’ u1, . . . , XËœÎ¸Ë†N âˆ’ uN )). (39)
The above problem is nonconvex, but can be approximated by alternating optimization in the same spirit
as AVMAX. The partial minimization problem of (39) w.r.t. uj can be expressed as
min
â€–ujâ€–2â‰¤r
kTj (XËœÎ¸Ë†j âˆ’ uj) + (âˆ’1)N+jdet(UNj), (40)
1Let Y be a nonempty continuous set. The pointwise infimum property states that if f(x,y) is concave in x for any y âˆˆ Y ,
then g(x) = infyâˆˆY f(x,y) is concave in x.
October 22, 2010 DRAFT
20
TABLE IV
A SUMMARY OF WAVMAX ALGORITHM
Given a convergence tolerance Îµ > 0, an error tolerance r, the dimension reduced
data matrix XËœ, the number of endmembers N , the subgradient step size Î³,
and the maximum number of subgradient iterations K .
Step 1. initialize (Î¸Ë†1, . . . , Î¸Ë†N) by AVMAX and obtain UË† by Table III.
Step 2. Alternating optimization over Î¸1, . . . ,Î¸N :
set j := 1, Nitr = 0, and compute % = det(âˆ†(XËœÎ¸Ë†1âˆ’ uË†1, . . . , XËœÎ¸Ë†Nâˆ’ uË†N )).
Step 3. A projected subgradient method for Î¸j
3.1. set k = 1 and Ï•best = 0.
3.2. calculate b(UË†) = [(âˆ’1)i+jdet(Qij)]Nâˆ’1i=1 where Qij is a submatrix
of âˆ†(XËœÎ¸Ë†1âˆ’ uË†1, . . . , XËœÎ¸Ë†N âˆ’ uË†N ) with the ith row and the jth column
removed.
3.3. update
Î¸j :=
{
Î¸j + Î³kXËœ
Tb(UË†)
}
S
,
where Î³k = Î³/
âˆš
k and {x}S is the projection x onto the simplex
using a water-filling method [40].
3.4. update UË† by Table III with the given (Î¸j , Î˜Ë†j).
3.5. update Î¸Ë†j := Î¸j if Ï•(Î¸j , Î˜Ë†j) > Ï•best and
Ï•best := max
{
Ï•best, Ï•(Î¸j , Î˜Ë†j)
}
.
3.6. update k := k + 1 and go to 3.2 until k > K .
Step 4. if (j modulo N) 6= 0,
then j := j + 1 and go to Step 3,
else
update Nitr := Nitr + 1, and compute UË† by Table III and %Â¯ =
det(âˆ†(XËœÎ¸Ë†1 âˆ’ uË†1, . . . , XËœÎ¸Ë†N âˆ’ uË†N )),
if |%Â¯âˆ’ %|/% > Îµ, then set % := %Â¯, j := 1, and go to Step 3,
otherwise, Î½Ë†j = XËœÎ¸Ë†j âˆ’ uË†j for all j.
Step 5. output (Î½Ë†1, . . . , Î½Ë†N ) as the robust Winterâ€™s estimates.
distance between endmembers and their estimates was used as a performance measure [23]:
Ï† = min
piâˆˆÎ N
âˆšâˆšâˆšâˆš 1
N
Nâˆ‘
i=1
[
arccos
(
aTi aË†pii
â€–aiâ€–â€–aË†piiâ€–
)]2
(41)
where pi = (pi1, . . . , piN ), and Î N = {pi âˆˆ RN | pii âˆˆ {1, 2, . . . , N}, pii 6= pij for i 6= j} is the set of
all the permutations of {1, 2, ..., N}. Clearly, the smaller the value of Ï†, the better the performance of
October 22, 2010 DRAFT
22
TABLE V
PERFORMANCE COMPARISON OF AVERAGE Ï† (DEGREES) AND AVERAGE T (SECS), AND AVERAGE Nitr OVER DIFFERENT
ENDMEMBER EXTRACTION METHODS FOR N = 8, L = 1000 AND VARIOUS SNRS.
Algorithms
SNR (dB)
5 10 15 20 25 âˆž
VCA
Ï† 15.59 8.66 3.81 2.22 1.32 0
T 0.17 0.17 0.11 0.09 0.10 0.06
SGA
Ï† 14.51 8.18 3.48 1.84 0.96 0
T 0.28 0.27 0.25 0.24 0.24 0.24
N-FINDR
Ï† 14.22 8.03 3.56 1.93 1.05 0
T 3.07 3.72 3.59 3.08 2.69 0.51
Nitr 39.14 38.32 30.76 25.34 20.72 7.96
NSS-N-FINDR
Ï† 15.05 8.14 3.96 2.12 1.13 0
T 0.50 0.49 0.48 0.48 0.47 0.47
AVMAX
Ï† 13.90 8.18 3.56 1.93 1.05 0
T 0.06 0.05 0.04 0.04 0.03 0.03
Nitr 4.11 3.93 3.61 3.23 3.16 2.00
SVMAX
Ï† 14.19 8.32 3.40 1.76 0.93 0
T 0.05 0.05 0.03 0.03 0.03 0.03
WAVMAX
Ï† 12.56 7.67 3.07 1.74 0.92 0
T 66.79 75.15 64.53 69.90 65.78 39.38
Nitr 2.27 3.12 2.52 3.04 3.01 2.00
B. Monte Carlo Simulations for Various Number of Pixels
The synthetic data were generated in the same manner as in Section VII-A, where the SNR fixed at 15
dB and the number of pixels varies from 250 to 8000. The average Ï† and T of the endmember extraction
methods for the synthetic data sets with different L are shown in Table VI. The error tolerance r for
WAVMAX is the same as that in Section VII-A for SNR=15 dB. One can see that for all the values of
L under test, WAVMAX outperforms all the other algorithms, and SVMAX spent less computation time
T than the others. Moreover, the number of iterations Nitr of N-FINDR slightly increases as L becomes
larger while that of AVMAX and WAVMAX seems independent of L.
C. Monte Carlo Simulations for Various Number of Endmembers
Again, the synthetic data were generated in the same manner as in Section VII-A, where SNR is set to
15 dB and the N endmembers varying from 4 to 14 are randomly picked from USGS library [42]. The
October 22, 2010 DRAFT
24
TABLE VII
PERFORMANCE COMPARISON OF AVERAGE Ï† (DEGREES), AVERAGE T (SECS), AND AVERAGE Nitr OVER DIFFERENT
ENDMEMBER EXTRACTION METHODS FOR L = 1000, SNR= 15 (DB) AND VARIOUS NUMBER OF ENDMEMBERS N .
Algorithms
The number of endmembers (N )
4 6 8 10 12 14
VCA
Ï† 2.15 2.59 3.47 5.93 9.01 9.99
T 0.07 0.08 0.10 0.13 0.18 0.24
SGA
Ï† 1.97 2.41 3.16 6.62 9.44 9.53
T 0.13 0.19 0.26 0.34 0.41 0.49
N-FINDR
Ï† 2.10 2.59 3.12 4.76 8.81 9.69
T 1.10 2.11 3.57 5.52 7.10 8.88
Nitr 16.04 23.41 31.51 43.32 51.79 57.12
NSS-N-FINDR
Ï† 2.19 2.84 3.44 6.30 9.33 10.05
T 0.24 0.36 0.38 0.63 0.77 0.92
AVMAX
Ï† 2.10 2.59 3.15 5.36 8.85 10.03
T 0.03 0.04 0.04 0.06 0.07 0.09
Nitr 2.82 3.15 3.68 4.32 4.20 4.78
SVMAX
Ï† 2.02 2.44 3.03 5.87 8.69 10.00
T 0.03 0.03 0.04 0.05 0.06 0.07
WAVMAX
Ï† 1.53 2.21 2.82 4.68 8.66 9.59
T 22.83 44.19 65.38 101.22 198.54 285.10
Nitr 2.29 3.00 2.82 3.76 6.68 10.14
identifiability of AVMAX and SVMAX in the absence of noise, and the convergence of AVMAX to
a stationary point. In addition, we have also established a worst-case Winterâ€™s endmember extraction
problem with the aim of accounting for the presence of noise in hyperspectral data, and have proposed a
worst-case AVMAX algorithm using a combination of alternating optimization and projected subgradients.
Some simulation results were presented to demonstrate the comparable performance of the AVMAX
and SVMAX algorithms with some existing benchmark endmember extraction algorithms, the superior
performance of the WAVMAX algorithm for finite SNRs, and the lower computational complexity of the
SVMAX algorithm than the benchmark methods.
October 22, 2010 DRAFT
26
C. Proof of Lemma 3.
Consider Step 4 in Table I for j = 1. Under the assumption that the solution of each alternating
maximizer is unique, we have
Î½Ë†1 = xËœ[`], ` = argn=1,...,Lmax bÂ¯
T
1 xÂ¯[n], (48)
where bÂ¯1 = [bT1 (âˆ’1)N+jdet(VN1)]T and xÂ¯[n] = [xËœ[n]T 1]T . Recall that xÂ¯[n] =
âˆ‘N
i=1 si[n]Î±Â¯i and
Î±Â¯i = [Î±
T
i 1]
T
. Then, we can infer from (48) and assumptions (A1) and (A2) that
max
n
bÂ¯T1 xÂ¯[n] = max
n
Nâˆ‘
i=1
si[n] bÂ¯
T
1 Î±Â¯i â‰¤ max
i=1,...,N
bÂ¯T1 Î±Â¯i. (49)
Assume w.l.o.g. that bÂ¯T1 Î±Â¯1 = maxi=1,...,N bÂ¯T1 Î±Â¯i. By the solution uniqueness in (48), it can be easily
verified that the equality in (49) holds if and only if Î½Ë†1 = xËœ[`] = Î±1. Next, consider j â‰¥ 2. Since bÂ¯Tj is
the jth row vector of the adjoint matrix of âˆ†(Î±1, . . . ,Î±jâˆ’1, Î½Ë†j , . . . , Î½Ë†N ) [38], it satisfies
bÂ¯Tj Î±Â¯i = 0, i = 1, . . . , j âˆ’ 1, (50)
due to adj(B)B = det(B)I where adj(B) is the adjoint matrix of a square matrix B. Hence, it follows
from (50) and the assumptions (A1) and (A2) that
max
n
bÂ¯Tj xÂ¯[n] = max
n
Nâˆ‘
i=j
si[n] bÂ¯
T
j Î±Â¯i â‰¤ max
iâˆˆ{j,...,N}
bÂ¯Tj Î±Â¯i. (51)
By employing the same proof above, we obtain, w.l.o.g., Î½Ë†j = xËœ[`] = Î±j . The proof of Lemma 3 is
therefore complete. 
D. Proof of Lemma 4
To prove Lemma 4, we first provide the following lemma:
Lemma 7. Let A âˆˆ RmÃ—p, B âˆˆ RmÃ—q and C = [A B]. The orthogonal complement projector of C,
denoted by PâŠ¥C, is identical to
PâŠ¥C = P
âŠ¥
BËœ
PâŠ¥A,
where BËœ = PâŠ¥AB.
Proof: Let D = [A BËœ]. As a basic result of matrix analysis, the range spaces of C and D are identical.
Thus, their orthogonal projectors are identical, i.e., PC = PD. Let us consider PD:
PD = D(D
TD)â€ DT = [A BËœ]
ï£®
ï£° ATA 0
0 BËœT BËœ
ï£¹
ï£»
â€  ï£®
ï£° AT
BËœT
ï£¹
ï£»
= A(ATA)â€ AT + BËœ(BËœT BËœ)â€ BËœT = PA +PBËœ. (52)
October 22, 2010 DRAFT
28
It can be easily verified that the equality above is achieved if and only if Î¸ = e` for any ` âˆˆ
argmaxn=1,...,L â€–xÂ¯[n]â€–2. Hence, the solution wË†1 = xÂ¯[`] is arrived. The proof for (29b) is the same
as above, and hence is omitted for brevity. 
F. Proof of Lemma 6
Let Î±Â¯i = [Î±Ti 1]T . By assumptions (A1) and (A2) and triangle inequality, we have
â€–xÂ¯[n]â€–2 =
âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
Nâˆ‘
i=1
si[n]Î±Â¯i
âˆ¥âˆ¥âˆ¥âˆ¥âˆ¥
2
â‰¤
Nâˆ‘
i=1
si[n]â€–Î±Â¯iâ€–2 â‰¤ max
i
â€–Î±Â¯iâ€–2, (60)
where the equality above holds if and only if n = ` such that xÂ¯[`] = Î±Â¯i for any i âˆˆ argmaxk=1,...,N â€–Î±Â¯kâ€–2.
Assume w.l.o.g. that â€–Î±Â¯1â€– = maxi=1,...,N â€–Î±Â¯iâ€–2. Hence, we can obtain wË†1 = Î±Â¯1. Next, consider j â‰¥ 2.
Suppose that WË†1:(jâˆ’1) = [Î±Â¯1, . . . , Î±Â¯jâˆ’1] are obtained. By â€–PâŠ¥WË†1:(jâˆ’1) Î±Â¯iâ€–2 = 0 for all i < j, it holds that
â€–PâŠ¥
WË†1:(jâˆ’1)
xÂ¯[n]â€–2 â‰¤
Nâˆ‘
i=j
si[n]â€–PâŠ¥WË†1:(jâˆ’1) Î±Â¯iâ€–2 â‰¤ maxiâˆˆ{j,...,N} â€–P
âŠ¥
WË†1:(jâˆ’1)
Î±Â¯iâ€–2, (61)
where the equality holds if and only if n = ` such that xÂ¯[`] = Î±Â¯i for any i âˆˆ argmaxk=j,...,N â€–PâŠ¥
WË†1:(jâˆ’1)
Î±Â¯kâ€–2.
Using the same argument as above, we get wË†j = Î±Â¯j w.l.o.g. The proof of Lemma 6 is therefore complete.

REFERENCES
[1] T. M. Lillesand, R. W. Kiefer, and J. W. Chipman, Remote Sensing and Image Interpretation, 2nd ed. New York: Wiley,
2004.
[2] J. A. Richards, â€œAnalysis of remotely sensed data: The formative decades and the future,â€ IEEE Trans. Geosci. Remote
Sens., vol. 43, no. 2, pp. 422â€“432, Mar. 2005.
[3] B. A. Campbell, Radar Remote Sensing of Planetary Surfaces. New York: Cambridge University Press, 2002.
[4] R. N. Clark, G. A. Swayze, K. E. Livo, R. F. Kokaly, S. Sutley, J. B. Dalton, R. R. McDougal, and C. A. Gent, â€œImaging
spectroscopy: Earth and planetary remote sensing with the USGS tetracorder and expert systems,â€ Journal of Geophysical
Research, vol. 108, no. 12, pp. 5â€“44, Dec. 2003.
[5] G. Shaw and D. Manolakis, â€œSignal processing for hyperspectral image exploitation,â€ IEEE Signal Process. Mag., vol. 19,
no. 1, pp. 12â€“16, Jan. 2002.
[6] N. Keshava and J. Mustard, â€œSpectral unmixing,â€ IEEE Signal Process. Mag., vol. 19, no. 1, pp. 44â€“57, Jan. 2002.
[7] A. Plaza, P. Martinez, R. Perez, and J. Plaza, â€œA quantitative and comparative analysis of endmember extraction algorithms
from hyperspectral data,â€ IEEE Trans. Geosci. Remote Sens., vol. 42, no. 3, pp. 650â€“663, Mar. 2004.
[8] M. Parente and A. Plaza, â€œSurvey of geometric and statistical unmixing algorithms for hyperspectral images,â€ in Proc.
Second IEEE Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS),
Reykjavik, Iceland, June 14-16, 2010.
October 22, 2010 DRAFT
30
[28] A. Ambikapathi, T.-H. Chan, W.-K. Ma, and C.-Y. Chi, â€œA robust minimum-volume enclosing simplex algorithm for
hyperspectral unmixing,â€ in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, Dallas,
Texas, USA, Mar. 14-19, 2010, pp. 1202â€“1205.
[29] H. Akaike, â€œA new look at the statistical model identification,â€ IEEE Trans. Autom. Control, vol. 19, no. 6, pp. 716â€“723,
Dec. 1974.
[30] M. Wax and T. Kailath, â€œDetection of signals by information theoretic criteria,â€ IEEE Trans. Acoust., Speech, Signal
Process., vol. 33, no. 2, pp. 387â€“392, Apr. 1985.
[31] C.-I. Chang and Q. Du, â€œEstimation of number of spectrally distinct signal sources in hyperspectral imagery,â€ IEEE Trans.
Geosci. Remote Sens., vol. 42, no. 3, pp. 608â€“619, Mar. 2004.
[32] J. M. Bioucas-Dias and J. M. P. Nascimento, â€œHyperspectral subspace identification,â€ IEEE Trans. Geosci. Rem. Sens.,
vol. 46, no. 8, pp. 2435â€“2445, 2008.
[33] T.-H. Chan, W.-K. Ma, C.-Y. Chi, and Y. Wang, â€œA convex analysis framework for blind separation of non-negative
sources,â€ IEEE Trans. Signal Processing, vol. 56, no. 10, pp. 5120â€“5134, Oct. 2008.
[34] W.-K. Ma, T.-H. Chan, C.-Y. Chi, and Y. Wang, â€œConvex analysis for non-negative blind source separation with application
in imaging,â€ in Chapter 7, Convex Optimization in Signal Processing and Communications, Editors: D. P. Palomar and Y.
C. Eldar, UK: Cambridge University Press, 2010.
[35] J. W. Boardman, â€œAutomating spectral unmixing of AVIRIS data using convex geometry concepts,â€ in Proc. Summ. 4th
Annu. JPL Airborne Geosci. Workshop, vol. 1, Dec. 9-14, 1993, pp. 11â€“14.
[36] â€”â€”, â€œGeometric mixture analysis of imaging spectrometry data,â€ in Proc. IEEE International Geoscience and Remote
Sensing Symposium, vol. 4, Pasadena, CA, Aug. 8-12, 1994, pp. 2369â€“2371.
[37] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge Univ. Press, 2004.
[38] G. Strang, Linear Algebra and Its Applications, 4th ed. CA: Thomson, 2006.
[39] D. P. Bertsekas, Nonlinear Programming. MA: Athena Scientific, 1999.
[40] A. Zymnis, S.-J. Kim, J. Skaf, M. Parente, and S. Boyd, â€œHyperspectral image unmixing via alternating projected
subgradients,â€ in Proc. 41st Asilomar Conference on Signals, Systems, and Computers, Pacific Grove, CA, Nov. 4-7,
2007.
[41] H. W. Kuhn, â€œThe Hungarian method for the assignment method,â€ Naval Research Logistics Quarterly, vol. 2, pp. 83â€“97,
1955.
[42] Tech. Rep., available online: http://speclab.cr.usgs.gov/cuprite.html.
[43] F.-Y. Wang, C.-Y. Chi, T.-H. Chan, and Y. Wang, â€œNon-negative least-correlated component analysis for separation of
dependent sources by volume maximization,â€ IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 5, pp.
875â€“888, May 2010.
October 22, 2010 DRAFT
R
N is the nth abundance vector comprising N fractional abundances
and L is the total number of observed pixel vectors.
EE algorithms aim to estimate the endmember signature matrix
A from the observed hyperspectral pixel vectors (or simply pixels)
x[1], . . . ,x[L], assuming that N is known a priori. The following
are the general assumptions in HU:
(A1) (Non-negativity condition) si[n] â‰¥ 0 âˆ€i, n.
(A2) (Full additivity condition) âˆ‘N
i=1 si[n] = 1 âˆ€n.
(A3) min{L,M} â‰¥ N and A is of full column rank.
(A4) (Pure pixel assumption) There exists an index set
{l1, l2, . . . , lN}, such that x[li] = ai, for i = 1, . . . , N .
3. DIMENSION REDUCTION
Like many other HU algorithms [1], we begin with dimension reduc-
tion of the observed pixels. The affine set fitting procedure in [10] is
utilized for dimension reduction. The dimension-reduced pixel vec-
tors xËœ[n] are obtained by the following affine transformation of x[n]:
xËœ[n] = CT (x[n]âˆ’ d) âˆˆ RNâˆ’1, (2)
where (C,d) is the affine set fitting solution given by
d =
1
L
Lâˆ‘
n=1
x[n], (3)
C = [ q1(UU
T ), q2(UU
T ), . . . , qNâˆ’1(UU
T ) ], (4)
in which U = [ x[1]âˆ’ d, . . . ,x[L]âˆ’ d ] âˆˆ RMÃ—L, and qi(UUT )
denotes the unit-norm eigenvector associated with the ith principal
eigenvalue of the matrix UUT . Further, due to (A2), and by substi-
tuting the signal model (1) into (2), we have
xËœ[n] =
Nâˆ‘
j=1
sj [n]Î±j , (5)
where
Î±j = C
T (aj âˆ’ d) âˆˆ R
Nâˆ’1 (6)
is the jth dimension-reduced endmember, by finding which, the cor-
responding aj can be obtained by aj = CÎ±j + d,âˆ€j [10]. Also, it
follows from (5) that under (A4),
xËœ[li] = Î±i, âˆ€i, (7)
and xËœ[n] lies in the simplex [11] formed by Î±1, . . . ,Î±N [10].
4. SIMPLEX ESTIMATION BY PROJECTION
In this section, let us present the new EE algorithm, SIMPLE-Pro.
We begin by considering the p-norm of the dimension-reduced data
cloud XËœ = [ xËœ[1], . . . , xËœ[L] ]. By the triangle inequality, (A1), and
(A2), one can infer from (5) that for all n,
â€–xËœ[n]â€–p â‰¤
Nâˆ‘
i=1
si[n]â€–Î±iâ€–p â‰¤ max
i=1,...,N
{â€–Î±iâ€–p}, (8)
where p â‰¥ 1. The inequality in (8) holds with equality if and only if
n = li (a pure pixel index) for any i âˆˆ arg maxk=1,...,N{â€–Î±kâ€–p}
(by (7)). Thus, a dimension-reduced endmember can be identified as
stated in the following lemma:
Lemma 1. Under (A1)-(A4), a dimension-reduced endmember can
be identified by
Î±i = xËœ[li], (9)
for any li âˆˆ arg maxn=1,...,L{â€–xËœ[n]â€–p}.
Î±1
(0, 0)
Î±2
d?
xËœ[n]
yTd? = 0
xËœ[n]Td?
Î±3 = xËœ[l3], l3 âˆˆ arg minn{xËœ[n]
Td?}
Fig. 1. Illustration of SIMPLE-Pro for N = 3. Assume that Î±1 and
Î±2 have been found. The vector d? is orthogonal to the affine hull of
Î±1 and Î±2 and the third endmember is found as Î±3 = xËœ[l3], where
l3 âˆˆ arg minn=1,...,L{xËœ[n]
Td?}.
Now, suppose that the dimension-reduced endmembers
Î±1, . . . ,Î±k (where k < N ) are already identified. To find the other
endmembers, we consider the following optimization problem:
min
dâˆˆRNâˆ’1
â€–dâ€–22 (10)
s.t. d âˆˆ aff{Î±1, . . . ,Î±k},
where aff{Î±1, . . . ,Î±k} is the affine hull of {Î±1, . . . ,Î±k}, defined
as [11]
aff{Î±1, . . . ,Î±k} =
{
x =
kâˆ‘
i=1
Î¸iÎ±i
âˆ£âˆ£âˆ£âˆ£ 1Tk Î¸ = 1, Î¸ âˆˆ Rk
}
, (11)
in which Î¸ = [Î¸1, . . . , Î¸k]T . Note that (10) is a quadratic convex
problem, and it can be easily shown that its closed-form solution is:
d
? = (Iâˆ’BBâ€ )Î±k = P
âŠ¥
BÎ±k, (12)
where B = [Î±1 âˆ’Î±k, . . . ,Î±kâˆ’1 âˆ’Î±k] âˆˆ R(Nâˆ’1)Ã—(kâˆ’1) and PâŠ¥B
is the orthogonal complement projector of B. By projecting all the
dimension-reduced data onto d?, and by (A1) and (A2), we have
xËœ[n]Td? =
Nâˆ‘
i=1
si[n]Î±
T
i d
? â‰¥ min
i=1,...,N
{Î±Ti d
?}, (13)
and the inequality in (13) holds with equality if and only if n = lz
for any z âˆˆ arg mini{Î±Ti d?}. The (k + 1)th dimension-reduced
endmember can then be found as Î±k+1 = xËœ[lz] where
lz âˆˆ arg min
n=1,...,L
{xËœ[n]Td?}. (14)
The above procedure is illustrated in Figure 1, for the N = 3 case.
Next, in the following lemma we show that xËœ[lz] is different from
those endmember estimates already found.
Lemma 2. Suppose that {Î±1, . . . ,Î±k} is the set of endmembers
already found and d? is obtained by (12). Then, under (A1)-(A4),
xËœ[lz] âˆˆ {Î±k+1, ...,Î±N} for any lz âˆˆ arg minn=1,...,L{xËœ[n]Td?}.
Proof: It is well known that the projector PâŠ¥B satisfies PâŠ¥BB = 0,
implying
P
âŠ¥
B(Î±q âˆ’Î±k) = 0, q = 1, . . . , k âˆ’ 1. (15)
Table 3. Average Ï†en (degrees) and average computation time Tc (secs) over the various EE methods for different purity levels (Ï) and SNRs.
Methods Ï
Ï†en (degrees)
Tc (secs)SNR (dB)
0 5 10 15 20 25 30 35 40
N-FINDR
0.6 19.61 14.95 11.13 9.14 8.41 8.42 8.57 8.53 8.60
0.8 19.08 14.49 10.39 8.03 6.51 6.31 5.31 5.25 5.25 3.61
1 19.06 14.42 10.55 8.02 5.47 2.93 1.38 0.85 0.53
VCA
0.6 19.34 14.75 10.84 8.74 8.00 7.71 8.32 9.13 9.19
0.8 18.94 14.14 10.24 7.87 6.61 5.92 8.01 7.60 7.12 0.66
1 18.83 14.14 10.15 8.03 5.86 3.72 8.49 7.56 6.25
SGA
0.6 18.77 14.16 10.68 8.69 7.83 7.74 7.72 7.58 7.61
0.8 18.30 13.89 10.12 7.77 6.72 6.09 5.62 5.49 5.37 0.35
1 18.40 13.93 10.20 8.02 6.09 3.37 1.24 0.75 0.41
SIMPLE-Pro
0.6 19.09 14.27 10.58 8.63 7.89 7.92 7.83 7.65 7.63
0.8 18.49 13.63 10.18 7.78 6.50 6.25 5.74 5.67 5.48 0.21
1 18.34 13.74 10.07 7.99 5.93 3.63 1.32 0.79 0.47
0.6 19.98 14.77 10.99 8.45 7.92 7.79 7.66 7.77 7.74
TRI-P 0.8 19.42 14.35 10.11 7.79 6.41 5.70 5.14 4.78 4.54 0.22
(p = 2) 1 19.40 14.53 10.25 7.69 5.68 3.19 1.13 0.63 0.36
6. SIMULATIONS AND CONCLUSIONS
The performance evaluation of the proposed two EE algorithms,
SIMPLE-Pro and TRI-P (with p = 2), by simulation is presented
in this section. Other EE algorithms that are compared are N-
FINDR, VCA, and SGA. The root-mean-square (rms) spectral angle
Ï†en [7,10] between the true and the estimated endmember signatures
is used as the performance index. In the simulations the number of
endmembers is 12 (N = 12) and the number of observed pixels is
1000 (L = 1000). The endmember signatures are chosen from the
USGS library [12], and it has 224 bands (M = 224). In each run,
1000 noise-free observed pixel vectors were synthetically generated
following the signal model in (1), and the abundance vectors s[n]
were generated following Dirichlet distribution (as in [7]), for dif-
ferent purity levels Ï [10]. The noisy data were generated by adding
independent and identically distributed zero-mean Gaussian noise to
the noise-free data for different signal-to-noise ratios (SNRs), where
SNR=
âˆ‘L
n=1 â€–x[n]â€–
2
2/MLÏƒ
2 and Ïƒ2 is the noise variance. For
each scenario 100 independent runs are performed and the average
Ï†en (over 100 runs) and the average computation time Tc (over all
the scenarios under consideration) of each algorithm (implemented
in Matlab R2008a and running in a desktop computer equipped with
Dual Core CPU 2.80 GHz, 2 GB memory) are calculated, for differ-
ent purity levels (Ï = 0.6, 0.8, 1) and SNRs ranging from 0 dB to
40 dB, in steps of 5 dB.
The obtained simulation results are shown in Table 3. The bold-
faced numbers in Table 3 correspond to the minimum rms spectral
angle for a specific pair of (Ï, SNR), over all the algorithms under
test. It can be observed from Table 3 that although the performances
of all the EE algorithms are competitive, SIMPLE-Pro and TRI-P
still outperform the other algorithms in most of the scenarios. On
the other hand, the average computation times (Tcs) for SIMPLE-
Pro and TRI-P are almost the same and they are about 17 times, 3
times and 1.5 times smaller than that of N-FINDR, VCA, and SGA,
respectively. These simulation results demonstrate the efficacy and
computational efficiency of the proposed two EE algorithms.
In summary, we have presented two effective and computation-
ally efficient EE algorithms namely, SIMPLE-Pro and TRI-P and
we have theoretically proved their endmember identifiability under
the assumptions (A1)-(A4). It is shown via simulations that either
SIMPLE-Pro or TRI-P yields the best performance in most of the
scenarios under consideration, and the computational complexities
of SIMPLE-Pro and TRI-P are lower than some existing benchmark
EE algorithms. The application of SIMPLE-Pro and TRI-P algo-
rithms to real hyperspectral data is currently under investigation.
7. REFERENCES
[1] N. Keshava and J. Mustard, â€œSpectral unmixing,â€ IEEE Signal Process.
Mag., vol. 19, no. 1, pp. 44-57, Jan. 2002.
[2] J. W. Boardman, F. A. Kruse, and R. O. Green, â€œMapping target signa-
tures via partial unmixing of AVIRIS data,â€ in Proc. Summ. JPL Air-
borne Earth Sci. Workshop, Pasadena, CA, Dec. 9-14, 1995, pp. 23-26.
[3] M. E. Winter, â€œN-findr: An algorithm for fast autonomous spectral
end-member determination in hyperspectral data,â€ in Proc. SPIE Conf.
Imaging Spectrometry, Pasadena, CA, Oct. 1999, pp. 266-275.
[4] A. Ifarraguerri and C.-I. Chang, â€œMultispectral and hyperspectral im-
age analysis with convex cones,â€ IEEE Trans. Geosci. Remote Sens.,
vol. 37, no. 2, pp. 756-770, Mar. 1999.
[5] C. I. Chang, C. C. Wu, W. M. Liu, and Y. C. Ouyang, â€œA new grow-
ing method for simplex-based endmember extraction algorithm,â€ IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 10, pp. 2804-2819, Oct. 2006.
[6] C. I. Chang, C. C. Wu, C. S. Lo, and M. L. Chang, â€œReal-time simplex
growing algorithms for hyperspectral endmember extraction,â€ IEEE
Trans. Geosci. Remote Sens., vol. 48, no. 4, pp. 1834-1850, Apr. 2010.
[7] J. M. P. Nascimento and J. M. B. Dias, â€œVertex component analysis:
A fast algorithm to unmix hyperspectral data,â€ IEEE Trans. Geosci.
Remote Sens., vol. 43, no. 4, pp. 898-910, Apr. 2005.
[8] T.-H. Chan, C.-Y. Chi, W.-K. Ma, and A. Ambikapathi, â€œHyperspec-
tral unmixing from a convex analysis and optimization perspective,â€ in
Proc. First IEEE WHISPERS, Grenoble, France, Aug. 26-28, 2009.
[9] D. Heinz and C.-I. Chang, â€œFully constrained least squares linear
mixture analysis for material quantification in hyperspectral imagery,â€
IEEE Trans. Geosci. Remote Sens., vol. 39, no. 3, pp. 529-545, 2001.
[10] T.-H. Chan, C.-Y. Chi, Y.-M. Huang, and W.-K. Ma, â€œA convex anal-
ysis based minimum-volume enclosing simplex algorithm for hyper-
spectral unmixing,â€ IEEE Trans. Signal Processing, vol. 57, no. 11,
pp. 4418-4432, Nov. 2009.
[11] S. Boyd and L. Vandenberghe, Convex Optimization, UK: Cambridge
Univ. Press, 2004.
[12] Tech. Rep., Available online:
http://speclab.cr.usgs.gov/cuprite.html.
MA et al.: A KHATRIâ€“RAO SUBSPACE APPROACH 2169
In this paper, we propose a subspace approach to DF-QSS.
Our work is based on the assumptions that:
i) the sources are mutually uncorrelated and wide-sense
quasi-stationary;
ii) the noise is wide-sense stationary, but with an unknown,
possibly nonwhite, spatial covariance; and
iii) the sensor array exhibits a uniform linear structure.
Our idea is to exploit the subspace characteristics of the
time-variant second-order statistics (SOSs) of the quasi-sta-
tionary source signals. Since this development involves
subspace formed by the self Khatriâ€“Rao (KR) product of the
array response, we call the proposed approach the KR subspace
approach. We should point out that the established KR sub-
space approach has its criterion different from the LSF and JD
criteria used in BSS-QSS. A meaningful result with the KR
subspace formulation is that for a physically underdetermined
problem, the DOA estimation problem under the KR subspace
can be â€˜virtually overdeterminedâ€™ under some conditions. In
essence, our identifiability analysis proves that for a sensor
array of elements, the degree of freedom under the KR sub-
space formulation is . This translates into an advantage
that a KR subspace method can unambiguously identify up to
sources. This is a significant improvement, compared
to the conventional subspace SOS-based approach where the
limit is sources. The KR subspace approach is also
convenient to implement, since available subspace algorithms
or concepts (e.g., MUSIC1) can be carried forward at a certain
point of the KR formulation. In addition, the KR subspace
formulation provides a convenient way of removing the spatial
noise covariance matrix from the received signal SOSs, without
knowing the noise covariance. Thus, the KR subspace approach
is effective against spatially colored noise with unknown spatial
covariance. Moreover, we will propose a novel idea where the
KR problem dimension is reduced prior to the subspace pro-
cessing. This dimension reduction idea helps save complexity
in implementations, and it does so without losing the effective
degree of freedom of the virtual array.
Since the proposed KR subspace approach enables underde-
termined DOA estimation of quasi-stationary signals, it would
be appropriate to mention other existing underdetermined DOA
estimation approaches. Underdetermined DOA estimation is
possible when the source signals are non-Gaussian stationary,
and that necessitates the use of higher order statistics (HOSs).
In the HOS-DF approach [21]â€“[24], it has been discovered
that the higher order cumulants of the observed signals provide
a virtual array structure that coincides with the one found in
this paper when such cumulants are of fourth order. Neverthe-
less, the DF-QSS and HOS-DF approaches follow different
formulations and they target different signals and applications.
For speech signals, our simulation results will show that the
proposed DF-QSS approach yields better DOA estimation ac-
curacies than the HOS-DF approach (though it is fair to say that
we would expect to see the opposite for strongly non-Gaussian,
weakly quasi-stationary signals).
The development in this paper concentrates on the narrow-
band DOA estimation scenario. Using the frequency-domain
1MUSIC stands for multiple signal classification.
signal subspace processing methods [3]â€“[5], one can readily ex-
tend the proposed KR subspace approach to the wideband DOA
estimation scenario. The wideband extension is also consid-
ered and tested, where the simulation results will illustrate that
the proposed wideband KR subspace algorithm is successful in
identifying DOAs of real speech signals under underdetermined
environments.
Notations: We denote matrices and vectors by boldfaced cap-
ital letters and lower-case letters, respectively. The space of
complex (real) -dimensional vectors is denoted by .
Likewise, the space of complex (real) matrices is de-
noted by . The th element of a vector
is denoted by . The th column of a matrix is
denoted by either or . The superscripts â€œ â€ and
â€œ â€ stand for the transpose and conjugate transpose, respec-
tively. For a given vector denotes its Euclidean
norm. Its matrix counterpart, namely the Frobenius norm, is also
denoted by . The expectation operator is denoted by .
The identity matrix is denoted by . The notation
stands for the all-one vector.
For a given matrix , the range space and the
orthogonal complement subspace are denoted by and
, respectively. The notation stands for vectoriza-
tion; i.e., if then .
For a given vector means a diagonal matrix
with the diagonals given by .
II. PROBLEM STATEMENT
We will first describe the signal model and assumptions of
the DOA estimation problem considered in this paper. Then, we
will study the second-order statistics model of the problem.
A. Signal Model and Assumptions
Consider a scenario in which a number of narrowband
far-field sources are observed by a sensor array of elements.
The array is assumed to have a uniform linear array (ULA)
structure. We denote by the observed signal of the th
sensor, and the signal emitted by the th source. By letting
and ,
the received signal is modeled as
(1)
Here, represents the spatial noise,
is the array response matrix
where is the direction of arrival (DOA) of
source , and
(2)
denotes the steering vector function with and being the in-
tersensor spacing and the signal wavelength, respectively. Some
common assumptions are made as follows:
A1) The source signals , are mutually
uncorrelated and have zero-mean.
A2) The source DOAs , are distinct to one
another, i.e., for all .
Authorized licensed use limited to: National Tsing Hua University. Downloaded on August 10,2010 at 07:52:04 UTC from IEEE Xplore.  Restrictions apply. 
MA et al.: A KHATRIâ€“RAO SUBSPACE APPROACH 2171
It is interesting to note from (10) or (11) that is reminiscent
of an array signal model where is virtually
the array response matrix and the source signal vector. The
virtual array dimension, given by , is greater than the phys-
ical array dimension for , and that essentially provides
us with the capability of processing cases where there are less
sensors than sources. This will be shown in the ensuing devel-
opment.
We should note that each column of in (12) describes the
powers of the respective source signal with respect to frames, or
simply speaking, the long-term power profile over time. Let us
assume the following:
A5) The matrix is of full column
rank.
Assumption A5) physically implies the followings: First, the
source power distributions over the time frames (or the columns
of ) are different so that can maintain a full column rank
condition. Second, any linear combination of the sources cannot
result in a WSS source (i.e., for any coefficients ,
the signal cannot be WSS), otherwise can
be a linear combination of the columns of which violates
A5). While the necessary condition for fulfilling A5) is to have
, in practice it would be desirable to use a much
larger so as to obtain sufficient long-term source power vari-
ations to well satisfy A5). Certainly, availability of large
depends on applications. For example, for speech applications
where is generally proportional to a physical time duration of
20 to 25 ms, we can obtain more than 40 frames (or )
in 1 s. Moreover, increasing the value of would generally
be useful in improving the conditioning of . Hence, in the
presence of estimation errors in the local covariances , em-
ploying a larger would be helpful in suppressing the subse-
quent error effects.
Under A5), we can eliminate the unknown noise covariance
effectively and easily. The noise covariance elimination is done
by denoting an orthogonal complement projector
, and then by performing a projection
(13)
Under A5), we have . In other
words, the noise covariance elimination operation does not
damage the rank condition of the covariance model.
Now consider the subspaces of . For ease of exposition
of idea, we assume for the time being that is of full
column rank. We will soon provide conditions under which this
assumption is valid. When both and in (13)
have full column rank, we can have [27]
(14)
Denote the singular value decomposition (SVD) of by
(15)
where and are the left and right
singular matrices associated with the nonzero singular values,
respectively, and are
the left and right singular matrices associated with the zero sin-
gular values, respectively, and is a diagonal matrix
whose diagonals contain the nonzero singular values. Based on
the standard SVD result that
(16)
we know that the source DOAs satisfy
(17)
for . We hence propose the following KR subspace
criterion for DOA estimation of quasi-stationary sources
(18)
To satisfy the KR subspace criterion in (18), we can use the
idea of MUSIC in the conventional subspace-based DOA esti-
mation approach. The further details will be considered in the
next subsection.
Like the development in the subspace discipline, it is crucial
to determine conditions under which (18) is satisfied only if
is a true DOA, viz., the identifiability conditions. The following
two propositions provide the key results of the theoretical iden-
tifiability of the KR subspace approach.
Proposition 1: Under A2), the sufficient and necessary con-
dition for the virtual array response matrix to yield
full column rank (or simply ) is when
Proposition 2: Assume that A1)â€“A5) hold. The KR subspace
criterion in (18) is achieved by any one of the true angles
, if and only if
(19)
The proofs of Propositions 1 and 2 are given in Appendix B.
Simply speaking, the idea behind the proofs is to use the KR
product krank property in Property 2, and to have explicit ex-
position of the structures of . Proposition 1 justifies our
assumption in the KR subspace development above, and Propo-
sition 2 provides the identifiability condition of the KR subspace
criterion. In particular, Proposition 2 gives the appealing impli-
cation that DOA estimation can be done for underdetermined
cases under the KR subspace framework.
C. KR-Based DOA Estimators With Dimension Reduction
We could develop a KR-based DOA estimator by directly ap-
plying a subspace method (say, MUSIC) to the KR subspace
criterion (18). But, prior to applying a subspace method, we can
reduce the problem dimension as hinted in the proof of Propo-
sitions 1 and 2 in Appendix B.
Authorized licensed use limited to: National Tsing Hua University. Downloaded on August 10,2010 at 07:52:04 UTC from IEEE Xplore.  Restrictions apply. 
MA et al.: A KHATRIâ€“RAO SUBSPACE APPROACH 2173
TABLE II
GENERATION OF SYNTHETIC QUASI-STATIONARY SIGNALS
where and denote the true and estimated DOAs, respec-
tively. The first three simulation examples are based on the stan-
dard narrowband scenario, where the performance of the KR
subspace methods is examined under various conditions. We
also compared the KR subspace methods with some other ex-
isting methods, such as the standard MUSIC algorithm, and
a fourth-order cumulant based HOS-MUSIC algorithm called
4-MUSIC [21]â€“[23].
In the first three narrowband simulation examples, the quasi-
stationary source signals were synthetically generated by
a random generation procedure given in Table II. This proce-
dure generates a locally stationary zero-mean complex Lapla-
cian process, with the variances randomly varying from one
frame to another. Moreover, the duration of each local time
frame is randomly drawn following a uniform distribution
on . The purpose of doing so is to simulate a more
realistic situation where the local stationary periods are uncer-
tain and varying; e.g., in speech. Strictly speaking, such signals
violate A4) which assumes all local stationary intervals to be the
same and known. Nevertheless, the simulation results to be pre-
sented soon will show that the KR subspace approach is not too
sensitive to the effects of uncertain local stationary intervals.
In the last simulation example, we simulate a realistic wide-
band microphone array processing system with real speech sig-
nals employed as the sources.
A. Simulation Example 1: Underdetermined Narrowband
DOA Estimation
We consider a narrowband, underdetermined case where
. The true DOAs are
. The sensor array is uni-
form linear with . The SNR is 10 dB. The spatial
noise is zero-mean, uniformly white complex Gaussian.
The synthetic signal generation procedure in Table II is used
to generate the source signals , with allowable range of
the frame periods . In Fig. 1 we
illustrate a segment of the synthetic quasi-stationary signals.
Notice that the dashed lines in the figure mark the actual local
stationary intervals of the signals. As we can see, the frame
intervals of the 6 source signals are not uniform and not syn-
chronized. We apply the KR subspace methods by choosing
a fixed frame period of , which is approximately the
mean of the actual frame periods. The number of frames is set
to . Fig. 2(a) shows one realization of the KR-MUSIC
DOA spectrum. As seen, the peaks of the spectra are in good
agreement with the true DOAs. This also demonstrates that the
Fig. 1. Illustration of a segment of synthetic quasi-stationary source signals
   . The dashed lines mark the local stationary intervals. Due to space limit,
only the real parts of the signals are plotted. The appearance of the imaginary
signal components is similar to that of the real.
Fig. 2. DOA spectra of the various algorithms in the narrowband underdeter-
mined case. The dashed lines mark the true DOA positions. (a) KR-MUSIC. (b)
4-MUSIC.
KR subspace approach exhibits robustness against uncertainties
and asynchronism of the actual locally stationary intervals.
Authorized licensed use limited to: National Tsing Hua University. Downloaded on August 10,2010 at 07:52:04 UTC from IEEE Xplore.  Restrictions apply. 
MA et al.: A KHATRIâ€“RAO SUBSPACE APPROACH 2175
Fig. 4. RMS angle error performance of the various algorithms when the
sources are locally complex Gaussian.
in Fig. 3(a), we found that the RMS angle errors of KR-MUSIC
and KR-Capon do not change with the distributions. This veri-
fies the distribution insensitivity of the KR-based methods.
C. Simulation Example 3: Narrowband DOA Estimation in the
Presence of Spatially Nonuniform and Nonwhite Noise
This simulation example considers a narrowband overdeter-
mined case where , and
where the true DOAs are given by
. We tried two kinds of noise models. The first is a spatially
nonuniform white complex Gaussian noise with covariance
(26)
With the other simulation settings being the same as Simula-
tion Example 1, we performed a Monte Carlo simulation for
KR-MUSIC, 4-MUSIC, and the conventional MUSIC algorithm
which can be applied in this overdetermined example. Fig. 5(a)
shows the performance of the various algorithms. We observe
that at high SNRs where the noise covariance becomes neg-
ligible, it is the conventional MUSIC algorithm that prevails.
However, at low SNRs, KR-MUSIC gives lower RMS estimation
errors than the conventional MUSIC. This verifies that the pro-
posed KR subspace approach is immune to the unknown noise
covariance effects. In fact, the conventional MUSIC is known to
be sensitive to colored or nonuniform spatial noise covariance
effects [28]. Fig. 5(a) also shows that 4-MUSIC is robust against
Gaussian noise with unknown noise covariance.
The second noise model we tested is a uniform, spatially col-
ored complex Gaussian noise with the th entry of the co-
variance given by
(27)
The results are shown in Fig. 5(b). We see that the performance
behaviors are the same as those of the previous noise model.
D. Simulation Example 4: Narrowband DOA Estimation of
Two Closely Spaced Sources
This example examines the performance of KR-MUSIC
when dealing with two closely spaced sources. We set
Fig. 5. RMS angle error performance of the various algorithms in the presence
of nonuniform or colored noise. (a) Spatially nonuniform white noise. (b) Spa-
tially colored noise.
Fig. 6. RMS angle error performance for two closely spaced sources.
. We tried two different settings of the
DOAs, namely and .
The rest of the simulation settings are the same as Simulation
Example 1. The results are shown in Fig. 6. For the very closely
spaced case , we observe that KR-MUSIC
Authorized licensed use limited to: National Tsing Hua University. Downloaded on August 10,2010 at 07:52:04 UTC from IEEE Xplore.  Restrictions apply. 
MA et al.: A KHATRIâ€“RAO SUBSPACE APPROACH 2177
Fig. 8. DOA spectra of KR-MUSIC and 4-MUSIC in the wideband real-speech
case.        . The gray dashed lines mark the true DOA positions.
(a) KR-MUSIC, with ISSM. (b) 4-MUSIC, with ISSM.
V. CONCLUSION AND DISCUSSION
This paper has addressed the DOA estimation problem of mu-
tually uncorrelated wide-sense quasi-stationary signals in the
presence of wide-sense stationary noise and with uniform linear
arrays. We have established a KR subspace approach that ju-
diciously utilizes the long-term time-variant characteristics of
quasi-stationary SOSs to achieve two advantages. First, the KR
subspace approach can be applied to cases where there are less
sensors than sources. Our theoretical identifiability analysis has
revealed that the proposed KR subspace approach can uniquely
identify the source DOAs when
where and denote the numbers of sources and sensors, re-
spectively. In addition, the approach can effectively cope with
the effects of wide-sense stationary noise with unknown covari-
ance. We have used synthetic signals and real speech signals
to perform a number of simulations, where the effectiveness of
the KR subspace approach has been successfully demonstrated
under various problem settings.
In essence, this paper has revealed how quasi-stationarity can
be utilized to provide advantages in DOA estimation. As a future
Fig. 9. DOA spectra of KR-MUSIC and 4-MUSIC in the wideband real-speech
case.       . The gray dashed lines mark the true DOA positions.
(a) KR-MUSIC, with ISSM. (b) 4-MUSIC, with ISSM.
direction, it would be interesting to study how quasi-stationarity
may bring a difference to DOA estimation performance limits;
e.g., by exploring the CramÃ©râ€“Rao lower bound in the quasi-sta-
tionary case. In addition, it would be interesting to perform a
theoretical analysis on the error performance of the KR sub-
space methods.
APPENDIX
A. Proof of Property 1
The matrix product can be written as
(33)
Applying vectorization to (33) and using (7), we get
(34)
Authorized licensed use limited to: National Tsing Hua University. Downloaded on August 10,2010 at 07:52:04 UTC from IEEE Xplore.  Restrictions apply. 
MA et al.: A KHATRIâ€“RAO SUBSPACE APPROACH 2179
has linearly dependent columns. But, by Lemma 1, (45) is lin-
early independent if and only if . This completes
the proof of Proposition 2.
ACKNOWLEDGMENT
The authors would like to thank Prof. Kon Max Wong at Mc-
Master University, Canada, for his valuable advice that have
helped us improve this manuscript significantly. Many thanks
also go to the anonymous reviewers, who gave useful and in-
sightful advice that have enabled us to further enhance the paper
in many different ways.
REFERENCES
[1] R. O. Schmidt, â€œMultiple emitter location and signal parameter estima-
tion,â€ IEEE Trans. Antennas Propag., vol. 34, no. 3, pp. 276â€“280, Mar.
1986.
[2] H. Krim and M. Viberg, â€œTwo decades of array signal processing re-
search: The parametric approach,â€ IEEE Signal Process. Mag., vol. 13,
no. 4, pp. 67â€“94, Jul. 1996.
[3] H. Wang and M. Kaveh, â€œCoherent signal-subspace processing for the
detection and estimation of angles of arrival of multiple wide-band
sources,â€ IEEE Trans. Acoust., Speech, Signal Process., vol. 33, no.
4, pp. 823â€“831, Aug. 1985.
[4] E. D. Di Claudio and R. Parisi, â€œWAVES: Weighted average of signal
subspaces for robust wideband direction finding,â€ IEEE Trans. Signal
Process., vol. 49, no. 10, pp. 2179â€“2191, Oct. 2001.
[5] Y.-S. Yoon, L. M. Kaplan, and J. H. McClellan, â€œTOPS: New DOA
estimator for wideband signals,â€ IEEE Trans. Signal Process., vol. 54,
no. 6, pt. 1, pp. 1977â€“1989, Jun. 2006.
[6] C. Kwan, K. C. Ho, G. Mei, Y. Li, Z. Ren, R. Xu, Y. Zhang, D. Lao, M.
Stevenson, V. Stanford, and C. Rochet, â€œAn automated acoustic system
to monitor and classify birds,â€ EURASIP J. Appl. Signal Process., vol.
2006, pp. 1â€“19, 2006, Article ID 96706.
[7] F. Asano, S. Hayamizu, T. Yamada, and S. Nakamura, â€œSpeech en-
hancement based on the subspace method,â€ IEEE Trans. Speech Audio
Process., vol. 8, no. 5, pp. 497â€“507, Sep. 2000.
[8] L. Parra and C. Spence, â€œConvolutive blind separation of non-sta-
tionary sources,â€ IEEE Trans. Speech Audio Process., vol. 8, no. 3, pp.
320â€“327, May 2000.
[9] D.-T. Pham and J.-F. Cardoso, â€œBlind separation of instantaneous mix-
tures of nonstationary sources,â€ IEEE Trans. Signal Process., vol. 49,
no. 9, pp. 1837â€“1848, Sep. 2001.
[10] K. Rahbar, J. P. Reilly, and J. H. Manton, â€œBlind identification
of MIMO FIR systems driven by quasistationary sources using
second-order statistics: A frequency domain approach,â€ IEEE Trans.
Signal Process., vol. 52, no. 2, pp. 406â€“417, Feb. 2004.
[11] K. Rahbar, J. P. Reilly, and J. H. Manton, â€œA frequency domain method
for blind source separation of convolutive audio mixtures,â€ IEEE Trans.
Signal Process., vol. 13, no. 5, pp. 832â€“844, Sep. 2005.
[12] R. K. Olsson and L. K. Hansen, â€œBlind separation of more sources than
sensors in convolutive mixtures,â€ in Proc. ICASSP 2006, May 2006, pp.
V657â€“V660.
[13] K. N. Mokios, N. D. Sidiropoulos, and A. Potamianos, â€œBlind speech
separation using parafac analysis and integer least squares,â€ in Proc.
ICASSP 2006, May 2006, pp. V73â€“V76.
[14] K. N. Mokios, A. Potamianos, and N. D. Sidiropoulos, â€œOn the effec-
tiveness of PARAFAC-based estimation for blind speech separation,â€
in Proc. ICASSP 2008, Mar. 2008, pp. 153â€“156.
[15] R. A. Harshman and M. E. Lundy, â€œThe PARAFAC model for three-
way factor analysis and multidimensional scaling,â€ in Research
Methods for Multimode Data Analysis, H. G. Law, C. W. Snyder, Jr.,
J. Hattie, and R. P. McDonald, Eds. New York: Praeger, 1984, pp.
122â€“215.
[16] N. D. Sidiropoulos and R. Bro, â€œOn the uniqueness of multilinear
decomposition of N-way arrays,â€ J. Chemomet., vol. 14, no. 3, pp.
229â€“239, May 2000.
[17] N. D. Sidiropoulos, R. Bro, and G. B. Giannakis, â€œParallel factor anal-
ysis in sensor array processing,â€ IEEE Trans. Signal Process., vol. 48,
no. 8, pp. 2377â€“2388, Aug. 2000.
[18] A. Yeredor, â€œNon-orthogonal joint diagonalization in the least-squares
sense with application in blind source separation,â€ IEEE Trans. Signal
Process., vol. 50, no. 7, pp. 1545â€“1553, Jul. 2002.
[19] A. Ziehe, P. Laskov, and G. Nolte, â€œA fast algorithm for joint diago-
nalization with non-orthogonal transformations and its application to
blind source separation,â€ J. Mach. Learn. Res., vol. 5, pp. 801â€“818,
2004.
[20] R. Vollgraf and K. Obermayer, â€œQuadratic optimization for simulta-
neous matrix diagonalization,â€ IEEE Trans. Signal Process., vol. 54,
no. 9, pp. 3270â€“3278, Sep. 2006.
[21] B. Porat and B. Friedlander, â€œDirection finding algorithms based on
high-order statistics,â€ IEEE Trans. Signal Process., vol. 39, no. 9, pp.
2016â€“2023, Sep. 1991.
[22] M. Dogan and J. Mendel, â€œApplications of cumulants to array pro-
cessing. I: Aperture extension and array calibration,â€ IEEE Trans.
Signal Process., vol. 43, no. 5, pp. 1200â€“1216, May 1995.
[23] P. Chevalier and A. Ferreol, â€œOn the virtual array concept for the
fourth-order direction finding problem,â€ IEEE Trans. Signal Process.,
vol. 47, no. 9, pp. 2592â€“2595, Sep. 1999.
[24] P. Chevalier, L. Albera, A. Ferreol, and P. Comon, â€œOn the virtual
array concept for higher order array processing,â€ IEEE Trans. Signal
Process., vol. 53, no. 4, pp. 1254â€“1271, Apr. 2005.
[25] C. G. Khatri and C. R. Rao, â€œSolutions to some functional equations
and their applications to characterization of probability distributions,â€
The Indian J. Stat. Series A, vol. 30, no. 2, pp. 167â€“180, 1968.
[26] J. B. Kruskal, â€œThree-way arrays: Rank and uniqueness of trilinear de-
compositions, with applications to arithmetic complexity and statis-
tics,â€ Linear Algebra and Its Appl., vol. 18, pp. 95â€“138, 1977.
[27] R. A. Horn and C. R. Johnson, Matrix Analysis. New York: Cam-
bridge Univ. Press, 1985.
[28] D. Torrieri and K. Bakhru, â€œThe effects of nonuniform and correlated
noise on superresolution algorithms,â€ IEEE Trans. Antennas Propag.,
vol. 45, no. 8, pp. 1214â€“1218, Aug. 1997.
[29] H. L. V. Trees, Detection, Estimation, and Modulation Theory, Part IV,
Optimum Array Processing. New York: Wiley, 2002.
[30] N. D. Sidiropoulos and X. Liu, â€œIdentifiability results for blind beam-
forming in incoherent multipath with small delay spread,â€ IEEE Trans.
Signal Process., vol. 49, no. 1, pp. 228â€“236, Jan. 2001.
[31] P. Stoica and R. Moses, Spectral Analysis of Signals. Englewood
CLiffs, NJ: Pearson Prentice-Hall, 2005.
Wing-Kin Ma (Sâ€™96â€“Mâ€™01) received the B.Eng.
(first-class honors) degree in electrical and electronic
engineering from the University of Portsmouth,
Portsmouth, U.K., in 1995 and the M.Phil. and Ph.D.
degrees, both in electronic engineering, from the
Chinese University of Hong Kong (CUHK), Hong
Kong, in 1997 and 2001, respectively. His Ph.D.
dissertation was commended to be â€œof very high
quality and well deserved honorary mentioningâ€ by
the Faculty of Engineering, CUHK, in 2001.
He is currently an Assistant Professor with the De-
partment of Electronic Engineering, CUHK. From 2005 to 2007, he was also an
Assistant Professor with the Institute of Communications Engineering, National
Tsing Hua University, Taiwan, R.O.C., where he is still holding an adjunct po-
sition. Prior to becoming a faculty member, he held various research positions
with McMaster University, Canada; CUHK; and the University of Melbourne,
Australia. His research interests are in signal processing and communications,
with a recent emphasis on MIMO techniques and convex optimization.
Dr. Ma currently serves as an Associate Editor of the IEEE TRANSACTIONS
ON SIGNAL PROCESSING and the IEEE SIGNAL PROCESSING LETTERS, and a
Guest Editor of IEEE SIGNAL PROCESSING MAGAZINE on the Special Issue ti-
tled Convex Optimization for Signal Processing.
Tsung-Han Hsieh received the B.S. degree from
the Department of Electrical Engineering, National
Chi Nan University, Nantou, Taiwan, in 2006, and
the M.S. degree from the Institute of Communica-
tions Engineering, National Tsing Hua University,
Hsinchu, Taiwan, in 2008.
He is currently working as a Digital IC Design En-
gineer at Realtek Semiconductor Corp., Hsinchu. His
research interests are in signal processing and high-
speed interface implementation.
Authorized licensed use limited to: National Tsing Hua University. Downloaded on August 10,2010 at 07:52:04 UTC from IEEE Xplore.  Restrictions apply. 
ALTERNATING PROJECTIONS FOR KHATRI-RAO SUBSPACE BLIND CHANNEL
IDENTIFICATION OF QUASI-STATIONARY SIGNALS
Wing-Kin Ma, Ka-Kit Lee Yi-Lin Chiou, Tsung-Han Chan, Chong-Yung Chi
Dept. Electronic Eng., Chinese Univ. Hong Kong Inst. Commun. Eng., National Tsing Hua Univ.
Shatin, N.T., Hong Kong Hsinchu, Taiwan
E-mail: wkma@ieee.org,kklee@ee.cuhk.edu.hk E-mail: thchan@ieee.org,cychi@ee.nthu.edu.tw
ABSTRACT
In this paper we propose a fast convergent method for blind channel
identification of quasi-stationary signals (BCI-QSS), with an em-
phasis on overdetermined linear mixtures. The proposed method
is based on the Khatri-Rao (KR) subspace criterion; this criterion
enables us to decouple the BCI-QSS problem into a number of per-
channel-column identification problems that exhibit a problem struc-
ture less complex than a full BCI-QSS problem. We propose a spe-
cialized alternating projections (AP) algorithm for handling those
per-channel-column problems. Interestingly, we prove that under an
ideal model condition, the proposed AP algorithm converges to a
true channel column almost surely, and it does so within one itera-
tion. Our simulation results show that the proposed method termi-
nates in 3 âˆ’ 4 iterations on average when noise is present, and that
it achieves satisfactory channel estimation performance as compared
to some other existing BCI-QSS methods.
Index Termsâ€” Khatri-Rao subspace, Quasi-stationary signals,
Second-order statistics, Blind channel identification
1. INTRODUCTION
This paper considers the problem of blind channel identification
(BCI) of quasi-stationary signals (QSS), termed BCI-QSS here for
short. In this context, the source signals are assumed to be station-
ary within local time intervals, but have time-varying statistics in a
global time scale. BCI-QSS finds applications in blind separation of
speech and audio signals, and it has received much interest. The idea
behind BCI-QSS is to exploit the inherent time-varying statistics
(usually the second-order statistics [SOSs]) of the sources to identify
the channel. BCI-QSS can be formulated as a data fitting problem
involving three-way arrays, popularly known as parallel factor analy-
sis (PARAFAC) [1â€“3]. Alternatively, one can formulate BCI-QSS as
a problem of diagonalizing multiple matrices; i.e., the so-called joint
diagonalization (JD) [4â€“6]. In devising a BCI-QSS method, under
either PARAFAC or JD, we are generally required to deal with some
kinds of four-order optimization with respect to the channel matrix
(or its inverse), and such problems are challenging to solve. Rep-
resentative algorithms under the PARAFAC framework include tri-
linear alternating least squares (TALS) [2] and alternating-columns
diagonal-centers (AC-DC) [3], while those under JD include Phamâ€™s
diagonalization [4], fast Frobenius diagonalization (FFDiag) [5], and
quadratic diagonalization (QDiag) [6].
This work was supported by a General Research Fund of Hong Kong
Research Grant Council (Project No. CUHK415908) and by the National
Science Council (R.O.C.) under Grants NSC 99-2221-E-007-003-MY3 and
NSC 96-2628-E-007-003-MY3.
In this paper we propose a BCI-QSS method based on the
Khatri-Rao (KR) subspace formulation [7]. KR subspace is an al-
ternative to the PARAFAC and JD framework; the former is differ-
ent in the sense that it advocates decoupled channel estimation on a
column-by-column basis. A potential advantage with this is that the
resulting per-channel-column identification problem exhibits a less
complex problem structure than a full BCI-QSS problem. In [7] we
have utilized such an advantage to develop simple, efficient DOA es-
timators for QSS. This paper deals with the more challenging case of
BCI, with an emphasis on overdetermined mixtures. To this end, we
will propose a channel column identification method using alternat-
ing projections (AP) [8]. It will be shown that the AP method, when
aided by the prewhitening procedure, yields appealing convergence
results. In particular, we will prove that in the absence of modeling
errors, the AP method (with prewhitening) almost surely converges
to a true channel column in one iteration. A method that uses AP to
effectively identify all the channel columns will also be developed.
Our simulation results will demonstrate that in the presence of noise,
the proposed KR subspace AP algorithm still exhibits fast conver-
gence (3-4 iterations on average), while providing competitive BCI
performance as compared to some other BCI-QSS algorithms.
2. PROBLEM STATEMENT
Consider an N -by-K linear mixing model as follows:
x(t) = As(t) + v(t), t = 0, 1, 2, . . . (1)
where x(t) = [x1(t), . . . , xN(t)]T is the received signal vector,
A = [a1, . . . ,aK ] âˆˆ C
NÃ—K is the channel matrix, s(t) =
[s1(t), . . . , sK(t)]
T is the source signal vector, and v(t) represents
noise. Our model assumptions are as follows:
(A1) The source signals sk(t), k = 1, . . . ,K, are zero-mean and
statistically independent of each other.
(A2) N > K, and A âˆˆ CNÃ—K is of full column rank.
(A3) The noise vector v(t) is wide-sense stationary with zero mean
and covariance Ïƒ2I, and it is statistically independent of the
source signals.
(A4) Each source signal sk(t) is wide-sense quasi-stationary with
frame length L; specifically, E{|sk(t)|2} = dmk, âˆ€t âˆˆ
[(mâˆ’ 1)L+ 1, mL].
Note that assumption (A4) is key to BCI-QSS techniques [2â€“6].
Physically, it means that the source SOSs are static within a short,
local period of time, but are time-variant over a global time scale.
Let us examine the local SOSs of the received signal x(t). Let
Rm = E{x(t)x(t)
H}, for any t âˆˆ [(mâˆ’ 1)L+ 1,mL], (2)
It can be easily verified that the solution to (15) is h =
UsU
H
s (Î±a
âˆ— âŠ— a). Moreover, the partial minimization w.r.t. (Î±, a)
min
|Î±|=1,â€–aâ€–2=1
â€–Î±aâˆ— âŠ— aâˆ’ hâ€–2 (16)
also has a closed form. To show this, consider the objective function
of (16). We have, for any |Î±| = 1, â€–aâ€–2 = 1,
â€–Î±aâˆ— âŠ— aâˆ’ hâ€–2 = |Î±|2â€–aâˆ— âŠ— aâ€–2 âˆ’ 2Re{Î±âˆ—(aâˆ— âŠ— a)Hh}+ â€–hâ€–2
= 1âˆ’ 2Re{Î±âˆ—aHvecâˆ’1(h)a}+ â€–hâ€–2 (17)
â‰¥ 1âˆ’ 2|aHvecâˆ’1(h)a|+ â€–hâ€–2 (18)
where (17) is due to (8). Note that equality in (18) holds when
Î± = exp(jÏ†(aHvecâˆ’1(h)a)) where Ï†(x) is the phase of x. More-
over, (18) is minimized when a equals a unit-norm eigenvector of
vecâˆ’1(h) associated with the eigenvalue of largest absolute value,
which we will denote by qmax(vecâˆ’1(h)). In summary, a =
qmax(vec
âˆ’1(h)), Î± = exp(jÏ†(aHvecâˆ’1(h)a)) serve as a solution
to (16). We summarize the AP method as follows:
ALTERNATING PROJECTION ALGORITHM FOR (13)
given an orthogonal basis matrix Us âˆˆ CN
2Ã—K
, and an
initial point h âˆˆ R(Us).
repeat
a := qmax(vec
âˆ’1(h)),
Î± := exp(jÏ†(aHvecâˆ’1(h)a)),
h := UsU
H
s (Î±a
âˆ— âŠ— a),
until a stopping criterion is satisfied.
output the vector a.
For general applications, AP may exhibit slow convergence.
Rather unexpectedly, we find that for semi-unitary A (i.e.,AHA =
I), the convergence of AP becomes dramatically different:
Theorem 1. (Convergence of AP for semi-unitaryA) Suppose
that A is semi-unitary, that the initialization is generated by
h = UsÎ¶ where Î¶ âˆ¼ CN (0, I) (i.e., i.i.d. zero-mean unit variance
complex Gaussian), and that there is no modeling error in (4).
Then, with probability one and in one iteration, the AP algorithm
converges to any one of a1, . . . ,aK up to a scale factor.
Proof: It can be verified that for a semi-unitaryA, (Aâˆ—A) is also
semi-unitary. With this property, we can show that
Us = (A
âˆ— A)Î“ (19)
for some unitary Î“ âˆˆ CKÃ—K . Let Î· = Î“Î¶. Then, we have Î· âˆ¼
CN (0, I). Subsequently, the initialization can be expressed as
h = UsÎ¶ = (A
âˆ— A)Î·. (20)
Let us consider the devectorization of h. Applying (7) to (20) yields
vecâˆ’1(h) = ADiag(Î·)AH (21)
Since A is semi-unitary, the right hand side of (21) is in fact an
eigenvalue decomposition (EVD) of vecâˆ’1(h). The remaining ques-
tion is whether (21) is the unique EVD. If it does, then the AP
step a := qmax(vecâˆ’1(h)) will pick up a principal eigenvector of
vecâˆ’1(h), which is one of the ak up to a scale factor. It is known
that if the eigenvalues Î·1, . . . , Î·K are distinct, then the respective
EVD is unique. As Î·i = Î·j holds with probability zero for any
i 6= j, we assert that the AP algorithm almost surely converges to a
true channel column in its first iteration. 
5. KR SUBSPACE AP-BASED ALGORITHM
We now consider building a complete BCI-QSS algorithm, by taking
advantage of rapid convergence of AP shown in Theorem 1. Theo-
rem 1 applies to semi-unitary A only. While the channel matrix A
is not semi-unitary in general, we can enforce that by prewhitening.
The prewhitening process is as follows. Let
RÂ¯ ,
1
M
Mâˆ‘
m=1
Rm = ADÂ¯A
H (22)
be the time-average covariance matrix, where DÂ¯ = 1
M
âˆ‘M
m=1Dm.
We perform a square root factorization on RÂ¯ (e.g., EVD) to obtain
RÂ¯ = BBH , where B âˆˆ CNÃ—K . Then, we carry out the following
prewhitening procedure on R1, . . . ,RM :
RËœm = B
â€ 
Rm(B
â€ )H , m = 1, . . . ,M. (23)
where Bâ€  is the pseudo-inverse of B. Then, it can be shown that
RËœm = GDËœmG
H , m = 1, . . . ,M, (24)
where DËœm = DÂ¯âˆ’1Dm, and G âˆˆ CKÃ—K is unitary and satisfies
G = Bâ€ ADÂ¯1/2. (25)
Hence, we can apply AP on RËœ1, . . . , RËœM to obtain the columns of
G. Once the wholeG is identified, we estimate the original channel
matrix by BG; cf., (25).
As a side product of prewhitening, we can efficiently iden-
tify all the columns of G by exploiting its column orthogo-
nality. Suppose that we have identified gr . Let Gâˆ’r =
[g1, . . . , grâˆ’1,gr+1, . . . ,gK ]. It can be easily shown that
R(Gâˆ—âˆ’r Gâˆ’r) = R
(
P
âŠ¥
gâˆ—
r
âŠ—gr (G
âˆ— G)
)
= R
(
P
âŠ¥
gâˆ—
r
âŠ—grUs
)
wherePâŠ¥gâˆ—
r
âŠ—gr = Iâˆ’ (g
âˆ—
r âŠ— gr)(g
âˆ—
r âŠ— gr)
H is the orthogonal com-
plement projector of gâˆ—r âŠ— gr . By applying AP on PâŠ¥gâˆ—
r
âŠ—grUs, we
can identify another gi, i 6= r.
We finish this section by providing the complete pseudo-code of
the proposed KR subspace algorithm in Table 1.
Table 1. The Proposed KR subspace AP-based Algorithm
Given local covariance matrices R1, . . . ,RM .
Step 1. compute ÏƒË†2 = minm=1,...,M Î»min(Rm), and obtain
RË†m = Rm âˆ’ ÏƒË†
2I, m = 1, . . . ,M .
Step 2. compute RÂ¯ = 1
M
âˆ‘M
m=1 RË†m, and perform a square-root
factorization RÂ¯ = BBH , where B âˆˆ CNÃ—K .
Step 3. compute RËœm = Bâ€ RË†m(Bâ€ )H , m = 1, 2, . . . ,M .
Step 4. form Y = [vec(RËœ1), . . . , vec(RËœM )], and compute its
compact SVD; i.e.,Y = UsÎ£sVHs , and set i = 1.
Step 5. apply the AP algorithm with the basis matrix Us and an
initial point h = UsÎ¶ for Î¶ âˆ¼ CN (0, I), and record its
output as gi.
Step 6. compute PâŠ¥gâˆ—
i
âŠ—gi
= I âˆ’ (gâˆ—i âŠ— gi)(g
âˆ—
i âŠ— gi)
H and ob-
tain the basis matrix of PâŠ¥gâˆ—
i
âŠ—gi
Us, denoted by Qs âˆˆ
C
K2Ã—(Kâˆ’i)
. Then, update Us := Qs.
Step 7. set i := i+ 1 and goto Step 5. until i > K.
Step 8. output AË† = BG, where G = [g1, . . . ,gK ].
 1
ï¥«è¨ªå±±æ±å¤§å­¸ã€æ¸…è¯å¤§å­¸ã€è¯ä¸­ç§‘æŠ€å¤§å­¸å¿ƒå¾—å ±å‘Š 
2010/1/29 
 
è¨ˆç•«ç·¨è™Ÿ NSC 96-2628-E-007-003-MY3  
è¨ˆç•«åç¨± é€šè¨Šèˆ‡ç”Ÿç‰©é†«å­¸æˆåƒä¹‹å‰çž»ç›²è”½è¨Šè™Ÿæºåˆ†ï§ª 
å‡ºåœ‹äººå“¡å§“å 
æœå‹™æ©Ÿé—œåŠè·ç¨± 
ç¥å¿ å‹‡ 
åœ‹ï§·æ¸…è¯å¤§å­¸é€šè¨Šå·¥ç¨‹ç ”ç©¶æ‰€ æ•™æŽˆ 
ï¥«è¨ªåœ°é»ž 
1/3/2010-1/20/2010 æ¿Ÿå—å±±æ±å¤§å­¸  
1/20/2010-1/23/2010 ï¥£äº¬æ¸…è¯å¤§å­¸ 
1/23/2010-1/27/2010 æ­¦æ¼¢è¯ä¸­ç§‘æŠ€å¤§å­¸ 
 
ä¸€ã€ï¥«è¨ªå±±æ±å¤§å­¸ç¶“éŽ (1/3-1/20)  
 
ç­†è€…æ–¼ 99ï¦Ž 1æœˆ 3æ—¥(æ—¥) æ—©ä¸Šå¾žæ¡ƒåœ’æ©Ÿå ´å‡ºç™¼ï¼Œæ–¼é¦™æ¸¯è½‰æ©Ÿï¼Œæ–¼ç•¶æ—¥ä¸‹åˆç´„ä¸‰é»žæŠµé”
æ¿Ÿå—ï¼Œä½é€²å±±æ±å¤§å­¸ä¹‹å­¸äººå¤§å»ˆã€‚ 
 
1æœˆ 4æ—¥ä¸‹åˆä¸‰é»žçµ¦ç¬¬ä¸€å ´æ¼”è¬›å¦‚ä¸‹:   
 
Talk A: A Linear Fractional Semidefinite Relaxation Approach to 
Maximum-Likelihood Detection of Higher Order QAM OSTBC in Unknown Channels  
 
Abstract: While the blind maximum-likelihood (ML) detection problem in multiple-input 
multiple-output (MIMO) flat-fading channels for general space-time codes is difficult to solve, it has 
been shown that for orthogonal space-time block codes (OSTBCs) with constant modulus 
constellations, this problem can be formulated as a discrete quadratic program, and then handled by a 
powerful convex approximation technique known as semidefinite relaxation (SDR). In this talk, we 
turn our attention to the case of higher order QAM OSTBCs. Due to the nonconstant modulus nature 
of higher order QAM signals, the blind ML detection problem turns out to be a discrete Rayleigh 
quotient maximization problem, and as a result the current SDR technique is no longer directly 
applicable. We present a linear fractional SDR (LFSDR) approach to this problem. This approach 
first relaxes the higher order QAM blind ML detection problem into a quasiconvex problem, 
followed by a simple solution approximation procedure. In general, quasiconvex problems are 
computationally more complex to solve than convex problems, but we show that an optimum 
solution of our quasiconvex problem can be efficiently obtained by solving a convex semidefinite 
program.  
   
 3
transmitter, a legitimate receiver (LR) and an unauthorized receiver (UR), we present a multi-stage 
training-based discriminatory channel estimation (DCE) scheme that aims to optimize the channel 
estimation performance of the LR while limiting the channel estimation performance of the UR. The 
key idea is to exploit the channel estimate fed back from the LR at the beginning of each stage to 
enable the judicious use of artificial noise (AN) in the training signal. Specifically, with knowledge of 
the LRâ€™s channel, AN can be properly superimposed with the training data to degrade the URâ€™s 
channel without causing strong interference on the LR. The channel estimation performance of the 
LR in earlier stages may not be satisfactory due to the inaccuracy of the channel estimate and 
constraints on the URâ€™s estimation performance, but can improve rapidly in later stages as the quality 
of channel estimate improves. The training data power and AN power are optimally allocated by 
minimizing the normalized mean squared error (NMSE) of the LR subject to a lower limit constraint 
on the NMSE of the UR. The presented DCE scheme is then extended to the case with multiple LRs 
and multiple URs. Simulation results are presented to demonstrate the effectiveness of the proposed 
DCE scheme. 
 
æ–¼æ¼”è¬›ä¸­èˆ‡å‡ºå¸­å¸«ç”Ÿå€‘å……åˆ†è¨Žï¥ç ”ç©¶å•é¡Œï¼Œäº’å‹•ï¥¼å¥½ï¼Œå—ç›Šï¥¼å¤šã€‚ä¸‹åˆï¥¸é»žè‡³å››é»žåŠ
èˆ‡æ¸…è¯å¤§å­¸ç„¡ç·šèˆ‡ç§»å‹•é€šä¿¡æŠ€è¡“ç ”ç©¶ä¸­å¿ƒå¸«ç”Ÿåº§è«‡ã€‚ 
 
1æœˆ 22æ—¥(äº”)ä¸Šåˆçµ¦ç¬¬äºŒå ´æ¼”è¬› (Talk A)ï¼Œä¸‹åˆå—é‚€æ–¼ï¥£äº¬äº¤é€šå¤§å­¸çµ¦ä¸€å ´æ¼”è¬› (Talk 
A)ï¼Œæ–¼æ¼”è¬›ä¸­èˆ‡å‡ºå¸­å¸«ç”Ÿå€‘å……åˆ†è¨Žï¥ç ”ç©¶å•é¡Œï¼Œäº’å‹•ï¥¼å¥½ï¼Œå—ç›Šï¥¼å¤šã€‚ 
 
ä¸‰ã€ï¥«è¨ªè¯ä¸­ç§‘æŠ€å¤§å­¸ç¶“éŽ (1/23-1/27)  
 
1æœˆ 23æ—¥(ï§‘)ä¸Šåˆè‡ªï¥£äº¬æ­æ©Ÿèµ´æ­¦æ¼¢ï¼Œæ–¼ä¸‹åˆå››é»žæŠµé”è¯ä¸­ç§‘æŠ€å¤§å­¸æ ¡åœ’å…§ä¹‹åœ‹éš›å°ˆ
å®¶å…¬å¯“ã€‚ 
1æœˆ 24æ—¥(æ—¥)ä¸Šåˆä¹é»žçµ¦ç¬¬ä¸€å ´æ¼”è¬› (Talk A)ï¼Œä¸‹åˆä¸‰é»žè‡³äº”é»žèˆ‡è¯ä¸­ç§‘æŠ€å¤§å­¸é€šä¿¡èˆ‡
æŽ¢æ¸¬ä¸­å¿ƒå¸«ç”Ÿåº§è«‡ã€‚ 
1æœˆ 26æ—¥(äºŒ)ä¸‹åˆä¸‰é»žçµ¦ç¬¬äºŒå ´æ¼”è¬› (Talk B)ï¼Œæ–¼æ¼”è¬›ä¸­èˆ‡å‡ºå¸­å¸«ç”Ÿå€‘å……åˆ†è¨Žï¥ç ”ç©¶å•
é¡Œï¼Œäº’å‹•ï¥¼å¥½ï¼Œå—ç›Šï¥¼å¤šã€‚ 
 
ç­†è€…æ–¼ 99ï¦Ž 1æœˆ 27æ—¥ä¸Šåˆå¾žæ­¦æ¼¢å‡ºç™¼ï¼Œæ–¼é¦™æ¸¯è½‰æ©Ÿï¼Œæ–¼ç•¶æ—¥ä¸‹åˆä¸‰é»žåŠæŠµé”æ¡ƒåœ’æ©Ÿ
å ´ã€‚ 
 
å››ã€è‡´è¬ï¼šæ„Ÿè¬åœ‹ç§‘æœƒè£œåŠ©ï¦ƒè²»ï¥«è¨ªå±±æ±å¤§å­¸ã€æ¸…è¯å¤§å­¸åŠè¯ä¸­ç§‘æŠ€å¤§å­¸ã€‚ 
 
äº”ã€é™„ä»¶ï¼šé™„ä¸Šé‚€è«‹å‡½åŠæ¼”è¬›å…¬å‘Šã€‚ 

ÎµzIs-ÎµsÎµÎµâˆ âˆ ä¹™90I98+1ã€‚âŠ¥
9sä¹™å†«6âˆ ä¹™90I98+âˆ¶Xâ…¤J
uo npÎ¸ Îµnâ…¡3u0â€™â€™lpsuOLlzâˆ¶ IFLtIâ…¡
Ou!Ll0è¯…dâ€˜V8000Iå–Ÿu!qã€‚gâ€˜lã€‚!JlsICI uBâˆ¶pIOHä»„l!sIÎ¸Î›â… un Bnq8uâ…  sâŠ¥
â–³o,uoD cIÎ³ä¸¬K3Â°Io1ã€LIoã€‚âŠ¥uoâ… lâ’“ã€‚!un1utLloD oIä¸¨ qoN Puv ssÎ¸ IÎ¸â–³,Ë‡\â€˜â†“s!luÎ¸!os Jã€‚!11D
??:è¯¥
;tå·æ°µ0ç¤»æ°µ:lÂ°
Jd
â…¨lÎ¸JÎ¸ã€‚uâ… s
i uoIIOJÂ°dÂ°Â°ã€‚
IÎ·Jl!nlJ aulåˆ‚  Jno uÎ¸do â…¡â… Î›ã€ P1åœ¯ Â°uo lã€—lJssã€‚ã€‚ns B Î¸q IIË‡
`` I!s!Î› JnoK ã€‚Î›Î¸â…¡oq I
0Îµâˆ¶lI-00âˆ¶01 'è¿‚Vâ€˜zä¹™-I0-0IOzâˆ¶ã€‚lvCl
sIã€‚uuâ’“1l0
uç¬Â°uä¸¬un u!DgâŠ¥sO wâ…¤0ã€‘opâ–³o1Î¸â…¡å¹‚IH Jo uã€‚lã€‘ã€‚âˆžocl pooLlmä¸« â–³-Luu11Iâ… xBW
oJ  lã€‚vÂ°JddV  uo!}Bxâ’“ IÎ¸ä¸¬  ã€‚)âˆ¶uåˆ‚Î¸pâ… tuos  IOuo!lÂ° PJd  ã€‘Oã€‚u!â–³  â…¤  âˆ¶II  Î¸1Rlã€‚Î¸â–³  z
0Îµâˆ¶ll-00âˆ¶0I 'è¾½Vâ€˜Iä¹™-I0-010ä¹™:ã€‚ltâ€™Q
smÎ˜lså¤­s OwI1^Ïƒ ss0lÎ¸J!/V`
u! uo!ltâ€™Luâ… ;sEI lÎ¸uuOLlO Kâ–³Â°lBuImâ… Jã€‚s!Cl Jqâˆ« u8â… sã€‚G Â°ã€‚uÎ¸ubos 8uâ… IBIâ–³ âˆ¶I 0Jlll0Î¸â–³ â–³
Kâ†“Isã€‘oÎ›!uf1ÎµnH8uâˆ¶bJjIBuoIltIN lB sluomoÎ› Î¸!qã€‚O
puB sÎ˜Ilâ… Î›!lã€‚æ°µ qo1Boso1 âŠ¥n0äºº uo sä¸¬IOJ ã€‚Î›ã€l å¹‚u!ç¬Â°â…¡oJ Â°ql Î¸Î›!cQ puâ’“ 0IOz K1vnuÎµf
"lÎµ
z puâ’“pJ0ä¹™uooÎ›â€²)Î¸q dnoâ–³81IoJBosã€‚Jâ–³nol!s!Î› ol noå¤­ol!Î›uIÂ°lÎ¸ä¸¬â…¡ pInoÎ›â€²I Îºl!SJoÎ›:un
Bnq8uâ… sJj JO(â–³HË‡)KBoIÂ°uâ…¡ã€‚ã€‚â–³uoIlOmloJtIl JÂ° Î˜nlIlsuI qmBÎ¸ soAä¸¬u!(ã€‘ã€‚JuÎ¸0ssÎ¸IÎ¸Jbã€)
1Î¸)uÎ¸D cå››Ë‡ KBoIã€‚uLlooâŠ¥ uoIIOã€‚â… 11nLuLuoD aIlqoæŠ¤ è¾½ put ssÎ¸ IÎ¸aIÎ›
` 
ã€‚â…¡l JÂ°JIOâ…¡ã€‚q uâ—‹
0I0ä¹™Js1Buuå®ƒfu!PuBIuâ… O1u eu!qD I!Sâ… Î› ol su!uuOld oã€‘ P IlK,kkouä¸¬ol KddLâ€™qK1Î¸Î› tuL,I
â€˜
â… qD8ula k-8uoâ…¡ D Jo1d aBã€‚Cl
0IOz ÎºJBnuuÎ“ â…¡âˆ¶ÎºIâˆ¶sJaÎ›âˆ¶â…¡f1Lâ€™nâ–³8uâˆ¶sâŠ¥lâˆ¶sâˆ¶Î›oâ—†uoâˆ¶Izlâˆ¶Î›uI
âˆ I-ä¹™ã€-600z Î¸Iå®ƒCI
âˆ 8âˆ ã€sâ€˜sLË‰988+:xâ’“J
9sIICâˆ sÎµ988+:IaJj
AtlÂ·npÎ¸Â·nq;â…¡Â·aÎ¸â‘¡âˆ¶qa(ã€‚:Iâˆ¶zâ…¢-â…¡
ÎµI00g â…¡uÎ›`âˆ¶Lã€â€˜nâ…¡aâ…¡!sHâ€˜Â·pu nJ8â…¡uâ…¡ä¸¬â€˜ä¹™Â·aã€‚sâ€˜10ã€
ÎºIâˆ¶sã€‘aNulâ–³å£nH Bâ…¡IsJjå©uoâˆ¶Iâ’“N
E3uâˆ¶J|,aâ…¡IÎ™;uu IL,ã€‚IJIiâ€™(â€™Iâ’‰Jo luaâ…¢lã€uda([I
çŠ­Pâ€˜â… ;uIJÎ¸Î¸uâˆ¶â… 3uIl suoâˆ¶ IBiâ€™âˆ¶â…¡nmâ…¢o())Jo aInlIIsuI
âˆ¶â…¡D8unk-8â…¡oâ…¡D JOssaJoJd
:oJj
eu!Llaâ€˜V80oo18uIâˆagå†«8000Iè¦ƒÎ·rå›¤lâ…¡
äººlIsJaÎ›Iunâ’“nâ…¡8ué˜L
å½yçº›éº´

å°æ¹¾æ¸…åŽå¤§å­¦ç”µå­å·¥ç¨‹ç³»Chong-Yung Chi (ç¥å¿ å‹‡)æ•™æŽˆå°†äºŽ 2010å¹´ 1æœˆ 26æ—¥ä¸‹
åˆæ¥æˆ‘ç³»ä½œå­¦æœ¯æŠ¥å‘Šï¼Œæœ‰å…³ä¿¡æ¯å¦‚ä¸‹ï¼š 
æ—¶é—´ï¼š1æœˆ 26æ—¥ï¼ˆæ˜ŸæœŸäºŒï¼‰ä¸‹åˆ 3ç‚¹   
åœ°ç‚¹ï¼šç”µä¿¡ç³»ä¼šè®®å®¤ï¼ˆå—ä¸€æ¥¼ä¸‰æ¥¼ä¸œå¤´ï¼‰ 
é¢˜ç›® 2ï¼šTraining Sequence Design for Discriminatory Channel Estimation in 
Wireless MIMO Systems ï¼ˆæ— çº¿MIMOç³»ç»Ÿå·®å¼‚ä¿¡é“ä¼°è®¡ä¸­çš„è®­ç»ƒåºåˆ—è®¾è®¡ï¼‰ 
æŠ¥å‘Šäººï¼šChong-Yung Chi (ç¥å¿ å‹‡)æ•™æŽˆ 
This talk presents a training-based channel estimation scheme for achieving quality-of-service discrimination 
between legitimate and unauthorized receivers in wireless multiple-input multiple-output (MIMO) channels. This 
scheme has applications ranging from user discrimination in wireless TV broadcast systems to the prevention of 
eavesdropping in secret communications. By considering a wireless MIMO system that consists of a 
multiple-antenna transmitter, a legitimate receiver (LR) and an unauthorized receiver (UR), we present a 
multi-stage training-based discriminatory channel estimation (DCE) scheme that aims to optimize the channel 
estimation performance of the LR while limiting the channel estimation performance of the UR. The key idea is to 
exploit the channel estimate fed back from the LR at the beginning of each stage to enable the judicious use of 
artificial noise (AN) in the training signal. Specifically, with knowledge of the LRâ€™s channel, AN can be properly 
superimposed with the training data to degrade the URâ€™s channel without causing strong interference on the LR. 
The channel estimation performance of the LR in earlier stages may not be satisfactory due to the inaccuracy of the 
channel estimate and constraints on the URâ€™s estimation performance, but can improve rapidly in later stages as 
the quality of channel estimate improves. The training data power and AN power are optimally allocated by 
minimizing the normalized mean squared error (NMSE) of the LR subject to a lower limit constraint on the NMSE 
of the UR. The presented DCE scheme is then extended to the case with multiple LRs and multiple URs. 
Simulation results are presented to demonstrate the effectiveness of the proposed DCE scheme. 
Biography  
Chong-Yung Chi (Sâ€™83-Mâ€™83-SMâ€™89) received the Ph.D. degree in Electrical Engineering from the 
University of Southern California in 1983. From 1983 to 1988, he was with the Jet Propulsion Laboratory, 
Pasadena, California. He has been a Professor with the Department of Electrical Engineering since 1989 and the 
Institute of Communications Engineering (ICE) since 1999 (also the Chairman of ICE for 2002-2005), National 
Tsing Hua University, Hsinchu, Taiwan. He co-authored a technical book, Blind Equalization and System 
Identification, published by Springer 2006, and published more than 150 technical (journal and conference) papers. 
His current research interests include signal processing for wireless communications, convex analysis and 
optimization for blind source separation, biomedical imaging and hyperspectral imaging.  
Dr. Chi is a senior member of IEEE. He has been a Technical Program Committee member for many IEEE 
sponsored and co-sponsored workshops, symposiums and conferences on signal processing and wireless 
communications, including Co-organizer and general Co-chairman of IEEE SPAWC 2001, and Co-Chair of Signal 
Processing for Communications Symposium, ChinaCOM 2008 & ChinaCOM 2009. He was an Associate Editor of 
IEEE Trans. Signal Processing (5/2001-4/2006), IEEE Trans. Circuits and Systems II (1/2006-12/2007), and a 
member of Editorial Board of EURASIP Signal Processing Journal (6/2005-5/2008), and an editor 
(7/2003-12/2005) as well as a Guest Editor (2006) of EURASIP Journal on Applied Signal Processing. Currently, 
he is an Associate Editor of IEEE Signal Processing Letters and IEEE Trans. Circuits and Systems I, and a member 
of IEEE Signal Processing Committee on Signal Processing Theory and Methods. 
 
æ¬¢è¿Žæ„Ÿå…´è¶£çš„è€å¸ˆå’ŒåŒå­¦å±Šæ—¶å‚åŠ ã€‚ 
ç”µå­ä¸Žä¿¡æ¯å·¥ç¨‹ç³» 
 2
3/18 (Thursday): Participated in  
(a) Plenary talk (8:30-9:30)  
(b) SPCOM-L4: Relay and Cooperative Systems I (10:00-12:00)  
(c) SPCOM-P8: Beamforming and MIMO II (13:30-15:30)  
(d) SPCOM-L5: Beamforming and MIMO I (16:00-18:00)  
 
3/19 (Friday): Participated in  
(e) Plenary talk (8:30-9:30)  
(f) ç­†è€…æ–¼session SAM-L3: Multichannel Communications é€²ï¨ˆï¥æ–‡å£é ­ç™¼è¡¨ 
(10:00-12:00)  
(g) DISPS-L2: Efficient Implementation for Communications (13:30-15:30)  
 
3/20-3/21: 3/20æ—¥ä¸­åˆç”± Dallas DFWåœ‹éš›æ©Ÿå ´ç¶“ Los Angelsè½‰æ©Ÿæ–¼ 3/21æ—¥æ™šé–“ 10:35åˆ†æŠµé”æ¡ƒ
åœ’åœ‹éš›æ©Ÿå ´  
 
æ”œå›žè³‡ï¦¾ï¼šICASSP-2010æœƒè­°ï¥æ–‡é›†ä¹‹å…‰ç¢Ÿä¸€ç‰‡ã€‚  
 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
 
ICASSP ç‚ºè¨Šè™Ÿè™•ï§¤åŠé€šè¨Šï¦´åŸŸä¹‹å…¨çƒæœ€å¤§åœ‹éš›æœƒè­°ï¼Œå› æ­¤åŒ¯é›†ï¦ºä¸–ç•Œå„åœ°çš„çŸ¥åå­¸è€…åŠå°ˆ
å®¶ã€‚é€™æ¬¡ç‚ºç­†è€…ç¬¬ä¸‰æ¬¡ï¥«èˆ‡æœƒè­°ï¼Œä¸¦ä¸”å…¨ç¨‹ï¥«èˆ‡å„å€‹æ™‚æ®µå¤§æœƒæ‰€å®‰æŽ’çš„ workshopsåŠï¥æ–‡ç™¼è¡¨ã€‚
åœ¨æœ¬æ¬¡æœƒè­°ä¸­ï¼Œç­†è€…æ‰€ç™¼è¡¨ä¹‹ï¥æ–‡å—åˆ°è¨±å¤šåœ‹éš›å°ˆå®¶å­¸è€…çš„é—œæ³¨ï¼Œåœ¨ï¥æ–‡ç™¼è¡¨å¾Œèˆ‡å¤šä½å­¸è€…é€²
ï¨ˆï¦ºäº¤ï§Šï¼Œåœ¨å¸æ”¶æ–°çŸ¥åŠå»ºï§·äººéš›é—œä¿‚å—ç›Šï¥¼å¤šã€‚é€™æ¬¡ï¥«èˆ‡ ICASSPå¦‚æ­¤å¤§åž‹åŠé«˜æ°´å¹³çš„åœ‹éš›
æœƒè­°ï¼Œè‘—å¯¦å¤§å¤§æ¿€ï¥¿ï¦ºæˆ‘åœ¨ç ”ç©¶ä¸Šï¤è¦è¿½æ±‚å‰µæ–°åŠå“è³ªã€‚ 
 
è‡´è¬ï¼šæ„Ÿè¬åœ‹ç§‘æœƒè£œåŠ©ï¦ƒè²»ã€‚  
 
more power efficient than that without AN, in discriminating Bob
and Evesâ€™ reception performances.
2. SIGNAL MODEL AND PROBLEM STATEMENT
Consider a wireless downlink system that consists of a transmitter
(Alice), a legitimate receiver (Bob) and ð‘€ unauthorized receivers
(Eves). We assume that Alice is equipped withð‘ð‘¡ transmit antennas,
and both Bob and Eves are equipped with only one receive antenna.
Let x(ð‘¡) âˆˆ â„‚ð‘ð‘¡ denote the signal vector transmitted from Alice, and
let h âˆˆ â„‚ð‘ð‘¡ and ð’ˆð‘š âˆˆ â„‚ð‘ð‘¡ denote the channel vectors from Alice
to Bob and from Alice to the ð‘šth Eve, respectively. The received
signals at Bob and Eves can be expressed as
ð‘¦ð‘(ð‘¡) = h
ð»
x(ð‘¡) + ð‘›(ð‘¡), (1)
ð‘¦ð‘’,ð‘š(ð‘¡) = ð’ˆ
ð»
ð‘šx(ð‘¡) + ð‘£ð‘š(ð‘¡), ð‘š = 1, . . . ,ð‘€, (2)
where ð‘›(ð‘¡) and ð‘£ð‘š(ð‘¡) are the additive noise at Bob and Eve ð‘š,
respectively. They are assumed to be independent and identically
distributed (i.i.d.) zero-mean, complex Gaussian random variables
with variances equal to ðœŽ2ð‘› and ðœŽ
2
ð‘£,ð‘š, respectively.
The aim of Alice is to design the transmit signal vector x(ð‘¡) âˆˆ
â„‚ð‘ð‘¡ such that Bob can retrieve the information signal with a desired
quality of service (QoS) while all Eves can only have limited recep-
tion performance. Taking the received signal-to-noise ratios (SNRs)
(or the received signal-to-interference-plus-noise ratios (SINRs) if
interference are also present) as the QoS measure, QoS discrimina-
tion can be achieved via transmit beamforming [9], that is, by letting
x(ð‘¡) = ð’˜ð‘ (ð‘¡) where ð’˜ âˆˆ â„‚ð‘ð‘¡ is the beamforming vector and ð‘ (ð‘¡)
is the information signal. Assuming that ð‘ (ð‘¡) has unit variance, one
can show by (1) and (2) that the SNRs at Bob and Eves are respec-
tively given by
SNRð‘ = ð’˜
ð»
Râ„Žð’˜/ðœŽ
2
ð‘›, SNRð‘’,ð‘š = ð’˜
ð»
Rð‘”,ð‘šð’˜/ðœŽ
2
ð‘£,ð‘š, (3)
whereRâ„Ž = hhð» andRð‘”,ð‘š = ð’ˆð‘šð’ˆð»ð‘š if the instantaneous CSIs of
Bob and Eves are available to Alice; whileRâ„Ž = E{hhð»} àª° 0 and
Rð‘”,ð‘š = E{ð’ˆð‘šð’ˆð»ð‘š} àª° 0 (hereð‘¿ àª° 0 means thatð‘¿ is Hermitian
positive semidefinite (PSD)) if only the channel correlation matrices
are known to Alice. Hence, with target SNR values ð›¾ð‘ for Bob and
ð›¾ð‘’ for Eves (ð›¾ð‘ > ð›¾ð‘’), one can design the beamforming vectorð’˜ by
considering the following optimization problem
ð‘ƒ â˜… = min
ð’˜âˆˆâ„‚ð‘ð‘¡
âˆ¥ð’˜âˆ¥2 (4a)
subject to (s.t.) ð’˜ð»Râ„Žð’˜/ðœŽ
2
ð‘› â‰¥ ð›¾ð‘, (4b)
ð’˜
ð»
Rð‘”,ð‘šð’˜/ðœŽ
2
ð‘£,ð‘š â‰¤ ð›¾ð‘’, ð‘š = 1, . . . ,ð‘€. (4c)
We should mention that the same design formulation as in (4) has
been used in transmit beamforming for spectrum sharing in cogni-
tive radios [9], a different application. Problem (4) is in general
a difficult nonconvex optimization problem, but interestingly when
Râ„Ž = hh
ð» (i.e., Alice perfectly knows Bobâ€™s CSI) problem (4) can
be recast as a convex quadratic program as follows (see, e.g., [10]):
min
ð’˜âˆˆâ„‚ð‘ð‘¡
âˆ¥ð’˜âˆ¥2 (5a)
s.t. Real{hð»ð’˜} â‰¥
âˆš
ð›¾ð‘ ðœŽ2ð‘›, Im{hð»ð’˜} = 0, (5b)
ð’˜
ð»
Rð‘”,ð‘šð’˜ â‰¤ ð›¾ð‘’ðœŽ2ð‘£,ð‘š, ð‘š = 1, . . . ,ð‘€, (5c)
where Real{hð»ð’˜} and Im{hð»ð’˜} denote the real and imaginary
parts of hð»ð’˜.
The design formulation in (4) depends on the degree of freedoms
(DOFs) of multiple antennas (which is ð‘ð‘¡) in discriminating Bob
and Evesâ€™ SNRs. Therefore, when ð‘€ is comparable to ð‘ð‘¡ or when
the target SNR ratio ð›¾ð‘/ð›¾ð‘’ is set too high, the beamformer ð’˜ has
to spend most of its DOFs in fulfilling (4c), but leaves very limited
number of DOFs for directing the main beam power toward Bob. In
that case, the optimumð’˜â˜… of problem (4) inevitably require to scale
up the transmit power âˆ¥ð’˜â˜…âˆ¥2 in order to meet Bobâ€™s SNR demand
in (4b).
3. PROPOSED TRANSMIT BEAMFORMING USING
ARTIFICIAL NOISE
In the section, we propose a new transmit beamforming strategy by
incorporating artificial noise (AN) into signal transmission design.
We will show how the associated beamforming problem can be effi-
ciently handled by the SDP relaxation method [7].
3.1. Proposed Beamforming Strategy Using AN
We propose to increase the interference level at Eves by incorporat-
ing AN [5] in the transmit beamforming design. In the AN-aided
approach, the transmit signal x(ð‘¡) is modified as
x(ð‘¡) = ð’˜ð‘ (ð‘¡) + z(ð‘¡), (6)
where
z(ð‘¡) âˆ¼ ð’žð’© (0,Î£), (7)
that is, z(ð‘¡) follows the zero-mean complex Gaussian distribution
with covariance matrix Î£ àª° 0. Under the AN-aided setting in (6),
we can deduce from the models in (1) and (2) that the SINRs at Bob
and Eves are
SINRð‘=
ð’˜ð»Râ„Žð’˜
Tr(Râ„ŽÎ£) + ðœŽ2ð‘›
, SINRð‘’,ð‘š=
ð’˜ð»Rð‘”,ð‘šð’˜
Tr(Rð‘”,ð‘šÎ£) + ðœŽ2ð‘£,ð‘š
, (8)
respectively, and the average transmit power is given by
E{âˆ¥x(ð‘¡)âˆ¥2} = âˆ¥ð’˜âˆ¥2 + Tr(Î£), (9)
where Tr(â‹…) denotes the trace of a matrix. While the SINRð‘ in (8)
seems degraded compared to the SNRs in (3) due to the added AN,
by joint design of the beamforming vector ð’˜ and AN covariance
matrix Î£, we shall be able to alleviate the degradation and in the
meantime effectively enhance the performance discrimination be-
tween Bob and Eves. Following the power minimization criterion
in (4), the design formulation for the AN-aided case is as follows:
ð‘ƒ â˜…AN = min
ð’˜,Î£
âˆ¥ð’˜âˆ¥2 +Tr(Î£) (10a)
s.t.
ð’˜ð»Râ„Žð’˜
Tr(Râ„ŽÎ£) + ðœŽ2ð‘›
â‰¥ ð›¾ð‘, (10b)
ð’˜ð»Rð‘”,ð‘šð’˜
Tr(Rð‘”,ð‘šÎ£) + ðœŽ2ð‘£,ð‘š
â‰¤ ð›¾ð‘’, ð‘š = 1, . . . ,ð‘€, (10c)
Î£ àª° 0. (10d)
A benefit that we can immediately see from (10) is that ð‘ƒ â˜…AN â‰¤
ð‘ƒ â˜…; i.e., the proposed formulation (10) is more power efficient than
its no-AN counterpart (4), given the same specifications of ð›¾ð‘ and
ð›¾ð‘’. The reason for this is that the feasible set of problem (4), together
with Î£ = 0, is merely a subset of the feasible set of (10). Problem
(10) is more difficult to handle than (4), however. In particular, one
can check that, unlike problem (4), problem (10) does not admit a
convex reformulation as in (5) whenRâ„Ž = hhð» . However, we will
present next that problem (10) can be efficiently handled by the SDP
relaxation technique [7].
2563
        







AN-aided design [in (10)]
No-AN design [in (4)]
1/ðœŽ2ð‘£ (dB)
A
ve
ra
ge
tr
an
sm
it
po
w
er
(d
B
)
Fig. 1. Simulation results (average transmit power versus 1/ðœŽ2ð‘£) for
ð›¾ð‘ = 10 dB, ð›¾ð‘’ = 3 dB, 1/ðœŽ2ð‘› = 10 dB,Râ„Ž = hh
ð» and Rð‘”,ð‘š =
ð’ˆð‘šð’ˆ
ð»
ð‘š for all ð‘š = 1, 2, 3.
As the final simulation example, we considered problems (4) and
(10) withRâ„Ž = hhð» andRð‘”,ð‘š = E{ð’ˆð‘šð’ˆð»ð‘š} for ð‘š = 1, 2, 3; i.e.,
Alice has the perfect Alice-to-Bob CSI but only knows the correla-
tion matrices of Alice-to-Eves channels. The channel vector h was
complex Gaussian distributed ð’žð’© (0, Ið‘ð‘¡/ð‘ð‘¡) and the covariance
matrices E{ð’ˆð‘šð’ˆð»ð‘š} were given by
E{ð’ˆð‘šð’ˆð»ð‘š} = ð›¼ Â¯ð‘šÂ¯
ð»
ð‘š
âˆ£âˆ£Â¯ð‘šâˆ£âˆ£2 + (1âˆ’ ð›¼)Î›, ð‘š = 1, 2, 3, (16)
where Â¯ð‘š âˆ¼ ð’žð’© (0, Ið‘ð‘¡ ), [Î›]ð‘–,ð‘— = 0.1âˆ£ð‘–âˆ’ð‘—âˆ£/ð‘ð‘¡ specifies the de-
gree of correlation between transmit antennas, and ð›¼ âˆˆ [0, 1] was
used to control the level of uncertainty of channel coefficients. In
this particular example, we found that problem (4) or problem (10)
may be infeasible (or have no solution) sometimes, especially if the
given problem instances Râ„Ž,Rð‘”,1, . . .Rð‘”,ð‘€ are harsh. In Fig. 3,
we present the percentage of infeasibility (%) of problems (4) and
(10) out of 1000 simulation trials for ð›¾ð‘’ = 3 dB. One can see from
the figure that for ð›¼ = 0.8 and 1/ðœŽ2ð‘£ â‰¥ 10 dB problem (4) has
about 30% infeasibility rate, and it can increase to about 55% for
ð›¼ = 0.6. The proposed problem (10) by contrast is always feasible
for both ð›¼ = 0.6 and ð›¼ = 0.8. The presented simulation results well
demonstrate the effectiveness of the proposed beamforming strategy
in power consumption as well as in feasibility.
5. REFERENCES
[1] A. D. Wyner, â€œThe wiretap channel,â€ Bell System Technical
Journal, vol. 54, pp. 1355â€“1387, 1975.
[2] P. Gopala, L. Lai, and H. El Gamal, â€œOn the secrecy capacity
of fading channels,â€ IEEE. Trans. Inform. Theory, vol. 54, no.
10, pp. 4687â€“4698, 2008.
[3] X. Li and J. Hwu, â€œUsing antenna array redundancy and chan-
nel diversity for secure wireless transmissions,â€ Journal of
Commun., vol. 2, no. 3, pp. 24â€“32, May 2007.
[4] L. Dong, Z. Han, A. Petropulu, and H. V. Poor, â€œSecure wire-
less communications via cooperation,â€ in Proc. Allerton Con-
ference on Communication, Control, and Computing, Monti-
cello, IL, USA, Sept. 23-26, 2008, pp. 1132â€“1138.
âˆ’50 0 50
0
5
10
15
20
25
30
Eve3Eve1 Bob Eve2
Beam pattern ofð’˜â˜…
dB
Direction (degree)
(a) Problem (4)
âˆ’50 0 50
0
5
10
15
20
25
30
Eve3Eve1 Bob Eve2
Beam pattern ofð’˜â˜…
Beam pattern of Î£â˜…
dB
Direction (degree)
(b) Problem (10)
Fig. 2. Optimum beam patterns of problems (4) and (10) for
ð›¾ð‘ = 20 dB, ð›¾ð‘’ = 0 dB, 1/ðœŽ2ð‘› = 0 dB, 1/ðœŽ
2
ð‘£ = 20 dB, Râ„Ž =
v(0)vð»(0), and Rð‘”,ð‘š = v(ðœ™ð‘š)vð»(ðœ™ð‘š) where {ðœ™1, ðœ™2, ðœ™3} =
{20âˆ˜,âˆ’20âˆ˜, 30âˆ˜}.
        













AN-aided design [in (10)], ð›¼ = 0.6
No-AN design [in (4)], ð›¼ = 0.6
AN-aided design [in (10)], ð›¼ = 0.8
No-AN design [in (4)], ð›¼ = 0.8
In
fe
as
ib
ili
ty
ra
te
(%
)
1/ðœŽ2ð‘£ (dB)
Fig. 3. Infeasibility rate (%) of problems (4) and (10) for ð›¾ð‘ = 10
dB, ð›¾ð‘’ = 3 dB and 1/ðœŽ2ð‘› = 10 dB.
[5] S. Goel and R. Negi, â€œGuaranteeing secrecy using artificial
noise,â€ IEEE. Trans. Wireless Commun., vol. 7, no. 6, pp.
2180â€“2189, June 2008.
[6] A. L. Swindlehurst, â€œFixed SINR solutions for the MIMO
wiretap channel,â€ in Proc. of IEEE ICASSP, Taipei, Taiwan,
April 19-24, 2009, pp. 2437â€“2440.
[7] N. D. Sidiropoulos, T. D. Davidson, and Z.-Q. Luo, â€œTransmit
beamforming for physical-layer multicasting,â€ IEEE Trans.
Signal Process., vol. 54, no. 6, pp. 2239â€“2251, June 2006.
[8] S. Boyd and L. Vandenberghe, Convex Optimization, Cam-
bridge University Press, Cambridge, UK, 2004.
[9] K. T. Phan, S. A. Vorobyov, N. D. Sidiropoulos, and C. Tellam-
bura, â€œSpectrum sharing in wireless networks via QoS-aware
secondary multicast beamforming,â€ IEEE Trans. Signal Pro-
cess., vol. 57, no. 6, pp. 2323â€“2335, June 2009.
[10] A. Wiesel, Y. C. Eldar, and S. Shamai, â€œLinear precoding via
conic optimization for fixed MIMO receivers,â€ IEEE Trans.
Signal Process., vol. 54, no. 1, pp. 161â€“176, Jan. 2006.
[11] J. F. Sturm, â€œUsing SeDuMi 1.02, a MATLAB toolbox for
optimization over symmetric cones,â€ Optimization Methods
and Software, vol. 11-12, pp. 625â€“653, 1999.
2565
 2
3/18 (Thursday): Participated in  
(a) Plenary talk (8:30-9:30);  
(b) Presented Paper #1 (A Robust Minimum Volume Enclosing Simplex Algorithm for 
Hyperspectral Unmixing) in Session IVMSP-P7.PG3;  
(c) And participated in several technical sessions (on signal processing in wireless 
communications, blind source separation, biomedical imaging and hyperspectral imaging.  
 
3/19 (Friday): Participated in  
(d) Plenary talk (8:30-9:30);  
(e) Session SAM-L3.L45 where Paper #2 â€œJoint Transmit Beamforming and Artificial Noise 
Design for Qos Discrimination in Wireless Downlinkâ€ was presented by Dr. Tsung-Hui 
Chang (a co-author of our paper) (10:00-12:00);  
(f) Participated in Signal Processing Theory and Methods (SPTM) Technical Program 
Committee Meeting (12:00-13:30);  
(g) Served as the chair of Session SPTM-P9 â€œSignal and System Modeling and Estimation IIâ€ 
(13:30-15:30).  
 
3/19-3/21: Flied from Dallas (in the evening 3/19) back to Taiwan, and arrived inæ¡ƒåœ’ä¸­æ­£æ©Ÿå ´ in 
the morning 3/21ã€‚  
 
æ”œå›žè³‡ï¦¾ï¼šICASSP-2010æœƒè­°ï¥æ–‡é›†ä¹‹å…‰ç¢Ÿä¸€ç‰‡ã€‚  
 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
 
åœ¨æœ¬æ¬¡æœƒè­°ä¸­ï¼Œç­†è€…æ‰€ç™¼è¡¨ä¹‹ 2 ç¯‡ï¥æ–‡å—åˆ°è¨±å¤šåœ‹éš›å°ˆå®¶å­¸è€…çš„é—œæ³¨ï¼Œåœ¨å¸æ”¶æ–°çŸ¥åŠ
å»ºï§·äººéš›é—œä¿‚å—ç›Šï¥¼å¤šã€‚Meanwhile, during the conference, I was invited by Dr. Seungjin 
Choi (Professor of Computer Science Department of Computer Science, Pohang University 
of Science and Technology San 31 Hyoja-dong, Nam-gu, Pohang 790-784, Korea), as an 
invited speaker, to give a talk on biomedical imaging at 2010 International Symposium on IT 
convergence Engineering to be held at POSTECH, Korea, 8/19 and 8/20. I have accepted his 
invitation.  
 
 
è‡´è¬ï¼šæ„Ÿè¬åœ‹ç§‘æœƒè£œåŠ©ï¦ƒè²» for the participation of ICASSP-2010ã€‚ 
 
the endmember signature matrix whose ith column vector ai is the
ith endmember signature, s[n] = [ s1[n], . . . , sN [n] ]T âˆˆ RN is the
nth abundance vector comprising N fractional abundances, w[n] =
[ w1[n], . . . , wM [n] ]
T is the zero-mean white Gaussian noise vector
(i.e.,N (0, Ïƒ2IM ), where Ïƒ2 is the noise variance) and L is the total
number of observed pixel vectors.
The goal of hyperspectral unmixing is to estimate the endmem-
ber signature matrix A and the abundances s[1], . . . , s[L] from the
noisy observed pixels y[1], . . . ,y[L], assuming that the number of
endmembers N is known a priori.
We consider the following general assumptions that are applica-
ble to HU algorithms :
(A1) (Non-negativity condition) si[n] â‰¥ 0 âˆ€ i, n.
(A2) (Full additivity condition)   Ni=1 si[n] = 1 âˆ€ n.
(A3) min{L,M} â‰¥ N and A is of full column rank.
(A1) and (A2) are valid assumptions in hyperspectral imaging
because the abundances are fractional proportions [1â€“4]. In addition,
the number of pixels and that of spectral bands involved are larger
than the number of endmembers and each endmember has its unique
signature, which justifies (A3).
Before getting into the core of development, we need to describe
a basic, essential concept in MVES, namely convex hull and simplex
[9]. Given a set of vectors {a1, . . . , aN} âŠ‚ RM , the convex hull of
{a1, . . . , aN} is defined as
conv{a1, . . . , aN} =  x =
N

i=1
Î¸iai 



1
T
NÎ¸ = 1, Î¸  0  , (3)
where Î¸ = [ Î¸1, . . . , Î¸N ]T âˆˆ RN . In addition, a convex
hull conv{a1, . . . , aN} is called a simplex if M = N âˆ’ 1 and
a1, . . . , aN are affinely independent.
3. REVIEW OF MVES PROBLEM FORMULATION
The noise-free signal model given by (2) is considered here for re-
viewing the formulation and theory behind MVES problem [8]. Like
many other HU algorithms [1], we begin with dimension reduction
of the observed pixels by projecting them onto an observed-pixel-
constructed affine set [8, 10], as given in the following lemma:
Lemma 1. (Dimension reduction by affine set fitting [8]) Under
(A2) and (A3), the dimension-reduced pixel vector xËœ[n] can be
obtained by an affine transformation of x[n]:
xËœ[n] = CT (x[n]âˆ’ d) âˆˆ RNâˆ’1, (4)
where (C,d) is the affine set fitting solution given by
d =
1
L
L

n=1
x[n], (5)
C = [ q1(UU
T ), q2(UU
T ), . . . , qNâˆ’1(UU
T ) ], (6)
where U = [ x[1]âˆ’d, . . . ,x[L]âˆ’d ] âˆˆ RMÃ—L, and qi(R) denotes
the eigenvector associated with the ith principal eigenvalue of R.
An important remark related to Lemma 1 is as follows:
(R1) The affine set fitting solution of the noisy observed pixels
(CË†, dË†) (obtained from (5) and (6) by replacing x[n] withy[n])
gives a best approximation in the least-squares sense [10], and
it asymptotically approaches the true (C,d) for large L.
Since   Nj=1 sj [n] = 1 [(A2)], it follows by substituting the
noise-free signal model (2) into (4) that
xËœ[n] =
N

j=1
sj [n]Î±j , (7)
where Î±j = CT (aj âˆ’ d) âˆˆ RNâˆ’1 is the jth dimension-reduced
endmember signature. Moreover, due to si[n] â‰¥ 0 [(A1)], it has
been proven in [8] that
xËœ[n] âˆˆ conv{Î±1, . . . ,Î±N} âŠ‚ RNâˆ’1, âˆ€n (8)
and conv{Î±1, . . . ,Î±N} is a simplex.
Based on Craigâ€™s belief [4], the unmixing problem of finding a
minimum volume simplex enclosing all the dimension-reduced pix-
els can now be written as the following optimization problem [8]:
min
Î²1,...,Î²NâˆˆR
Nâˆ’1
V (Î²1, . . . ,Î²N)
s.t. xËœ[n] âˆˆ conv{Î²1, . . . ,Î²N}, âˆ€ n,
(9)
where V (Î²1, . . . ,Î²N) is the volume of the simplex conv{Î²1, . . . ,
Î²N} given by [11]
V (Î²1, . . . ,Î²N) =
|det(B)|
(N âˆ’ 1)! (10)
and B = [ Î²1 âˆ’ Î²N , . . . ,Î²Nâˆ’1 âˆ’ Î²N ] âˆˆ R(Nâˆ’1)Ã—(Nâˆ’1) . In
addition, by (3), we rewrite the constraint of (9) in terms of B as
xËœ[n] = Î²N + BÎ¸n, (11)
where Î¸n  0 and 1TNâˆ’1Î¸n â‰¤ 1. Hence, problem (9) can be equiv-
alently written as
min
BâˆˆR(Nâˆ’1)Ã—(Nâˆ’1),
Î²N ,Î¸1,...,Î¸LâˆˆR
Nâˆ’1
|det(B)|
s.t. Î¸n  0, 1TNâˆ’1Î¸n â‰¤ 1,
xËœ[n] = Î²N + BÎ¸n, âˆ€ n.
(12)
By letting Î¸n = HxËœ[n] âˆ’ g for all n where H = Bâˆ’1 and g =
Bâˆ’1Î²N , one can eliminate the variables Î¸n for all n in (12) and
come up with
max
HâˆˆR(Nâˆ’1)Ã—(Nâˆ’1), gâˆˆRNâˆ’1
|det(H)|
s.t. 1TNâˆ’1(HxËœ[n]âˆ’ g) â‰¤ 1,
HxËœ[n] âˆ’ g  0, âˆ€ n.
(13)
Next, we will show how the MVES problem in (13) can be reformu-
lated for the noisy scenario.
4. ROBUST MVES PROBLEM AND ALGORITHM
Let us consider the noisy signal model given by (1). From (1), (4)
and (R1), we get
yËœ[n]  CË†T (y[n]âˆ’ dË†) âˆ¼= xËœ[n] + CË†Tw[n], (14)
where CË†Tw[n] is a random vector following N (0, Ïƒ2INâˆ’1) (since
CË†T is a semi-unitary matrix). Following the footsteps of the MVES
problem formulation, the robust MVES problem can be expressed
by replacing xËœ[n] by yËœ[n]âˆ’ CË†Tw[n] in (13):
max
H, g
|det(H)|
s.t. 1TNâˆ’1(HyËœ[n]âˆ’HCË†Tw[n]âˆ’ g) â‰¤ 1,
HyËœ[n]âˆ’HCË†Tw[n]âˆ’ g  0, âˆ€ n,
(15)
where 1TNâˆ’1HCË†
Tw[n]  z[n] and HCË†Tw[n]  u[n]
are a random variable and a random vector with distribu-
tionsN (0, Ïƒ21TNâˆ’1HHT1Nâˆ’1) and N (0, Ïƒ2HHT ), respectively.
Since the constraints in (15) involve randomness, they can be man-
ifested as chance constraints so as to mitigate the noise effects in
1203
Table 1. Average Ï†en and Ï†ab (degrees) over the various unmixing methods for different purity levels (Ï) and SNRs.
Methods Ï
Ï†en Ï†ab
SNR (dB) SNR (dB)
20 25 30 35 40 20 25 30 35 40
N-FINDR-FCLS
0.7 5.45 5.31 5.24 5.11 5.16 22.54 21.86 21.63 19.76 19.82
0.85 2.65 2.67 2.66 2.65 2.61 9.60 8.37 8.03 7.93 7.77
1 1.15 0.58 0.33 0.18 0.10 6.14 3.59 2.13 1.24 0.72
MVSA
0.7 5.95 4.03 2.67 2.12 1.40 20.80 14.56 7.88 4.81 3.14
0.85 5.99 3.75 2.61 2.07 1.27 19.65 12.12 7.17 4.16 2.34
1 6.12 3.96 2.71 2.14 1.33 18.93 11.55 6.68 3.85 2.15
MVES
0.7 5.17 3.26 2.43 1.73 1.01 16.66 10.58 6.51 3.81 2.17
0.85 5.28 3.59 2.65 1.85 1.11 16.88 10.98 7.20 4.26 2.38
1 6.67 4.37 3.35 2.50 1.55 19.81 13.09 9.58 6.81 4.50
RMVES
0.7 1.69 1.09 0.76 0.46 0.43 9.21 5.37 3.21 1.98 1.32
0.85 1.90 1.30 1.01 0.53 0.44 8.34 5.90 3.48 2.03 1.38
1 2.89 2.27 2.05 1.69 1.40 9.75 6.27 5.31 3.32 2.85
for all n = 1, . . . , L. To ensure (A1), the negative values of  s[n] are
made zero. The above procedure is referred to as RMVES algorithm.
The proposed RMVES algorithm uses the well known VCA [3]
for the initialization of (24) [6]. The optimization problem in (24) is
then solved by sequential quadratic programming (SQP). Moreover,
in each iteration of the RMVES algorithm (which involves N âˆ’ 1
row updates of H), we use the solution of the previous iteration as
the initialization for the current iteration.
5. SIMULATIONS AND CONCLUSIONS
In this section, the efficacy of the proposed RMVES algorithm is
demonstrated using 50 Monte Carlo runs for various purity lev-
els and SNRs. In each run, 1000 noise-free observed pixel vec-
tors were synthetically generated following the signal model in (2),
where 6 endmembers (i.e., Alunite, Buddingtonite, Calcite, Copi-
apite, Kaolinite, and Muscovite) with 417 bands are selected from
USGS library [12], and the abundance vectors s[n] were generated
following Dirichlet distribution D(s[n], Î¼) with Î¼ = 1
N
1N [3], for
different purity levels Ï = 0.7, 0.85, 1 [8]. The noisy data were
obtained by adding independent and identically distributed (i.i.d.)
zero-mean Gaussian noise to the noise-free data for different SNRs,
where SNR =  Ln=1 â€–x[n]â€–22/Ïƒ2ML. To maintain non-negativity
of the noisy observed pixels, we artificially set the negative values of
the noisy pixels to zero. For performance comparison, we also tested
three existing algorithms, N-FINDR-FCLS [2,13] (where FCLS [13]
was used to find the associated abundances for the endmember esti-
mates of N-FINDR), MVSA [6], and the original MVES [8].
The root-mean-square (rms) spectral angle between the true one
and estimated one (which has been widely used in HU [1, 3, 8]) is
used as the performance index. We here denote rms spectral angle
between endmembers and their estimates as Ï†en, and that between
abundance maps and their estimates as Ï†ab. By our extensive nu-
merical experience we found that Î· (which depends on Ï and SNR)
should be less than 0.5 (this can also be justified from Figure 1).
The average Ï†en and Ï†ab of the unmixing algorithms over SNR
= 20, 25, ..., 40 dB and Ï = 0.7, 0.85, 1 are shown in Table 1,
where each bold-faced number denotes the minimum rms spectral
angle associated with a specific pair of (Ï,SNR) over all the algo-
rithms. Table 1 shows that the proposed RMVES algorithm yields
the best performance for Ï = 0.7 and 0.85. For all the values of
Ï, the RMVES algorithm is better than its predecessor, the MVES
algorithm. In addition, to investigate the problem natures of (22)
and (24), we simply performed Monte Carlo simulations by directly
solving (22) using SQP for Ï = 0.7 and SNR = 20 dB. The average
Ï†en and Ï†ab were 9.04 and 24.73 degrees, respectively, which are
apparently much worse than those (1.69 and 9.21) of solving (24) in
an alternating fashion (RMVES algorithm). This may be attributed
to local optimality issues associated with the highly non-convex na-
ture of (22).
In conclusion, we have presented a robust hyperspectral unmix-
ing method, namely the RMVES algorithm which can effectively
unmix the highly mixed and noisy data. The RMVES algorithm ap-
plies chance constraints to accommodate the noise effects, and can
be suitably implemented using SQP solvers in an alternating fashion.
Simulation results showed that the RMVES algorithm outperforms
some existing benchmark algorithms and its predecessor MVES al-
gorithm. The application of RMVES algorithm to real hyperspectral
data would be our future direction.
6. REFERENCES
[1] N. Keshava and J. Mustard, â€œSpectral unmixing,â€ IEEE Signal Process.
Mag., vol. 19, no. 1, pp. 44-57, Jan. 2002.
[2] M. E. Winter, â€œN-findr: An algorithm for fast autonomous spectral
end-member determination in hyperspectral data,â€ in Proc. SPIE Conf.
Imaging Spectrometry, Pasadena, CA, Oct. 1999, pp. 266-275.
[3] J. M. P. Nascimento and J. M. B. Dias, â€œVertex component analysis:
A fast algorithm to unmix hyperspectral data,â€ IEEE Trans. Geosci.
Remote Sens., vol. 43, no. 4, pp. 898-910, Apr. 2005.
[4] M. D. Craig, â€œMinimum-volume transforms for remotely sensed data,â€
IEEE Trans. Geosci. Remote Sens., vol. 32, no. 3, pp. 542-552, 1994.
[5] N. Dobigeon, S. Moussaoui, M. Coulon, J.-Y. Toumeret, and
A. O. Hero, â€œJoint Bayesian endmember extraction and linear un-
mixing for hyperspectral imagery,â€ IEEE Trans. Signal Processing,
vol. 57, no. 11, pp. 4355-4368, Nov. 2009.
[6] J. Li, and J. M. B. Dias, â€œMinimum volume simplex analysis: A fast
algorithm to unmix hyperspectral data,â€ in Proc. IEEE IGARSS, vol.
4, Boston, MA, Aug. 8-12, 2008, pp. 2369-2371.
[7] J. M. B. Dias, â€œA variable splitting augmented Lagrangian approach
to linear spectral unmixing,â€ in Proc. IEEE WHISPERS, Grenoble,
France, Aug. 26-28, 2009.
[8] T.-H. Chan, C.-Y. Chi, Y.-M. Huang, and W.-K. Ma, â€œA convex analy-
sis based minimum-volume enclosing simplex algorithm for hyper-
spectral unmixing,â€ IEEE Trans. Signal Processing, vol. 57, no. 11,
pp. 4418-4432, Nov. 2009.
[9] S. Boyd and L. Vandenberghe, Convex Optimization, UK: Cambridge
Univ. Press, 2004.
[10] T.-H. Chan, W.-K. Ma, C.-Y. Chi, and Y. Wang, â€œA convex analysis
framework for blind separation of non-negative sources,â€ IEEE Trans.
Signal Processing, vol. 56, no. 10, pp. 5120-5134, Oct. 2008.
[11] G. Strang, Linear Algebra and Its Applications, CA: Thomson, 4th
edition, 2006.
[12] Tech. Rep., Available online: http://speclab.cr.usgs.gov/
cuprite.html.
[13] D. Heinz and C.-I. Chang, â€œFully constrained least squares linear
mixture analysis for material quantification in hyperspectral imagery,â€
IEEE Trans. Geosci. Remote Sens., vol. 39, no. 3, pp. 529-545, 2001.
1205
more power efficient than that without AN, in discriminating Bob
and Evesâ€™ reception performances.
2. SIGNAL MODEL AND PROBLEM STATEMENT
Consider a wireless downlink system that consists of a transmitter
(Alice), a legitimate receiver (Bob) and ð‘€ unauthorized receivers
(Eves). We assume that Alice is equipped withð‘ð‘¡ transmit antennas,
and both Bob and Eves are equipped with only one receive antenna.
Let x(ð‘¡) âˆˆ â„‚ð‘ð‘¡ denote the signal vector transmitted from Alice, and
let h âˆˆ â„‚ð‘ð‘¡ and ð’ˆð‘š âˆˆ â„‚ð‘ð‘¡ denote the channel vectors from Alice
to Bob and from Alice to the ð‘šth Eve, respectively. The received
signals at Bob and Eves can be expressed as
ð‘¦ð‘(ð‘¡) = h
ð»
x(ð‘¡) + ð‘›(ð‘¡), (1)
ð‘¦ð‘’,ð‘š(ð‘¡) = ð’ˆ
ð»
ð‘šx(ð‘¡) + ð‘£ð‘š(ð‘¡), ð‘š = 1, . . . ,ð‘€, (2)
where ð‘›(ð‘¡) and ð‘£ð‘š(ð‘¡) are the additive noise at Bob and Eve ð‘š,
respectively. They are assumed to be independent and identically
distributed (i.i.d.) zero-mean, complex Gaussian random variables
with variances equal to ðœŽ2ð‘› and ðœŽ
2
ð‘£,ð‘š, respectively.
The aim of Alice is to design the transmit signal vector x(ð‘¡) âˆˆ
â„‚ð‘ð‘¡ such that Bob can retrieve the information signal with a desired
quality of service (QoS) while all Eves can only have limited recep-
tion performance. Taking the received signal-to-noise ratios (SNRs)
(or the received signal-to-interference-plus-noise ratios (SINRs) if
interference are also present) as the QoS measure, QoS discrimina-
tion can be achieved via transmit beamforming [9], that is, by letting
x(ð‘¡) = ð’˜ð‘ (ð‘¡) where ð’˜ âˆˆ â„‚ð‘ð‘¡ is the beamforming vector and ð‘ (ð‘¡)
is the information signal. Assuming that ð‘ (ð‘¡) has unit variance, one
can show by (1) and (2) that the SNRs at Bob and Eves are respec-
tively given by
SNRð‘ = ð’˜
ð»
Râ„Žð’˜/ðœŽ
2
ð‘›, SNRð‘’,ð‘š = ð’˜
ð»
Rð‘”,ð‘šð’˜/ðœŽ
2
ð‘£,ð‘š, (3)
whereRâ„Ž = hhð» andRð‘”,ð‘š = ð’ˆð‘šð’ˆð»ð‘š if the instantaneous CSIs of
Bob and Eves are available to Alice; whileRâ„Ž = E{hhð»} àª° 0 and
Rð‘”,ð‘š = E{ð’ˆð‘šð’ˆð»ð‘š} àª° 0 (hereð‘¿ àª° 0 means thatð‘¿ is Hermitian
positive semidefinite (PSD)) if only the channel correlation matrices
are known to Alice. Hence, with target SNR values ð›¾ð‘ for Bob and
ð›¾ð‘’ for Eves (ð›¾ð‘ > ð›¾ð‘’), one can design the beamforming vectorð’˜ by
considering the following optimization problem
ð‘ƒ â˜… = min
ð’˜âˆˆâ„‚ð‘ð‘¡
âˆ¥ð’˜âˆ¥2 (4a)
subject to (s.t.) ð’˜ð»Râ„Žð’˜/ðœŽ
2
ð‘› â‰¥ ð›¾ð‘, (4b)
ð’˜
ð»
Rð‘”,ð‘šð’˜/ðœŽ
2
ð‘£,ð‘š â‰¤ ð›¾ð‘’, ð‘š = 1, . . . ,ð‘€. (4c)
We should mention that the same design formulation as in (4) has
been used in transmit beamforming for spectrum sharing in cogni-
tive radios [9], a different application. Problem (4) is in general
a difficult nonconvex optimization problem, but interestingly when
Râ„Ž = hh
ð» (i.e., Alice perfectly knows Bobâ€™s CSI) problem (4) can
be recast as a convex quadratic program as follows (see, e.g., [10]):
min
ð’˜âˆˆâ„‚ð‘ð‘¡
âˆ¥ð’˜âˆ¥2 (5a)
s.t. Real{hð»ð’˜} â‰¥
âˆš
ð›¾ð‘ ðœŽ2ð‘›, Im{hð»ð’˜} = 0, (5b)
ð’˜
ð»
Rð‘”,ð‘šð’˜ â‰¤ ð›¾ð‘’ðœŽ2ð‘£,ð‘š, ð‘š = 1, . . . ,ð‘€, (5c)
where Real{hð»ð’˜} and Im{hð»ð’˜} denote the real and imaginary
parts of hð»ð’˜.
The design formulation in (4) depends on the degree of freedoms
(DOFs) of multiple antennas (which is ð‘ð‘¡) in discriminating Bob
and Evesâ€™ SNRs. Therefore, when ð‘€ is comparable to ð‘ð‘¡ or when
the target SNR ratio ð›¾ð‘/ð›¾ð‘’ is set too high, the beamformer ð’˜ has
to spend most of its DOFs in fulfilling (4c), but leaves very limited
number of DOFs for directing the main beam power toward Bob. In
that case, the optimumð’˜â˜… of problem (4) inevitably require to scale
up the transmit power âˆ¥ð’˜â˜…âˆ¥2 in order to meet Bobâ€™s SNR demand
in (4b).
3. PROPOSED TRANSMIT BEAMFORMING USING
ARTIFICIAL NOISE
In the section, we propose a new transmit beamforming strategy by
incorporating artificial noise (AN) into signal transmission design.
We will show how the associated beamforming problem can be effi-
ciently handled by the SDP relaxation method [7].
3.1. Proposed Beamforming Strategy Using AN
We propose to increase the interference level at Eves by incorporat-
ing AN [5] in the transmit beamforming design. In the AN-aided
approach, the transmit signal x(ð‘¡) is modified as
x(ð‘¡) = ð’˜ð‘ (ð‘¡) + z(ð‘¡), (6)
where
z(ð‘¡) âˆ¼ ð’žð’© (0,Î£), (7)
that is, z(ð‘¡) follows the zero-mean complex Gaussian distribution
with covariance matrix Î£ àª° 0. Under the AN-aided setting in (6),
we can deduce from the models in (1) and (2) that the SINRs at Bob
and Eves are
SINRð‘=
ð’˜ð»Râ„Žð’˜
Tr(Râ„ŽÎ£) + ðœŽ2ð‘›
, SINRð‘’,ð‘š=
ð’˜ð»Rð‘”,ð‘šð’˜
Tr(Rð‘”,ð‘šÎ£) + ðœŽ2ð‘£,ð‘š
, (8)
respectively, and the average transmit power is given by
E{âˆ¥x(ð‘¡)âˆ¥2} = âˆ¥ð’˜âˆ¥2 + Tr(Î£), (9)
where Tr(â‹…) denotes the trace of a matrix. While the SINRð‘ in (8)
seems degraded compared to the SNRs in (3) due to the added AN,
by joint design of the beamforming vector ð’˜ and AN covariance
matrix Î£, we shall be able to alleviate the degradation and in the
meantime effectively enhance the performance discrimination be-
tween Bob and Eves. Following the power minimization criterion
in (4), the design formulation for the AN-aided case is as follows:
ð‘ƒ â˜…AN = min
ð’˜,Î£
âˆ¥ð’˜âˆ¥2 +Tr(Î£) (10a)
s.t.
ð’˜ð»Râ„Žð’˜
Tr(Râ„ŽÎ£) + ðœŽ2ð‘›
â‰¥ ð›¾ð‘, (10b)
ð’˜ð»Rð‘”,ð‘šð’˜
Tr(Rð‘”,ð‘šÎ£) + ðœŽ2ð‘£,ð‘š
â‰¤ ð›¾ð‘’, ð‘š = 1, . . . ,ð‘€, (10c)
Î£ àª° 0. (10d)
A benefit that we can immediately see from (10) is that ð‘ƒ â˜…AN â‰¤
ð‘ƒ â˜…; i.e., the proposed formulation (10) is more power efficient than
its no-AN counterpart (4), given the same specifications of ð›¾ð‘ and
ð›¾ð‘’. The reason for this is that the feasible set of problem (4), together
with Î£ = 0, is merely a subset of the feasible set of (10). Problem
(10) is more difficult to handle than (4), however. In particular, one
can check that, unlike problem (4), problem (10) does not admit a
convex reformulation as in (5) whenRâ„Ž = hhð» . However, we will
present next that problem (10) can be efficiently handled by the SDP
relaxation technique [7].
2563
        







AN-aided design [in (10)]
No-AN design [in (4)]
1/ðœŽ2ð‘£ (dB)
A
ve
ra
ge
tr
an
sm
it
po
w
er
(d
B
)
Fig. 1. Simulation results (average transmit power versus 1/ðœŽ2ð‘£) for
ð›¾ð‘ = 10 dB, ð›¾ð‘’ = 3 dB, 1/ðœŽ2ð‘› = 10 dB,Râ„Ž = hh
ð» and Rð‘”,ð‘š =
ð’ˆð‘šð’ˆ
ð»
ð‘š for all ð‘š = 1, 2, 3.
As the final simulation example, we considered problems (4) and
(10) withRâ„Ž = hhð» andRð‘”,ð‘š = E{ð’ˆð‘šð’ˆð»ð‘š} for ð‘š = 1, 2, 3; i.e.,
Alice has the perfect Alice-to-Bob CSI but only knows the correla-
tion matrices of Alice-to-Eves channels. The channel vector h was
complex Gaussian distributed ð’žð’© (0, Ið‘ð‘¡/ð‘ð‘¡) and the covariance
matrices E{ð’ˆð‘šð’ˆð»ð‘š} were given by
E{ð’ˆð‘šð’ˆð»ð‘š} = ð›¼ Â¯ð‘šÂ¯
ð»
ð‘š
âˆ£âˆ£Â¯ð‘šâˆ£âˆ£2 + (1âˆ’ ð›¼)Î›, ð‘š = 1, 2, 3, (16)
where Â¯ð‘š âˆ¼ ð’žð’© (0, Ið‘ð‘¡ ), [Î›]ð‘–,ð‘— = 0.1âˆ£ð‘–âˆ’ð‘—âˆ£/ð‘ð‘¡ specifies the de-
gree of correlation between transmit antennas, and ð›¼ âˆˆ [0, 1] was
used to control the level of uncertainty of channel coefficients. In
this particular example, we found that problem (4) or problem (10)
may be infeasible (or have no solution) sometimes, especially if the
given problem instances Râ„Ž,Rð‘”,1, . . .Rð‘”,ð‘€ are harsh. In Fig. 3,
we present the percentage of infeasibility (%) of problems (4) and
(10) out of 1000 simulation trials for ð›¾ð‘’ = 3 dB. One can see from
the figure that for ð›¼ = 0.8 and 1/ðœŽ2ð‘£ â‰¥ 10 dB problem (4) has
about 30% infeasibility rate, and it can increase to about 55% for
ð›¼ = 0.6. The proposed problem (10) by contrast is always feasible
for both ð›¼ = 0.6 and ð›¼ = 0.8. The presented simulation results well
demonstrate the effectiveness of the proposed beamforming strategy
in power consumption as well as in feasibility.
5. REFERENCES
[1] A. D. Wyner, â€œThe wiretap channel,â€ Bell System Technical
Journal, vol. 54, pp. 1355â€“1387, 1975.
[2] P. Gopala, L. Lai, and H. El Gamal, â€œOn the secrecy capacity
of fading channels,â€ IEEE. Trans. Inform. Theory, vol. 54, no.
10, pp. 4687â€“4698, 2008.
[3] X. Li and J. Hwu, â€œUsing antenna array redundancy and chan-
nel diversity for secure wireless transmissions,â€ Journal of
Commun., vol. 2, no. 3, pp. 24â€“32, May 2007.
[4] L. Dong, Z. Han, A. Petropulu, and H. V. Poor, â€œSecure wire-
less communications via cooperation,â€ in Proc. Allerton Con-
ference on Communication, Control, and Computing, Monti-
cello, IL, USA, Sept. 23-26, 2008, pp. 1132â€“1138.
âˆ’50 0 50
0
5
10
15
20
25
30
Eve3Eve1 Bob Eve2
Beam pattern ofð’˜â˜…
dB
Direction (degree)
(a) Problem (4)
âˆ’50 0 50
0
5
10
15
20
25
30
Eve3Eve1 Bob Eve2
Beam pattern ofð’˜â˜…
Beam pattern of Î£â˜…
dB
Direction (degree)
(b) Problem (10)
Fig. 2. Optimum beam patterns of problems (4) and (10) for
ð›¾ð‘ = 20 dB, ð›¾ð‘’ = 0 dB, 1/ðœŽ2ð‘› = 0 dB, 1/ðœŽ
2
ð‘£ = 20 dB, Râ„Ž =
v(0)vð»(0), and Rð‘”,ð‘š = v(ðœ™ð‘š)vð»(ðœ™ð‘š) where {ðœ™1, ðœ™2, ðœ™3} =
{20âˆ˜,âˆ’20âˆ˜, 30âˆ˜}.
        













AN-aided design [in (10)], ð›¼ = 0.6
No-AN design [in (4)], ð›¼ = 0.6
AN-aided design [in (10)], ð›¼ = 0.8
No-AN design [in (4)], ð›¼ = 0.8
In
fe
as
ib
ili
ty
ra
te
(%
)
1/ðœŽ2ð‘£ (dB)
Fig. 3. Infeasibility rate (%) of problems (4) and (10) for ð›¾ð‘ = 10
dB, ð›¾ð‘’ = 3 dB and 1/ðœŽ2ð‘› = 10 dB.
[5] S. Goel and R. Negi, â€œGuaranteeing secrecy using artificial
noise,â€ IEEE. Trans. Wireless Commun., vol. 7, no. 6, pp.
2180â€“2189, June 2008.
[6] A. L. Swindlehurst, â€œFixed SINR solutions for the MIMO
wiretap channel,â€ in Proc. of IEEE ICASSP, Taipei, Taiwan,
April 19-24, 2009, pp. 2437â€“2440.
[7] N. D. Sidiropoulos, T. D. Davidson, and Z.-Q. Luo, â€œTransmit
beamforming for physical-layer multicasting,â€ IEEE Trans.
Signal Process., vol. 54, no. 6, pp. 2239â€“2251, June 2006.
[8] S. Boyd and L. Vandenberghe, Convex Optimization, Cam-
bridge University Press, Cambridge, UK, 2004.
[9] K. T. Phan, S. A. Vorobyov, N. D. Sidiropoulos, and C. Tellam-
bura, â€œSpectrum sharing in wireless networks via QoS-aware
secondary multicast beamforming,â€ IEEE Trans. Signal Pro-
cess., vol. 57, no. 6, pp. 2323â€“2335, June 2009.
[10] A. Wiesel, Y. C. Eldar, and S. Shamai, â€œLinear precoding via
conic optimization for fixed MIMO receivers,â€ IEEE Trans.
Signal Process., vol. 54, no. 1, pp. 161â€“176, Jan. 2006.
[11] J. F. Sturm, â€œUsing SeDuMi 1.02, a MATLAB toolbox for
optimization over symmetric cones,â€ Optimization Methods
and Software, vol. 11-12, pp. 625â€“653, 1999.
2565
 2
6/15 (Tuesday): Second day of the workshop 
08:30-09:30 Attended the Plenary talk 2: â€œFeature Mining from a HS Data Cube for Information 
Mapping: 3D and Beyondâ€ by Prof. Xiuping Jia, Univ. of New South Wales, Australian Defence 
Force Academy, Australia. 
09:30-11:10 Session tue-o-1-a: Attended the paper-presentation session on Machine Learning for 
Analysis of Hyperspectral Data (1)  
11:40-13:00 Session tue-p-a: Attended the poster-presentation session on Applications in Biomedical 
Imagery, Chemistry and Mineralogy. 
14:00-15:40 Session tue-o-2-a: Attended the paper-presentation session on Spatial-Spectral 
Hyperspectral Image Analysis (2).  
16:10-17:50 Session tue-o-3-a: Attended the paper-presentation session on Statistical vs Geometric 
Unmixing Algorithms for Hyperspectral Data (1) . 
18:30-22:30: Attended the workshopâ€™s Banquet dinner. 
 
6/16 (Wednesday): Third day of the workshop 
11:40-13:00 Session wed-p-a: Attended the paper-presentation session on Endmembers and 
Unmixing.  
14:00-15:40 Session wed-o-2-a: Attended the paper-presentation session on Feature Extraction and 
Dimension Reduction.  
16:10-17:50 Session wed-o-3-b: Attended the paper-presentation session on Statistical Vs Geometric 
Unmixing Algorithms for Hyperspectral Data (2). I presented our paper (details mentioned above) in 
this session of the workshop. 
17:50-18:00 Finally there were some concluding remarks and closing of the workshop.  
 
æ”œå›žè³‡ï¦¾ï¼šA hard copy of the Workshop Program, and a CD of the Workshop Proceedings  
 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
On all the three days of the workshop, I was actively involved in various discussions with the 
speakers and the participants. I also raised many questions during the discussion session of 
the paper presentations. Through the workshop, I could meet various international experts of 
this field and some of those experts, whom I met, are listed as under: 
1. Prof. Jose M. Bioucas-Dia (Instituto de Telecommunicaces, Instituto Superior 
Tecnico),  
2. Prof. Antonio Plaza (University of Extremadura),  
3. Prof. Jean-Yves Tourneret (University of Toulouse),  
4. Prof. Xiuping Jia (The University of New South Wales),  
5. Prof. Qian Du (Mississippi State University) 
6. Prof. Dobigeon (University of Toulouse). 
A ROBUST ALTERNATING VOLUME MAXIMIZATION ALGORITHM FOR ENDMEMBER
EXTRACTION IN HYPERSPECTRAL IMAGES
ArulMurugan Ambikapathiâ€ , Tsung-Han Chanâ€ , Wing-Kin Maâ€ âˆ— and Chong-Yung Chiâ€ 
â€ Inst. Commun. Eng., National Tsing Hua Univ. âˆ—Dept. Electronic Eng., Chinese Univ. Hong Kong
Hsinchu, Taiwan Shatin, N.T., Hong Kong
E-mail: (tsunghan@mx,cychi@ee).nthu.edu.tw E-mail: wkma@ieee.org
ABSTRACT
Accurate estimation of endmember signatures and the associated
abundances of a scene from its hyperspectral observations is at
present, a challenging research area. Many of the existing hyper-
spectral unmixing algorithms are based on Winterâ€™s belief, which
states that the vertices of the maximum volume simplex inside
the data cloud (observations) will yield high fidelity estimates of
the endmember signatures if pure-pixels exist. Based on Winterâ€™s
belief, we recently proposed a convex analysis based alternating vol-
ume maximization (AVMAX) algorithm. In this paper we develop
a robust version of the AVMAX algorithm. Here, the presence of
noise in the hyperspectral observations is taken into consideration
with the original deterministic constraints suitably reformulated as
probabilistic constraints. The subproblems involved are convex
problems and they can be effectively solved using available convex
optimization solvers. Monte Carlo simulations are presented to
demonstrate the efficacy of the proposed RAVMAX algorithm over
several existing pure-pixel based hyperspectral unmixing methods,
including its predecessor, the AVMAX algorithm.
Index Termsâ€” Hyperspectral unmixing, Convex analysis,
Chance constraints, Second-order cone program
1. INTRODUCTION
A hyperspectral sensor records the electromagnetic scattering pat-
terns of distinct materials over hundreds of spectral bands that range
from visible to near-infrared wavelength region. The limited spatial
resolution of the sensor used for hyperspectral imaging demands an
effective hyperspectral unmixing (HU) scheme to extract the under-
lying endmember signatures (or simply endmembers) and the asso-
ciated abundance maps distributed over a scene of interest [1]. Con-
ventional HU algorithms based on a linear mixing model (to be ex-
plained later) are based on the assumption that in a given set of hy-
perspectral observations, there exists a pure-pixel for each endmem-
ber, namely, pixels that are fully contributed by a single endmember.
HU algorithms based on the pure-pixel assumption includes pixel
purity index (PPI) [2], N-finder (N-FINDR) [3] and vertex compo-
nent analysis (VCA) [4]. A recent addition to this group is the AV-
MAX [5] algorithm, which is a convex analysis based optimization
algorithm based on Winterâ€™s belief. It employs an alternating linear
programming approach to solve the optimization problem. Never-
theless, the performance of the above mentioned algorithms is de-
graded when the observations are corrupted by noise.
This work was supported by the National Science Council (R.O.C.) un-
der Grant NSC 96-2628-E-007-003-MY3, and by General Research Funds
of Hong Kong Research Grant Council (Project Nos. CUHK415509,
CUHK415908).
In this paper, we extend our AVMAX [5] algorithm to a ro-
bust AVMAX (RAVMAX) algorithm that accounts for the noise ef-
fects by employing chance constraints. We first reformulate the AV-
MAX subproblems into equivalent problems to which the chance
constraint can be suitably applied, and then reformulate them as
second-order cone problems. Hence, the chance constrained prob-
lem can be efficiently solved by any convex optimization solvers in
an alternating fashion. We finally show some Monte-Carlo simula-
tions to demonstrate the efficacy of the proposed RAVMAX algo-
rithm, in comparison with the conventional pure-pixel based algo-
rithms, including its predecessor.
In the remainder of the paper, the following notations are em-
ployed. RM and RMÃ—N represent the sets of all real M Ã— 1 vectors
and M Ã—N matrices, respectively. The symbol  denotes compo-
nentwise inequality. 1N , IM and 0 respectively represent the N Ã— 1
all one vector, the M Ã—M identity matrix and an all-zero vector of
proper dimension. A Gaussian distribution with mean vector Âµ and
covariance matrix Î£ is denoted as N (Âµ,Î£). The notation sign(b)
denotes a vector whose elements are the signs of the elements in the
vector b, |b| denotes a column vector whose elements are the ab-
solute values of the individual elements in b, and diag(b) denotes
a diagonal matrix with the elements of b as its diagonal elements.
The symbol [a]i and Aij denote the ith element of the vector a and
(i, j)th element of matrixA, respectively. Finally, the symbol [A]i,:
corresponds to the ith row vector of A.
2. SIGNAL MODEL AND ASSUMPTIONS
Consider a scenario in which a hyperspectral sensor measures solar
electromagnetic radiation from N distinct substances. Each pixel of
the hyperspectral images measured over M spectral bands can be
represented by the following M Ã—N linear mixing model [1, 3â€“6]
y[n] = x[n] +w[n], (1)
x[n] = As[n] =
Nâˆ‘
i=1
si[n]ai, âˆ€n = 1, . . . , L. (2)
In (1), y[n] = [ y1[n], . . . , yM [n] ]T represents the nth ob-
served pixel vector comprising M spectral bands and x[n] =
[ x1[n], . . . , xM [n] ]
T corresponds to its noise-free counterpart.
The noise vector, w[n] = [ w1[n], . . . , wM [n] ]T in (1), is zero-
mean, uniform white Gaussian noise vector (i.e., N (0,D), where
D = Ïƒ2IM and Ïƒ denotes the standard deviation of the noise). In
(2), A = [ a1, . . . ,aN ] âˆˆ RMÃ—N denotes the endmember signa-
ture matrix with the ith column vector ai being the ith endmember
signature, s[n] = [ s1[n], . . . , sN [n] ]T âˆˆ RN is the nth abundance
vector comprising N fractional abundances and L is the total num-
ber of observed pixels. Assuming prior knowledge of the number of
q? = min
Î½jâˆˆR
Nâˆ’1,Î¸jâˆˆR
L
b
T
j GGÎ½j + (âˆ’1)
N+jdet(VNj)
s.t. GÎ½j = GXËœÎ¸j , Î¸j  0, 1
T
LÎ¸j = 1.
(16)
Then, by change of variables, we let
Î±j = BÎ½j (17)
and
Î²j = GÎ½j . (18)
To facilitate the application of chance constraints to the AVMAX
problem, we relax the first equality constraints of (15) and (16) and
thus the corresponding subproblems are given as below:
p? = max
Î±jâˆˆR
Nâˆ’1,Î¸jâˆˆR
L
|bj |
T
Î±j + (âˆ’1)
N+jdet(VNj)
s.t. Î±j  BXËœÎ¸j , Î¸j  0, 1
T
LÎ¸j = 1,
(19)
q? = min
Î²jâˆˆR
Nâˆ’1,Î¸jâˆˆR
L
âˆ’ |bj |
T
Î²j + (âˆ’1)
N+jdet(VNj)
s.t. Î²j  GXËœÎ¸j , Î¸j  0, 1
T
LÎ¸j = 1.
(20)
Although we relax the first constraints of the subproblems, we still
can show that the optimal solutions of (19) and (20) are equivalent to
that of (13) and (14), respectively, as proved in the following lemma:
Lemma 2. (Equivalence of subproblems) The subproblems (19)
and (20) are equivalent to (13) and (14), respectively.
The proof of Lemma 2 is given in Appendix.
4.2. Robust AVMAX Algorithm
Now we consider the unmixing problem with noisy observations
given by (1). Substituting (1) into (4), we get the following dimen-
sion reduced observations
yËœ[n] , CË†T (y[n]âˆ’ dË†) âˆ¼= xËœ[n] + CË†
T
w[n], (21)
where CË†Tw[n] is a random vector followingN (0, CË†TDCË†). In ma-
trix form (considering all the pixels, n = 1, . . . , L), we can write the
above equation as: YËœ = XËœ+ CË†TW, where YËœ = [yËœ[1], . . . , yËœ[L]],
XËœ = [xËœ[1], . . . , xËœ[L]], and W = [w[1], . . . ,w[L]]. The first in-
equality constraint in (19) now becomes:
Î±j  B(YËœ âˆ’ CË†
T
W)Î¸j = BYËœÎ¸j + zj , (22)
where zj , âˆ’BCË†TWÎ¸j âˆ¼ N (0,BCË†TDCË†BT â€–Î¸jâ€–22).
Since the noise-induced vector zj is random and unknown, our
approach is to replace (22) with chance constraints, as shown below
Pr{[Î±j ]i âˆ’ [B]i,:YËœÎ¸j â‰¤ [zj ]i} â‰¥ Î·,âˆ€ i = 1, . . . , N âˆ’ 1, (23)
where 0 < Î· < 1 is a design parameter. A similar equation can be
written for the first inequality constraint of (20), i.e.,
Pr{[Î²j ]i âˆ’ [G]i,:YËœÎ¸j â‰¤ [zj ]i} â‰¥ Î·,âˆ€ i = 1, . . . , N âˆ’ 1. (24)
The second-order cone equivalence of a chance constraint
has been discussed in [7]. Specifically, for a random variable
Îµ âˆ¼ N (Âµ, Î´2) and t âˆˆ R, one can show that Pr(Îµ â‰¤ t) â‰¥ Î· is true
as t â‰¥ Î´Î¦âˆ’1(Î·) + Âµ, where Î¦âˆ’1 is the inverse of the cumulative
distribution function of the standard normal random variable.
By letting Q = BCË†TDCË†BT âˆˆ R(Nâˆ’1)Ã—(Nâˆ’1) and applying
the above mentioned chance constraint procedure to (23) and (24),
we have âˆš
Qiiâ€–Î¸jâ€–2Î¦
âˆ’1(1âˆ’ Î·) â‰¥ [Î±j ]i âˆ’ [B]i,:YËœÎ¸j , (25)
and âˆš
Qiiâ€–Î¸jâ€–2Î¦
âˆ’1(1âˆ’ Î·) â‰¥ [Î²j ]i âˆ’ [G]i,:YËœÎ¸j . (26)
for all i = 1, . . . , Nâˆ’1. By replacing the first constraints of (19) and
(20) with (25) and (26), respectively, the robust AVMAX problem
can then be written as:
p? = max
Î±jâˆˆR
Nâˆ’1,Î¸jâˆˆR
L
|bTj |Î±j + (âˆ’1)
N+jdet(VNj)
s.t.
âˆš
Qiiâ€–Î¸jâ€–2Î¦
âˆ’1(1âˆ’ Î·) â‰¥ [Î±j ]i âˆ’ [B]i,:YËœÎ¸j ,
Î¸j  0, 1
T
LÎ¸j = 1, âˆ€ i = 1, 2, . . . , N âˆ’ 1.
(27)
q? = min
Î²jâˆˆR
Nâˆ’1,Î¸jâˆˆR
L
âˆ’ |bTj |Î²j + (âˆ’1)
N+jdet(VNj)
s.t.
âˆš
Qiiâ€–Î¸jâ€–2Î¦
âˆ’1(1âˆ’ Î·) â‰¥ [Î²j ]i âˆ’ [G]i,:YËœÎ¸j ,
Î¸j  0, 1
T
LÎ¸j = 1, âˆ€ i = 1, 2, . . . , N âˆ’ 1.
(28)
The values of Î· affect the feasible sets of (27) and (28); specif-
ically, their convexity. The following are the three possible cases:
When Î· > 0.5 (i.e., Î¦âˆ’1(1 âˆ’ Î·) < 0), the first constraints of both
subproblems are second-order cone constraints and hence subprob-
lems (27) and (28) are convex. If Î· = 0.5 (i.e., Î¦âˆ’1(1âˆ’Î·) = 0), the
first constraints of both subproblems reduce to those of the original
AVMAX problem (as in (13) and (14)), i.e., the constraints become
linear (convex). Finally, if Î· < 0.5 (i.e., Î¦âˆ’1(1 âˆ’ Î·) > 0), the
constraints become non-convex. The effect of Î· is illustrated in Fig-
ure 1. From our extensive numerical experiences we found that for
satisfactory performance, the Î· value should lie between 0.9 and 1,
in which case the subproblems (27) and (28) are convex.
true simplex
noisy observed pixels 
Î· > 0.5
Î· = 0.5
Î· < 0.5
Fig. 1. Illustration of the effect of Î· in RAVMAX for N = 3.
Some more technical aspects are discussed as follows. The sub-
problems (27) and (28) are solved in an alternating fashion, simi-
lar to the original AVMAX explained in Section 3, except for the
following difference: after each execution of the subproblems, the
corresponding Î½j = BÎ±j (by (17)) is obtained if |p?| > |q?|, else
Î½j = GÎ²j (by (18)) is obtained. The proposed RAVMAX algo-
rithm uses any N pixel vectors in XËœ for its initialization. As the
subproblems are convex (for our desired choice, Î· > 0.5), they can
be solved effectively by using available convex optimization solvers.
5. SIMULATIONS
This section demonstrates the efficacy of the proposed RAVMAX
through comparison with other pure-pixel based HU algorithms. The
algorithms considered are N-FINDR [3], VCA [4] and AVMAX [5].
 1
ï¥«è¨ªç¶­å‰å°¼äºžï§¤å·¥æš¨å·žï§·å¤§å­¸å¿ƒå¾—å ±å‘Š 
2009/10/20 
 
è¨ˆç•«ç·¨è™Ÿ NSC 96-2628-E-007-003-MY3  
è¨ˆç•«åç¨± é€šè¨Šèˆ‡ç”Ÿç‰©é†«å­¸æˆåƒä¹‹å‰çž»ç›²è”½è¨Šè™Ÿæºåˆ†ï§ª 
å‡ºåœ‹äººå“¡å§“å 
æœå‹™æ©Ÿé—œåŠè·ç¨± 
ç¥å¿ å‹‡ 
åœ‹ï§·æ¸…è¯å¤§å­¸é€šè¨Šå·¥ç¨‹ç ”ç©¶æ‰€ æ•™æŽˆ 
ï¥«è¨ªåœ°é»ž 10/5/2009-10/18/2009, Virginia Polytechnic and State University (Virginia Tech.)  
 
ä¸€ã€ï¥«è¨ª Virginia Polytechnic and State University (Virginia Tech.)ç¶“éŽ (10/5-10/18)  
 
ç­†è€…æ–¼ 98ï¦Ž 10æœˆ 5æ—¥(ä¸€) æ™šä¸Šå¾žæ¡ƒåœ’æ©Ÿå ´å‡ºç™¼ï¼Œæ–¼ Los Angelesæ©Ÿå ´è½‰æ©Ÿï¼Œåˆæ–¼èŠåŠ 
å“¥æ©Ÿå ´è½‰æ©Ÿï¼Œæ–¼ 10æœˆ 6æ—¥(äºŒ)ä¸ŠåˆæŠµé”Washington Reganæ©Ÿå ´ï¼Œä½é€²Comfort Inn Ballston 
(1211 North Glebe Road, Arlington, VA 22201, US)ã€‚ 
 
10æœˆ 7æ—¥è‡³ 9æ—¥èˆ‡æˆ‘å€‘çš„åœ‹éš›åˆä½œå–®ä½ä¹‹å°æ‡‰æ•™æŽˆ(Prof. Yue Wang)ï¼ŒåŠå…¶ç ”ç©¶åœ˜éšŠ(è¶…
éŽåååšå£«ç ”ç©¶ç”Ÿ)é€²ï¨ˆç ”ç©¶è¨Žï¥åŠå ±å‘Šï¼Œä¸¦æ–¼ 10æœˆ 9æ—¥(äº”)ä¸‹åˆçµ¦ç¬¬ä¸€å ´æ¼”è¬›å¦‚ä¸‹:  
 
Talk 1: Non-negative Least-correlated Component Analysis for Separation of 
Dependent Sources by Volume Maximization  
Abstract: Although significant efforts have been made in developing non-negative blind source 
separation techniques, accurate separation of positive yet dependent sources remains a challenging 
task. In this talk, a joint correlation function of multiple signals is proposed to reveal and confirm that 
the observations after non-negative mixing would have higher joint correlation than the original 
unknown sources. Accordingly, a new non-negative least-correlated component analysis (nLCA) 
method is presented to design the unmixing matrix by minimizing the joint correlation function 
among the estimated nonnegative sources. In addition to a closed-form solution for unmixing two 
mixtures of two sources, the general algorithm of nLCA for the multi-source case is developed based 
on an iterative volume maximization (IVM) principle and linear programming. The source 
identifiability and required conditions are discussed and proved. The proposed nLCA algorithm, 
denoted as nLCA-IVM, is evaluated with both simulation data and real biomedical data to 
demonstrate its superior performance over several existing benchmark methods.  
 
æ–¼æ¼”è¬›ä¸­èˆ‡å‡ºå¸­å¸«ç”Ÿå€‘å……åˆ†è¨Žï¥ç ”ç©¶å•é¡Œï¼Œäº’å‹•ï¥¼å¥½ï¼Œå—ç›Šï¥¼å¤šã€‚ 
 
 V I R G I N I A  P O L Y T E C H N I C  I N S T I T U T E  A N D  S T A T E  U N I V E R S I T Y  
A n  e q u a l  o p p o r t un i t y ,  a f f i r ma t i v e  ac t i o n  i n s t i t u t i on  
 
 Invent the Future 
College of Engineering 
National Capital Region 
 
Bradley Department of Electrical & Computer Engineering 
4300 Wilson Blvd., Suite. 750 
Arlington, VA  22203 
703.528.5500  Fax: 703.528.5543 
www.ece.vt.edu 
Chong-Yung Chi 
Professor, Institute of Communications Engineering, & 
Department of Electrical Engineering 
National Tsing Hua University 
101, Sec. 2, Kuang Fu Rd., Hsinchu, Taiwan 30013 
 
September 18, 2009 
 
RE: Invitation Letter for Prof. C-Y Chiâ€™s Visit 
 
 
Dear Prof. Chi: 
 
It is my great pleasure to write to you and formally invite you to visit our research group here, in 
the Bradley Department of Electrical and Computer Engineering at Virginia Tech.  
 
During your visiting period of 10/5/09-10/18/09, we have arranged two formal seminars to be 
given by you: 
 
October 9, 2009: Non-negative Least-correlated Component Analysis for Separation of 
Dependent Sources by Volume Maximization 
 
October 16, 2009:  A Convex Analysis and Optimization Perspective to Unmixing of 
Hyperspectral Images  
     
I am very excited about your coming and look forward to working with you. 
 
Yours sincerely, 
 
 
Yue (Joseph) Wang, PhD & CBIL Director, AIMBE Fellow 
The Grant A. Dove Professor of Electrical and Computer Engineering 
Virginia Polytechnic Institute and State University (Virginia Tech) 
Email: yuewang@vt.edu 
Office: (703) 387-6056  
http://www.ece.vt.edu/faculty/ywang.html 
 

Outline
1 Introduction
2 Problem Formulation and Assumptions
3 Non-negative Least-correlated Component Analysis (nLCA)
4 Dimension and Noise Reduction by Principal Component
Analysis (PCA)
5 Experimental Results
6 Conclusions
2 / 51
Introduction
Some recent nBSS developments do not require sources to be
statistically independent (or uncorrelated).
NMF4 [Lee99] and CAMNS5 [Chan07].
Non-negative least-correlated component analysis (nLCA)
Assume non-negativity for sources and mixing matrix, without requiring
any statistical dependence/independence assumptions or conditions on
sources.
Minimize a joint correlation function (not in any statistical sense) of all
the separated sources for the design of the demixing matrix.
The nLCA to be presented is fulfilled by an iterative volume maximization
(IVM) algorithm, denoted as nLCA-IVM, which shows better
performance than some existing benchmark methods.
A closed-form solution is available for unmixing two mixtures of two
sources.
4
NMF: Non-negative Matrix Factorization.
5
CAMNS: Convex Analysis of Mixtures of Non-negative Sources.
4 / 51
Notations
R, RN , RMÃ—N Set of real numbers, N -vectors, M Ã—N
matrices
R+, RN+ , RMÃ—N+ Set of non-negative real numbers, N -vectors,
M Ã—N matrices
1N All-one column vector of dimension N
IN N Ã—N identity matrix
ei Unit vector with the ith entry equal to 1
 Componentwise inequality
â€– Â· â€– Euclidean norm of a vector or Forbenius norm
of a matrix
diag{Î±1, . . . , Î±N} N Ã—N diagonal matrix with diagonal entries
Î±1, . . . , Î±N
6 / 51
Problem Formulation
Our goal is to design a demixing matrix W = [wij ] âˆˆ RNÃ—M such
that
y[n] =Wx[n] = (WA)s[n] = Ps[n], (2)
where
y[n] = (y1[n], y2[n], . . . , yN [n])T is the extracted (unmixed)
source point,
P =WA âˆˆ RNÃ—N is a permutation matrix.
Define
xi = (xi[1], xi[2], . . . , xi[L])T , i = 1, ...,M, (ith observation)
si = (si[1], si[2], . . . , si[L])T , i = 1, ..., N, (ith source)
yi = (yi[1], yi[2], . . . , yi[L])T , i = 1, ..., N, (ith extracted source)
8 / 51
Assumptions
The nLCA-IVM to be presented is based on the following
non-statistical assumptions:
(A1) (Non-negative sources) For each j, sj âˆˆ RL+.
(A2) (Non-negative mixing matrix) A âˆˆ RMÃ—N+ .
(A3) M â‰¥ N and A is of full column rank, i.e., rank(A) = N .
(A4) (Unit row sum) A1N = 1M .
(A1) and (A2) are usually true in imaging analysis.
(A3) is generally true in nBSS problems.
(A4) is satisfied in magnetic resonance imaging (MRI) due to
the partial volume effect [Wang06], in contrast to the full
additivity condition on the sources (i.e., sT [n]1N = 1, âˆ€n) in
hyperspectral imaging [Stein02].
10 / 51
3. nLCA: Geometric Insights through Convex Analysis
Convex Hull
Convex hull of a set of vectors {z1,z2, . . . ,zN} âŠ‚ RL:
conv{z1,z2, . . . ,zN} =
{
z
âˆ£âˆ£âˆ£ z = Nâˆ‘
i=1
Î±izi,
Nâˆ‘
i=1
Î±i = 1, Î±i âˆˆ R+
}
.
Solid Region
Solid region of a set of vectors {z1,z2, . . . ,zN} âŠ‚ RL:
sol{z1,z2, . . . ,zN} =
(
z



z =
N
X
i=1
Î±izi,
N
X
i=1
Î±i â‰¤ 1, Î±i âˆˆ R+
)
.
(Please refer to [Boyd04] for further details of convex analysis)6
6[Boyd04] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge,
UK, Cambridge University Press, 2004.
12 / 51
nLCA: Geometric Insights
For the case of M = 2, the conventional pair-wise correlation
coefficient between x1 and x2 is widely known as
Ï(x1,x2) =
xT1 x2
â€–x1â€– Â· â€–x2â€– = cos(Î¸(x1,x2)),
where 0 â‰¤ Î¸(x1,x2) â‰¤ pi/2 is the angle between the vectors
x1  0 and x2  0. It is straightforward to see
min Ï(x1,x2) â‡” max Î¸(x1,x2).
14 / 51
nLCA: Joint Correlation Function of Multiple Signals
We propose a joint correlation function of M â‰¥ 2 signal vectors
defined as the inverse of the volume of the solid region formed by
these signal vectors, i.e.,
corr(x1,x2, . . . ,xM ) =
1
V (sol{x1,x2, . . . ,xM})
where
V (sol{x1,x2, . . . ,xM}) =
âˆ«
sol{x1,x2,...,xM}
dÎ¾,
namely, the multiple integral over sol{x1,x2, . . . ,xM}.
16 / 51
nLCA: nBSS problem
Owing to one-to-one mapping between corr(Â·) and V (sol{Â·}), the
nBSS problem can be reformulated as
max
W
V (sol{y1, . . . ,yN})
s.t. W1M = 1N , yi = Î£
N
j=1wijxj  0, âˆ€i = 1, . . . , N.
(6)
Case of M = N : the IVM algorithm for solving (6) will be
presented next, which is applicable only for M = N .
Case of M > N : the principal component analysis (PCA)
[Hastie02] is considered to obtain best N noise-reduced and
rank-reduced observations to be processed by the IVM
algorithm.
18 / 51
nLCA: IVM Algorithm (M = N)
The nBSS problem (7) can be decomposed into the following
two optimization subproblems:
p? = max
W
det(W)
s.t. W1N = 1N , Î£Nj=1wijxj  0, âˆ€i = 1, 2, . . . , N.
(8a)
q? = min
W
det(W)
s.t. W1N = 1N , Î£Nj=1wijxj  0, âˆ€i = 1, 2, . . . , N.
(8b)
The optimal solution of (7), denoted by W?, is chosen as the
optimal solution of (8a) if |p?| > |q?| or the optimal solution
of (8b) if |q?| > |p?|.
20 / 51
nLCA: IVM Algorithm (M = N)
Each subproblem of (8) becomes a linear program (LP) problem
p? = max
wi
N
X
j=1
(âˆ’1)i+jwijdet(Wij)
s.t. wTi 1N = 1, yi = Î£
N
j=1wijxj  0,
(9a)
q? = min
wi
N
X
j=1
(âˆ’1)i+jwijdet(Wij)
s.t. wTi 1N = 1, yi = Î£
N
j=1wijxj  0,
(9b)
and can be solved using interior-point methods with the computational
complexity
O(L(N âˆ’ 1) + (N âˆ’ 1)3) ' O(L(N âˆ’ 1))
on average since L N .
The resulting iterative algorithm is the nLCA-IVM Algorithm with
complexity O(N2LÎ·) where Î· denotes the number of iterations involved.
22 / 51
nLCA: Insights of the IVM Algorithm
Move yi to the boundary of the feasible set such that
V (sol{y1, . . . ,yN}) is maximum while fixing
y1, . . . ,yiâˆ’1,yi+1, . . . ,yN , i.e., the V (sol{y1, . . . ,yN}) is
guaranteed to increase maximally.
Figure: A geometric illustration of the IVM algorithm for N = M = 3.
24 / 51
nLCA: Computational Complexity of IVM Algorithm
Complexity comparison of the nLCA-IVM Algorithm with two
non-iterative pure-source sample based algorithms, nLCA-ES,
CAMNS, and five iterative algorithms, nICA, FastICA
[Hyvarine01], NMF, Quasi-Newton NMF (QNMF) [Zdunek07], and
Sparse NMF (sNMF) [Liu03]:
Methods nLCA-IVM nLCA-ES CAMNS nICA
Complexity orders O(N2rÎ·) O(N3L) O((N âˆ’ 1)2L) O(N2LÎ·)
Methods FastICA NMF QNMF sNMF
Complexity orders O(N3LÎ·) O(N2LÎ·) O(N2LÎ·) O(N2LÎ·)
Î·: number of iterations involved
r: number of extreme points in conv{x[1],x[2], . . . ,x[L]}
The nLCA-IVM Algorithm has lower complexity order than all the
other iterative algorithms.
26 / 51
nLCA-IVM: Closed-form Solution for M = N = 2
Proposition 1
Under (A1) to (A4), problem (7) has a closed-form solution W? where
w?11 = max
n
n âˆ’x2[n]
x1[n]âˆ’ x2[n]



x1[n] > x2[n], n = 1, 2, . . . , L
o
,
w?21 = min
n
n âˆ’x2[n]
x1[n]âˆ’ x2[n]



x1[n] < x2[n] n = 1, 2, . . . , L
o
,
and w?12 = 1âˆ’ w?11 and w?22 = 1âˆ’ w?21.
Remarks:
nLCA-ES also has the same solution for W? above for the case of
M = N = 2.
The nLCA-IVM obtains W? in one iteration (besides one iteration for
convergence check) for the case of M = N = 2.
28 / 51
Dimension and Noise Reduction by PCA
By the rank inequality [Horn85],
rank(S)+rank(A)âˆ’N â‰¤ rank(SAT ) â‰¤ min{rank(S), rank(A)},
one can see, from rank(A) = N and rank(S) = N , that
rank(SAT ) = N < M.
However,
N â‰¤ rank(X) â‰¤M, as Î¨ 6= 0,
implying that usually rank(X) = M as Î¨ 6= 0 while rank(X) = N
ideally (as Î¨ = 0).
=â‡’ Use PCA for simultaneous noise reduction and rank reduction.
30 / 51
Dimension and Noise Reduction by PCA
The optimum solutions of Âµ, Î»i, and V are given by
Âµ? =
1
M
Mâˆ‘
i=1
xi,
Î»?i = (V
?)T (xi âˆ’ Âµ?),
and V? is a matrix comprising (N âˆ’ 1) principal left singular
vectors of [x1 âˆ’ Âµ?, . . . ,xM âˆ’ Âµ?] âˆˆ RLÃ—M .
Selection of N Noise-reduced Observations
The selected N noise-reduced (rank-reduced) observations x?i âˆˆ RL+ to be
processed by the IVM algorithm are the ones with the smallest approximation
errors â€–x?i âˆ’ xiâ€–2, and meanwhile all the negative components of the selected
x?i are replaced by zero.
32 / 51
Experimental Results
 Averaged cross-correlation coefficient Ïave between the true
sources and extracted sources, averaged computation time
Tave per realization, and averaged number of iterations Î·ave
were measured for comparison.
 For verifying the source identifiabilty of nLCA, we artificially
added pure-source samples in the original source images in
Experiment 1.
 All the experiments were performed using Toshiba Satellite
A100 laptop personal computer (equipped with Intel Centrino
Duo Processor T2050 CPU 1.6GHz, 1 GB memory and
Microsoft windows XP home edition version 2002 with service
pack 2).
34 / 51
Experiment 1
Averaged cross-correlation coefficient (Ïave) between the true sources and
extracted sources, averaged computation time (Tave), and averaged number of
iterations (Î·ave) of the various BSS methods in Experiment 1 (L = 16384).
Exp. 1 (Human face image separation)
Testing M = N = 6, SNR=âˆž M = 20, N = 6, SNR=25 dB M = N = 6, SNR=âˆž
cases (No pure-source sample) (No pure-source sample) (Pure-source samples added)
Method Ïave Tave (sec) Î·ave Ïave Tave (sec) Î·ave Ïave Tave (sec) Î·ave
nLCA-IVM 0.9952 22.95 3.92 0.9213 15.60 5.48 1 3.94 2.26
nLCA-ES 0.7661 0.05 â€“ 0.6383 0.06 â€“ 1 0.06 â€“
CAMNS 0.8973 13.66 â€“ 0.7685 9.54 â€“ 1 12.23 â€“
nICA 0.4694 54.19 2000 0.4590 54.62 2000 0.5032 54.82 2000
FastICA 0.4357 52.80 83.68 0.3889 58.99 77.76 0.4050 61.78 81.22
NMF 0.6156 58.98 2000 0.6959 59.27 2000 0.6307 60.07 2000
QNMF 0.5614 77.14 2000 0.5465 76.44 2000 0.5093 74.64 2000
sNMF 0.5968 93.84 2000 0.7079 94.69 2000 0.6017 95.11 2000
1
36 / 51
Experiment 1
FastICA
NMF
QNMF
sNMF
nICA
PDF Created with deskPDF PDF Writer - Trial :: http://www.docudesk.com
Figure: (Continued) Human face images for the noise-free case in Exp. 1.
38 / 51
Experiments 2 and 3
Averaged cross-correlation coefficient (Ïave) between the true sources and
extracted sources, averaged computation time (Tave), and averaged number of
iterations (Î·ave) of the various BSS methods in Experiments 2 (L = 3526) and
3 (L = 26896).
Exp. 2 (Infrared spectra decomposition) Exp. 3 (Dual-energy X-ray image decomposition)
Testing M = N = 3, SNR=âˆž, M = N = 2, SNR=âˆž,
cases (No pure-source sample) (Pure-source samples exist)
Methods Ïave Tave (sec) Î·ave Ïave Tave (sec) Î·ave
nLCA-IVM '1 2.15 3.50 1 0.46 2
nLCA-ES 0.9999 0.01 â€“ 1 0.06 â€“
CAMNS 0.9994 1.59 â€“ 1 3.07 â€“
nICA 0.5184 7.53 2000 0.6660 23.68 2000
FastICA 0.9221 0.52 9.40 0.3451 16.73 5.76
NMF 0.8654 4.89 2000 0.8201 22.15 2000
QNMF 0.8545 9.47 2000 0.9448 28.19 2000
sNMF 0.8841 10.11 2000 0.8044 48.10 2000
1
40 / 51
Performance comparisons in Exp. 2 and 3
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
nICA
FastICA
NMF
QNMF
sNMF
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
0 2000 4000
0
0.5
1
PDF Created with deskPDF PDF Writer - Trial :: http://www.docudesk.com
Figure: (Continued) Infrared spectra decomposition in Exp. 2.
42 / 51
Real Data Experiment: Fluorescence microscopy signals
Fluorescence microscopy produces multispectral images by staining the
components of a specimen with different fluorescence biomarkers, and
captures fluorescence images by an optical sensor array (e.g. CCD
camera).
In the investigation of the multiple labeled fluorescent specimen, two or
more emission signals are often overlapped in the measured images since
the spatial positions of the labeled components are very close. This is
so-called crosstalk, which can be modeled as an nBSS problem.
A set of images of a dividing newt lung cell is taken from
http://publications.nigms.nih.gov/insidethecell/chapter1.html.
44 / 51
Real Data Experiment: DCE-MRI
Dynamic Contrast Enhancement-MRI (DCE-MRI) can distinguish
vascular heterogeneity and elucidate features that characterize angiogenic
blood vessels from their normal counterparts.
DCE-MRI also has potential utility in measuring the efficacy of
angiogenesis inhibitors in cancer treatment in vivo.
Nineteen breast DCE-MRI images (observations) (M = 19 and N = 2) at
different time points provided from Prof. Yue Wang, Virginia Polytechnic
Institute and State University, USA, were used in the experiment.
Each column of the mixing matrix A = [aij ]19Ã—2 is so-called â€œTime
Activity Curveâ€ (TAC) of one source. Non-negative least squares estimate
of TACs can be obtained from X (= SAT +Î¨) and SË†.
46 / 51
Real Data Experiment: DCE-MRI
TACs can show the metabolism variations of the injected contrast agent
for different physiologies.
The descending curve (dashed line) shows the fast contrast agent
perfusion, a phenotype of cancer tumors in metabolism. The ascending
curve (solid line) exhibits the slow contrast agent perfusion of normal
tissues.
1 3 5 7 9 11 13 15 17 19
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
TAC (a
i1
 and a
i2
)
time (i)
PDF Created with deskPDF PDF Writer - Trial :: http://www.docudesk.com
Figure: Obtained TACs for contrast agent perfusion.
48 / 51
Conclusions
Experimental results show the superior performance of the
nLCA-IVM over some existing BSS algorithms
â™¦ Non-iterative pure-source sample criterion based algorithms:
nLCA-ES and CAMNS;
â™¦ Iterative ICA based algorithms: nICA and FastICA;
â™¦ Iterative NMF based algorithms: NMF, QNMF, and sNMF;
and the superior computational efficiency over the above iterative
algorithms.
The paper based on the presented research results has been
accepted for publication in IEEE Trans. Pattern Analysis and
Machine Intelligence as a regular paper, and the paper preprint has
been posted on IEEE Xplore.
Thank you very much for your attention.
50 / 51
A Convex Analysis and Optimization Perspective to
Unmixing of Hyperspectral Images
Chong-Yung Chi
Institute of Communications Engineering &
Department of Electrical Engineering
National Tsing Hua University, Taiwan 30013
Tel: +886-3-5731156, Fax: +886-3-5751787
E-mail: cychi@ee.nthu.edu.tw
http://www.ee.nthu.edu.tw/cychi/
Invited talk at Virginia Tech., Arlington, VA, 2009/10/16.
Acknowledgment: Dr. Tsung-Han Chanâ€™s helps for viewgraphs preparation
1 / 62
Notations
R, RN , RMÃ—N Set of real numbers, N Ã— 1 vectors, M Ã—N matrices
R+, RN+ , R
MÃ—N
+ Set of non-negative real numbers, N Ã— 1 vectors,
M Ã—N matrices
Xâ€  Moore-Penrose inverse or pseudo-inverse of matrix X
1N N Ã— 1 vector with all the components equal to unity
IN N Ã—N identity matrix
ei Unit vector of proper dimension with the ith entry
equal to 1
 Componentwise inequality
â€– Â· â€– Euclidean norm
det(X) Determinant of a square matrix X
3 / 62
Figure: Illustration of hyperspectral imaging for remote sensing. Courtesy
to G. Shaw and D. Manolakis [Shawâ€™02].
5 / 62
Problem: Estimation of endmember signatures a1, . . . ,aN and
abundance vectors s[1], . . . , s[L] from the given observed pixels
x[1], . . . ,x[L]. (A blind source separation (BSS) problem, or a blind
detection and channel estimation problem in wireless communications)
Figure: An illustration of linear mixing model for hyperspectral unmixing.
7 / 62
Existing Algorithms for Hyperspectral Unmixing
Endmember extraction algorithms based on the pure pixel
assumption [(A4)] that only estimate endmembers (A); e.g.,
pixel purity index (PPI) [Boardmanâ€™95],
n-finder (N-FINDR) [Winterâ€™99],
vertex component analysis (VCA) [Nascimentoâ€™05],
convex cone analysis (CCA) [Ifarraguerriâ€™99].
Endmember estimation is usually followed by abundance
estimation using, for instance, fully constrained least squares
(FCLS) [Heinzâ€™01].
Unmixing algorithms that estimate endmembers (A) and
abundances s[n] simultaneously; e.g.,
alternating projected subgradients (APS) [Zymnisâ€™07],
iterated constrained endmembers (ICE) [Bermanâ€™04],
minimum volume transform (MVT) [Craigâ€™94],
minimum volume constrained non-negative matrix factorization
(MVC-NMF) [Miaoâ€™07],
minimum volume simplex analysis (MVSA) [Liâ€™08].
9 / 62
Our Contributions
It can be proven by convex analysis that both Winterâ€™s and
Craigâ€™s unmixing criteria yield the same N true endmembers
when N pure pixels exist.
We illustrate how the two (non-convex) optimization problems
can be efficiently solved by alternating linear programming,
leading to AVMAX algorithm (by Winterâ€™s criterion) and
AVMIN algorithm (by Craigâ€™s criterion).
Simulation results show their superior performance over some
existing benchmark unmixing methods for synthetic data, and
the AVMIN algorithm also works well for real hyperspectral
image data (Cuprite mining site, Nevada, 1997).
11 / 62
Affine Hull
An affine hull can always be represented by:
aff{a1, . . . ,aN} =
{
x = CÎ±+ d
âˆ£âˆ£ Î± âˆˆ RP }
for some (non-unique) d âˆˆ RM and C âˆˆ RMÃ—P , where
P â‰¤ N âˆ’ 1 is the affine dimension.
If {a1, . . . ,aN} is an affinely independent set (or
{a1 âˆ’ aN , . . . ,aNâˆ’1 âˆ’ aN} is linearly independent), then the
affine dimension P is maximal; i.e., P = N âˆ’ 1.
13 / 62
A point x âˆˆ conv{a1, . . . ,aN} is an extreme point (or
vertex) of conv{a1, . . . ,aN} if x is not any nontrivial convex
combination of {a1, . . . ,aN}. Intuitively, the extreme points
of conv{a1, . . . ,aN} are its corner points.
If {a1, . . . ,aN} is affinely independent, then
conv{a1, . . . ,aN} is called a simplex and its extreme points
(or vertices) are a1, . . . ,aN .
A simplex is called simplest simplex if M = N âˆ’ 1. For
instance, point (N = 1), line segment (N = 2), triangle and
its interior (N = 3).
In the following presentation, we use the term â€™simplexâ€™
instead of â€™simplest simplexâ€™ for the sake of simplicity.
15 / 62
A. Dimension Reduction:
Under (A2) and (A3), one can readily infer from (1) that
x[n] =
Nâˆ‘
i=1
si[n]ai âˆˆ aff{a1, . . . ,aN} = A(C,d), âˆ€ n
for some (C,d) âˆˆ RMÃ—(Nâˆ’1) Ã— RM and rank(C) = N âˆ’ 1.
Lemma 1 (Endmember affine set construction)
Let d = 1L
âˆ‘L
n=1 x[n] and U = [ x[1]âˆ’ d, . . . ,x[L]âˆ’ d ] âˆˆ RMÃ—L.
Suppose that (A2) and (A3) are true. Then
A(C,d) = aff{x[1], . . . ,x[L]},
where
C = [q1(UUT ), q2(UUT ), . . . , qNâˆ’1(UUT )],
in which qi(R) denotes the unit-norm eigenvector associated with the ith
principal eigenvalue of R.
17 / 62
Simplex Geometry for Hyperspectral Unmixing
Figure: Scatter plot of the dimension-reduced pixels for N = 3.
19 / 62
Pictorial illustration - Winterâ€™s and Craigâ€™s Unmixing Criteria
Figure: Scatter plot of the dimension-reduced pixels for N = 3, illustrating
Winterâ€™s and Craigâ€™s criteria for hyperspectral unmixing.
21 / 62
Alternating LP Approach to Volume Maximization
Theorem 1 (Endmember identifiability of Winterâ€™s criterion)
The optimal solution of (3) is uniquely given by Î±1, . . . ,Î±N if and
only if (A1)âˆ’ (A4) hold. Each Î±i corresponds to an endmember
ai = CÎ±i + d, where (C,d) is given in Lemma 1.
An equivalent form of (3) is given by
Maximum volume simplex fitting problem
max
Î½iâˆˆRNâˆ’1
Î¸1,...,Î¸NâˆˆRL
|det(âˆ†(Î½1, . . . ,Î½N ))|
s.t. Î½i = XËœÎ¸i, Î¸i  0, 1TLÎ¸i = 1 âˆ€ i,
(4)
where XËœ = [ xËœ[1], . . . , xËœ[L] ] âˆˆ R(Nâˆ’1)Ã—L.
It is a nonconvex problem, since the objective function is
nonconvex.
23 / 62
The partial maximization of (4) with respect to Î½j , while
fixing Î½1, . . . ,Î½jâˆ’1,Î½j+1, . . . ,Î½N :
max
Î½jâˆˆRNâˆ’1,Î¸jâˆˆRL
âˆ£âˆ£âˆ£ bTj Î½j + (âˆ’1)N+jdet(VNj)âˆ£âˆ£âˆ£
s.t. Î½j = XËœÎ¸j , Î¸j  0, 1TLÎ¸j = 1.
(4p)
Solve (5p) by breaking it into two LPs:
p? = max
Î½jâˆˆRNâˆ’1,Î¸jâˆˆRL
bTj Î½j + (âˆ’1)N+jdet(VNj)
s.t. Î½j = XËœÎ¸j , Î¸j  0, 1TLÎ¸j = 1,
(4p+)
q? = min
Î½jâˆˆRNâˆ’1,Î¸jâˆˆRL
bTj Î½j + (âˆ’1)N+jdet(VNj)
s.t. Î½j = XËœÎ¸j , Î¸j  0, 1TLÎ¸j = 1.
(4pâˆ’)
25 / 62
Pseudo-code of the AVMAX method
Given A convergence tolerance Îµ > 0, an endmember affine set
characterization (C,d), the dimension-reduced pixels xËœ[n] for
all n, and the number of endmembers N .
Step 1. Obtain some feasible initial Î½1, . . . ,Î½N by randomly selecting
any N dimension-reduced pixels from xËœ[1], . . . , xËœ[L].
Step 2. Set j := 1 and % := |det(âˆ†(Î½1, . . . ,Î½N ))|.
Step 3. Solve the LPs (4p) and (4p+),and obtain their optimal
solutions, denoted by (Î½Â¯j , Î¸Â¯j) and (Î½j ,Î¸j), respectively.
Step 4. If |p?| > |q?|, then update (Î½j ,Î¸j) := (Î½Â¯j , Î¸Â¯j). Otherwise,
update (Î½j ,Î¸j) := (Î½j ,Î¸j).
Step 5. If (j modulo N) 6= 0, then j := j + 1, and goto Step 3,
else
If |max{|p?|, |q?|} âˆ’ %|/% < Îµ, then Î½?j = Î½j for all j.
Otherwise, set % := max{|p?|, |q?|}, j := 1, and goto Step 3.
27 / 62
Problem Formulation by Craigâ€™s Criterion
Based on the belief of Craigâ€™s unmixing criterion, we have
Minimum volume simplex fitting problem
min
Î²1,...,Î²N
V (Î²1, . . . ,Î²N )
s.t. xËœ[n] âˆˆ conv{Î²1, . . . ,Î²N}, âˆ€ n.
(5)
Theorem 2 (Endmember identifiability of Craigâ€™s criterion)
Under (A1) through (A4), the optimal solution of (5) is uniquely
given by Î±1, . . . ,Î±N . Each Î±i corresponds to a true endmember
ai = CÎ±i + d, where (C,d) is given in Lemma 1.
Theorem 2 provides a sufficient identifiability condition (i.e.,
(A4)). Accurate endmember identification could still be
possible even as pure pixels are not existent by our experience.
29 / 62
Minimum volume simplex fitting problem
min
B, Î²N ,
sâ€²[1],...,sâ€²[L]
|det(B)|
s.t. sâ€²[n]  0, 1TNâˆ’1sâ€²[n] â‰¤ 1,
xËœ[n] = Î²N +Bsâ€²[n], âˆ€ n = 1, . . . , L.
Change of variables:
H = Bâˆ’1 âˆˆ R(Nâˆ’1)Ã—(Nâˆ’1), g = Bâˆ’1Î²N âˆˆ RNâˆ’1,
sâ€²[n] = Bâˆ’1(xËœ[n]âˆ’Î²N ) = HxËœ[n]âˆ’ g.
(7)
Minimum volume simplex fitting problem
max
H, g
|det(H)|
s.t. HxËœ[n]âˆ’ g  0,
1TNâˆ’1(HxËœ[n]âˆ’ g) â‰¤ 1, âˆ€ n = 1, Â· Â· Â· , L,
(8)
31 / 62
The ith partial maximization of (8) in hTi and gi:
max
hTi , gi
âˆ£âˆ£âˆ£Nâˆ’1âˆ‘
j=1
(âˆ’1)i+jhijdet(Hij)
âˆ£âˆ£âˆ£
s.t. 0 â‰¤ hTi xËœ[n]âˆ’ gi â‰¤ 1âˆ’
âˆ‘
j 6=i
(hTj xËœ[n]âˆ’ gj), âˆ€ n.
(7p)
Solve (7p) by breaking it into two LPs:
p? = max
hTi , gi
Nâˆ’1âˆ‘
j=1
(âˆ’1)i+jhijdet(Hij) (7p+)
s.t. 0 â‰¤ hTi xËœ[n]âˆ’ gi â‰¤ 1âˆ’
âˆ‘
j 6=i
(hTj xËœ[n]âˆ’ gj), âˆ€ n,
q? = min
hTi , gi
Nâˆ’1âˆ‘
j=1
(âˆ’1)i+jhijdet(Hij) (7pâˆ’)
s.t. 0 â‰¤ hTi xËœ[n]âˆ’ gi â‰¤ 1âˆ’
âˆ‘
j 6=i
(hTj xËœ[n]âˆ’ gj), âˆ€ n.
33 / 62
Initialization of AVMIN
To initialize (7p), a feasible (H,g) is needed.
We can find a feasible (H,g) by solving the following
feasibility problem:
find (H,g)
s.t. HxËœ[n]âˆ’ g  0,
1TNâˆ’1(HxËœ[n]âˆ’ g) â‰¤ 1, âˆ€ n = 1, . . . , L,
(9)
which can also be implemented by LP.
This approach is referred to as alternating volume
minimization (AVMIN), which simultaneously estimates
endmembers and abundances.
35 / 62
Step 5. Calculate Î±Ì‚N = (H?)âˆ’1g? and
[Î±Ì‚1, ..., Î±Ì‚Nâˆ’1] = Î±Ì‚N1TNâˆ’1 + (H
?)âˆ’1.
Step 6. Obtain endmember estimates aÌ‚i = CÎ±Ì‚i + d for i = 1, ..., N .
Step 7. Recover the abundance vectors
sÌ‚[n] = [ (H?xËœ[n]âˆ’ g?)T 1âˆ’ 1TNâˆ’1(H?xËœ[n]âˆ’ g?) ]T
for n = 1, ..., L.
Following the same complexity evaluation as in AVMAX, the
proposed AVMIN has a worst-case computational complexity
of O(N2L1.5) per iteration.
37 / 62
4. Computer Simulations
100 Monte Carlo runs were performed, and 1000 synthetic
pixels x[n] (L = 1000) were generated in each run.
The endmembers A with 417 spectral bands (M = 417) were
selected from U.S. geological survey (USGS) library [Clarkâ€™93].
The abundance vectors s[n] were generated following Dirichlet
distribution D(s[n],Âµ) with Âµ = 1N 1N , which automatically
enforces (A1) and (A3) [Nascimentoâ€™05].
Dirichlet distribution (probability density function)
D(s,Âµ) =
âˆN
i=1 Î“(Âµi)
Î“(
âˆ‘N
i=1 Âµi)
Nâˆ
i=1
sÂµiâˆ’1i
where s = (s1, . . . , sN )T , 0 â‰¤ si â‰¤ 1,
âˆ‘N
i=1 si = 1 and Î“(Â·)
denotes the Gamma function.
39 / 62
Performance Indices
aË†i denotes the estimated endmember.
si = [ si[1], . . . , si[L] ]T and sË† denote the true and estimated
abundance maps, respectively.
Root-mean-square spectral angle (error performance measure)
Ï†en = min
piâˆˆÎ N
âˆšâˆšâˆšâˆš 1
N
Nâˆ‘
i=1
[
arccos
(
aTi aË†pii
â€–aiâ€–â€–aË†piiâ€–
)]2
Ï†ab = min
piâˆˆÎ N
âˆšâˆšâˆšâˆš 1
N
Nâˆ‘
i=1
[
arccos
(
sTi sË†pii
â€–siâ€–â€–sË†piiâ€–
)]2
where
pi = (pi1, . . . , piN ),
Î N = {pi âˆˆ RN | pii âˆˆ {1, 2, . . . , N}, pii 6= pij for i 6= j}
is the set of all the permutations of {1, 2, ..., N}.
41 / 62
0.7 0.75 0.8 0.85 0.9 0.95 1
0
2
4
6
8
10
12
14
16
Ï (purity level)
Ï† e
n 
(de
gre
es
)
 
 
Nâˆ’FINDRâˆ’FCLS
VCAâˆ’FCLS
PPIâˆ’FCLS
APS
MVCâˆ’NMF
MVSA
ICE
AVMIN (proposed)
AVMAX (proposed)
Figure: Simulation results of the endmember estimates obtained by the various
algorithms under test for different purity levels (Ï†en).
43 / 62
Simulations for Various Number of Endmembers
The synthetic data were generated in the same manner as
above (M = 417 and L = 1000).
The purity level was fixed as Ï = 0.75 and the N endmembers
were randomly picked from USGS library.
We generated five data sets with different number of
endmembers N = 6, 8, 10, 12, 14 for performance evaluation.
45 / 62
6 8 10 12 14
0
10
20
30
40
50
60
Number of endmembers
Ï† a
b 
(de
gre
es
)
 
 
Nâˆ’FINDRâˆ’FCLS
VCAâˆ’FCLS
PPIâˆ’FCLS
APS
MVCâˆ’NMF
MVSA
ICE
AVMIN (proposed)
AVMAX (proposed)
Figure: Simulation results of the endmember estimates obtained by the various
algorithms under test for different number of endmembers (Ï†ab) where
Ï = 0.75.
47 / 62
Methods Ï
Ï†en Ï†ab
SNR (dB) SNR (dB)
20 25 30 35 40 20 25 30 35 40
PPI-FCLS
0.7 6.24 6.09 6.02 6.05 6.01 44.77 45.80 46.27 46.60 45.78
0.85 4.05 3.56 2.77 2.78 2.71 23.46 20.02 12.02 11.59 10.82
1 1.46 0.58 0.33 0.17 0.09 7.59 3.46 2.04 1.21 0.70
N-FINDR-FCLS
(Winter)
0.7 5.45 5.31 5.24 5.11 5.16 22.54 21.86 21.63 19.76 19.82
0.85 2.65 2.67 2.66 2.65 2.61 9.60 8.37 8.03 7.93 7.77
1 1.15 0.58 0.33 0.18 0.10 6.14 3.59 2.13 1.24 0.72
VCA-FCLS
0.7 5.77 5.56 5.64 5.56 5.50 31.57 29.97 29.71 28.54 28.38
0.85 2.79 2.70 2.67 2.71 2.61 10.83 9.45 9.00 8.89 8.82
1 1.12 0.61 0.32 0.18 0.11 6.00 3.45 2.05 1.23 0.76
APS
0.7 8.22 7.56 7.79 7.22 15.44 28.45 28.13 27.49 24.98 26.83
0.85 4.16 4.27 4.04 3.93 4.01 18.56 15.67 13.98 13.67 13.11
1 2.75 1.55 1.25 1.10 1.01 12.59 7.18 4.99 4.75 4.47
MVC-NMF
0.7 6.80 5.32 4.50 6.05 4.49 26.69 20.55 16.54 21.89 15.60
0.85 2.47 1.37 1.31 1.34 1.32 5.81 4.61 4.26 4.27 4.18
1 1.48 0.89 0.71 0.66 0.64 7.87 4.52 2.87 2.07 1.75
MVSA
0.7 5.95 4.03 2.67 2.12 1.40 20.80 14.56 7.88 4.81 3.14
0.85 5.99 3.75 2.61 2.07 1.27 19.65 12.12 7.17 4.16 2.34
1 6.12 3.96 2.71 2.14 1.33 18.93 11.55 6.68 3.85 2.15
ICE
0.7 6.43 5.13 4.21 4.22 4.22 24.13 17.85 12.03 12.23 12.62
0.85 2.86 2.76 2.79 2.78 2.79 9.45 8.66 8.58 8.40 8.48
1 1.69 1.50 1.47 1.42 1.36 7.21 6.63 6.28 5.64 4.88
AVMAX
(Winter)
0.7 5.50 5.36 5.39 5.13 5.10 24.60 21.94 20.95 18.77 16.48
0.85 2.77 2.64 2.65 2.69 2.65 9.15 7.96 7.10 6.70 6.48
1 1.14 0.61 0.33 0.18 0.10 6.39 3.66 2.13 1.22 0.70
AVMIN
(Craig)
0.7 5.17 3.26 2.43 1.73 1.01 16.66 10.58 6.51 3.81 2.17
0.85 5.28 3.59 2.65 1.85 1.11 16.88 10.98 7.20 4.26 2.38
1 6.67 4.37 3.35 2.50 1.55 19.81 13.09 9.58 6.81 4.50
Each bold-face blue number denotes the minimum spectral angle among all the
algorithms associated with a fixed pair of (Ï,SNR).
49 / 62
5. Real Data Experiments
Algorithms under test include VCA-FCLS, MVC-NMF, and the
proposed AVMIN.
Real data were captured by Airborne Visible/Infrared Imaging
Spectrometer (AVIRIS) flight over the Cuprite mining site, Nevada,
in 19971.
The subimage (200Ã— 200,
L = 40000) comprises 224
bands over wavelength from
0.4 to 2.5 Âµm.
The bands 1-2, 104-113,
148-167, and 221-224 were
removed due to the low SNR
as well as the water-vapor
absorption. (M = 188)
AVIRIS hyperspectral image data
for the 10th band
1
The free data are available online: http://aviris.jpl.nasa.gov/html/aviris.freedata.html.
51 / 62
Estimated Abundance Map Comparison
53 / 62
Estimated Abundance Map Comparison
55 / 62
Mean-removed Spectral Angle Comparison
AVMIN identifies 14 minerals that contain less prevalent minerals, i.e.,
Goethite and Halloysite, but MVC-NMF and VCA only identifies 12
minerals.
AVMIN MVC-NMF VCA
Muscovite 35.64 33.93 32.70
Goethite 15.08 - -
Halloysite 13.10 - -
Nontronite 29.74 20.21 16.14
Montmorillonite 25.54 19.81 15.98
Alunite 19.55 18.97 23.48
Buddingtonite 20.68 36.91 27.25
Pyrope 32.37 14.49 19.97
Kaolinite #1 22.95 27.74 (31.84) 22.55 (22.04)
Kaolinite #2 21.32 - -
Chalcedony 26.01 23.02 31.09
Desert Varnish 14.74 15.69 16.13
Kaolinite #3 17.52 - -
Andradite 26.80 19.21 18.16
Kaolinite #4 - 19.77 (12.00) 18.17 (21.05)
Dumortierite - 33.34 20.44
Average Ï† 22.92 23.35 21.80
Those numbers in the parentheses denote the mean-removed spectral angles of
the endmember estimates classified as the same mineral.
57 / 62
Some simulation results demonstrated the same performance
of AVMAX and AVMIN when pure pixels exist, and the
superior performance of the latter (AVMIN) over some existing
benchmark algorithms, especially for highly mixed data.
Experimental results with real hyperspectral image data
showed that AVMIN can estimate endmembers and
abundances in high agreement with the reported ground truth.
Besides Winterâ€™s and Craigâ€™s criteria, it may well be
worthwhile to analyze other hyperspectral unmixing
criteria/beliefs (by convex analysis) for devising new effective
algorithms.
Hyperspectral unmixing with noise effects (not seriously
considered in this presentation) taken into account is currently
under investigation.
59 / 62
References
Bermanâ€™04 M. Berman, H. Kiiveri, R. Lagerstrom, A. Ernst, R. Dunne, and J. F. Huntington, â€œICE: A statistical
approach to identifying endmembers in hyperspectral images,â€ IEEE Trans. Geosci. Remote Sens., vol. 42,
no. 10, pp. 2085â€“2095, Oct. 2004.
Boardmanâ€™95 J. W. Boardman, F. A. Kruse, and R. O. Green, â€Mapping target signatures via partial unmixing of AVRIS
data,â€ in Proc. Summ. JPL Airborne Earth Sci. Workshop, vol. 1, Pasadena, CA, Dec. 9-14, 1995, pp.
23-26.
Boydâ€™04 S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, 2004.
Changâ€™04 C.-I. Chang and Q. Du, â€Estimation of number of spectrally distinct signal sources in hyperspectral
imagery,â€ IEEE Trans. Geosci. Remote Sens., vol. 42, no. 3, pp. 608-619, Mar. 2004.
Clarkâ€™93 R. N. Clark, G. A. Swayze, A. Gallagher, T. V. King, and W. M. Calvin, â€The U.S. geological survey digital
spectral library: version 1: 0.2 to 3.0 Âµm,â€ in U.S. Geol. Surv., Denver, CO., 1993, pp. 93-592.
Craigâ€™94 M. D. Craig, â€Minimum-volume transforms for remotely sensed data,â€ IEEE Trans. Geosci. Remote Sens.,
vol. 32, no. 3, pp. 542-552, May 1994.
Heinzâ€™01 D. Heinz and C.-I. Chang, â€Fully constrained least squares linear mixture analysis for material quantification
in hyperspectral imagery,â€ IEEE Trans. Geosci. Remote Sens., vol. 39, no. 3, pp. 529-545, Mar. 2001.
Ifarraguerriâ€™99 A. Ifarraguerri and C.-I. Chang, â€Multispectral and hyperspectral image analysis with convex cones,â€ IEEE
Trans. Geosci. Remote Sens., vol. 37, no. 2, pp. 756-770, Mar. 1999.
Keshavaâ€™02 N. Keshava and J. Mustard, â€Spectral unmixing,â€ IEEE Signal Process. Mag., vol. 19, no. 1, pp. 44-57,
Jan. 2002.
Miaoâ€™07 L. Miao and H. Qi, â€œEndmember extraction from highly mixed data using minimum volume constrained
nonnegative matrix factorization,â€ IEEE Trans. Geosci. Remote Sens., vol. 45, no. 3, pp. 765â€“777, Mar.
2007.
Liâ€™08 J. Li and J. Bioucas-Dias, â€œMinimum volume simplex analysis: A fast algorithm to unmix hyperspectral
data,â€ in Proc. IEEE International Geoscience and Remote Sensing Symposium, vol. 4, Boston, MA, Aug.
8-12, 2008, pp. 2369-2371.
Lustigâ€™94 I. J. Lustig, R. E. Marsten, and D. F. Shanno, â€œInterior point methods for linear programming:
Computational state of the art,â€ ORSA Journal on Computing, vol. 6, no. 1, pp. 1-14, 1994.
61 / 62
ç„¡ç ”ç™¼æˆæžœæŽ¨å»£è³‡æ–™ 
å…¶ä»–æˆæžœ 
(ç„¡æ³•ä»¥ï¥¾åŒ–è¡¨é”ä¹‹æˆ
æžœå¦‚è¾¦ï§¤å­¸è¡“æ´»å‹•ã€ç²
å¾—çŽé …ã€é‡è¦åœ‹éš›åˆ
ä½œã€ç ”ç©¶æˆæžœåœ‹éš›å½±éŸ¿
ï¦ŠåŠå…¶ä»–å”åŠ©ç”¢æ¥­æŠ€
è¡“ç™¼å±•ä¹‹å…·é«”æ•ˆï¨—äº‹
é …ç­‰ï¼Œè«‹ä»¥æ–‡å­—æ•˜è¿°å¡«
ï¦œã€‚) 
ç„¡ 
 æˆæžœé …ç›® ï¥¾åŒ– åç¨±æˆ–å…§å®¹æ€§è³ªç°¡è¿° 
æ¸¬é©—å·¥å…·(å«è³ªæ€§èˆ‡ï¥¾æ€§) 0  
èª²ç¨‹/æ¨¡çµ„ 0  
é›»è…¦åŠç¶²ï¤·ç³»çµ±æˆ–å·¥å…· 0  
æ•™æ 0  
èˆ‰è¾¦ä¹‹æ´»å‹•/ç«¶è³½ 0  
ç ”è¨Žæœƒ/å·¥ä½œåŠ 0  
é›»å­å ±ã€ç¶²ç«™ 0  
ç§‘ 
æ•™ 
è™• 
è¨ˆ 
ç•« 
åŠ  
å¡« 
é … 
ç›® è¨ˆç•«æˆæžœæŽ¨å»£ä¹‹ï¥«èˆ‡ï¼ˆé–±è½ï¼‰äººï¥© 0  
