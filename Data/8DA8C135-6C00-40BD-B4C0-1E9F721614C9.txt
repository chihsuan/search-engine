ä¸­æ–‡æ‘˜è¦ 
 
æœ¬è¨ˆç•«é‡å° Watch List å³ Open-Set äººè‡‰è¾¨ï§¼çš„å•é¡Œï¼Œæå‡ºå…©é …è§£æ±ºæ–¹æ¡ˆï¼Œä¸€ç‚º Fusion 
Facial Modelã€å¦ä¸€ç‚º Facial Trait Codesã€‚ å‰è€…åˆ©ç”¨ HMM (Hidden Markov Model)å’Œ
SVM (Support Vector Machine)ç›¸äº’çµåˆï¼Œè€Œå½¢æˆä¸€å€‹æ–°çš„èåˆåˆ†é¡å™¨ã€‚åˆ©ç”¨ HMM æ“·å–
ä¸€äººè‡‰å…§çš„å–®ä¸€å€‹é«”è®ŠåŒ–é‡ (intrapersonal variation)ï¼Œå†çµåˆ SVM å¯é‡æ¸¬å¤šå€‹é«”çš„äººè‡‰
ä¹‹é–“çš„å·®ç•°é‡ (interpersonal variation)ï¼Œæ•…ä¸åƒ…å¯å°‡å„å€‹é«”çš„äººè‡‰ç¨ç‰¹çš„ç‰¹å¾µè®Šç•°é‡æ“·å–
å‡ºï¼Œä¸¦å¯å¼·åŒ–ä¸åŒäººè‡‰ä¹‹é–“çš„ç‰¹å¾µå·®ç•°ã€‚æœ¬è¨ˆç•«æå‡ºä¹‹ Fusion Classifier å¯ç¶­æŒ SVM åœ¨è™•
ç†ä¸åŒè¦–è§’ä¸‹äººè‡‰è¾¨ï§¼çš„ä½èª¤æ‹’ç‡(FRR)ï¼Œä¸¦å¯åŒæ™‚åˆ©ç”¨SVMå¤§å¹…é™ä½èª¤å—ç‡(FAR)ã€‚Facial 
Trait Codes åˆ©ç”¨å¤§å°èˆ‡æ¯”ä¾‹ä¸åŒçš„çŸ©å½¢è¦–çª—æƒæäººè‡‰ï¼Œæ“·å–äººè‡‰ä¸­ä¸åŒå°ºå¯¸èˆ‡åˆ†ä½ˆçš„ç‰¹å¾µ
å€å¡Šã€‚ç”±åŒä¸€çŸ©å½¢è¦–çª—åœ¨ä¸åŒäººè‡‰ä¸­æ‰€æ“·å–åˆ°çš„å½±åƒç‰¹å¾µï¼Œé€²è¡Œå±€éƒ¨åˆ†é¡è™•ç†ã€‚å†ç”±ä¸åŒ
çŸ©å½¢è¦–çª—å…§å·²åˆ†é¡å¾Œçš„å±€éƒ¨ç‰¹å¾µï¼Œé€²è¡Œäººè‡‰è¾¨ï§¼æœ‰æ•ˆæ€§æ“·å–ã€‚å†å°‡æ“·å–å‡ºçš„ç‰¹å¾µå€å¡Šèˆ‡å„
å€å¡Šå…§çš„é¡åˆ¥é€²è¡Œäººè‡‰ç‰¹å¾µç·¨ç¢¼ (Facial Trait Code)ï¼Œä¸¦åˆ©ç”¨äººè‡‰ç‰¹å¾µç·¨ç¢¼é€²è¡Œäººè‡‰è¾¨ï§¼ã€‚ 
 
 
è‹±æ–‡æ‘˜è¦ 
 
Two approaches are proposed for face recognition in watch-list or open-set scenarios, 
which are considered the most challenging cases in face recognition. The first approach 
combines a HMM and a SVM to form a fusion classifier. The HMM part captures the 
evolution of facial features across a subjectâ€™s face without referencing to the faces of 
others. Because of the captured subject-oriented features, the HMM retains certain 
robustness against pose variations, yielding low FRR, on the price of high FAR because 
it is built upon within-class variation only. The SVM part is developed following a 
special design able to substantially diminish the FAR and further lower down the FRR. 
The second approach, called Facial Trait Codes (FTC), first extracts the distinctive trait 
patterns (DTP). The extraction of DTP involves clustering and boosting for maximizing 
the discrimination between faces. The extracted DTPâ€™s can be symbolized and used to 
make up facial trait codes. A given face can be encoded at some prescribed facial traits to 
render a facial trait code with each symbol in its codeword corresponding to the closest 
DTP. Both the fusion model and FTC are experimentally proven effective for watch-list 
face recognition.  
 
 
é—œéµè©: Face recognitionã€open-set face identificationã€fusion classifierã€
hidden Markov models (HMM)ã€support vector machines (SVM) 
 
 
ä¸€å¼µç”±æœå°‹çµ„å…§å–å¾—äººè‡‰å½±åƒï¼Œï§¼åˆ¥ç³»çµ±å¿…å®šæœƒæ±ºå®šä¸€ä½ç›¸å°æ‡‰çš„è¨»å†Šè€…ã€‚ 
3. åå–®æœå°‹ (Watch List) æ•ˆèƒ½ï¼šæœå°‹çµ„å…§å«æœ‰è¨±å¤šéè¨»å†Šè€…çš„äººè‡‰å½±åƒï¼Œè¾¨ï§¼ç³»çµ±å¿…
é ˆæ±ºå®šä»»æ„ä¸€å¼µç”±æœå°‹çµ„å…§å–å¾—çš„äººè‡‰å½±åƒæ˜¯å¦ç‚ºè¨»å†Šè€…ï¼Œè‹¥ç‚ºè¨»å†Šè€…ï¼Œå‰‡é ˆæ±ºå®š
å°æ‡‰è¨»å†Šè€…çš„èº«åˆ†ã€‚ 
    åå–®æœå°‹ (Watch List) æ•ˆèƒ½æ¸¬è©¦é æ¯”è‡‰éƒ¨ç¢ºèªå’Œè‡‰éƒ¨ï§¼åˆ¥å¯ŒæŒ‘æˆ°æ€§ï¼ŒFRVT 2002 è©•é‘‘
å–®ä½ç”±åŸæœå°‹çµ„ä¸­ä»»æ„å–å‡ºä¸€å¼µäººè‡‰å½±åƒï¼Œåƒè©¦è€…å¿…é ˆå›ç­”è¨»å†Šçµ„ä¸­æ˜¯å¦å­˜æœ‰èˆ‡æ­¤äººç¬¦åˆ
çš„è‡‰éƒ¨æ¨¡å‹ï¼Œå¦‚æœ‰ï¼Œå¿…é ˆä¸€ä½µè¼¸å‡ºäººåã€‚ä»¥åœ¨è¨»å†Šçµ„å« 25 äººç‚ºä¾‹ï¼Œæœå°‹æ­£ç¢ºç‡åƒ…é” 77%ã€‚
å¦‚å°‡è¨»å†Šçµ„æ“´å¤§è‡³ 100 äººï¼Œæœå°‹çµ„å« 200 äººçš„ 400 å¼µå½±åƒï¼Œåœ¨ FAR1%ä¸‹ï¼Œæœå°‹æ­£ç¢ºç‡ä¸‹é™
åˆ° 68%ï¼Œç”±æ­¤å¯è¦‹åå–®æœå°‹çš„é«˜é›£åº¦ã€‚äº‹å¯¦ä¸Šï¼Œåå–®æœå°‹çš„æ•ˆèƒ½ç›´æ¥å½±éŸ¿äº†è‡‰éƒ¨è¾¨ï§¼ç§‘æŠ€
æ˜¯å¦å¯æ‡‰ç”¨æ–¼çŠ¯ç½ªé˜²æ²»ï¼Œå¾ FRVT 2002 çš„æ¸¬è©¦çµæœçœ‹ä¾†ï¼Œä¼¼ä¹ä»æœ‰ä¸€æ®µç›¸ç•¶å€¼å¾—åŠªåŠ›çš„ç©º
é–“ã€‚æœ¬ç ”ç©¶çš„ç›®çš„å³åœ¨è¨­è¨ˆé©ç”¨æ–¼åå–®æœå°‹å¼çš„è‡‰éƒ¨è¾¨ï§¼æ–¹æ³•ã€‚æœ¬ç ”ç©¶æ‰€æå‡ºçš„æ–¹æ³•æœ‰äºŒï¼š 
1. åˆ©ç”¨ HMM (Hidden Markov Model)å’Œ SVM (Support Vector Machine)ç›¸äº’çµåˆï¼Œ
è€Œå½¢æˆä¸€å€‹æ–°çš„èåˆåˆ†é¡å™¨ã€‚åˆ©ç”¨ HMM æ“·å–ä¸€äººè‡‰å…§çš„å–®ä¸€å€‹é«”è®ŠåŒ–é‡ 
(intrapersonal variation)ï¼Œå†çµåˆ SVM å¯é‡æ¸¬å¤šå€‹é«”çš„äººè‡‰ä¹‹é–“çš„å·®ç•°é‡ 
(interpersonal variation)ï¼Œæ•…ä¸åƒ…å¯å°‡å„å€‹é«”çš„äººè‡‰ç¨ç‰¹çš„ç‰¹å¾µè®Šç•°é‡æ“·å–å‡ºï¼Œä¸¦
å¯å¼·åŒ–ä¸åŒäººè‡‰ä¹‹é–“çš„ç‰¹å¾µå·®ç•°ã€‚æœ¬è¨ˆç•«æå‡ºä¹‹ Fusion Classifier å¯ç¶­æŒ SVM åœ¨è™•
ç†ä¸åŒè¦–è§’ä¸‹äººè‡‰è¾¨ï§¼çš„ä½èª¤æ‹’ç‡(FRR)ï¼Œä¸¦å¯åŒæ™‚åˆ©ç”¨ SVM å¤§å¹…é™ä½èª¤å—ç‡
(FAR)ã€‚ 
2. åˆ©ç”¨å¤§å°èˆ‡æ¯”ä¾‹ä¸åŒçš„çŸ©å½¢è¦–çª—æƒæäººè‡‰ï¼Œæ“·å–äººè‡‰ä¸­ä¸åŒå°ºå¯¸èˆ‡åˆ†ä½ˆçš„ç‰¹å¾µå€å¡Šã€‚
ç”±åŒä¸€çŸ©å½¢è¦–çª—åœ¨ä¸åŒäººè‡‰ä¸­æ‰€
æ“·å–åˆ°çš„å½±åƒç‰¹å¾µï¼Œé€²è¡Œå±€éƒ¨åˆ†
é¡è™•ç†ã€‚å†ç”±ä¸åŒçŸ©å½¢è¦–çª—å…§å·²
åˆ†é¡å¾Œçš„å±€éƒ¨ç‰¹å¾µï¼Œé€²è¡Œäººè‡‰è¾¨
ï§¼æœ‰æ•ˆæ€§æ“·å–ã€‚å†å°‡æ“·å–å‡ºçš„ç‰¹
å¾µå€å¡Šèˆ‡å„å€å¡Šå…§çš„é¡åˆ¥é€²è¡Œäºº
è‡‰ç‰¹å¾µç·¨ç¢¼ (Facial Trait Code)ï¼Œ
ä¸¦åˆ©ç”¨äººè‡‰ç‰¹å¾µç·¨ç¢¼é€²è¡Œäººè‡‰è¾¨
ï§¼ã€‚ 
 
äººè‡‰è¾¨ï§¼çš„æ•ˆèƒ½å¸¸ä»¥å…‰æºã€è¦–è§’èˆ‡
è¡¨æƒ…ä¸‰å¤§åƒæ•¸é€²è¡Œè©•ä¼°ã€‚æœ¬ç ”ç©¶å°ˆæ³¨æ–¼
ä¸åŒè¦–è§’ä¸‹çš„äººè‡‰è¾¨ï§¼ï¼Œä¸¦ä»¥åå–®æœå°‹å¼çš„è‡‰éƒ¨è¾¨ï§¼ç‚ºæ¸¬è©¦çš„è©•æ¯”æ¨™æº–ï¼Œä¸¦ä»¥ CMU PIE è³‡
æ–™åº«ç‚ºä¸»è¦çš„æ¸¬è©¦æ¨£æœ¬ã€‚CMU PIE è³‡æ–™åº«æä¾› 13 ç¨®ä¸åŒçš„äººè‡‰è§’åº¦ï¼Œå…¶ä¸­æœ‰ 8 ç¨®ç‚ºå´é¢
æˆ–è§’åº¦éå¤§çš„å½±åƒï¼Œæœ¬ç ”ç©¶æš«ä¸è€ƒæ…®ï¼Œæœ¬ç ”ç©¶è€ƒæ…®çš„ç¯„åœåŒ…æ‹¬æ­£é¢äººè‡‰ã€å‘ä¸Š 20 åº¦ã€å‘ä¸‹
20 åº¦ã€å¾€å·¦å¾€å³å„ 30 åº¦ç­‰äº”ç¨®è¦–è§’ï¼Œå¦‚ä¸Šåœ–æ‰€ç¤ºã€‚ 
 
ç¬¬ä¸‰ç¯‡è«–æ–‡æ‘˜è¦ 
The Facial Trait Code (FTC) is proposed to encode human facial images. The FTC is 
motivated by the discovery of the basic types of local facial features, called facial trait 
bases, which can be extracted from a large number of faces. In addition, the fusion of 
these facial trait bases can accurately capture the appearance of a face. Extraction of the 
facial trait bases involves clustering and boosting approaches, leading to the best 
discrimination of the faces in the FTC face set. The extracted facial trait bases are 
symbolized and make up the n-ary facial trait codes. A given face can be then encoded at 
the patches specified by the traits to render an n-ary facial trait code with each symbol in 
its codeword corresponding to the closest trait base. Encoding and decoding can be 
made application-oriented using different FTC face sets. We applied the FTC to three 
interesting problems: (1) Synthesis of male (female) faces from female (male) faces, and 
(2) Normalization of the illumination condition over a given face, and (3) A typical face 
identification problem with a hard matching and soft matching schemes. All have 
yielded satisfactory results.   
 
æ–‡ç»æ¢è¨ï¼š(æœ¬ç¯€å…¨ä»¥è‹±æ–‡æ•˜è¿°) 
    The most well known methods for face recognition includes PCA (Principal Component 
Analysis)ã€LDA (Linear Discriminant Analysis)ã€EBGM (Elastic Bunch Graph Matching)ã€Kernel 
Methodsã€AAM (An Active Appearance Model)ã€3-D Morphable Modelã€SVM (Support Vector 
Machine)ã€HMM (Hidden Markov Models)ã€and Boosting & Ensemble. Each of the above will be 
briefly reviewed below.  
 
PCA (Principal Component Analysis) 
PCA-based method is derived from Karhunen-Loeve's transformation. Given an 
s-dimensional vector representation of each face in a training set of images, Principal 
Component Analysis (PCA) tends to find a t-dimensional subspace whose basis vectors 
correspond to the maximum variance direction in the original image space. This new subspace is 
normally lower dimensional (t<<s). If the image elements are considered as random variables, 
the PCA basis vectors are defined as eigenvectors of the scatter matrix [1~4]. 
 
LDA (Linear Discriminant Analysis) 
Linear Discriminant Analysis (LDA) finds the vectors in the underlying space that best 
discriminate among classes. For all samples of all classes the between-class scatter matrix SB and 
the within-class scatter matrix SW are defined. The goal is to maximize SB while minimizing SW, 
in other words, maximize the ratio det|SB|/det|SW| . This ratio is maximized when the column 
vectors of the projection matrix are the eigenvectors of (SW-1 Ã— SB) [5~10].  
 
EBGM (Elastic Bunch Graph Matching) 
All human faces share a similar topological structure. Faces are represented as graphs, with 
nodes positioned at fiducial points. (exes, nose...) and edges labeled with 2-D distance vectors. 
Each node contains a set of 40 complex Gabor wavelet coefficients at different scales and 
very accurate (strong) classifier. Viola and Jones build the first real-time face detection system by 
using AdaBoost, which is considered a dramatic breakthrough in the face detection research. On 
the other hand, the papers by Guo et al. are the first approaches on face recognition using the 
AdaBoost methods [33~38].  
 
è¨ˆç•«æˆæœè‡ªè©•ï¼š 
 
æœ¬è¨ˆç•«åŸè¨‚ç›®æ¨™ç‚ºåˆ©ç”¨é«˜ç•«è³ªèˆ‡ 3D äººè‡‰å½±åƒé€²è¡Œ Watch List äººè‡‰è¾¨ï§¼æ–¹æ³•ä¹‹ç ”ç©¶ï¼Œ
é›–å› è£œåŠ©é‡‘é¡æœ‰é™ï¼Œä¸è¶³ä»¥æ¡è³¼ 3D äººè‡‰æƒæå™¨ã€‚ä½†é€éé™³ç¥–ç¿°æ•™æˆæä¾›çš„ CMU PIE äººè‡‰
è³‡æ–™åº«ï¼Œæœ¬è¨ˆç•«ä¾ç„¶å®ŒæˆåŸè¨ˆç•«æ›¸è¦åŠƒä¹‹å¹¾é …ä¸»è¦ç ”ç©¶é …ç›®ï¼Œå³åˆ©ç”¨ 2D äººè‡‰å½±åƒï¼Œæ¨å‹•
å¯æ‡‰ç”¨æ–¼ Watch List å¼ä¹‹äººè‡‰è¾¨ï§¼æ–¹æ³•çš„ç ”ç©¶ã€‚åœ¨å£¹å¹´çš„åŸ·è¡ŒæœŸå…§ï¼Œé™¸çºŒå®Œæˆ Fusion 
Classifier (èåˆåˆ†é¡å™¨)èˆ‡ Facial Trait Code (äººè‡‰ç‰¹å¾µç·¨ç¢¼)çš„éƒ¨ä»½æ–¹æ³•è¨­è¨ˆèˆ‡ç›¸é—œå¯¦é©—ã€‚æœ¬
è¨ˆç•«æˆæœå¯æ¦‚è¿°å¦‚ä¸‹ï¼š 
 
1. Fusion Classifier æŒ‘æˆ°åœ¨ä¸åŒè¦–è§’ä¸‹çš„äººè‡‰è¾¨ï§¼æ¼”ç®—æ³•ã€‚åˆ©ç”¨ä¸€ä¸²è¯æ©Ÿåˆ¶å°‡ HMM 
(éš±è—å¼é¦¬å¯å¤«æ¨¡å‹)èˆ‡ SVM (æ”¯æŒå‘é‡æ©Ÿ)çµåˆï¼Œä¸²è¯æ©Ÿåˆ¶åŒ…æ‹¬å…©è€…æ‰€æ“·å–çš„ç‰¹å¾µï¼Œ
åŠå…©è€…æ¨¡å‹è¨“ç·´å’Œæ‡‰ç”¨æ–¼è¾¨ï§¼çš„éƒ¨ä»½ã€‚æœ¬ä¸²è¯æ©Ÿåˆ¶ä¿æŒäº†å…©è€…å€‹åˆ¥æ‡‰ç”¨æ–¼äººè‡‰è¾¨ï§¼
æ™‚çš„å„ªé»ï¼Œä¸¦åˆ©ç”¨ç›¸äº’ä¹‹é–“çš„äº’è£œæ€§ï¼Œå½Œè£œäº†å…©è€…å€‹åˆ¥çš„ç¼ºé»ã€‚æœ¬ä¸²è¯å¼è‡‰éƒ¨è¾¨ï§¼
ç³»çµ±å¯æœ‰æ•ˆçµåˆè‡ªæˆ‘å·®ç•°(intra-personal variation)èˆ‡äººæˆ‘å·®ç•°(inter-personal 
variation)ï¼Œä¸¦ç”±æ­¤çµåˆç”¢ç”Ÿä½ FAR èˆ‡ä½ FRR ä¹‹äººè‡‰è¾¨ï§¼å™¨ã€‚ 
2. Facial Trait Code åˆ©ç”¨å¤§é‡çš„äººè‡‰è³‡æ–™ï¼Œå°‡è¨±å¤šäººè‡‰å±€éƒ¨ç‰¹å¾µé€²è¡Œå±€éƒ¨åˆ†é¡ï¼›å†å°‡
é€™äº›å±€éƒ¨åˆ†é¡å¾Œçš„ç‰¹å¾µï¼Œåˆ©ç”¨å…¶åœ¨å®Œæ•´äººè‡‰åˆ†é¡çš„èƒ½åŠ›ï¼Œæ“·å–è¼ƒå…·äººè‡‰åˆ†é¡çš„æ¬Šé‡
çµ„åˆã€‚å†åˆ©ç”¨é€™äº›å…·æœ‰äººè‡‰åˆ†é¡æ¬Šé‡çš„å±€éƒ¨ç‰¹å¾µçš„é¡åˆ¥ï¼Œé€²è¡Œäººè‡‰ç·¨ç¢¼ï¼Œä¸¦åˆ©ç”¨ç·¨
ç¢¼çš„çµæœé€²è¡Œäººè‡‰è¨»å†Šèˆ‡è¾¨ï§¼ã€‚ 
3. ä¸Šè¿°äºŒé …å‡æœ‰ç›¸é—œè«–æ–‡ç™¼è¡¨æ–¼åœ‹éš›èˆ‡åœ‹å…§ç ”è¨æœƒï¼Œä¸¦ä¸”äºŒé …ç›®å‰æˆæœçš„å»¶ä¼¸èˆ‡è©³è¿°
ç‰ˆæœ¬å‡åœ¨é€²è¡Œä¸­ï¼Œæº–å‚™é€²è¡Œç›¸é—œé ˜åŸŸæœŸåˆŠçš„æŠ•ç¨¿ã€‚ 
 
è‹¥ä»¥ä¸‹è¡¨ç‚ºæˆæœè‡ªè©•æ¨™æº–:  
 
ç­‰ç´š ç‰¹å„ª å„ª ä½³ å¯ æ¬ ä½³ 
è©•åˆ¤æ¨™æº– 
å®ŒæˆåŸè¨ˆç•«
å¤§éƒ¨ä»½è¦åŠƒ
å·¥ä½œï¼Œä½œå“å¾—
åˆ°åœ‹éš›(å¦‚çŸ¥
åæœŸåˆŠã€å°ˆåˆ©
ç­‰)ä¹‹è‚¯å®šï¼Œ
æˆ–å¼•ç™¼å»£æ³›
ç”¢æ¥­é—œæ³¨ã€‚ 
 
å®ŒæˆåŸè¨ˆç•«
å¤§éƒ¨ä»½è¦åŠƒ
å·¥ä½œï¼Œä½œå“å¾—
åˆ°å»£æ³›çš„è‚¯
å®šï¼Œå¦‚é ˜åŸŸå…§
åœ‹éš›é ‚å°–ç ”
è¨æœƒæˆ–ç›¸é—œ
å°ˆåˆ©ç­‰ã€‚ 
å®ŒæˆåŸè¨ˆç•«
å¤§éƒ¨ä»½è¦åŠƒ
å·¥ä½œï¼Œä½œå“å¾—
åˆ° äº› è¨± è‚¯
å®šï¼Œå¦‚é ˜åŸŸå…§
ä¸€èˆ¬ç ”è¨æœƒ
ç­‰ã€‚ 
å®ŒæˆåŸè¨ˆç•«
å¤§éƒ¨ä»½è¦åŠƒ
å·¥ä½œï¼Œä½†ä½œå“
å°šæœªå¾—åˆ°å…¶
å®ƒå–®ä½è‚¯å®šã€‚ 
æœªå®ŒæˆåŸè¨ˆ
ç•«å¤§éƒ¨ä»½è¦
åŠƒå·¥ä½œã€‚ 
 
å› æœ¬è¨ˆç•«ç”¢ç”Ÿæ•¸ç¯‡åœ‹å…§å¤–ç ”è¨æœƒè«–æ–‡ï¼Œå…¶ä¸­ä¸€ç¯‡ç™¼è¡¨æ–¼ CVPR (IEEE International 
 å¯ä¾›æ¨å»£ä¹‹ç ”ç™¼æˆæœè³‡æ–™è¡¨ 
â–“ å¯ç”³è«‹å°ˆåˆ©  â–“ å¯æŠ€è¡“ç§»è½‰                                      æ—¥æœŸï¼š98 å¹´ 9 æœˆ 10 æ—¥ 
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•« 
è¨ˆç•«åç¨±ï¼šä»¥é«˜ç•«è³ªå½±åƒåŠä¸‰ç¶­è‡‰éƒ¨æƒç„é€²è¡Œ Watch List äººè‡‰è¾¨ï§¼ 
è¨ˆç•«ä¸»æŒäººï¼šå¾ç¹¼è–        
è¨ˆç•«ç·¨è™Ÿï¼šNSC 97-2218-E-011-004- å­¸é–€é ˜åŸŸï¼šåœ–å‹è¾¨ï§¼ 
æŠ€è¡“/å‰µä½œåç¨± ä¸²è¯å¼è‡‰éƒ¨è¾¨ï§¼ç³»çµ± 
ç™¼æ˜äºº/å‰µä½œäºº å¾ç¹¼è– 
æŠ€è¡“èªªæ˜ 
ä¸­æ–‡ï¼šæœ¬ç™¼æ˜åˆ©ç”¨ä¸€ä¸²è¯æ©Ÿåˆ¶å°‡ HMM(éš±è—å¼é¦¬å¯å¤«æ¨¡å‹)èˆ‡ SVM 
(æ”¯æŒå‘é‡æ©Ÿ)çµåˆï¼Œé€²è¡Œå¿«é€Ÿä¸”ç²¾æº–çš„äººè‡‰è¾¨ï§¼ã€‚ä¸²è¯éƒ¨ä»½åŒ…æ‹¬å…©
è€…æ‰€æ“·å–çš„ç‰¹å¾µéƒ¨ä»½ï¼ŒåŠå…©è€…æ¨¡å‹è¨“ç·´å’Œæ‡‰ç”¨æ–¼è¾¨ï§¼çš„éƒ¨ä»½ã€‚æœ¬ä¸²
è¯æ©Ÿåˆ¶ä¿æŒäº†å…©è€…å€‹åˆ¥æ‡‰ç”¨æ–¼äººè‡‰è¾¨ï§¼æ™‚çš„å„ªé»ï¼Œä¸¦åˆ©ç”¨ç›¸äº’ä¹‹é–“
çš„äº’è£œæ€§ï¼Œå½Œè£œäº†å…©è€…å€‹åˆ¥çš„ç¼ºé»ã€‚æœ¬ä¸²è¯å¼è‡‰éƒ¨è¾¨ï§¼ç³»çµ±å¯æœ‰æ•ˆ
çµåˆè‡ªæˆ‘å·®ç•°(intra-personal variation)èˆ‡äººæˆ‘å·®ç•°(inter-personal 
variation)ï¼Œä¸¦ç”±æ­¤çµåˆç”¢ç”Ÿä½ FAR èˆ‡ä½ FRR ä¹‹äººè‡‰è¾¨ï§¼å™¨ã€‚ 
è‹±æ–‡ï¼šA cascade architecture is proposed to fuse HMM (Hidden 
Markov Model) and SVM (Support Vector Machine) for face 
recognition. The fusion is on feature extraction, training, and 
recognition. The proposed method keeps the advantages of 
HMM and SVM when each applies for face recognition, while 
dampening the associated disadvantages using the cascade 
architecture. Because both the intra-personal and inter-personal 
variations are considered in this architecture, it yields a classifier 
with low FAR and FRR.  
å¯åˆ©ç”¨ä¹‹ç”¢æ¥­ 
åŠ 
å¯é–‹ç™¼ä¹‹ç”¢å“ 
1. äººè‡‰é–€ç¦æ§åˆ¶ç³»çµ± 
2. äººè‡‰ç™»éŒ„ç³»çµ± 
3. äººè‡‰æœå°‹èˆ‡çŠ¯ç½ªé˜²åˆ¶ç›¸é—œç³»çµ±ç”¢å“ 
æŠ€è¡“ç‰¹é» 
1. å¿«é€Ÿäººè‡‰ç™»éŒ„è¨»å†Šï¼Œä»¥ 800MHz CPU èˆ‡ RAM 256MB çš„
Windows OS ç‚ºä¾‹ï¼Œå€‹äººç™»éŒ„è¨»å†Šæ™‚é–“å°‘æ–¼ä¸€ç§’(äººè‡‰åµæ¸¬æ™‚é–“
é™¤å¤–) 
2. å¿«é€Ÿäººè‡‰è¾¨ï§¼ï¼Œä»¥ä¸Šè¿°ç‚ºä¾‹ï¼Œå€‹äººè¾¨ï§¼æ™‚é–“å°‘æ–¼ 0.1 ç§’ 
3. ä»¥ 100 äººåœ¨ä¸€èˆ¬å®¤å…§ç’°å¢ƒä¸‹ï¼ŒFAR 0.005 æ™‚ï¼ŒFRR é” 0.008 
æ¨å»£åŠé‹ç”¨çš„åƒ¹å€¼ 
æœ¬æŠ€è¡“å¯æ¨å»£è‡³é–€ç¦æ§åˆ¶ã€å®‰å…¨ç›£æ§ã€çŠ¯ç½ªé˜²åˆ¶ã€é›»è…¦èˆ‡å…‰å­¸ç”¢å“
ä¹‹åŠ å€¼è»Ÿé«”ç­‰ç›¸é—œç³»çµ±ä¹‹è¨­è¨ˆèˆ‡è£½é€ å•†ï¼Œææ˜‡ç¾æœ‰ç”¢å“åŠŸèƒ½ï¼Œä¸¦åŠ 
å¼·ç”¢å“åœ‹éš›ç«¶çˆ­åŠ›ã€‚ 
â€» 1.æ¯é …ç ”ç™¼æˆæœè«‹å¡«å¯«ä¸€å¼äºŒä»½ï¼Œä¸€ä»½éš¨æˆæœå ±å‘Šé€ç¹³æœ¬æœƒï¼Œä¸€ä»½é€ è²´å–®ä½ç ”ç™¼æˆæœ
æ¨å»£å–®ä½ï¼ˆå¦‚æŠ€è¡“ç§»è½‰ä¸­å¿ƒï¼‰ã€‚ 
â€» 2.æœ¬é …ç ”ç™¼æˆæœè‹¥å°šæœªç”³è«‹å°ˆåˆ©ï¼Œè«‹å‹¿æ­éœ²å¯ç”³è«‹å°ˆåˆ©ä¹‹ä¸»è¦å…§å®¹ã€‚ 
â€» 3.æœ¬è¡¨è‹¥ä¸æ•·ä½¿ç”¨ï¼Œè«‹è‡ªè¡Œå½±å°ä½¿ç”¨ã€‚ 
é™„ä»¶ 1 
Fusion Classifier for Open-Set Face Recognition 
with Pose Variations 
Gee-Sern Jison Hsu
Abstractâ€”A fusion classifier composed of two modules, one 
made by a hidden Markov model (HMM) and the other by a support 
vector machine (SVM), is proposed to recognize faces with pose 
variations in open-set recognition settings. The HMM module cap-
tures the evolution of facial features across a subjectâ€™s face using 
the subjectâ€™s facial images only, without referencing to the faces of 
others. Because of the captured evolutionary process of facial fea-
tures, the HMM module retains certain robustness against pose vari-
ations, yielding low false rejection rates (FRR) for recognizing faces 
across poses. This is, however, on the price of poor false acceptance 
rates (FAR) when recognizing other faces because it is built upon 
within-class samples only. The SVM module in the proposed model 
is developed following a special design able to substantially dimin-
ish the FAR and further lower down the FRR. The proposed fusion 
classifier has been evaluated in performance using the CMU PIE 
database, and proven effective for open-set face recognition with 
pose variations. Experiments have also shown that it outperforms 
the face classifier made by HMM or SVM alone.  
Index Termsâ€”Face recognition, open-set identification, hidden 
Markov model, support vector machines.
T
I. INTRODUCTION 
H
va
preted
IS paper aims at open-set face recognition with pose 
riations. Open-set face recognition can be better inter-
 using a gallery set and a probe set. A gallery set con-
tains the subjects enrolled to the system with one or a few 
facial images per subject, and a probe set refers to the facial 
images unseen to the system and presented to the system for 
recognition. The images in both sets are disjoint. When both 
sets have the same individuals, it is known as closed-set iden-
tification, and each probe face has one and only one matched 
subject in the gallery. Many former algorithms were evalu-
ated using this scenario. The closed-set identification is often 
not the case in real life, but the open-set recognition is. In 
open-set recognition, the probe set is larger than the gallery 
set, and those in the probe set but not in the gallery act as 
imposters trying to break in the gallery. In such a scenario, 
one must first determine whether a probe face exists in the 
gallery, and if it exists, one will have to identify what the 
matched subject is from the gallery set. Open set face recog-
nition is considered more general, and thus more difficult, 
than closed-set identification because it actually adds in a 
detection task on top of an identification task. It is reported in 
[1] that in an open-set scenario a small size of gallery is eas-
ier to recognize than a large size.  
Poses, illumination conditions, and expressions are gen-
erally acknowledged as three challenging parameters in face 
recognition. Quite a number of methods for recognizing faces 
across poses use 3D approaches [1]-[7], and among them the 
3D morphable model [1, 2] may be the most well-known and 
considered an effective tool for handling poses. However, the 
3D approaches suffer from intensive computation, possible 
imprecise alignments, and undesirable artifacts generated on 
the model-based virtual views. Therefore, many researchers 
have been working on approaches with perspectives different 
from 3D ones.    
A geometry assisted probabilistic approach is reported in 
[8], which approximates a head with a 3D ellipsoid model, so 
that any face image is a 2D projection of such a 3D ellipsoid 
at a certain pose. Linear object classes (LOC) are introduced 
in [9, 10], which are formed by the prototypical views to a 
specific class of objects, as faces for example. LOC has the 
properties that the virtual views of any object of the same 
class under uniform affine 3D transformations can be gener-
ated if the corresponding transformed views are known for 
the set of prototypes. If a training set consists of frontal and 
rotated views of a set of prototypical faces, any rotated view 
of a new face can be generated from a single frontal view. 
Two issues have limited the application of LOC in practice, 
one is the finding of correspondence between the model and 
an image, and the other is the completeness of available ex-
amples for building the prototypes. The virtual view genera-
tion problem is reformulated as a prediction problem in [11], 
and solved by linear regression. This method is inspired by 
the idea that the linear mapping between non-frontal patches 
and frontal patches maintains better than that of the global 
case in the case of coarse alignment.  
This paper reports a model that fuses a Hidden Markov 
Model (HMM) with a Support Vector Machine (SVM), each 
of which has been applied to face identification and recogni-
tion, and each has some specific advantages and weakness 
[12]-[19]. The proposed fusion model can keep their advan-
tages and compensate for their weakness. It has been re-
ported and also observed in our experiments that HMM is 
good for closed-set face identification with pose variations, 
but poor for open-set face recognition [20] with unacceptable 
false acceptance rates (FAR). The poor FAR is primarily 
caused by the HMM built upon within-class samples only. 
To exploit the HMMâ€™s strength in recognizing faces across 
poses and effectively diminish the FAR it induces, the pro-
posed fusion model connects a HMM module with a SVM 
module following a special architecture. When enrolling a 
subject to the gallery, the HMM module, built from the sub-
jectâ€™s facial images, searches for those in the gallery whose 
faces look similar to the subjectâ€™s face, and uses these similar 
faces as part of the training sample for building the SVM 
Gee-Sern Jison Hsu is with the National Taiwan University of Science and 
Technology (phone: +886-2-2730-3234, email: jison@mail.ntust.edu.tw,) 
World Academy of Science, Engineering and Technology 56 2009
563
The sign of d(x) shows on which side of the separating 
hyperplane x is located, and its magnitude gives the distance 
of x away from the hyperplane. The larger the distance, the 
more reliable the classification result.    
The feature vectors in [16, 17] are generated by Eigenface 
and Fisherface decompositions, and those in [18] are normal-
ized gray-valued pixel vectors formed by the face images 
after histogram equalization and lighting intensity subtrac-
tion. SVM is good at extracting the information for the dis-
crimination of unclassified features, e.g., the features from 
PCA; however, it is prone to be over-trained with well-
classified features, e.g., the features from LDA [17].  
SVM aims at binary classification, but can be extended to 
multiple classification with two schemes. One is the one-to-
the-rest scheme. If there are n classes in the training set, the 
negative samples of one specific class are the conglomerate 
of all the rest n-1 classes. The other is pairwise approach, 
each classifier only involves two out of the n classes, so there 
will be n (n -1)/2 classifiers in total.     
The major disadvantage of the two schemes is the expen-
sive computation due to the huge amount of training data. In 
the one-to-the-rest scheme, the support vectors of each class 
require the training upon the whole data set; and in the pair-
wise approach, there are too many classes to train. Training 
upon a large number of data poses a serious threat to the ap-
plication scope of SVMs [22].      
III. FUSION OF HMM AND SVM
Aiming at open set face recognition, we propose a fusion  
model, which keeps the advantages of both HMM and SVM 
methods, and effectively overcomes the weakness of each. 
The fused model is composed of a HMM module and a SVM 
module. In the enrollment stage, a subjectâ€™s HMM module is 
built from the subjectâ€™s facial images, and acts as a filter to 
select those in the gallery whose faces look similar to the 
subjectâ€™s face. These similar faces are then used as part of the 
training sample for building the SVM module, substantially 
reducing the amount of training samples for the SVM, and 
also effectively suppressing the HMM-induced FAR in the 
recognition stage. Furthermore, in the recognition stage the 
similarity between a probe face and each subject in the gal-
lery can be readily measured by the subjectâ€™s HMM module, 
and the rank-n pool can be quickly determined which in-
cludes n subjects whose faces are considered similar to the 
probe face. The way of developing the HMM modules is 
similar to those reported in [12]-[15], the originality of this 
work is on (1) the fusion of the HMM module and the SVM 
module so that both can be integrated, and (2) the training of 
SVM using a generic negative sample set and a subject-
oriented negative sample set.  
A. Features for the Fusion Model 
The features for the HMM module must preserve the 
variations of local facial features captured by the window 
moving from left to right and top to bottom. Similar to the 
work in [10, 11, 12], the DCT coefficients taken from each 
patch captured by a moving square window are used in this 
work. Only the low frequency parts, i.e., those in the upper 
triangular of each patchâ€™s DCT coefficients, are extracted. 
The DCT coefficients from the overlapping patches are good 
for building the HMM module. But they are inappropriate for 
building the SVM module, because of the high dimensional-
ity of the feature space formed by these coefficients. Con-
sider a case with a 64x64 face and a 8x8 patch overlapped 
with its neighbors for 3 pixels, if the largest 15 DCT coeffi-
cients are taken from each patch, it will result in a feature 
vector of 5415 in dimension. This size of dimension can 
paralyze a computer when it is running a SVM session with a 
large number of training samples.  
A solution to the above is to downsample the features and 
use some subset of the features. Assuming that each patch in 
the above example overlaps its neighbors for 2 pixels only 
and just the largest 9 DCT coefficients are taken, a feature 
dimension of 900 is attained. In our experimental study, we 
tried 5 DCT coefficients from each patch, and the accuracy 
degraded at a negligible degree but came with faster training 
due to the reduced feature dimension of 500. 
B. Development of the SVM module in the Fusion Model 
We propose a special design to the generation of the SVM 
module for each subject when enrolling to the gallery set. 
This design consists of the following steps: 
1. For each subject, the positive sample set is formed by 
the subjectâ€™s own face images, but the negative sample 
set is composed of a generic (or white) set and a sub-
ject-oriented set of face images.  
2. The generic negative set aims at carrying a wide spec-
trum of face variations across individuals, poses, illu-
minations and other factors. The generic negative set 
can be made, or approximated, by selecting the repre-
sentative samples from a large face database using self-
organizing maps (SOMâ€™s) and principal component 
analysis (PCA). Each subject in the gallery set shares 
this same generic negative set, but has a subject-
oriented (or tailor-made) negative sample set.   
3. Using the easy-to-be-misclassified samples to strategi-
cally carve the SVM hyperplane, the subject-oriented 
negative sample set is meant to reduce the false accep-
tance rate (FAR) and improve the recognition rate. This 
sample set is formed by the face images of those in the 
gallery who look similar to the enrolling subject, and 
the similarity is measured by the subjectâ€™s HMM mod-
ule. The subjectâ€™s HMM module can select a rank-m
pool of m similar faces to form the subject-oriented 
negative samples. This subject-oriented negative set and 
the generic negative set constitute the complete nega-
tive training set for building the subjectâ€™s SVM module. 
We have tested a few kernels, and decided to use the 
radial basis function (RBF) because it gives better per-
formance than others.  
bKy
Ky
d
s
s
n
i
iiin
i
iiii
 Â¦
Â¦  
 
1
1
)(
)(
1)( xx,
x,x
x D
D
(2)
World Academy of Science, Engineering and Technology 56 2009
565
A. Test Protocols for Performance Evaluation 
Two different test protocols were considered. One studied 
the impact of different number of images available for train-
ing each subjectâ€™s fusion model in the gallery set, and the 
other studied the performance variation with different gallery 
sizes. Details are as follows: 
Protocol-1. Different number of facial poses available for 
each subject to be enrolled to the gallery: for the fusion mod-
el we ran two different test scenarios. The Test-1 scenario 
started with 2 poses, frontal (F) and left-sided (L); and then 3 
poses, frontal (F), left-sided (L), and right-sided (R); then 4 
poses, F, L, R, and upward (U); and finally with all 5 poses, 
F, L, R, U, and downward (D). The Test-2 scenario again 
starts at 2 poses, but with F and U; then 3 poses, F, U, and D; 
and then 4 poses, F, U, D, and L. In each scenario, we ran-
domly selected 34 subjects out of the 68 available from the 
PIE database for enrolling to the gallery. For each subject in 
the gallery, we randomly selected the facial image samples 
from one out of the seven selected illumination conditions for 
building the fusion model. The samples in the rest six illumi-
nation conditions were used to compute the FRR (false rejec-
tion rate). The samples of the other 34 individuals were used 
to compute the FAR (false acceptance rate). The randomized 
selection scheme was repeated for 12 times, and the average 
rate was reported. The samples used in the Test-1 scenario 
were also used to build a SVM model and a HMM model for 
performance comparison. To attain a fair comparison, the 
thresholds in all algorithms were adjusted to make the FAR 
at 0.005, except for the HMM model. If the FAR of the 
HMM was set at 0.005, the FRR would have been over 0.9; 
therefore, the FAR was set to 0.15.  
Protocol-2. Different galley sizes and probe sizes: the gal-
lery size and probe size refer to the number of individuals in 
the gallery and probe sets, respectively. Given an upper 
bound of 68 subjects in the PIE database, we tested the gal-
lery sizes of 10, 20, and 34 with the probe sizes of 20, 40, 
and 68, respectively. Those in the gallery set were also in the 
probe set, but with different sets of images. The data partition 
was same as in the Protocol-1, but aims at open-set settings: 
for each gallery size, an equal size of imposters are there 
trying to break in. For each gallery size, we repeated the test 
with a random selection on the subjects for enrollment, and 
then a random selection of one out of the seven illumination 
conditions to provide training samples. As this protocol aims 
at the performance of the proposed fusion model handling 
galleries of different sizes in open-set settings, we used all 5 
poses available from the selected illumination condition for 
enrollment.  
B. Sample Preprocessing and Test Results 
All faces were aligned by the eyes, and normalized to 
64x64 pixels in size according to the distance between the 
eyes. Each facial image was converted from the original col-
or image into an 8-bit gray-scale image. We subtracted the 
best-fit linear plane from each image to reduce possible illu-
mination impacts, and then equalized its intensity histogram.  
To make the generic negative set for the SVM module, we 
collected a large number of face images from other bench-
mark databases, including FRGC [24, 25], AR [26], and 
XM2VTS [27], and some from the internet. Our collection 
had 8,156 facial images with different poses, expressions, 
ethnic backgrounds, and under various illumination condi-
tions. 626 representative ones were extracted using a self-
organizing map (SOM) with facial features extracted by PCA 
(Principal Component Analysis).   
Rank-5 candidate pool was used in all experiments, for ei-
ther training or testing, i.e., n, the number of similar faces 
selected to train the HMM module is 5, and the number of 
candidates selected to validate a probe face is also 5. For the 
proposed fusion model, 8x8 squares overlapped for 4 pixels 
have been chosen along with the major 15 coefficients from 
each squareâ€™s DCT map served as the features for the HMM 
module. As mentioned in Section III-A, part of these features 
were used by the SVM module to form the feature vector of 
dimension 500. The HMM-only for comparison used the 
same features as those used in the fusion model, but the 
SVM-only used the 666 low-frequency DCT coefficients 
extracted from each image.   
Fig. 4. Seven similar illumination conditions were selected, and 
the yellow box shows the facial area considered in our experi-
ments.
All test results are the average of 10 randomized selections 
of the gallery sets with the associated probe set. The per-
formance for Protocol-1 is shown in Fig. 5 where the two 
ways of pose variations, Test-1 and Test-2, are compared 
with its SVM-only and HMM-only counterparts. With a pre-
selected FAR at 0.005, the FRR of the fusion model with 
Fig. 5. Performance of the fusion model varies with the number 
of images available for enrollment to the galley set, compared 
with the HMM and the SVM algorithms (FAR=0.005).  
World Academy of Science, Engineering and Technology 56 2009
567
tern Recognition, (CVPR) 2005, vol.1, pp. 502 â€“ 509.  
[9] T. Vetter and T. Poggio, â€œLinear object classes and image synthesis 
from a single example image,â€ PAMI, vol.19, no.7, pp.733â€“742, 1997. 
[10] T. Vetter, â€œSynthesis of novel views from a single face image,â€ Intâ€™l J. 
Computer Vision (IJCV), vol.28, no.2, 1998, pp.103â€“116. 
[11] X. Chai, S. Shan, X. Chen and W. Gao, â€œLocally Linear Regression for 
Pose-Invariant Face Recognition,â€ IEEE Trans. Image Processing, vol. 
16, Iss.7, pp. 1716-1725, 2007.  
[12] A.V. Nefian and M.H. Hayes III, â€œHidden Markov Models for Face 
Recognition,â€ Proc. ICASSP, 1998, pp. 2721-2724.  
[13] A.V. Nefian and M.H. Hayes III, â€œAn Embedded HMM Based Ap-
proach for Face Detection and Recognition,â€ Proc. ICASSP, vol.6, 
1999, pp. 3553-3556.  
[14] S. Eickeler, S. MÃ¼ller, and G. Rigoll, â€œImproved Face Recognition 
using Pseudo 2-D Hidden Markov Models,â€ in Workshop on Advances 
in Facial Image Analysis and Recognition Technology (AFIART), Frei-
burg, Germany, 1998.  
[15] F. Samaria and S. Young, â€œHMM-based architecture for face identi-
fication,â€ Image and Vision Computing, 12(8), pp. 537-543, 1994. 
[16] P.J. Phillips, â€œSupport Vector Machines Applied to Face Recognition,â€ 
in Advances in Neural Information Processing Systems, vol.11, M.J. 
Kearns et. al., eds., MIT Press, 1999.    
[17] K. Jonsson, J. Matas, J. Kittler, and Y.P. Li, â€œLearning Support Vectors 
for Face Verification and Recognition,â€ Proc. IEEE Intâ€™l Conf. on Au-
tomatic Face and Gesture Recognition (FG), 2000, pp. 208-213.   
[18] B. Heisele, P. Ho, and T. Poggio, â€œFace Recognition with Support 
Vector Machines: Global versus Component-Based Approach,â€ Com-
puter Vision and Image Understanding (CVIU), vol.91, no. 1/2, pp. 6-
21, 2003. 
[19] B. Heisele, T. Serre and T. Poggio, â€œA Component-based Framework 
for Face Detection and Identification,â€ IJCV, vol.74, no.2,  pp. 167-
181, 2007.  
[20] P.H. Lee, Y.W. Wang, J. Hsu and Y.P. Hung, â€œFacial Features Extracted 
by 2-D HMM for Face Recognition with Pose Variations,â€ Proc. of 
IAPR Conference on Machine Vision Applications (MVA), pp. 392~395, 
2007.   
[21] L. R. Rabiner, "A Tutorial on Hidden Markov Models and Se-lected 
Applications in Speech Recognition," Proc. of IEEE, vol. 77, no. 2, pp. 
257-286, 1989.     
[22] C.J.C. Burges, "Simplified Support Vector Decision Rules," Proc. 13th 
Int Conf. on Machine Learning, pp. 71-78, 1996. 
[23] T. Sim, S. Baker, and M. Bsat, "CMU pose illumination and expres-
sion(PIE) database," PAMI, IEEE Trans, vol.25,  NO.12, Dec 2003. pp. 
1613 â€“ 1618, 2003. 
[24] Frgc    P.J. Phillips, P.J. Flynn, T. Scruggs, K.W. Bowyer, J. Chang, K. 
Hoffman, J. Marques, M. Jaesik, W. Worek, "Overview of the Face 
Recognition Grand Challenge," CVPR 2005, vol.1, 20-25, pp.947-954. 
[25] P.J. Phillips, P.J. Flynn, T. Scruggs, K.W. Bowyer, W. Worek, " Preliminary 
Face Recognition Grand Challenge Results," Proc. 7th Intâ€™l Conf Automatic 
Face and Gesture Recognition, pp. 15-24, 2006.  
[26] A.M. Martinez and R. Benavente, "The AR Face Database," CVC Technical 
Report #24 , June 1998. 
[27] K. Messar, J. Matas, and J. Kittler, "XM2VTSDB: The Extended M2VTS 
Database, " Proc. 2nd Intâ€™l Conf. Audio and Video-based Biometric Person 
Authentication (AVBPAâ€™99â€™),  pp. 2 â€“ 14, 1999.
World Academy of Science, Engineering and Technology 56 2009
569
Figure 1. Illustration of Facial Trait Code.
encode it using the numbers of the patterns at all facial traits
that best describes the appearance of this face. These num-
bers constitute the facial trait code for this face. An illustra-
tive example is given in Figure 1, where three facial traits
are shown in each of the three faces. According to FTC, the
faces can be encoded into [1, 2, 1], [3, 7, 4], and [9, 9, 6],
respectively.
Putting the facial trait patterns into codes has the follow-
ing advantages:
1. A face can be effectively denoted by a finite length of
codeword in which each symbol gives not just a spe-
cific facial trait with a fixed size and location, but also
the pattern in this facial trait that best describes the face
in that specific facial trait. This implies that the FTC
offers an effective descriptor to a given face.
2. Face identification and verification problems can be
formulated as code matching problems, and thus some
merits from coding perspective can be preserved. Er-
ror correcting is one such merit needed in the develop-
ment of the FTC, and more details will be given sub-
sequently.
3. Coding consists of encoding and decoding. The for-
mer transforms a face into a codeword, and the lat-
ter, according to the proposed FTC, transforms a code-
word to a subject in the gallery for face identification.
This is, however, just an option in the FTC decoding.
Our on-going research show that FTC decoding can be
made to generate faces with different levels of simi-
larity and some caricature faces that preserve certain
features of a real face. This implies that the FTC may
also be useful for other applications, for example in
animation or entertainment areas.
1.1. Related Works
A few works were proposed that put together coding and
facial recognition. Kittler et al [13] and Windeatt et at [23]
applied an Error Correcting Output Coding (ECOC) ap-
proach for face verification. This work shows that ECOC
can decompose a multi-classification problem into a set of
complimentary binary classification problems solvable by,
for example, MLP (Multi-Layer Perceptron) binary classi-
fiers. The input to the binary classifiers are the holistic fa-
cial features extracted using Principal Component Analysis
(PCA) and Linear Discriminant Analysis (LDA). The out-
put of the binary classifiers defines the ECOC feature space,
where the patterns of different faces are claimed to be well
separated. Followed by [13], Xie and Kumar proposed Face
Class Code (FCC) [24] to encode each subjectâ€™s facial class
into a binary string. They designed classifiers to discrimi-
nate â€™1â€™ or â€™0â€™ for each bit in the binary string for the deter-
mination of the class label. [24] aimed to fix the computa-
tionally expensive one-classifier-per-subject problem when
the number of subjects is large. Given an N -subject recog-
nition task that generally requiresN binary classifiers, FCC
only creates log2(N) binary classifiers.
The proposed FTC is distinctive from all of the previous
works upon coding for facial recognition in following as-
pects. Firstly, the codes are developed from unperceivable
and local features. Secondly, each symbol in a codeword
denotes some specific pattern existing in a facial trait, i.e., a
local rectangle patch of a certain size and location. Thirdly,
the FTC is an n-ary codeword instead of the binary ones in
all the previous works. This paper begins with the spec-
ification of the FTC in Section 2 that elaborates how the
facial trait codes are determined and specified from a large
collection of faces. When the specification of the FTC is
determined, one can encode a given facial image and de-
code a FTC codeword for face recognition, and the details
are given in Section 3. The experimental setup and the
databases used for the performance evaluation is given in
Section 4. This paper ends at a conclusion in Section 5.
2. Extraction of Facial Traits and Their Pat-
terns
Given a facial image, one can specify a local patch by
a bounding box {x, y, w, h}, where x and y are the 2-D
pixel coordinates of this bounding boxâ€™s upper-left corner,
and w and h are the width and height of this bounding box,
respectively. If this bounding box is moved from left to right
and top to bottom in the face with a step size ofâˆ†x andâˆ†y
pixels in each direction, one can obtain many patches with
the same size but different locations. If w and h can further
change from some small values to large values, we will end
up with some exhaustive set of local patches across the face.
Some similar set of such an exhaustive collection of local
1614
Authorized licensed use limited to: National Taiwan Univ of Science and Technology. Downloaded on September 15, 2009 at 06:28 from IEEE Xplore.  Restrictions apply. 
Given N patches and their associated PPMiâ€™s stacked
to form a L Ã— L Ã— N dimensional array, there are L(Lâˆ’1)2
N -dimensional binary vectors along the depth of this array
because each PPMi is symmetric matrix and one can con-
sider the lower triangular part of it. Let vp,q (1 â‰¤ q <
p â‰¤ L) denote one of the N -dimensional binary vectors,
then vp,q reveals the local similarity between the p-th and
the q-th subjects in terms of these N local patches. More
unities in vp,q indicates more differences between this pair
of subjects, and on the contrary, more zeros shows more
similarities.
The binary vector vp,q motivates our application of the
Error Correcting Output Code (ECOC) [6] to this research.
If each subjectâ€™s face is encoded using the most discrim-
inant patches, or the facial traits, then the induced set of
[vp,q]1â‰¤q<pâ‰¤L can be used to define the minimum and max-
imum Hamming distance among all encoded faces in the
corresponding code space. The vp,q with the least (most)
of unities gives the minimum (maximum) Hamming dis-
tance. To maximize the robustness against possible recogni-
tion errors in the decoding phase, we propose an Adaboost
algorithm to maximize the dmin, the minimum Hamming
distance, for the determination of the facial traits from the
overall patches. This algorithm is summarized in Algorithm
1.
Algorithm 1 Extraction of Facial Trait Patterns
Require: PPMi, i = 1 âˆ¼M
Ensure: selected N facial traits that yield maximum dmin
F = {the set ofM patches}; FË† = {âˆ…}
C(p, q) = 0; Ï‰(p, q) = 1, where p = 1 âˆ¼ L and q =
1 âˆ¼ L
for t = 1 to N do
Normalizing the weight Ï‰(p, q) = Ï‰(p,q)âˆ‘
p,q Ï‰(p,q)
.
For every element fi âˆˆ F , Î±(fi) =âˆ‘
p,q PPMi(p, q)Ï‰(p, q)
Select fË†t = argmaxi Î±(fi)
Update C(p, q) = C(p, q) + PPMi(p, q)
Calculate dmin(dmax), which is the mini-
mum(maximum) element in C(p, q)
Ï‰(p, q) =
ï£±ï£²ï£³ L if C(p, q) = dmin0 if C(p, q) = dmax1 otherwise
Update sets F = F âˆ’ fË†t and FË† = FË†
â‹ƒ
fË†t.
end for
In Algorithm 1, F is the set of the overall patches, ini-
tially contains M patches. FË† is the set of selected facial
traits, which will finally reachN traits in total. C is a LÃ—L
dimensional array where C(p, q) gives the number of ones
in vp,q. Ï‰ is a weight array with the same dimension as that
of C, and Ï‰(p, q) is the weight for the subject pair p and q.
In each run, the patch able to maximize the updated dmin is
selected as one new facial trait.
The N facial traits with their trait patterns symbolized
from 1, 2, ..., N define the basic structure of the proposed
Facial Trait Code. Each codeword in the FTC is of lengthN
and n-ary where n is the largest number of the trait patterns
found in one single trait. And the smallest distance between
codewords is dmin. In summary, given a set of frontal faces,
we can define N facial traits,
âˆ‘N
j=1 kj trait patterns, andâˆN
j=1 kj faces (or FTC codewords).
3. FTC Encoding, Decoding, and Application
to Face Recognition
The following facial sets need to be defined so that one
can apply the proposed FTC for face recognition.
1. Trait Extraction Set: A large collection of neutral
frontal faces which best covers both genders and a
wide range of ages, races, and possibly other param-
eters. Those with variations caused by poses, illumi-
nations, and expressions are excluded. The facial trait
patterns and thus the facial trait codes are defined upon
such a set. This set specifies (1) the number of facial
traits, and thus the codeword length for a given face;
(2) the patterns in each facial trait, and thus the range
of each symbol in a codeword; (3) the location and the
size of each facial trait.
2. Trait Variation Set: This is the set that encompasses
the images with the variations excluded in the trait ex-
traction set. This set does not alter any specifications
given by the trait extraction set, but substantially en-
riches the spectrum of each predetermined trait pattern
by adding in samples with variations.
3. Gallery Set: This is the set that those enrolled used to
register their faces to a face recognition system. It is
allowed in the FTC and other algorithms that images
with the aforementioned variations can be included.
4. Probe Set: This is a disjoint set from the gallery set,
and is used to test the recognition rate and other perfor-
mance indices. This set includes the images of those
enrolled but taken at different time and conditions, and
also the images of those not enrolled. It often includes
images with a tremendous amount of the aforemen-
tioned variations.
With a pre-selected length of the FTC codeword, N , the
trait extraction set defines N facial traits of different sizes,
orientations, and locations, and also the patterns in each fa-
cial trait. Each facial trait pattern is tagged with a number,
which will be used as a symbol in the FTC codeword. In
1616
Authorized licensed use limited to: National Taiwan Univ of Science and Technology. Downloaded on September 15, 2009 at 06:28 from IEEE Xplore.  Restrictions apply. 
(a)
Figure 4. FTC identification performance with different numbers
of facial traits.
2. Subset Identification: To study the identification per-
formance with different sizes of datasets, three subsets
were randomly selected from Sg . Sg1 had 100 sub-
jects, Sg2 had 200 subjects, and Sg3 had 400 subjects.
The three corresponding probe sets, Sp1, Sp2, and Sp3
were also selected from Sp. This test was repeated for
more than 20 times to obtain some average statistics.
3. Subset Verification: The three probe sets Sp1, Sp2, and
Sp3 were used along with three disjoint but equal-sized
probe sets, Sq1, Sq2, and Sq3, respectively, to match
against Sg1, Sg2, and Sg3. Sq1, Sq2, and Sq3 were
used as imposters who were trying to break in using
false identities. This test was also repeated for more
than 20 times.
4. Generalization Test: This test was designed to eval-
uate the generalization performance of the FTC. The
840 subjects were randomly selected to form three dis-
joint sets, V1 had 440 subjects, V2 had 200 subjects,
and V3 had the rest 200 subjects. V2 was further par-
titioned into two disjoint subsets V2a and V2b, which
had the same subjects but with images taken at differ-
ent time. V1 was used as the trait extraction set and
trait variation set in the FTC, and also as the basis ex-
traction set for the approaches as PCA and LDA.When
the basis components were defined byV1, the faces in
V2a, V2b and V3 would be decomposed accordingly.
V2a was then used as the gallery set,V2b as the probe
set, and V3 as another probe set formed by imposters
only. V2a andV2b were used in identification test, and
withV3 added in for verification test. This test was re-
peated for at least 20 times for 4 different gallery sizes,
20, 40, 80, and 160.
With the first setupOverall Identification, figure 4 shows
the the FTCâ€™s performance with different numbers of facial
traits, and thus with different length of codeword. It can be
seen that the identification rate increases with the number of
the facial traits, and it reaches some upper bound between
64 and 128 traits. If 127 traits were selected, we would end
Figure 5. An example of the FTC codebook.
up with a [127, 840, 41] FTC3, which could correct up to
41 error digits in the 127 symbol positions. The BCH code
proposed in Kittler01 in our experiment could only correct
27 error bits. We also found that the number of trait patterns
in each different trait was typically from 15 to 30. A few
major facial traits with their patterns are shown in figure 5.
Figure 6 (a) and (b) gives results for the Subset Identi-
fication and Subset Verification, respectively. The verifica-
tion performance was evaluated as the Hit Rate when the
False Acceptance Rate was 0.001. The most difficult test
scenario was given by theGeneralization Test, and the iden-
tification and verification rates are shown in Figure 7 (a) and
(b), respectively. In both Figure 6 and 7, we use 127 facial
traits for the proposed FTC method and a 127-bit BCH code
for the algorithm Kittler01.
Our experiments have shown that the ECOC algorithm
Kittler01 performs well in most setups except the most
challenging Generalization Test. This is because its bina-
rization at each bit is determined solely by the BCH cod-
ing method, resulting in dichotomies irrelevant to the ap-
pearance of a human face. It sure fails in the generaliza-
tion test. In all our experiments, FTC outperformed other
methods, including Ahonen06, which drew some attention
in the face recognition community recently. FTC performed
better than Ahonen06 in the subset(generalization) test for
23.9%(15.8%) higher in the verification rate.
Table 2 summarizes the performance of all algorithms
evaluated in our experiments in terms of average identifica-
tion/verification rates. Heisele03 were not feasible for the
study of the generalization performance, and thus excluded
in this table.
3We denote a [N,M, dminâˆ’1
2
] FTC as a code with M valid n-ary
codewords of lengthN . And the smallest distance between theseM code-
words is dmin.
1618
Authorized licensed use limited to: National Taiwan Univ of Science and Technology. Downloaded on September 15, 2009 at 06:28 from IEEE Xplore.  Restrictions apply. 
larger out-of-plane rotations. One better solution is to use
3-D facial data. It is worthy of mention that the proposed
algorithm can be extended to incorporate 3-D facial data
straightforwardly. The whole algorithm remains the same,
except that the raw features become range data instead of
image intensities.
Acknowledgement
This work is supported in part under the grant 97-EC-17-
A-02-S1-032.
References
[1] J. Ahlberg. Facial feature extraction using deformable graphs
and statistical pattern matching. In in Swedish Symposium on
Image Analysis, SSAB, 1999.
[2] T. Ahonen, A. Hadid, and M. Pietikainen. Face description
with local binary patterns: Application to face recognition.
In PAMI, pages 2037â€“2041, 2006.
[3] E. Bart, E. Byvatov, and S. Ullman. View-invariant recogni-
tion using corresponding object fragments. In Proceedings
of the Seventh European Conference on Computer Vision,
pages Vol II: 152â€“165, 2004.
[4] P. Belhumeur, J. Hespanha, and D. Kriegman. Eigenfaces
vs. fisherfaces: Recognition using class specific linear pro-
jection. PAMI, 19(7):711â€“720, 1997.
[5] C.-C. Chang and C.-J. Lin. LIBSVM: a library for
support vector machines, 2001. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
[6] T. G. Dietterich and G. Bakiri. Solving multiclass learning
problems via error-correcting output codes. Journal of Arti-
ficial Intelligence Research, 2:263â€“286, 1995.
[7] M. Figueiredo and A. Jain. unsupervised learning of finite
mixture models. PAMI, 24:381â€“396, 2002.
[8] V. Guruswami and A. Sahai. Multiclass learning, boosting,
and error-correcting codes. In Proceedings of the twelfth
annual conference on Computational learning theory, pages
145 â€“ 155, 1999.
[9] B. Heisele, P. Ho, J. Wu, and T. Poggio. Face recognition:
component-based versus global approaches. CVIU, 91(1):6â€“
12, 2003.
[10] B. Heisele, S. Thomas, P. Sam, and T. Poggio. Hierar-
chical classification and feature reduction for fast face de-
tection with support vector machines. Pattern Recognition,
36(9):2007â€“2017, 2003.
[11] Y. Ivanov, B. Heisele, and T. Serre. Using component fea-
tures for face recognition. In FGRâ€™04, page 421, 2004.
[12] M. J. Jones and P. Viola. Face recognition using boosted
local features. Technical report, 2003.
[13] J. Kittler, R. Ghaderi, T. Windeatt, and J. Matas. Face veri-
fication using error correcting output codes. In CVPR, vol-
ume 1, pages Iâ€“755â€“ Iâ€“760, 01.
[14] R. Liao and S. Z. Li. Face recognition based on multiple
facial features. In In Proc. of the 4th IEEE Int. Conf. on
Automatic Face and Gesture Recognition, pages 239â€“244.
Dekker Inc, 2000.
[15] A. Martinez and R. Benavente. The ar face database. Tech-
nical Report 24, CVC, 1998.
[16] K. Messer, J. Matas, J. Kittler, J. Luettin, and G. Maitre.
Xm2vtsdb: The extended m2vts database. In AVBPA, 1999.
[17] P. Phillips, P. Flynn, T. Scruggs, K. Bowyer, J. Chang,
K. Hoffman, J. Marques, J. Min, andW.Worek. Overview of
the face recognition grand challenge. In CVPR, pages 947â€“
954, 2005.
[18] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss. The
FERET evaluation methodology for face-recognition algo-
rithms. PAMI, 22(10):1090â€“1034, 2000.
[19] M. Savvides, R. Abiantun, J. Heo, S. Park, C. Xie, and B. Vi-
jayakumar. Partial and holistic face recognition on frgc-ii
data using support vector machine. In Proceedings of the
2006 Conference on Computer Vision and Pattern Recogni-
tion Workshop (CVPRW06), pages 48â€“48, 2006.
[20] T. Sim, S. Baker, and M. Bsat. The CMU pose, illumination,
and expression (PIE) database of human faces. Technical Re-
port CMU-RI-TR-01-02, Carnegie Mellon University, 2001.
[21] M. Turk and A. Pentland. Eigenfaces for recognition. Jour-
nal of Cognitive Neuroscience, 3(1):71â€“86, 1991.
[22] P. Viola andM. Jones. Rapid object detection using a boosted
cascade of simple features. In CVPR, volume 1, pages 511â€“
518, 2001.
[23] T. Windeatt and G. Ardeshir. Boosted ecoc ensembles for
face recognition. In VIE, pages 165 â€“168, 2003.
[24] C. Xie and B. V. Kumar. Face class code based feature
extraction for face recognition. In Fourth IEEE Workshop
on Publication Automatic Identification Advanced Technolo-
gies, pages 257â€“262, 2005.
[25] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld.
Face recognition: A literature survey. ACM Comput. Surv.,
35(4):399â€“458, 2003.
Appendix: Error-Correcting Property of the
ECOC
FTC inherits the error-correcting capability from the
ECOC. An (N,L, dmin) ECOC C is a set of L binary vec-
tors of dimensionN , called codewords, such that the Ham-
ming distance between every pair of distinct codewords is
at least dmin. This code can correct at least (dminâˆ’12 ) er-
ror bits. Since every codeword has distance at least dmin
from every other codeword, the closed Hamming balls of
radius (dminâˆ’12 ) around each codeword are disjoint. Hence
if a binary vector y differs from some codeword x âˆˆ C in at
most (dminâˆ’12 ) bit positions, then x is still the unique clos-
est codeword in C to y [8]. Consequently, an ECOC with
larger dmin is able to correct more error bits. It also means
that the L codewords in C are well separated in the Ham-
ming space of C.
1620
Authorized licensed use limited to: National Taiwan Univ of Science and Technology. Downloaded on September 15, 2009 at 06:28 from IEEE Xplore.  Restrictions apply. 
