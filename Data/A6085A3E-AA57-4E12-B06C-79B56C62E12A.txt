all ROIs have been covered. The ROI is scanned at 10 
scales each a factor of 1.25 larger than the last.
Step 4: To locate the potential candidate, the rectangle 
feature of W u H pixels in the feature template can be 
computed as follows  
|),(|][
0,0
Â¦
dd
 
HyWx
yxdi
DWF
LH
,                     (1) 
where d
DWF
LH  is the nonsubsampled version of discrete 
wavelet transform (DWF, discrete wavelet frames) in 
the LH horizontal subband and }...,,1{ IiÂ , I = 9 is the 
number of  rectangles in the proposed feature template. 
At each location within ROI, check the decision rule of 
equation (2) that indicates whether the face template is 
satisfied or not. 
 1  2  3 
 4  5  6 
 7  8  9 
( [1] > [4] )  && ( [3] > [6] ) &&  ( [1] > [2]  ||  [3] > 
[2] )  && ( [5] > [4] ) && ( [5] > [6] ) && ( [8] > [7] ) 
&& ( [8] > [9] )                                                             (2) 
Step 5: Merge templates that can be counted as 
enclosing the same face since it is common to have 
multiple findings with small displacements horizontally 
and/or vertically.  
Step 6: Normalize all the candidate regions â€“ resizing to 
24 u 24 pixels and using histogram equalization. 
Step 7: Classify the face candidate using the SVM
classifier. If the class corresponds to a face, we draw a 
rectangle around the face in the output image. 
Step 8: Normalize the detected face rectangle to size 
40Ã—60 pixels .  An entropy-based smoothing filter is 
introduced to move the center from pixel to pixel in the 
rectangle to remove the coefficients located at outside 
the facial component regions. This continues until all 
pixel locations have been covered and facial component 
object are to be created for extraction. The entropy 
filtering of the pixels in the 3 Ã— 3 (N  =  3)  
neighborhood defined by the mask is given by the 
expression 
),(log),(
1
_
1
0,
2
yxdyxd
N
entropyWavelet DWFLH
N
yx
DWF
LHÂ¦

 
 . (3)
According to anthropometry, we could perform the 
inter-orientation projection along the horizontal and 
vertical axes respectively to locate human eyes and 
mouth. Search for the centroid in the facial component 
region, we approach region segmentation by finding 
meaningful boundary based on point aggregation 
procedure. Choosing the center pixel of the component 
region is a natural starting point and grouping points to 
form the region of interest with paying attention to 4-
connectivity would yield a clustering result, when no 
more pixels for inclusion in the region. After growing, 
the region centroid is relocated and therefore eyes, 
mouth, and face size are also known. An adaptation is 
finally carried out to mark the eyes and mouth positions 
and refine the bounding rectangle as an ellipse of fitting 
facial oval shape (Fig. 1(b)). 
    Note that one needs to do more works as the 
following step for the pose estimation of human face 
while the final objective of face segmentation is 
performing face recognition. 
Step 9: The pose estimation procedure presented in Fig. 
2 is defined in the image plane. The points ),( LL yx  and 
),( RR yx  as displayed in the following illustration are 
the coordinates of the left and right eyes, respectively, 
and the point ),( MM yx  is the center of the mouth. We 
define the pan T  of equation (3) as the angle between 
the lines passing through the right eye and the center of 
the mouth and passing through the left eye and the 
center of the mouth. The face image with rotation in 
plane T  can be rotated back to the upright position 
(orientation alignment) and then the corresponding 
view-based classifier based on the skin color ratio 
between the left cheek and the right cheek can be 
chosen for recognition. The three multiview face 
classifiers are built according to the following angles: -
60q~-20q, -20q~+20q, +20q~+60q. The color ratio range 
is determined as (-1.2, +1.2) for the frontal view face 
recognition.  
ST /180
2/)(
2/)(
tan 1 uÂ¸Â¸Â¹
Â·
Â¨Â¨Â©
Â§

 
LRM
LRM
yyx
xxx .           (3) 
),( MMx
),( MyMx
),( RyRx
),( LyLx
T
The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00  Â© 2006
Y.Rodriguez, K.Kryszczuk, J.Czyz, L.Vandendorpe, J.Ng, 
H.Cheung, and B.Tang. â€œFace authentication competition on 
the BANCA database,â€ ICBA, Lecture Notes in Computer 
Science, vol. LNCS 3072, pp. 8-15, 2004. 
[11] http://www.humanscan.de/. 
[12] http://www.vision.caltech.edu/html-iles/archive.html. 
Fig. 1.  Function block diagram of the proposed approach. 
Input
Orientation 
Alignment
Segmentation
Output
Fig. 2.  Pose estimation. 
 (a)  (b) 
Ê³(c)
(d)
Fig. 3.  Live detection: (a) dim lighting, (b) bright lighting, (c) in crowds; and (d) photo detection.
FACE
CLASSIFICATION
WAVELET
FRAMES
(a) Input image; 
SKIN COLOR 
DETECTION
FACE LOCALIZATION
FACE
SEGMENTATATION
(b) Output image. 
The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00  Â© 2006
 äºŒã€æœƒè­°ï¨ˆç¨‹èˆ‡å…§å®¹ 
æ­¤æ¬¡æœƒè­°è‡ª 0112-0114 å…±ä¸‰å¤©ï¼Œæœƒè­°é–‹å§‹å‰ï¥¸æ—¥ç‚º tutorial sectionï¼Œä¸»é¡Œæ¶µ
æ¦‚ç¶²ï¤·æŠ€è¡“ã€æ™ºç”¢æ¬Šç®¡ï§¤ã€å‹•æ…‹å½±éŸ³è¦–è¨Šå£“ç¸®ç­‰æŠ€è¡“ã€‚æœ¬äººä¹‹å ±å‘Šä¿‚è¢«æ’
åœ¨ 0113 çš„æœ€å¾Œä¸€å€‹æ™‚æ®µï¼Œæ‰€ä»¥è½çœ¾é¡¯å¾—è¼ƒå°‘äº›ï¼Œï¥§éè­°ç¨‹ä¸»å¸­ Akiomi 
Kunisa (ï¤­è‡ªä¸‰æ´‹ï¥£ç¾ç ”ç©¶ä¸­å¿ƒ)ä¹Ÿï¨¦æœƒæå•ä»¥å¢åŠ å½¼æ­¤é–“çš„äº’å‹•ç†±é¬§æ°£
æ°›ã€‚ç”±æ–¼é€™å€‹æœƒè­°ä¸­å¿ƒç¶“å¸¸èˆ‰è¾¦åœ‹éš›æ€§çš„å±•å‡ºæ´»å‹•å› æ­¤é¢ç©ç›¸ç•¶ï§ƒé—Šï¼Œæ¯
é–“ç°¡å ±å®¤çš„ç©ºé–“ä¹Ÿæ¯”ä¸€èˆ¬åœ¨ï¨ªåº—èˆ‰è¾¦æœƒè­°çš„è­°äº‹å»³å¤§ä¸Šè¨±å¤šï¼Œå†åŠ ä¸ŠåŒæ™‚
æœ‰è¨±å¤šè­°ç¨‹é€²ï¨ˆäººå“¡åˆ†æ•£å„è™•ï¼Œæ‰€ä»¥æ„Ÿè¦ºèµ·ï¤­è¼ƒï¥§ç†±é¬§ã€‚é€™æ¬¡æ°é€¢ 25 é€±ï¦
æ…¶ï¼Œå› æ­¤å¤§æœƒåœ¨æ›å ´ä¼‘æ¯çš„é¤é»ä¸­ä¹Ÿç‰¹åˆ¥æº–å‚™ï¦ºå°æœ‰ icce25 çš„å¤§è›‹ç³•ä¾›å‡º
å¸­äººå“¡äº«ç”¨ä»¥ç¤ºæ…¶ç¥ã€‚ 
icce ä¹‹ä¸»é¡Œè¼ƒåé‡æ–¼æ¶ˆè²»é›»å­ç›¸é—œï¦´åŸŸï¼Œèˆ‡æœ¬äººåœ¨å®Œå…¨ç„¡æå¤±æµ®æ°´å°ä¹‹
ç ”ç©¶ç¯„ç–‡æ¥µç‚ºæ¥è¿‘ã€‚èˆ‡æœƒè€…æœ‰äº›æ˜¯ä»¥å‰åœ¨ç›¸é—œæ–‡ç»ä¸Šæ›¾ï¨Šéåå­—çš„ä½œè€…ï¼Œ
è¦ªç¹å…¶æœ¬å°Šä¹‹æ¼”ï¥¯å¦æœ‰ä¸€ç•ªæ„Ÿå—ã€‚æ­¤å¤–ï¼Œæœ¬äººï¤è¶æ­¤æ©Ÿæœƒåˆ°æµ·å ±å±•ç¤ºå€å»
ç­è§£å…¶ä»–ä½œè€…åœ¨é€™ï§ä¸»é¡Œä¹‹ç ”ç©¶æˆæœï¼Œèˆ‡ä»–å€‘äº¤æ›å¿ƒå¾—ä¸¦è¨ï¥æ­¤æŠ€è¡“æœªï¤­
ä¹‹ç™¼å±•æ–¹å‘ï¼Œæ”¶ç©«å¯è¬‚è±å¯Œã€‚ 
 
ä¸‰ã€çµï¥ 
ç”±æ–¼ä¸€èˆ¬çš„æµ®æ°´å°æŠ€è¡“å¿…éœ€ä¿®æ”¹åŸå§‹å½±åƒçš„å…§å®¹ï¼Œå¾€å¾€æœƒé€ æˆéƒ¨ä»½è³‡è¨Šçš„
å¤±çœŸã€‚è€Œæœ¬äººæå‡ºçš„å®Œå…¨ç„¡æå¤±æµ®æ°´å°æŠ€è¡“å‰‡å…‹æœï¦ºé€™å€‹ç¼ºé»ï¼Œåœ¨é†«å­¸å½±
