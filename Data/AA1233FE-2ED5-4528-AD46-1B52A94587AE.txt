1Â 
Â 
ï§ç”¨å¤šé‡æ ¸å¿ƒä¹‹æ”¯æ´å‘ï¥¾æ©Ÿæ–¼è›‹ç™½è³ªéç©©å®šå€æ®µçš„é æ¸¬ï¼ˆç¬¬ 2 ï¦ï¼‰ 
è¨ˆç•«ç·¨è™Ÿï¼šNSC 96-2221-E-032-051-MY2 
åŸ·ï¨ˆæœŸé™ï¼š2007/08/01~2009/07/31 
ä¸»æŒäººï¼šè¨±è¼ç…Œ æ·¡æ±Ÿå¤§å­¸ 
å…±åŒä¸»æŒäººï¼šé„­å»ºä¸­ å˜‰ç¾©å¤§å­¸  
è¨ˆç•«ï¥«èˆ‡äººå“¡ï¼šè¬æ­£è‘¦ æ·¡æ±Ÿå¤§å­¸, æ¥Šæœå‹³ æ·¡æ±Ÿå¤§å­¸, ï¦€æ˜é” æ·¡æ±Ÿå¤§å­¸ 
 
ä¸€ã€ä¸­æ–‡æ‘˜è¦ 
æœ¬ç ”ç©¶ä»¥é æ¸¬è›‹ç™½è³ªçš„ï¥§ç©©å®šå€æ®µç‚º
ä¸»é¡Œï¼Œåœ¨ä»¥å¾€çš„ç ”ç©¶ä¸­ï¼Œå…‰æ˜¯ï§ç”¨å„€å™¨é‡
å°æ™®é€šç©©å®šè›‹ç™½è³ªçš„çµæ§‹ç ”ç©¶åˆ†æï¼Œå°±å·²
ç¶“æ˜¯å€‹éå¸¸è€—æ™‚ä»¥åŠè³‡æºæ¶ˆè€—çš„è­°é¡Œï¼Œï¤
ï¥§ç”¨ï¥¯æ˜¯é‡å°è›‹ç™½è³ªä¸­çš„ï¥§ç©©å®šå€æ®µåšåµ
æ¸¬ã€‚ä¹Ÿå› å¦‚æ­¤ï¼Œï§ç”¨æ©Ÿå™¨å­¸ç¿’çš„ï§¤ï¥å¾è›‹
ç™½è³ªçš„ä¸€ç´šåºï¦œä¸­é æ¸¬å…¶ä¸‰ç´šçµæ§‹æ˜¯ä¸€ç¨®
æ–°çš„è¶¨å‹¢ä¹Ÿæ˜¯ä¸€ç¨®è¿«ï¨€çš„éœ€è¦ã€‚å¦‚æ­¤ï¼Œæ‰
èƒ½å¤§å¹…ï¨çš„æå‡è›‹ç™½è³ªéç©©å®šå€æ®µé æ¸¬çš„
æ•ˆèƒ½ã€‚æ‰€è¬‚çš„è›‹ç™½è³ªéç©©å®šå€æ®µåœ¨è›‹ç™½è³ª
çš„çµæ§‹ï§¨æ˜¯ï¥§å…·æœ‰å›ºå®šçš„äºŒç´šçµæ§‹ï¼Œå¦‚ä»¥
ä¸€ï¤­ï¤å¢åŠ ï¦ºçµæ§‹é æ¸¬çš„é›£ï¨ã€‚é™¤æ­¤ä»¥å¤–ï¼Œ
å…¶çµæ§‹çš„é‡è¦æ€§åœ¨æ–¼ä¸€äº›ç‰¹æ®ŠåŠŸèƒ½ä¸Šçš„é‹
ä½œã€‚åœ¨æ­¤ç ”ç©¶ä¸­ï¼Œæˆ‘å€‘ä¹Ÿé‡å°å…¶è¼¸å…¥ç‰¹å¾µ
ä¸­æŒ‘é¸å‡ºä¸€äº›é©ç•¶çš„ç‰¹å¾µé›†ã€‚å…¶ä¸­ï¼Œæˆ‘å€‘
ä¹Ÿï§ç”¨ï¦ºä¸‰ç¨®ï¥§åŒçš„ç‰¹å¾µé¸å–æ–¹æ³•å°æˆ‘å€‘
é¸å®šçš„è³‡ï¦¾é›†ä½œç‰¹å¾µé¸å–çš„å‹•ä½œã€‚å…¶æ–¹æ³•
åˆ†åˆ¥æ˜¯ F-score, information gain, ä»¥
åŠ K-medoids clusteringã€‚åœ¨ç‰¹å¾µé¸å–ä¹‹
å¾Œï¼Œæˆ‘å€‘ï§ç”¨ï¦ºæ”¯æ´å‘ï¥¾æ©Ÿ(support 
vector machine)åšç‚ºå­¸ç¿’é æ¸¬åˆ†ï§å™¨ã€‚åœ¨
æœ€ çµ‚ çš„ ç ” ç©¶ æˆ æœ ä¸­ ï¼Œ æˆ‘ å€‘ æ å‡º çš„
K-medoids clustering ç‰¹å¾µé¸å–æ–¹æ³•èƒ½å¤ 
æœ‰æ•ˆçš„ï¨‰ä½è¼¸å…¥çš„ç‰¹å¾µï¥¾ï¼Œå¾åŸå…ˆçš„ 440
å€‹ç‰¹å¾µï¨‰ä½è‡³ 150 å€‹ï¼Œæ­¤å¤–ï¼Œå…¶é æ¸¬çš„æ­£
ç¢ºï¥¡ä¹Ÿå¾åŸå…ˆçš„ 84.66%å‘ä¸Šæå‡è‡³
86.81%ã€‚å¦å¤–ï¼Œæˆ‘å€‘ä¹Ÿåœ¨å¯¦é©—çµæœä¸­è­‰æ˜
æ­¤æ–¹æ³•æ¯” F-score æˆ–æ˜¯ Information gain
æœ‰ï¤ç©©å®šçš„è¡¨ç¾ã€‚ 
é—œéµè©ï¼šè›‹ç™½è³ªéç©©å®šå€æ®µ; K-Medoids 
å¢é›†; ç‰¹å¾µé¸å–; è›‹ç™½è³ªé«”å­¸ 
Abstract 
Determining the structure of a protein is 
not an easy task which usually involved a 
time-consuming and costly process in the 
web lab. Using computational methods to 
predict a proteinâ€™s tertiary structure from its 
primary structure (the amino acid sequence) 
is desirable. Disordered regions are segments 
of a protein that do not have a fixed 
conformation, which makes the structure 
prediction much harder. Also, these 
disordered regions are functionally important 
for a protein. In this research, we would like 
to identify such regions with a focus on 
selecting a proper feature set. Three feature 
selection methods, namely F-score, 
information gain, and K-medoids clustering, 
are used for feature selection. The support 
vector machine is then used for classification. 
The results show that the classification 
accuracy can be raised with a smaller feature 
set. The K-medoids clustering feature 
selection can reduce the number of features 
from 440 to 150 and improve the accuracy 
from 84.66% to 86.81% in five-fold cross 
validation. It also has a more stable 
performance than F-score and information 
gain.  
3Â 
Â 
æ“·å–çµæœã€‚ 
æˆ‘å€‘åˆ†åˆ¥å°‡é€éä¸‰ç¨®ï¥§åŒçš„ç‰¹å¾µæ“·å–
æ–¹å¼æ‰€å¾—åˆ°çš„ç‰¹å¾µé›†è¼¸å…¥æ”¯æ´å‘ï¥¾æ©Ÿ
(SVM)ï¼Œç”¨å…¶å„ªï¥¢çš„åˆ†ï§èƒ½ï¦Šæä¾›æˆ‘å€‘ï¥¼å¥½
çš„éç©©å®šå€æ®µé æ¸¬çµæœã€‚è¡¨ 1 ç‚ºï§ç”¨ä¸‰ç¨®
ï¥§åŒçš„ç‰¹å¾µæ“·å–æ–¹å¼æ‰€å¾—åˆ°çš„é æ¸¬çµæœã€‚ 
è¡¨ 1. ä¸‰ç¨®ï¥§åŒçš„ç‰¹å¾µæ“·å–æ–¹å¼æ‰€å¾—åˆ°çš„åˆ†ï§å™¨
é æ¸¬çµæœ (å®Œæ•´440å€‹ç‰¹å¾µå€¼çš„æ­£ç¢ºï¥¡æ˜¯84.66%) 
æ–¹æ³• æ­£ç¢ºï¥¡ 
(5-fold cross validation) 
ç‰¹å¾µï¥©ç›® 
F-score 82.38% 400 
IG 83.53% 400 
K-medoids 86.81% 150 
 
å¾çµæœæˆ‘å€‘å¯ä»¥çœ‹åˆ°ï¼Œåœ¨å„åˆ¥ç‰¹å¾µæ“·
å–çš„æ–¹æ³•ä¸Šï¼Œæœ€å¥½çš„çµæœåˆ†åˆ¥æ˜¯ F-score
çš„ 82.38%ã€IG çš„ 83.53%ä»¥åŠ K-medoids
çš„ 86.81% ï¼Œ æ˜ é¡¯ çš„ æˆ‘ å€‘ æ‰€ æ å‡º çš„
K-medoids åœ¨çµæœä¸Šå„ªæ–¼å‰ï¥¸è€…ã€‚ä½†æ˜¯å°±
ä»¥ç‰¹å¾µï¥©è€Œï¥ï¼Œæˆ‘å€‘æ‰€æå‡ºçš„ K-medoids
æ–¹å¼ï¤èƒ½å¤ ï¨‰ä½ç‰¹å¾µï¥©ç›®ï¼Œé”åˆ° 150 å€‹ï¼Œ
è€ŒåŒæ™‚ï¼ŒF-score èˆ‡ IG å°±åªèƒ½ï¨‰ä½ 40 å€‹
ç‰¹å¾µï¥©ï¼Œé”åˆ° 400 å€‹ç‰¹å¾µï¥©ã€‚å°æ­¤è€Œè¨€ï¼Œ
ä¹‹å¾Œåˆ†ï§å™¨ï¤èƒ½å¤ æå‡æ•ˆèƒ½ï¼Œå¦å¤–ä¹Ÿï¤èƒ½
å»é™¤è¨Šæ¯ä¸­çš„é›œè¨Šï¼Œé”åˆ°ï¥­æ™‚åˆèƒ½æé«˜æ­£
ç¢ºï¥¡çš„æ•ˆæœã€‚ 
å¦å¤–ï¼Œæˆ‘å€‘ä¹Ÿé‡å°ç‰¹å¾µæ“·å–çš„ç©©å®šï¨
åšä¸€å€‹æ¯”è¼ƒï¼Œå¸Œæœ›èƒ½æ¯”è¼ƒå‡ºåœ¨ï¥§åŒçš„ç‰¹å¾µ
ï¥©ç›®é¸å–ä¸Šï¼Œç‰¹å¾µæ“·å–æ–¹å¼èƒ½å¤ æä¾›ä¸€å€‹
ç©©å®šä¸”ï¥¼å¥½çš„çµæœï¼Œï¥§èƒ½åœ¨å»é™¤ç‰¹å¾µï¥©ç›®
çš„åŒæ™‚ï¼Œå°‡é‡è¦çš„ç‰¹å¾µä¸€ä½µå»é™¤ï¼Œå¦‚æ­¤ä¸€
ï¤­ï¥¥ç„¡æ³•é”åˆ°ç©©å®šçš„æ•ˆèƒ½ã€‚è¡¨ 2 ç‚ºä¸‰ç¨®ç‰¹
å¾µæ“·å–æ–¹å¼çš„ç©©å®šï¨æ¯”è¼ƒã€‚ 
 
 
 
è¡¨ 2 ä¸‰ç¨®ç‰¹å¾µæ“·å–æ–¹å¼åœ¨è³‡ï¦¾é›†çš„è‡ªæˆ‘è¨“ï¦–ä»¥åŠ
æ¸¬è©¦çµæœ 
Methods
Number 
of 
features
Accuracy Recall/Sensitivity Precision Specificity
K-medoids
50 96.21% 0.91 98.22% 99.14%
100 99.47% 0.99 99.66% 99.82%
150 99.54% 0.99 99.78% 99.89%
200 99.82% 1.00 99.82% 99.91%
250 99.71% 0.99 99.78% 99.89%
300 99.77% 1.00 99.82% 99.91%
350 99.82% 1.00 99.85% 99.92%
400 99.86% 1.00 99.89% 99.94%
F-score
50 79.23% 0.43 92.97% 98.31%
100 79.93% 0.44 94.47% 98.65%
150 81.07% 0.50 90.76% 97.35%
200 81.69% 0.52 90.82% 97.26%
250 82.36% 0.54 91.08% 97.25%
300 82.85% 0.55 91.09% 97.17%
350 99.85% 1.00 100.00% 100.00%
400 82.98% 0.57 89.84% 96.64%
IG 
50 83.81% 0.65 84.03% 93.53%
100 98.72% 0.97 99.11% 99.54%
150 99.61% 0.99 99.69% 99.84%
200 99.63% 0.99 99.62% 99.80%
250 82.16% 0.54 90.15% 96.92%
300 82.52% 0.55 89.62% 96.64%
350 99.91% 1.00 99.88% 99.94%
400 84.38% 0.62 89.74% 96.33%
 
 å¾è¡¨ä¸­æˆ‘å€‘å¯ä»¥è§€æ¸¬åˆ°ï¼Œç„¡ï¥åœ¨ç‰¹å¾µ
ï¥©ç›®æ”¹è®Šæ™‚ï¼ŒK-medoids æ–¹å¼çš†èƒ½æä¾›ä¸€
å€‹ï¥¼å¥½ç©©å®šçš„çµæœï¼Œç›¸å°æ–¼ K-medoidsï¼Œ
å…¶ä»–ï¥¸ç¨®çš„ç‰¹å¾µæ“·å–æ–¹å¼çš„æ¸¬ï¥¾çµæœï¼Œ
K-medoids ç¸½æ˜¯èƒ½ä¿è­‰å…¶é æ¸¬ï¥¡ï¼Œï¥§æœƒåœ¨
åšç‰¹å¾µå‰”é™¤æ™‚ï¼Œå°‡é‡è¦ç‰¹å¾µçµ¦å»é™¤æ‰ï¼Œè€Œ
å°è‡´é æ¸¬çš„æ­£ç¢ºï¥¡ä¸‹ï¨‰ã€‚ 
 
 
PaperÂ listÂ  Â 
    Â Â Â Â Â Â Â Â  Cheng-Wei Hsieh, Hui-Huang Hsu, Ming-Da, Lu, â€œProtein Disordered Region 
Prediction by SVM with Post-Processing,â€ in Proc. the 2nd International 
Conference on Complex, Intelligent and Software Intensive Systems (CISIS 
2008), pp. 693-698, Barcelona, Spain, March 4-7, 2008. Â 
Â Â Â Â Â Â Â Â Â  Huiâ€HuangÂ Hsu,Â Chengâ€WeiÂ Hsieh,Â andÂ Mingâ€DaÂ Lu,Â â€œAÂ HybridÂ FeatureÂ SelectionÂ 
Mechanism,â€Â  inÂ  Proc.Â  EighthÂ  InternationalÂ  ConferenceÂ  onÂ  IntelligentÂ  SystemsÂ 
DesignÂ andÂ ApplicationsÂ  (ISDAÂ 2008),Â  vol.Â 2,Â pp.Â 271â€276,Â Kaohsiung,Â  Taiwan,Â 
Nov.Â 26â€28,Â 2008.Â 
Â Â Â Â Â Â Â Â Â  Huiâ€HuangÂ HsuÂ andÂ Chengâ€WeiÂ Hsieh,Â â€œFeatureÂ SelectionÂ ForÂ IdentifyingÂ ProteinÂ 
DisorderedÂ Regionsâ€Â submittedÂ toÂ BiomedicalÂ Engineering,Â Oct.Â 2009.Â 
Â 
Â Â Â Â Â Â Â  Â Â  Huiâ€HuangÂ  Hsu,Â  Chengâ€WeiÂ  Hsieh,Â  Mingâ€DaÂ  Lu,Â â€œHybridÂ  featureÂ  selectionÂ  byÂ 
combiningÂ filtersÂ andÂ wrappersâ€Â submittedÂ toÂ ExpertÂ SystemsÂ withÂ Applications,Â 
Oct.Â 2009.Â 
Â 
 acid properties are proved to be related to protein 
disordered region. We use the support vector machine 
with the above information to predict the disordered 
region of proteins. And finally, we use several post 
processing algorithms as smoothing functions to 
improve the prediction accuracy. 
The rest of the paper is organized as follows. 
Section 2 introduces the related work. Section 3 
describes the SVM training with smoothing algorithm. 
Section 4 presents the experimental results. And 
Section 5 draws the final conclusion.  
2. Related work 
 
In the past few years, several machine learning 
models were developed to solve problems in 
bioinformatics. Especially for structural biology, by 
analyzing the protein sequence composition, numerous 
issues could be solved.  For example, protein structure 
prediction [7], solvent accessibility prediction [8], 
residue contact prediction  [9], protein subcellular 
localization prediction [10],â€¦etc. In all the above 
predictions, few post process were taken to improve 
the prediction accuracy. Smoothing process is the most 
used method to correct the short prediction errors 
regions which perform an easy step to smooth the 
discontinuous prediction results.  
In this paper, the disordered proteinsâ€™ issue follows 
the above studiesâ€™ pattern. However, unlike protein 
structure prediction, normal physical features are not 
enough for disordered proteins prediction. The 
additional features of amino acids should also be 
provided for prediction. In the previous researches of 
disordered protein predictions, there are several 
successful works. For instance, the first study for 
disordered proteins prediction is by Williams et al. 
(1978) [11]. They noted the abnormally low 
charge/hydrophobic ratio for the two disordered 
proteins, and used this special property to predict. 
Uversky et al. [12] did the same analysis but on much 
larger set of proteins in 2000. And they made a list of 
disordered propensity for each amino acid. However, 
no post process of prediction was taken.  
In 2006, Keith Dunker et al. developed the VSL2 
[13] disordered region predictor which used an output 
smooth procedure for its prediction result. The 
smoothing algorithm is based on calculating the 
average of raw predictions for neighboring residues 
within an output window of an odd number length 61 
to remove occasional misclassifications. In that work, 
the prediction accuracy exceeds 85%.  Nevertheless, 
they did not show the smoothing effects. No 
description was made to explain the smoothing result. 
The only thing we know is that they use the average 
result within a sliding window as the central nodeâ€™s 
reference. In this paper, we modify the average idea to 
a weighted reference which will be described in 
Section 3. 
Several researches also use smoothing procedure to 
remove the misclassified node. Such as IUPred [14], 
which is a web server presented a novel algorithm for 
predicting such regions from amino acid sequences by 
estimating their total pairwise interresidue interaction 
energy. In the end, the IUPred use a simple smoothing 
step to smooth some misclassified node over a window 
size of 21. Another predictor is PONDR [15] which 
measures proteinsâ€™ complexity and encodes it with 
statistic information of amino acids. Then, PONDR 
trains the neural network with these features. The 
training error of it is about 17% based on the data set 
they collected. The smoothing procedure of it is by 
averaging over sliding windows of nine amino acids. 
In particular, the PONDR wouldnâ€™t smooth the first 
and last four sequence positions from the N- and C-
terminal. Since some short disordered regions usually 
exist in the edges of a sequence. 
The DisEMBL [16] predictor introduced a new idea 
to predict the protein disordered regions by a simple 
concept of â€œhot loopsâ€ which indicate the structure 
coils with high temperature factors. The DisEMBL 
also refer to the PONDRâ€™s smoothing algorithm. 
However, it did not improve the overall performance 
on their own datasets. 
In the above prediction model, the smoothing 
algorithms are of few variations and few discussions 
were made. Hence, in this paper we present two other 
improved smoothing algorithms and make a complete 
discussion on them. 
3. SVM training and smoothing algorithms 
 
3.1. Feature sets  
 
 For disordered protein prediction, basic protein 
sequence composition information is needed. 
Therefore, in the protein prediction stage, the target 
proteinsâ€™ composition should be analyzed. The first 
kind of composition information is target proteinsâ€™ 
occurrence frequencies. We developed a program with 
encoding and predicting functions. Hence, while the 
target proteinsâ€™ primary structure information is 
inputted, the 20 kinds of amino acidsâ€™ frequencies 
would be calculated as the first 20 features. 
 In advance, only frequencies feature may satisfy 
 a decision function is used.  
 
3.3. Datasets  
 
Our ordered and disordered sequence is collected 
from the DisProt [3] and PDB [2] database. The 
proteins in DisProt are all with disordered regions. 
The number of ordered regions is very few. If we use 
the SVM with these disordered data, this may cause 
the unbalanced training problem. Hence we also 
collect proteins from the PDB which contains much 
more ordered regions. Those data selected from 
DisProt are taken as positive training data, and the 
negative training data are derived from 
PDB_Select_25 [17] which is a nonredundant dataset 
of the Protein Data Bank (PDB). Finally, 119 protein 
sequences are collected and there are totally 21676 
residues. 
 
3.4. Smoothing Algorithm  
 
 After predicting by SVM model, the result would 
be saved in our program. In Fig. 2, one can easily find 
that some predicted disordered region is not continues. 
According to the disordered regionsâ€™ characteristic, it 
is not possible for only one or two residues to form a 
disordered region except for the edge regions. Hence, 
we develop two smoothing algorithms to recover these 
discontinued regions.  
 
 
 
 
 
Figure 2. The prediction result contains discontinued 
regions which should be smoothed. 
 
Algorithm 1: the disordered-density algorithm 
 Steps: 1. Calculate the neighborsâ€™ disordered 
frequencies of the central node. 2. Divide the above 
frequencies value by the window size. 3. If the 
quotient is great than the threshold, then the central 
node should be set to disordered, and vice versa. 
 The above algorithm is based on the idea that 
discontinued disordered/ordered regions should be 
smoothed. This idea is realized by inspecting the 
neighborhoodâ€™s disordered/ordered residues 
distribution. In other words, by using the sliding 
window with a fixed size, the central residueâ€™s 
disordered/ordered state should reference to its 
neighborsâ€™ disordered/ordered density. If the 
disordered density is greater than the threshold which 
is predefined by the user (default 0.5), the central 
residue of the current window should change its states 
from ordered to disordered, and vice versa. Fig. 3 
shows the smoothing process. Because the candidate 
node in Fig. 3 is 0 (ordered) while all its neighbors are 
1 (disordered), the calculation should be (number of 
disordered) / (window size). In this example, the trend 
score of disorder is 8/9 (win size=9) which is greater 
than 0.5 (default: 0.5). Therefore, the central node 
should be set to 1 (ordered).  
 
 
 
 
 
 
Figure 3. The central node of the sliding window would be 
changed from 0 to 1, because of the vote result within the 
window. (window size=9, number of 1=8, number of 0=1) 
 
Algorithm 2: the disordered-distance algorithm 
 Steps: 1. Calculate disordered nodesâ€™ distance 
value from the central node within the sliding window. 
2. The voting weight of each disordered node is one 
over its distance value. 3. Sum up the voting weights 
in Step 2. If the value is great than the threshold, then 
the central node should be set to disordered, and vice 
versa. 
  
 In the disordered-density algorithm, the residues 
within the windows have rights to vote the central 
residueâ€™s state. When the majority is on the disordered 
side, the central residue should be disordered. 
Nevertheless, it is not fair for the residues near the 
center. Each residue within the window should not 
share a equal voting right. The residue which is farer 
away from the central residue should have a smaller 
voting weight instead of taking the same weight as the 
residues near the central residue. Therefore, this 
algorithm is named disordered-distance algorithm that 
the voting weight is inversely related to its distance to 
the central residue. Fig. 4 gives an example.  
 
 
ïƒ¥
ïƒ¥
ïƒ¥ ïƒ¥
ï€½
ï€½
ï€½ ï€½
ï€«ïƒ—ïƒ—ï€½
ï€½ï€½ï‚³
ïƒ—ï€­
l
i
iii
l
i
iii
l
i
l
ji
jijijii
bxxyxf
yandli
xxyy
1
1
1 1,
2
1
))(sgn()(
0,,...,1,0
)(
ï¡
ï¡ï¡
ï¡ï¡ï¡
Maximize 
 
 
Subject to 
 
 
Decision function 
1111110111111111111111111 
Sliding window: Win size=9 
Protein sequence 
Answer 
Prediction result 
Discontinued 
region 
(2) 
 4.3. Discussion 
 
In the smoothing steps, two algorithms are 
performed with different parameter settings. In the 
first disordered-density smoothing algorithm, we can 
observe that no matter how we change the window 
size, the accuracy is still around 84.7%. It is only 
better than prediction without smoothing 0.1%. Then 
we change the threshold from 0.1 to 0.9, and the result 
is better than only change the window size. The best 
setting of the disordered-density smoothing algorithm 
is that the window size set to 31 and threshold set to 
0.3. However, this is not a satisfied result. Therefore, 
we develop a second smoothing algorithm, disordered-
distance smoothing. 
As the parameter setting with disordered-density 
smoothing algorithm, we changed the window size of 
it in the order of 7, 15, 21, 31and 41. The result shows 
that it works the same as the previous disordered-
distance smoothing algorithm. Nevertheless, when we 
change the threshold value from 0.1 to 0.9, the 
accuracy increased to 85%. The best parameter setting 
of disordered- distance smoothing algorithm is that 
window size set to 21 and threshold set to 0.3. 
However, in the disordered-distance smoothing 
experiment with different window size, we can observe 
that the accuracy of window size 21 and 31 is almost 
the same. Therefore, we integrate the two algorithmâ€™s 
best parameter setting. We found that the best 
parameter setting for these two algorithms is the same 
that window size set to 31 and threshold set to 0.3. 
The Table 5 displays the optimal parameter setting 
experiment results. Specially, the disordered-density 
algorithm improves accuracy from previous 
experiment result of 84.76% to 85.76%. 
After these smoothing steps, we can improve the 
prediction accuracies by 1%. This is benefit to the 
final prediction result. In the disordered protein 
prediction studies, itâ€™s not a easy work to improve the 
prediction result. For instance, the VSL2 predictor of 
DisProt only improves 2.1% accuracy since it uses 22 
additional features derived from the computationally 
expensive PSI-BLAST profiles (PSSM). Thus it can be 
seen that our work profit the prediction result. 
5. Conclusion 
In this paper, we use two different smoothing 
algorithms to improve the prediction accuracy of SVM 
disordered region prediction. The SVM is trained with 
48 features that we combined in our program. The 
accuracy of it is 84.66%. By performing the smoothing 
algorithm, almost 1% accuracy is improved. This 
could be helpful for the discontinued prediction result 
for different prediction models. In addition, the 
smoothing steps can be processed in linear time. No 
additional computationally expensive steps are taken. 
In the future, we will also apply structure features to 
improve the smoothing result. For example, the 
protein secondary structure information within the 
sliding node to smooth the central node. 
Acknowledgement 
This research is supported by the National Science 
Council under grant number NSC 96-2221-E-032-
051-MY2. 
 
References 
[1] C.R. Cantor, Principles of Protein X-Ray 
Crystallography, Springer Verlag, New York, 1996, pp. 1-
341.  
[2] H.M. Berman, J. Westbrook,  Z. Feng, G. Gilliland, 
T.N. Bhat, H. Weissig, I.N. Shindyalov and P.E Bourne, 
â€œThe Protein Data Bank,â€ Nucleic Acids Resource, Vol. 28, 
2000, pp. 235-242. 
[3] S, Vucetic, Z. Obradovic, V. Vacic, P. Radivojac, K. 
Peng, L.M. Iakoucheva, M.S. Cortese, J.D. Lawson, C.J. 
Brown, J.G. Sikes, C.D. Newton, and A.K. Dunker, 
â€œDisProt: A Database of Protein Disorder,â€ Bioinformatics, 
Vol. 21, 2005, pp. 137-140. 
[4] C. Bracken, â€œNMR spin relaxation methods for 
characterization of disorder and folding in proteins,â€ J Mol 
Graph Model, Vol. 19, 2001, pp. 3-12. 
[5] G.D. Fasman, Circular Dichroism and the 
Conformational Analysis of Biomolecules, Plenum Press, 
New York, 1996.  
[6] Protein Structure Prediction Center, 
http://predictioncenter.gc.ucdavis.edu/ (last accessed 
August 3, 2007). 
[7]  C. Bracken, L.M. Iakoucheva, P.R. Romero, and A.K. 
Dunker, â€œCombining prediction, computation and 
experiment for the characterization of protein disorder,â€ 
Curr. Opin. Struct. Biol, Vol. 14, 2004, pp. 570-576. 
[8] Jaehyun Sim, Seung-Yeon Kim and Julian Lee, 
â€œPrediction of protein solvent accessibility using fuzzy k â€“
nearest neighbor method,â€ Bioinformatics, Vol. 21, 2005, 
pp. 2844-2849. 
[9] J. Cheng and P. Baldi, â€œImproved residue contact 
prediction using support vector machines and a large feature 
set,â€ Bioinformatics, Vol. 8, 2007. 
[10] J.L. Gardy, M.R. Laird, F. Chen, S. Rey, C.J. Walsh, 
M. Ester, and F.S.L. Brinkman, â€œPsortb V.2.0: Expanded 
Prediction Of Bacterial Protein Subcellular Localization And 
Insights Gained From Comparative Proteome Analysis,â€ 
Bioinformatics, Vol. 21, 2005, pp. 617-623. 
[11] R.J. Williams, â€œThe conformational mobility of 
proteins and its functional significance,â€ Biochem. Soc. 
 Abstract 
This research intended to use the SVM to predict 
the protein disordered region. Nevertheless, the feature 
set used in this paper is about 440. Both time and 
calculation complexity is enormous for the support 
vector machine (SVM) training and testing. So this 
paper proposed a hybrid feature selection model to 
decrease the dimensionality of feature sets. Filter and 
wrapper feature selection models are combined to 
improve the SVMâ€™s predictability and decrease the 
processing complexity. Three main procedures are 
taken to remove irrelevant features and improve the 
prediction accuracy at the same time. The results show 
that the proposed hybrid mechanism is useful. 
Keywords: Feature Selection, Filter, Wrapper, Support 
Vector Machine, Protein Disordered Region Prediction 
1. Introduction 
Classification in the area of bioinformatics is not an 
easy task. Due to the â€œwideâ€ date set, gene and protein 
classification problem are always performed with 
hundreds or even thousands of features. To improve the 
processing time, the prediction accuracy, and the 
memory usage, feature selection model is needed.  
However, there is always a problem with features 
selection. The fast feature selection model wouldnâ€™t 
provide a good result such as filter model [1]. On other 
hand, good feature selection result doesnâ€™t come fast 
such as wrapper model [1]. This paper aims to improve 
the accuracy of filter model while improve the speed of 
wrapper model. A hybrid feature selection model is 
proposed here to solve the protein disordered region 
problem [2]. 
Disordered protein prediction is a special issue on 
protein structure prediction. Itâ€™s not like the secondary 
protein prediction [3]. Much information should be 
provided for learning machine to make more accurate 
prediction.  
The definition of disordered regions of a protein is 
that some part of a protein could not be fixed in shape. 
According to the central dogma of structural biology, 
the function of a protein is determined by its three-
dimensional structure. In the past views, these regions 
were thought to be useless or harmful while doing the 
protein crystal experiment. Also these regions are 
considered to be no function due to its unfixed shape. 
Even so, recent studies proved that these regions have 
special functions as signal controlling or regulator roles 
and this kind of region is defined as â€œdisordered region.â€  
In this paper, we use the SVM [4] to predict the 
disordered region. However, due to much information 
is applied while processing the prediction, the feature 
set is in a big dimension. This may cause two 
disadvantages. First, the training and testing time 
would be wasted for SVM to calculate these features. 
Second, some additional features would be taken as 
noise while doing the training and testing and this may 
decrease the prediction accuracy. 
The rest of the paper is organized as follows. 
Section 2 introduces the related work. Section 3 
describes the hybrid feature selection model. Section 4 
presents the leaning model SVM and feature set. 
Section 5 lists the experimental results. And finally 
Section 6 draws the final conclusion.  
2. Related work 
In the past researches, feature selection models are 
applied on classification issues in order to select the 
most simplify feature which make classifier more 
accurate and faster. Some specific issues are always 
processed with a large number of features. For instance, 
microarrays [5], transaction logs, and web data are very 
â€œwideâ€ datasets due to their huge amount features.  
In general, according to the result of feature 
selection, two types of feature selection models are 
chose. One kind of research only care about the error 
rate of classification result, they would choose the 
feature extraction model as feature selection procedure, 
such as principal component analysis [6], singular-
value decomposition, manifold learning and factor 
analysis. This kind of feature extraction model would 
transform the current feature space into a different 
feature space. The new feature space may smaller than 
the original one and has better resolution ability, but it 
lose the explain ability from the original feature set.  
A Hybrid Feature Selection Mechanism  
Hui-Huang Hsu, Cheng-Wei Hsieh , Ming-Da Lu 
Dept. of Computer Science and Information Engineering  
Tamkang University 
Taipei, Taiwan 
E-Mail: h_hsu@mail.tku.edu.tw, 892190108@s92.tku.edu.tw, 490191771@s90.tku.edu.tw 
 
 algorithm. Neural networks, basin networks, and SVM 
are often applied on different wrapper issue.  
 
Figure 2. Wrapper model 
 
From above description, we can summarize the 
comparison of filter and wrapper model as Table 1.  
 
Table 1. Comparison between filter and wrapper 
Comparison Filter Wrapper 
Processing time Fast Slow 
Accuracy for classification Depends High 
Rely on data features Yes No 
Depend on learning 
methods 
No Yes 
The processing time of filter model is faster than 
wrapper model, but the classification accuracy of filter 
is not as high as wrapper model. Therefore, in this 
paper we combine the benefit both of filter and 
wrapper model. Next, we will describe the system 
architecture. 
 
3.2 System Architecture 
 Filter model works fast, but itâ€™s not good for our 
classification result. Hence, we can consider it as the 
preprocess procedure. Let it works really as the â€œfilterâ€ 
step to remove part of redundant features. Wrapper 
works slow, but itâ€™s good for the classification result. 
We let it be the post-process procedure to improve the 
classification accuracy. Here we use two different filter 
models as the preprocessing model which contains F-
score, information and mutual information. As for the 
wrapper model, we choose the sequential floating 
search method. Fig.3 is our system architecture. It can 
be departed into three main steps. The first step is 
preliminary screening which contains two filter feature 
selection methods. The second step is feature sets 
combination, and there are several ways to combine. 
For example, intersection, union or weighted model to 
combine the feature sets. The third step is fine tuning. 
It works with the wrapper model to improve the 
classification result. In the following sections, the 
preliminary screening would be presented in section 
3.3. Next, the combination model is described in 
section 3.4. Finally, section 3.5 will introduce how the 
fine tuning works.  
 
Figure 3. System Architecture 
 
3.3. Preliminary screening  
In this paper we use two kinds of feature selection 
methods as the preprocess procedure. F-score and 
information gain both have its benefit and disadvantage. 
Hence we combine their result for the best benefit. 
F-score feature selection method is based on a 
simple concept which measures the discrimination of 
two sets of real numbers. Eq. 1 shows the calculation of 
F-score. 
ğ¹(ğ‘–) â‰¡
 x i
(+)
âˆ’x i  
2
+ x i
(âˆ’)
âˆ’x i  
2
1
n +âˆ’1
  x 
k ,i
(+)
âˆ’x 
i
(+)
 
2
n +
k =1 +
1
nâˆ’âˆ’1
  x 
k ,i
(âˆ’)
âˆ’x 
i
(âˆ’)
 
2
nâˆ’
k =1
     (1) 
 
x i
(+)
, x i
(âˆ’)
 and xi  are the average of the ith feature of 
the positive, negative and whole data sets. n+ and n- are 
the number positive and negative instances. 
x k,i
(+)
and x k,i
(âˆ’)
   are the ith feature of the kth positive instance 
and the ith feature of the kth negative instance. 
F-score is very simple and easy to perform. The 
result of it is also acceptable. However, it only 
measures the discrimination ability of â€œeach featureâ€. 
Without considering the mutual information between 
features, the result of F-score may not be always 
correct. The fig.4 present a situation that two features 
may perform good discrimination ability if they are 
both selected, but in this case, these two features would 
be removed because of the bad discrimination ability of 
each feature.  
 
Figure 4. Two separable classes in the two dimension space 
may not be separated by only observe one featureâ€™s 
distribution. 
+1 
-1 
F-score 
Original feature set 
Combination of feature sets 
Sequential floating search method 
Feature set 1 Feature set 2 
Final feature set 
Information gain 
Feature 
Generation 
Learning 
Algorithm 
Learning 
Algorithm 
Testing 
 
Subset Measurement 
Yes 
Best Subset Classifier 
Training 
data 
Training 
data 
No 
Combination Model 
Fine tuning 
Phase 1 
Phase 2 
Good or Stop 
Training 
data 
Preliminary Screening 
 two lines would be support vectors. Eq.5 demonstrates 
how the SVM processes. 
     
Figure 8. The SVM could find out the maximum margin and use the 
SVs to predict the prediction targets. The line H1 and H2 are located 
on these SVs. (dark circles are SVs). 
 
4.2. Datasets  
We use the protein disordered and ordered data to 
test our feature selection model. Because in general 
database, there is insufficient disordered proteins data 
sets for us to train. Hence, we choose the disordered 
protein data set from DisProt [12] database which 
collect more than 500 disordered proteins and use the 
data set from Protein Data Bank (PDB) [13] database 
as ordered protein sets. Taking data sets from the both 
database may avoid the unbalanced training problem. 
Finally, 119 protein sequences are collected and there 
are totally 21676 residues. 
 
4.3. Feature sets 
In this paper, we use three types of protein features 
which contain (1) position-specific scoring matrix 
(PSSM) [14] scoring matrix value, (2) statistic analysis 
on current sliding window of protein sequence, and (3) 
side chain properties of each amino acid in the protein 
sequence.  
The PSSM scoring matrix is calculated by PSI-
BLAST tool automatically, a sequence with length of 
N may map to a size of N x 20 matrix which contain 
the conserving information of current proteinâ€™s family. 
Therefore, we take the result of PSSM for each node 
with 20 features which are the scores of 20 kinds of 
amino acids. 
Next, we calculate each amino acids frequency 
within a predefined window size. This information 
gives the distribution of amino acids within a positionâ€™s 
neighborhood. There are totally 20 kinds of amino 
acids of a protein. Therefore the number of statistic 
features is 20. 
Finally, each amino acids side chain properties 
should also be considered. In this paper, we collect 8 
amino acidsâ€™ side chain properties. They are aliphatic, 
tiny, small, aromatic, hydropathy index (Kyte-
Doolittle), polar, charged, and hydrophobic. Hence, 
there are totally 440 protein features which are PSSM 
(20 values x15 window size=300), statistic values (20), 
and side chain properties (8 x 15 window size=120). 
5. Experimental results  
5.1. Results 
 In the first preliminary screening procedure, we test 
the two kinds of filter model, F-score and information 
gain. The result is list in Table 3.  
 
Table 3. Result of preliminary screening procedure 
Method Threshold 
Removed 
dimension 
Final 
Dimension 
Accuracy 
(5-fold 
cross 
validation) 
None  - - 440 82.75% 
F-score 0.0001 152 288 82.12% 
F-score 0.0002 220 220 81.58% 
IG 0.01 120 320 82.87% 
IG 0.02 291 149 80.66% 
  
In this procedure, the threshold setting is resolved 
from a grid process. We can observe that the original 
prediction accuracy is almost equal to the result of F-
score (threshold: 0.0001) and IG (threshold: 0.001) 
with removed features of 152 and 120, respectively. To 
remove these features, the core of wrapper may 
perform faster. In this procedure, the preliminary 
screening just works as a feature filter to remove the 
additional features and also keeps the prediction 
accuracy. 
 Next, the feature sets generated by previous 
procedure need to mix together. Table 4 shows the 
selected feature relationship between the F-score and 
IG. 
 
Table 4. Feature set comparison between F-score 
and IG 
Relationship Amount 
Total feature set 440 
F-scoreâˆªIG 355 
F-scoreâˆ©IG 253 
(F-scoreâˆªIG)- (F-scoreâˆ©IG) 102 
 
From Table 4, the combination procedure performs 
as following. There are 253 features (F-scoreâˆ© IG) 
which would not be removed by the next wrapper 
process, and only 102 feature ((F-scoreâˆªIG)- (F-score
âˆ© IG)) would be considered in next fine tuning 
procedure. This may greatly decrease the wrapperâ€™s 
process time, and limit the testing feature. In the end, 
the wrapper will work faster. 
Maximize 
 
Subject to 
 
Decision function 
ïƒ¥
ïƒ¥
ïƒ¥ ïƒ¥
ï€½
ï€½
ï€½ ï€½
ï€«ïƒ—ïƒ—ï€½
ï€½ï€½ï‚³
ïƒ—ï€­
l
i
iii
l
i
iii
l
i
l
ji
jijijii
bxxyxf
yandli
xxyy
1
1
1 1,
2
1
))(sgn()(
0,,...,1,0
)(
ï¡
ï¡ï¡
ï¡ï¡ï¡
H1 
H2 
(5) 
1 
 
FEATURE SELECTION FOR IDENTIFYING PROTEIN DISORDERED 
REGIONS  
 
Hui-Huang Hsu
*
 and Cheng-Wei Hsieh  
Dept. of Computer Science and Information Engineering, Tamkang University 
Taipei, 25137, Taiwan 
*
h_hsu@mail.tku.edu.tw, 892190108@s92.tku.edu.tw 
 
ABSTRACT 
Determining the structure of a protein is not an easy task which usually involved a time-consuming and costly process in the 
web lab. Using computational methods to predict a proteinâ€™s tertiary structure from its primary structure (the amino acid 
sequence) is desirable. Disordered regions are segments of a protein that do not have a fixed conformation, which makes the 
structure prediction much harder. Also, these disordered regions are functionally important for a protein. In this research, we 
would like to identify such regions with a focus on selecting a proper feature set. Three feature selection methods, namely F-
score, information gain, and K-medoids clustering, are used for feature selection. The support vector machine is then used for 
classification. The results show that the classification accuracy can be raised with a smaller feature set. The K-medoids 
clustering feature selection can reduce the number of features from 440 to 150 and improve the accuracy from 84.66% to 
86.81% in five-fold cross validation. It also has a more stable performance than F-score and information gain.  
 
Keywords: Disordered protein region; K-medoids clustering; Feature selection; Proteomics. 
 
 
INTRODUCTION 
In proteomics, a proteinsâ€™ function is always related to 
its structure. This is the central dogma of structural 
biology. The definition is that the function of a protein 
is determined by its three-dimensional structure. A 
protein is composed of 20 kinds of amino acids. Each 
amino acid has different attributes such as aliphatic, 
tiny, small, aromatic, hydropathy index, polar, charged, 
hydrophobic, and etc. Because of these different 
attributes, the structure of a protein has a distinct 
conformation. For example, fully folded, collapsed, and 
extended.  
However, in some special cases, some parts of a 
protein may not have a fix conformation. That means 
some parts of a protein fail to self-fold into a fixed 3D 
globular structure. These parts are called disorder 
regions. In traditional view, these regions do not have 
any function with it. Nevertheless, lots of new 
evidences show that there indeed exist important 
characteristics in these regions. For example, the signal 
controlling or regulating function is usually found in 
these regions. Since there is not a fixed conformation of 
disordered regions, these parts can work as the â€œhubâ€ 
for their flexible structure characteristics.  
Disordered regions often contain short linear peptide 
motifs, and these regions cause disordered proteins 
partially or fully unstructured. Moreover, various major 
protein conformational diseases are caused by 
disordered proteins such as synuclein, tau protein, and 
prion protein.  
The protein disordered regions have been collected 
and stored in several online databases. The protein 
structure database, protein data bank1 (PDB), records 
these unstable regions in its conformation files as 
REMARK-465 which contains the position and the 
length of each amino acid segment. The database of 
protein disorder regions, DisProt2, also collected more 
than 400 disordered proteins and about 1000 disordered 
regions.  
Various instruments are used to identify the 
disordered regions, such as nuclear magnetic resonance 
(NMR) spectroscopy3, X-ray crystallography3 and 
circular dichroism5. Nevertheless, through these wet-
lab experiments, it takes a lot time, money and 
manpower.  Thus it is desirable to use a computational 
method to identify the disorder regions of a protein. 
The current trend is to use machine learning 
technologies to discover the disordered regions of 
proteins. The most used models are neural networks, 
Bayesian networks, and support vector machines 
(SVMs)6,7. The inputs to these machine learning 
models are the attributes of the amino acid sequence of 
a protein and the output is the binary classification of 
disordered regions. With proper training, these 
machine learning models could predict the disordered 
regions. However, from previous researches, some 
limitations of disordered regions prediction were found 
while using these machine learning models. First, 
various forms of disordered regions cause the prediction 
rate not as high as expected. Next, the number of 
3 
 
METHODS 
Disordered Protein Feature Collection 
For disordered protein prediction, this research 
collected three kinds of attributes. The first one is 
protein sequence composition. As we know the protein 
is composed by amino acids. For 20 kinds of different 
amino acids, their properties are totally different. Some 
of them would construct a strong linkage when they 
meet each other. Some of them would stay away from 
water due to their hydrophobic property. Hence, the 
composition of amino acids has a strong relationship 
with their protein structure and even the disorder 
region can be affected by it. Therefore, the target 
proteinsâ€™ composition should be analyzed. Here, we 
calculate the target proteinsâ€™ occurrence frequencies in 
a predefined window. While the target proteinsâ€™ 
primary structure information is input, the 20 kinds of 
amino acidsâ€™ frequencies should be calculated as the 
first 20 attributes. 
Frequencies of amino acids might satisfy protein 
secondary structure prediction, but are not enough for 
disordered proteinsâ€™ prediction. Because the frequencies 
features only provide the ensemble information, the 
individual position information is not concerned. 
Consequently, the position data also have to be added 
in the prediction program. However, only one target 
proteinâ€™s position information is not sufficient. Since 
proteins would mutate in the natural environment, and 
we can find several close family proteins which have 
similar region conformation. These same family 
proteins may have similar structure and functions. If all 
proteins of the same family can be collected and an 
ensemble analysis is performed, the position 
information is much more useful than only one 
proteinâ€™s position information.  
Nevertheless, to describe a set of proteinsâ€™ position is 
not an easy work. In this paper, we use the position-
specific scoring matrix (PSSM) 17 which is performed 
by several steps. Firstly, by querying the target protein 
against the selected database, the closest family proteins 
would be found. Next, multiple sequence alignment is 
performed and the position of the selected protein set 
can be calculated. The PSSM also calculates the log-
likelihoods of the substring under a product 
multinomial distribution which is very useful for 
disordered protein prediction. Finally, we obtain a 
PSSM matrix data with a protein sequence. In this 
matrix, each inputted amino acid retrieves 20 values, 
and this is the second kind of attributes in our data, 20 
PSSM attributes. 
According to the past disordered region studies9, 
only sequenceâ€™s physical information is not sufficient. 
Some specific side chain properties of an amino acid 
should also be included in the prediction system. From 
previous studies, amino acidsâ€™ side chain properties 
would really affect a proteinâ€™s conformation. In this 
paper, several amino acidsâ€™ side chain properties are 
listed. They are aliphatic, tiny, small, aromatic, 
hydropathy index (Kyte-Doolittle), polar, charged, and 
hydrophobic, and this is the last 8 attributes included in 
our prediction data.  
Fig. 1 shows the 48 attributes for feature collection. 
Among them, PSSM and side chain properties are for a 
single amino acid, i.e., a position in a protein sequence.  
To include the sequence information, we use a sliding 
window technique to monitor the relationship of 
current amino acid and its neighbors. Here, a sliding 
window with a size of 15 is used and information of 
neighboring seven amino acids of the current amino 
acid on both sides is encompassed. PSSM and side 
chain properties of the 15 amino acids in the window 
are the features to be used, which results in (20+8) Ã—15 
= 420 features. The statistical features are originally 
analyzed for a segment of the protein sequence, so it 
does not need to be multiplied by the window size 15. 
Totally, 440 features of one amino acid position in the 
sequence are used in our experiments. 
 
 
Fig. 1  48 Collected attributes 
 
Feature Selection  
Feature selection is a major technique to improve the 
classification efficiency. To select the most class-related 
features is not an easy work. Sometimes feature 
selection is applied on different researches besides 
classification, for example, gene selection18. For feature 
selection, there are two kinds of feature selection 
models, filters and wrappers19. These feature selection 
models are widely applied to different problems. For 
those applications with a large feature set that need to 
lower the feature dimensionality quickly, the filters are 
the most suitable model.  From the viewpoint of the 
information theory, the information of a set of features 
could be calculated by various statistical methods, and 
that is the core of the filter type of feature selection 
methods. Because of the fast calculation, filters are 
often applied to high dimensional feature selection. As 
shown in Fig. 2, the filters have three main stages: 
5 
 
exclusive clustering, and that means each feature can 
only belong to one cluster. 
There are many kinds of clustering algorithms. For 
example, hierarchical clustering, fuzzy C-means, 
mixture of Gaussians, K-means, K-medoids and etcâ€¦ 
We chose the K-medoids clustering method to perform 
feature selection. It works in the following steps.  
(i) Randomly select k of the n data points as the 
medoids. 
(ii) Associate each data point to the closest medoid. 
(iii) For each medoid m and non-mediod data point o. 
Swap m and o and compute the total cost of the 
configuration.  
(iv) Select the configuration with the lowest cost.  
(v) Repeat Steps (ii) to (v) until there is no change in 
the medoids. 
(vi) Remove all the non-medoid data points. The 
remained features are the final selected features.  
Classification  
After the previous feature collection and feature 
selection, a machine learning model is used here for the 
classification of protein disordered regions. Here, we 
apply the support vector machine as the learning model 
for classification. The SVM is based on the SV (support 
vector) learning. That means the SVM would not 
always compare the prediction target to all the existing 
training examples. In contrast, the SVM selects several 
training examples as its SVs, and use these SVs to 
judge the label of the classification target. 
In the testing stage, the SVM model would use the 
SVs to do the classification. And also, these SVs would 
locate near the hyper-plane that causes the maximum 
margin of separation. The SVM has been rated an 
excellent classifier in practical applications. The SVM 
can handle more complex nonlinear problems through 
kernel transformation. Fig. 3 demonstrates the 
maximum margin between two classes which are 
separated by the hyperplane in the SVM model. The H1 
and H2 are the boundaries. And the nodes which are 
located on these two boundary lines are support vectors. 
Eq.5 demonstrates how the SVM processes. 
 
  
Fig. 3  The SVM could find out the maximum margin 
and use the SVs to predict the prediction targets. The 
line H1 and H2 are located on these SVs. 
 
 
EXPERIMENTAL RESULTS 
In this paper, we perform a standard procedure for 
recognizing the protein disordered regions. At first, we 
collect three kinds of feature sets, and they are statistics 
of 20 amino acids, the PSSM matrix, and the side chain 
properties. Then, we perform three different feature 
selection procedures to reduce the number of features. 
Next, a SVM learning process is performed to test the 
classification results.  
For feature selection, F-score, information gain, and 
K-medoids clustering feature selection are performed. 
Table 1 presents the best feature selection result (five-
fold cross validation) of these three kinds of models. 
The K-medoidsâ€™ best result is 86.81% with 150 features. 
While F-score and IG reach their best results, there are 
still 400 remaining features. For the purpose of raising 
prediction accuracy while removing redundant features, 
both F-score and IG did not do a good job as expected.  
 
Table 1. The feature selection results of 440 features. 
(The prediction accuracy of all 440 features is 84.66%) 
Method Accuracy 
(5-fold cross 
validation) 
Number of features 
F-score 82.38% 400 
IG 83.53% 400 
K-medoids 86.81% 150 
 
Because filter mode feature selection would not 
guarantee its classification accuracy, and its prediction 
result is also not stable. Occasionally filter mode feature 
selection methods could get a high accuracy while the 
other time gets not so satisfied results. In Table 2, we 
made a detail analysis of these three feature selection 
models. We compare their self training/testing results 
in number of features, accuracy, recall (sensitivity), 
precision, and specificity in Table 2. The definitions of 
them are in Eq.6.  
Accuracy = (TP + TN) / (P + N) 
Recall (Sensitivity) = TP / P 
Precision = TP / (TP + FP) 
ïƒ¥
ïƒ¥
ïƒ¥ ïƒ¥
ï€½
ï€½
ï€½ ï€½
ï€«ïƒ—ïƒ—ï€½
ï€½ï€½ï‚³
ïƒ—ï€­
l
i
iii
l
i
iii
l
i
l
ji
jijijii
bxxyxf
yandli
xxyy
1
1
1 1,
2
1
))(sgn()(
0,,...,1,0
)(
ï¡
ï¡ï¡
ï¡ï¡ï¡
Maximize 
 
 
Subject to 
 
 
Decision function 
(5) 
H2 H1 
(6) 
7 
 
12. Hopp TP, and Woods KR, A computer program for 
predicting protein antigenic determinants, Mol Immunol 
20:483-448,1983. 
13. Kyte J and Doolittle RF, A simple method for displaying 
the hydropathic character of a protein, J Mol Biol 
157:105-132, 1982. 
14. Yang ZR, Thomson R, McNeil P, Esnouf R, RONN: the 
bio-basis function neural network technique applied to 
the detection of natively disordered regions in proteins, 
Bioinformatics 21:3369-3376, 2005.  
15. Peng K, Radovojac P, Vucetic S, Dunker AK and 
Obradovic Z, Length-dependent prediction of protein 
intrinsic disorder, Bioinformatics 7, 2006. 
16. Huang CL, Chen EL, Chung PC, Fall detection using 
modular neural networks with back-projected optical flow, 
BME 19:415-424, 2007. 
17. Altschul SF, Gish W, Miller W, Myers EW, and Lipman 
DJ, Basic local alignment search tool, J. Mol. Biol. 
215:403-410, 1990. 
18. Chuang LY, Yang CS, Li JC, Yang CH, Combat GA-
based gene selection for classification of microarray data, 
BME 20:345-352, 2008. 
19. Kohavi R, John G, Wrappers for Feature Subset Selection, 
Artificial Intelligence 97:273-324, 1997.  
20. Chen X, Peng H, Hu J, K-medoids substitution clustering 
method and a new clustering validity index method, 
Intelligent Control and Automation 2:5896-5900, 2006. 
 
(features from protein primary structure). As for the gene selection of microarray cancer data, dealing with its 
thousands of features is essential. To identify the main disease genes from thousands of other regular genes in the 
microarray data, effective feature selection is always very helpful.  
In the remainder of the paper, related work is first discussed in Section 2. The proposed hybrid feature selection 
mechanism is then delineated in Section 3. The learning model and datasets are introduced in Section 4. The 
experimental results are presented in Section 5. Finally, a brief conclusion is drawn in Section 6.  
2. Related work  
Feature selection methods have been applied to classification problems in order to select a reduced feature set that 
makes the classifier more accurate and faster. Some specific problems are always processed with a great number of 
features. For instance, microarrays, transaction logs, and web data are all very wide datasets with a huge amount of 
features. Here we first review papers about the filters and the wrappers.  
Huang et al. (2006) used a filter approach for feature selection based on mutual information. In their point of view, 
there are two types of input features perceived as being unnecessary. They are features completely irrelevant to the 
output classes and features redundant given other input features. By using the mutual information test on features vs. 
classes and features vs. features, feature selection can be done. This is from the concept of information theorem which 
analyzes the relationship between features and classes to remove the most related (redundant) features or the most 
irrelevant to the class. In their research, a greedy feature selection algorithm was proposed.  
Another filter work was done by Deisy et al. (2006). They used the analysis of symmetrical uncertainty with 
information gain. By calculating the difference between the entropy of the whole class and the features, features with 
less information can easily be identified. In addition, some other feature selection methods are based on the featuresâ€™ 
discrimination ability. For example, Chen and Lin (2003) used F-score to perform feature selection. In their work, the 
support vector machine (SVM) was used as the feature set performance measurement. The F-score analyzes the 
decimation ability of each feature. Owing to the SVM also tries to find a separation hyper-plane to divide different 
classesâ€™ data apart, the F-score may helpful for SVM to remove some features of low decimation ability. 
Backstrom and Caruana (2006) presented an internal wrapper feature selection method for cascade correlation. The 
internal wrapper feature selection method selects features while hidden units are being added to the growing cascade 
correlation network architecture. Liu et al. (2008) developed a wrapper-based optimized SVM model for demand 
forecasting. At first, wrappers based on the genetic algorithm are employed to analyze the sales data of a product. 
Then the selection result is applied to build a SVM regression model. 
Next, two bioinformatics problems were tested in this research. One of the major inventions in biology is the 
microarray technique. Disease classification based on gene expression data from microarray is very important. This 
topic is also related to feature selection. Classification without feature selection would certainly affect both the 
processing time and the classification accuracy. When the microarray cancer data classification is performed, genes 
related to this particular cancer can also be identified through feature selection. Each gene on the microarray chip is 
considered as a unique feature. 
For the gene selection problem, the goal is to select a few important genes from thousands of genes. Thus, feature 
selection would be an essential step. Vapnik et al. (2002) applied the SVM to investigate the gene selection problem, 
measurement would be performed again. Hence, the final feature set would contain the most informative features. 
Finally, the testing step is proceeded by a learning algorithm, like SVMs or neural networks (NN).  The result 
includes the testing result of the selected features. 
Fig.2 presents the working procedure of wrappers. It is the same as that of the filters except that the measurement 
stage is replaced by a learning algorithm. And this is the main reason that the wrappers always perform slowly. On the 
other hand, owing to the learning algorithm, the wrappers could achieve better feature selection results in most cases. 
For the stopping criterion, when the result starts to get worse or the number of features reaches a predefined threshold, 
the procedure stops. 
Table 1    
The comparison of the filters and the wrappers 
Items The filters The wrappers 
Processing speed fast slow 
Classification accuracy depends high 
Depend on learning methods no yes 
Table 1 lists the pros and cons of the filters and the wrappers. The filters process quickly, but their results are not 
always acceptable. The wrappers have high classification accuracy, but process slowly. In addition, the filters calculate 
the information from features; therefore, its feature selection results will depend on the measured information of the 
features. The wrappers use the learning algorithm for the judgment; hence, their classification result is biased by the 
learning algorithm. 
In this study, we propose a new feature selection mechanism which utilizes the advantages of both filters and 
wrappers. By combining the filters and the wrappers, we not only can improve the classification accuracy of pure 
filters, but also decrease the processing time of pure wrappers. 
3.2. Hybrid feature selection 
Fig.3 shows the hybrid feature selection procedure. Two filter models were chosen as the preliminary screening 
to remove the most redundant or irrelevant features. F-score and information gain are the core of preliminary 
screening. These two resulted feature sets are combined together as the preprocessed feature set for fine tuning. This 
step is called the combination model. Finally, the wrapper model is applied to improve the classification accuracy, and 
this is the fine-tuning step. The following subsections describe these three critical steps in detail. 
3.2.1. Preliminary screening 
In the first step, we chose F-score and information gain to remove redundant and irrelevant features. F-score is a 
novel filter model which calculates the discriminative ability of each feature. That is to say, features with higher 
F-score have better separation ability in classification problems. F-score is defined in the following equation. 
3.2.2. Combination  
When the preliminary screening procedure is completed, two feature subsets are selected by F-score and IG, 
respectively. These features are considered as the most class-related features from all features. Putting all of above 
features together as the final feature set may not be a wise decision. Not only the training or testing procedure of the 
learning model would take a lot of time, but also the classification accuracy might not be good. The key here is to 
effectively combine the two feature subsets. To reduce redundant tests in the fine-tuning step, we divide the union of 
F-score and IGâ€™s feature sets into two parts: intersection (AND) and exclusive-OR (XOR). Fig.4 illustrates these two 
parts.  
Feature sets 1 and 2 are selected by F-score and IG, respectively.  The intersection part of feature sets 1 and 2 is 
recommended by both F-score and IG and the features might be conserved in the final feature set. As for the 
exclusive-OR part of feature sets 1 and 2, some of the features might be valuable and should be included. Thus, a 
fine-tuning step was designed to further test these selected features in both interaction and exclusive-OR parts. The 
wrapper procedure with a machine learning algorithm would further examine the features starting from the intersection 
part to the exclusive-OR part. The worst fine-tuning result, in respect to feature reduction, is the union of feature sets 1 
and 2 (the maximum number of features from the preliminary screening procedure). 
3.2.3. Fine tuning  
In the previous preliminary screening and combination steps, most redundant and irrelevant features are removed 
and useful features are kept for the next fine tuning stage. In this stage, we try to take advantage of the wrapper kind of 
feature selection, and that is to use a searching algorithm and a machine learning model to select a feature set that can 
result in higher classification accuracy. 
The wrappers are not suitable for wide feature set with thousands of features. Owing to the previous feature 
reduction procedure, the wrappers can be applied now with less computational effort. The sequential floating search 
method (SFSM) (Pudil et al., 1994) is modified to fit the fine-tuning procedure, which can avoid the nesting effect 
caused by using only sequential forward or backward search. The working flow of the SFSM is reversed with the 
sequential backward search (SBS) performed before the sequential forward search (SFS). The SFSM usually starts 
from an empty feature set, but our mechanism starts from the intersection part which already includes a set of 
important features. 
In Fig.5, the SBS starts with the whole feature set and removes one feature at one time, and then a learning 
model would be applied to test its result. It will be performed repeatedly until the stopping criterion is reached. The 
procedure stops when the test result starts to get worse or the number of features reaches a predefined threshold. Fig.6 
is the SFS which starts from an empty set and adds one feature at a time. For the stopping criterion, when the test 
result starts to get worse, the procedure stops. 
The original concept of the SFSM is to use the SFS first, which add and test features one by one. When the 
learning modelâ€™s classification accuracy is decreased, the SFS stops and the SBS is then executed to reduce the 
number of features. These steps will perform repeatedly until the stopping criterion is reached. In Fig.7, the inverse 
SFSM (iSFSM) is performed as the following steps: (1) The SBS is executed on the intersection part of the 
within a predefined window. This information gives the distribution of amino acids within a positionâ€™s neighborhood. 
Therefore the number of statistical features is 20 corresponding to 20 amino acids. Finally, side chain properties of 
each amino acid were also considered. We collected eight side chain properties. They are aliphatic, tiny, small, 
aromatic, hydropathy index (Kyte-Doolittle), polar, charged, and hydrophobic. Hence, there are totally 440 protein 
features with a window size of 15. They are PSSM (20 values Ã— 15 window size=300), statistical values (20), and side 
chain properties (8 Ã— 15 window size=120). 
4.3. Microarray cancer datasets 
In this research, we also tried the microarray cancer data classification problem. We used the AML & ALL 
(leukemia) dataset and the Lung cancer dataset. These datasets were downloaded from the Kent Ridge Bio-medical 
Data Set Repository which stores both experimental values and the gene names.  
In total, there are 72 samples in the AML & ALL dataset, each with 7,129 features (genes). Forty-seven of them 
are ALL data, and 25 are AML data. In the Lung Cancer dataset, there are 181 samples; each with 12,533 features 
(genes). Thirty-one of them are of MPM, and the other 150 samples are of ADCA. 
5. Experimental results 
5.1. Disordered protein prediction 
In the preliminary screening procedure, F-score and information gain (IG) are used to filter the features. The 
results are listed in Table 3. In this procedure, the threshold setting is resolved with a greedy process. We can observe 
that the accuracies for the two reduced feature sets (82.12% and 82.87%) maintained at the same level as when the 
original feature set was used (82.75%). Moreover, the number of features was reduced from 440 to 288 and 320, 
respectively. With removal of the features, the core of wrappers for fine tuning can perform much faster. Next, the 
feature sets generated in preliminary screening were combined. Table 4 shows the numbers of features with different 
combinations. 
Table 3    
Preliminary screening on disordered protein data 
Method Threshold 
Removed  
features 
Final  
features 
Accuracy 
(5-fold cross validation) 
None - - 440 82.75% 
F-score 0.0001 152 288 82.12% 
IG 0.01 120 320 82.87% 
 In Table 5, the threshold setting is resolved from a greedy process. Originally, the classification accuracy of 
AML & ALL and Lung Cancer datasets were 68.06% and 86.74%, respectively. After the preliminary screening 
procedure on F-score and IG, the AML & ALL feature set was reduced from 7,129 to 873 and 1,510, and the 
prediction accuracy was improved to 98.61%. As for the Lung Cancer dataset, F-score and IG reduced the features 
from 12,533 to 996 and 1,571, and also the accuracy was raised to 99.45%.  
Table 6 shows the combinations of features with the preliminary F-score and IG filtering. From Table 6, the 
AML & ALL dataset retained totally 2,107 (276 + 1,831) features for further examination by iSFSM. For the Lung 
cancer dataset, 2,241 (326 + 1,915) features were left. Next, we list the results after fine tuning in Table 7. The number 
of features reduced to 70 for both AML & ALL and Lung Cancer datasets and the classification accuracy was further 
improved to 98.61% and 100%, respectively.  For both cancer datasets, 70 genes (features) were picked as most 
related to the particular disease. This information is very useful in medicine.  
 
Table 6     
Combinations of microarray cancer data after screening 
Dataset Relationship Number of features 
AML & ALL 
Total feature set 7,129 
F-score âˆ© IG 276 
F-score XOR IG 1,831 
Lung cancer 
Total feature set 12,533 
F-score âˆ© IG 326 
F-score XOR IG 1,915 
 
Table 7    
Prediction accuracy after fine tuning 
Dataset Relationship Number of features 
Accuracy 
(5-fold cross validation) 
AML & ALL 
F-score âˆ© IG 276 98.61% 
Best feature set 70 98.61% 
Lung cancer 
F-score âˆ© IG 326 99.45% 
Best feature set 70 100% 
 
Acknowledgement 
This research was partially supported by the research grant NSC# 96-2221-E-032-051-MY2. 
References 
Altschul, S. F., Gish, W., Miller, W., Myers, E.W., & Lipman, D.J. (1990). Basic local alignment search tool, J. Mol. 
Biol., 215(3), 403-410.  
Backstrom, L., & Caruana, R. (2006). C2FS: An Algorithm for Feature Selection in Cascade Neural Networks, IEEE 
International Joint Conference on Neural Networks. Vancouver, BC, Canada (pp.4748-4753). 
Berman, H.M., Westbrook, J., Feng, Z., Gilliland, G., Bhat, T.N., Weissig, H., Shindyalov, I.N., & Bourne, P.E.  
(2000). The Protein Data Bank, Nucleic Acids Resource, 28, 235-242. 
Chen, C., & Lin, J. (2003). Libsvm: A library for support vector machines. Available from: 
<http://www.csie.ntu.edu.tw/~cjlin/libsvm>. 
Cho, S., & Ryu, J. (2002). Classifying gene expression data of cancer using classifier ensemble with mutually 
exclusive features, Proceedings of The IEEE, 90(11), 1744-1753. 
Cho, S., & Won, H. (2007). Cancer classification using ensemble of neural networks with multiple significant gene 
subsets, Applied Intelligence, 26(3), 243-250. 
Deisy, C., Subbulakshmi, B., Baskar, S., & Ramaraj, N. (2007). Efficient Dimensionality Reduction Approaches for 
Feature Selection, International Conference on Computational Intelligence and Multimedia Applications, 
Sivakasi, India (pp.121- 127). 
Fujibuchi, W., & Kato, T. (2007). Classification of heterogeneous microarray data by maximum entropy kernel, BMC 
Bioinformatics, 8, 267-277.  
Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene Selection for Cancer Classification using Support 
Vector Machines, Machine Learning, 46(1-3), 389-422. 
Huang, Jinjie, Cai, Yunze, Xu, & Xiaoming (2006). A Filter Approach to Feature Selection Based on Mutual 
Information, Proc. of the Firth IEEE International Conference on Cognitive Informatics, Beijing, China (pp.84 
-89). 
Ishida, T., & Kinoshita, K. (2007). PrDOS: prediction of disordered protein regions from amino acid sequence, 
Nucleic Acids Research, 35, 460-464. 
Kohavi, R., John, & G. (1997). Wrappers for Feature Subset Selection, Artificial Intelligence, 97, 273-324.  
Linding, R., Jensen, L.J., Diella, F., Bork, P., Gibson, T.J., & Russell, R.B. (2003). Protein disorder prediction: 
implications for structural proteomics, Structure, 11(11), 1453-1459.  
Liu, H., Dougherty, E.R., Dy, J.G., Torkkola, K., Tuv, E., Peng, H., Ding, C., Long, F., Berens, M., & Parsons, L. 
(2005). Evolving feature selection, Intelligent Systems, IEEE, 20(6), 64-76. 
Liu, Yue, Yin, Yafeng, Gao, Junjun, & Tan, Chongli (2008). Wrapper Feature Selection Optimized SVM Model for 
Demand Forecasting, The International Conference on Young Computer Scientists, Hunan, China (pp.953-958). 
Original 
features 
Feature set 
generation 
Stopping 
criterion 
Tested by learning 
algorithm Yes 
No 
Measurement 
Selected 
features 
 
Fig.1 The filters 
 
Original 
features 
Feature set 
generation 
Stopping 
criterion 
Tested by learning 
algorithm Yes 
No 
Learning 
Algorithm 
Selected 
features 
 
Fig.2 The wrappers 
  
Fig.5 Sequential backward searching (SBS) 
 
 
Start 
Add a feature to current feature set 
Stopping 
Criterion Stop 
Remove a feature from current feature set
Stopping 
Criterion Stop 
Start 
 
Fig.6 Sequential forward searching (SFS) 
 
SBS 
SFS 
Stopping 
Criterion 
No
Start
Stop
Yes
 
Fig.7 Inverse sequential floating search method (iSFSM) 
å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
è¨ˆç•«ç·¨è™Ÿ NSC 96-2221-E032-051-MY2 
è¨ˆç•«åç¨± ï§ç”¨å¤šé‡æ ¸å¿ƒä¹‹æ”¯æ´å‘ï¥¾æ©Ÿæ–¼è›‹ç™½è³ªéç©©å®šå€æ®µçš„é æ¸¬ (2/2) 
å‡ºåœ‹äººå“¡å§“å 
æœå‹™æ©Ÿé—œåŠè·ç¨± 
è¨±è¼ç…Œ æ·¡æ±Ÿå¤§å­¸è³‡è¨Šå·¥ç¨‹ç³»å‰¯æ•™æˆ 
æœƒè­°æ™‚é–“åœ°é» 2009/3/16~19 æ—¥æœ¬ï¨›å²¡ 
æœƒè­°åç¨± Third International Conference on Complex, Intelligent and Software Intensive Systems (CISIS 2009) 
ç™¼è¡¨ï¥æ–‡é¡Œç›® Outlier Filtering for Identification of Gene Regulations in Microarray Time-Series Data
ä¸€ã€ï¥«åŠ æœƒè­°ç¶“é 
CISIS ç ”è¨æœƒæ˜¯ç¬¬ä¸‰æ¬¡èˆ‰è¾¦ï¼Œå…¶ä¸»é¡Œæ¶µè“‹è¤‡é›œã€æ™ºæ…§å‹ã€è»Ÿé«”å¯†é›†ç³»çµ±çš„ç›¸é—œèª²é¡Œï¼Œæ˜¯
æ­æ´²çš„ DEXA å­¸æœƒ (Society for Database and Expert Systems Applications) æ‰€èªå¯çš„ç ”è¨æœƒï¼Œ
æœ¬æ¬¡å’Œå‰ï¥¸æ¬¡ä¸€æ¨£å’Œ ARES ç ”è¨æœƒå…±åŒèˆ‰è¾¦ï¼Œä¸¦æœ‰ 13 å€‹å·¥ä½œåŠ (workshops) ï¥«èˆ‡ ã€‚ç¸½è¨ˆæœ‰
ä¸‰åå€‹åœ‹å®¶ä»¥ä¸Šã€å››ç™¾é¤˜äººèˆ‡æœƒã€‚å€‹äººèˆ‡ä¸»è¾¦çš„æ—¥æœ¬å’Œå¥§åœ°ï§æ•™æˆç†Ÿï§¼ï¼Œé™¤ï¦ºå”åŠ©ç±Œè¾¦ä¸€å€‹
å·¥ä½œåŠï¼Œä¸¦æ“”ä»»æœ¬å±†çš„è­°ç¨‹ä¸»å¸­ã€‚ 
æœ¬æ¬¡å¤§æœƒåœ¨ 3/18 ä¸€æ—©å®‰æ’ï¦ºä¸€å ´çš„ keynote speechï¼Œç”±å¤§é˜ªå¤§å­¸ Shojiro Nishio æ•™æˆä¸»è¬›
CSTP Coordination Program of Science and Technology Projects in Japan: Very Large Information 
Integration and Application Platformï¼Œæ—¥æœ¬ç‚ºï¦ºæ‡‰ä»˜è¶…å¤§ï¥¾è³‡ï¦¾çš„è™•ï§¤å’Œæœå‹™æ‰€ä½œçš„å¤§ï¤Šé¡ã€
è·¨éƒ¨æœƒçš„å¤§å‹æ•´åˆè¨ˆç•«ï¼Œï¦¨äººå°è±¡æ·±åˆ»ã€‚å¦å¤–é‚„æœ‰ç¸½å…± 52 å€‹å ´æ¬¡ (sessions) çš„ï¥æ–‡ç™¼è¡¨ã€‚ä¸‰
å¤©ä¸‹ï¤­ï¼Œå€‹äººï¥«åŠ ï¦ºè¨±å¤šå ´æ¬¡çš„ï¥æ–‡å ±å‘Šï¼Œè®“æˆ‘åœ¨å¤šå€‹ä¸»é¡Œä¸Šç²ï¨—ï¥¼å¤šï¼Œä¹Ÿï¦ºè§£ï¦ºè¨±å¤šæœ€æ–°
çš„ç ”ç©¶å’Œç™¼å±•ã€‚å…¶ä¸­çš„ä¸€äº›å‰ç»æ€§çš„çœ‹æ³•ï¼Œéå¸¸æœ‰å•Ÿç™¼æ€§ï¼Œå°æ–¼å€‹äººæœªï¤­çš„ç ”ç©¶ç™¼å±•æœƒå¾ˆæœ‰
å¹«åŠ©ã€‚å€‹äººæœ¬æ¬¡èˆ‡æœƒçš„ä¸»è¦ä»»å‹™äº¦é”æˆå¦‚ä¸‹ï¼š 
1. ä»¥CISIS 2009 Program Chairçš„èº«åˆ†ï¥«èˆ‡å’Œå”èª¿ CISISâ€™09 ç ”è¨æœƒçš„é€²ï¨ˆï¼Œæ–¼3/16~18å…±
æœ‰14å€‹ï¥æ–‡ç™¼è¡¨å ´æ¬¡ã€‚ã€‚ 
2. ä»¥ä¸»è¾¦äºº (Workshop Chair) çš„èº«åˆ†ï¥«èˆ‡å’Œå”èª¿ IIBMâ€™08 workshopçš„é€²ï¨ˆï¼Œæ–¼3/19å…±æœ‰
ä¸‰å€‹ï¥æ–‡ç™¼è¡¨å ´æ¬¡ã€‚ 
3. æ–¼ï¥¸å€‹ï¥æ–‡ç™¼è¡¨å ´æ¬¡æ“”ä»»ä¸»æŒäººï¼Œåˆ†åˆ¥æ˜¯ CISISâ€™09 æ–¼ 3/18 11:00~12:30 çš„ Network 
Control and Performance Analysis II å’Œ IIBMâ€™09 æ–¼ 3/19 11:00~12:30 çš„ IIBM-S1 ã€‚ 
4. åœ¨IIBM-S3çš„å ´æ¬¡ä¸­ï¼Œä»¥å£é ­å ±å‘Šçš„å½¢å¼ç™¼è¡¨ç ”ç©¶çš„æˆæœ (3/19 16:00~18:00)ã€‚ 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
æ­¤æ¬¡æœƒè­°çš„ï¥«èˆ‡è€…ï¼Œæ—¥æœ¬å’Œæ­æ´²çš„å­¸è€…ä½”å¤§å¤šï¥©ã€‚æœƒä¸­ï§ç”¨ Coffee breaks å’Œï¥¸å ´çš„å®´æœƒï¼Œ
å€‹äººå’Œèˆ‡æœƒçš„å­¸è€…æœ‰ï¥§å°‘çš„äº¤ï§Šå’Œè¨ï¥ï¼Œé™¤ï¦ºå­¸è¡“çš„èª²é¡Œï¼Œä¹Ÿæ¯”è¼ƒï¦ºï¥§åŒåœ‹å®¶å¤§å­¸çš„æ•™å­¸å’Œ
ç ”ç©¶ç’°å¢ƒã€‚æœ¬æ¬¡çš„ç ”è¨æœƒé™¤ï¦ºå’ŒåŸæœ‰èªï§¼çš„å„åœ‹æ•™æˆäº¤ï§Šï¼Œä¹Ÿæ–°èªï§¼ï¦ºå¤šä½ç¾©å¤§ï§ã€å¾·åœ‹ã€
å¥§åœ°ï§ã€æ—¥æœ¬ã€æ¾³æ´²ã€éŸ“åœ‹å’Œä¸­åœ‹ç­‰åœ°çš„æ•™æˆï¼Œå¾€å¾Œåœ¨è¨±å¤šçš„åœ‹éš›å­¸è¡“æ´»å‹•ä¸Šï¨¦æœƒæœ‰ï¤é€²ä¸€
æ­¥çš„åˆä½œã€‚æ”œå›è³‡ï¦¾åç¨±å’Œå…§å®¹: (1) CISIS 2009 ï¥æ–‡é›†å…‰ç¢Ÿä¸€ç‰‡ (IEEE CS Press å‡ºç‰ˆ)ã€(2) 
CISIS 2010 å’Œå…¶ä»–ï¥©å€‹åœ‹éš›ç ”è¨æœƒçš„ Call for Papers ç­‰æ›¸é¢è³‡ï¦¾ã€‚ 
the output gene is increased with the presence of the input 
gene, and vice versa. In other words, an activator gene 
regulates the activatee gene in the biological process so that 
the gene expression level of the two genes forms the trend 
of positive correlations. On the contrary, a trend of negative 
correlations results from the inhibition regulations. 
Table 1. Microarray Time-series Data 
Gene Time Slot 1 Time Slot 2 â€¦ Time Slot n 
Gene #1 0.56 0.80 0.90 
Gene #2 -0.24 -0.1 0.60 
Gene #3 0.12 0.24 0.50 
â€¦ â€¦ â€¦ â€¦ 
Gene #n 0.78 -0.14 -0.56 
 
As a result, the aim of the analysis on microarray 
time-series data is to observe and find out whether there 
exists any pair of genes that have highly-correlated relations. 
Researches on this issue have been worked for these years, 
and a variety of approaches are proposed. Common 
proposed solutions include clustering analysis [5, 6, 7, 8], 
spectral analysis [9, 10], similarity analysis [11, 12], and 
Bayesian networks [13, 14]. These approaches are widely 
applied on the inference and prediction of gene-gene 
relations in microarray time-series data. Although some of 
these proposed approaches may have a success for the 
analysis of the microarray time-series data, they may not 
work for particular datasets or even need an exhausted 
computational time. Pearson correlation coefficient is a 
commonly-used statistical and mathematical method to 
measure the correlation for the two sets of data. It has been 
successfully applied in many fields but it is not suitable for 
the analysis of microarray time-series data due to some 
limitations such as the existence of outliers. Moreover, local 
similarity plays an important role rather than the 
consideration for the whole gene expression levels in 
microarray time-series data. This brings the disadvantage for 
the application of traditional Pearson correlation coefficient. 
The paper presents a method based on Pearson 
correlation coefficient to measure the gene-gene correlation 
relations by filtering gene expression values at particular 
time slots in microarray time-series data. With the proposed 
method, significant outliers of raw data are removed so that 
more known gene regulations can be identified. 
Implementation of this method on the commonly-used 
dataset is performed, and the results show that the proposed 
method can search out more known gene regulations 
compared with original Pearson correlation coefficient. 
Additionally, the proposed method is very simple and it 
does not require too much computational time.  
Remaining of this paper is organized as follows. In 
Section 2, we give a brief description about the involved 
microarray time-series dataset. Our proposed method is 
shown in details in Section 3. Analysis and discussions for 
the experimental results are presented in Section 4. The 
concluding remarks are made in Section 5 with future work. 
 
2. Datasets 
 
This section draws the description of the dataset 
involved in our evaluations. Spellman et al. and Cho et al. 
provided the yeast microarray dataset (http://genome-
www.stanford.edu/cellcycle) [3, 7]. The data was obtained 
for genes of Yeast Saccharomyces cerevisiae cells that were 
collected with four synchronization methods: alpha-factor, 
cdc15, cdc28, and elutriation [15]. These four subsets of the 
dataset contain totally 6178 gene ORF profiles with their 
expression values across individual amounts of time slots. 
For example, the alpha subset contains 18 time points with 
seven minutes as the time interval, while the cdc28 contains 
17 time points with ten minutes as the time interval. These 
four kinds of subsets record the gene expression reactions 
during different phases in cell cycle. However, some of the 
6178 gene ORF profiles are incomplete with missing values 
at certain time slots. The Spellmanâ€™s dataset is as shown in 
Figure 1. 
 
 
 
Figure 1. Spellmanâ€™s Yeast Dataset 
 
Filkov et al. reviewed related literatures and collected 
all known gene regulations of alpha and cdc28 subsets in 
Spellmanâ€™s yeast cell dataset [16]. A database for recording 
all these known gene regulations was also constructed. In 
our evaluation, the known gene regulations recorded in 
Filkovâ€™s database are taken as the validation datasets. In the 
database, number of recorded gene activations and 
inhibitions for alpha subset is 343 and 96 respectively, while 
for cdc28 subset is 469 and 155. All these regulations come 
in the format of A (+) B which denotes gene A is an 
activator that activates gene B. Similarly, C (-) D represents 
an inhibitor gene C inhibits gene D. For example, ABF1 (+) 
855
number of hitting regulations for each situation. The 
flowchart of the proposed method is shown in Figure 2. 
 
 
 
Figure 2. Flowchart of the Proposed Method 
 
The detail algorithm of the proposed method is described as 
follows. 
 
Algorithm for the proposed method: 
 
I. First calculate the PCC with all time slot points, we 
get PCCall 
II. For involved time slot points (originally 1, 2, 3 â€¦ N) 
in each pair of genes, remove one time slot point from 
time slot number one to number N, recalculate PCC 
for each filter phase. Suppose PCC generated are 
PCC1, PCC2, PCC3 ,â€¦, PCCn  
III. Calculate the difference (absolute value) of (PCC1, 
PCCall), (PCC2, PCCall), (PCC3, PCCall) â€¦ (PCCn, 
PCCall), we get Difference1, Difference2, 
Difference3 ,â€¦, DifferenceN 
IV. Choose the Maximum value Difference_i which 
increases with filtering time slot point i from 
Difference1 to DifferenceN 
V. Set time slot point i as the outlier and remove time slot 
point i from the involved time slot points 
VI. Repeat step II to step V for four times without filtered 
time slot point. Record all the correlation coefficients 
for each time 
 
4. Experimental Results and Discussion 
 
After the generation of calculating results for each 
outlier filtering, the number of known regulations on the 
results with marked symbols is gathered. The calculating 
results are shown in Figure 3.  
 
 
 
Figure 3. Output Sample 
 
As shown in Figure 3, each row represents one of the 
pairwise combinational pairs. Rows with (+) or (-) and 
number marks are known activations and inhibitions. All the 
marked combinations are referred to Filkovâ€™s known gene 
transcriptional regulations. We then sort this data with 
Pearson correlation coefficient on all time slot points and 
Pearson correlation coefficient on time slot points without 
filtered outliers for each time. Subsequently, we select 
significant gene combinations with highly-correlated 
Pearson correlation coefficient. Here we set the threshold 
value with 0.7 because Pearson correlation coefficient is 
usually said as characteristic when its absolute value is 
857
