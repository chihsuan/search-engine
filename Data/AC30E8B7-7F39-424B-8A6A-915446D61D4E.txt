a conventional AR model, each pixel in the to-be-
interpolated frame is modeled as a linear combination 
of temporal neighborhood and spatial neighborhood. 
This project proposed a temporal AR model (called 
TAR) utilizing temporal neighborhoodï¼› and a spatial 
AR model (called SAR) utilizing spatial neighborhood. 
We also proposed a mechanism which selects TAR or SAR 
adaptively according to motion information in the 
video sequence. By selecting appropriate AR model, 
unnecessary variables can be eliminated from 
regression process and computational cost can be 
greatly reduced.  
  In addition, we also proposal variable training 
window sizes for the AR model. In conventional auto-
regressive model, fixed training window size is 
adopted within a video sequence. Therefore, it may 
result in worse interpolated frames. We proposed a 
scheme which selects the best training window sizes 
in the encoder side. The best training window sizes 
are encoded into a binary file and transferred to the 
decoder side. When the decoder side performs FRUC, 
this binary file provides better training window 
sizes to improve the quality of the interpolated 
frames. The experiments show that, with variable 
training window sizes, auto-regression model can 
achieve better interpolated frame quality Althought 
it has more computational cost in the encoder side. 
In the second year of the project, we design an auto-
regressive model with various training window sizes 
for multiple description coding (MDC). Our approach 
utilizes the best window size information to enhance 
the error resilience of multiple description coding. 
In our MDC structure, we encode a video stream in two 
descriptions: one contains all odd frames, and the 
other contains all even frames. Both are encoded 
standard H.264/AVC coder. Besides, the descriptor 
with all odd frames will also carry the best window 
size maps for all the even frames. Similarly, the 
descriptor with all even frames will also carry the 
best window size maps for all the odd frames. In the 
decoder side, when there is any packet loss, we 
conceal the missing frame by using auto-regressive 
è¡Œæ”¿é™¢åœ‹å®¶ç§‘å­¸å§”å“¡æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•« â–  æˆ æœ å ± å‘Š   
â–¡æœŸä¸­é€²åº¦å ±å‘Š 
 
ç ”ç©¶è‡ªå‹•è¿´æ­¸æŠ€è¡“åœ¨æå‡å½±ç‰‡æ’­æ”¾é€Ÿç‡èˆ‡å¤šé‡æè¿°è¦–è¨Šç·¨ç¢¼ä¸Šä¹‹æ‡‰ç”¨ 
Study on auto-regressive model for its applications on frame-rate up conversion 
and multiple description video coding 
 
è¨ˆç•«é¡åˆ¥ï¼šâ–  å€‹åˆ¥å‹è¨ˆç•«  â–¡ æ•´åˆå‹è¨ˆç•« 
è¨ˆç•«ç·¨è™Ÿï¼šNSC 100ï¼2628ï¼Eï¼009ï¼026ï¼MY2 
åŸ·è¡ŒæœŸé–“ï¼š 100å¹´ 08æœˆ 01æ—¥è‡³ 102å¹´ 07æœˆ 31æ—¥ 
è¨ˆç•«ä¸»æŒäººï¼š è”¡æ–‡éŒ¦ 
è¨ˆç•«åƒèˆ‡äººå“¡ï¼šæ¥Šå·§å®‰ é»ƒè‡´é  è•­æˆæ†²  
              ç‹æ•¬åš´ æ—å»ºå„’ é«˜å½¬å€¬ èƒ¡æŒ¯é” é™³å®£å‹ é‚±æŸç‘ 
æˆæœå ±å‘Šé¡å‹(ä¾ç¶“è²»æ ¸å®šæ¸…å–®è¦å®šç¹³äº¤)ï¼šâ–¡ç²¾ç°¡å ±å‘Š  â– å®Œæ•´å ±å‘Š 
æœ¬æˆæœå ±å‘ŠåŒ…æ‹¬ä»¥ä¸‹æ‡‰ç¹³äº¤ä¹‹é™„ä»¶ï¼š 
â–¡èµ´åœ‹å¤–å‡ºå·®æˆ–ç ”ç¿’å¿ƒå¾—å ±å‘Šä¸€ä»½ 
â–¡èµ´å¤§é™¸åœ°å€å‡ºå·®æˆ–ç ”ç¿’å¿ƒå¾—å ±å‘Šä¸€ä»½ 
â– å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘ŠåŠç™¼è¡¨ä¹‹è«–æ–‡å„ä¸€ä»½ 
â–¡åœ‹éš›åˆä½œç ”ç©¶è¨ˆç•«åœ‹å¤–ç ”ç©¶å ±å‘Šæ›¸ä¸€ä»½ 
è™•ç†æ–¹å¼ï¼šé™¤ç”¢å­¸åˆä½œç ”ç©¶è¨ˆç•«ã€æå‡ç”¢æ¥­æŠ€è¡“åŠäººæ‰åŸ¹è‚²ç ”ç©¶è¨ˆç•«ã€åˆ—
ç®¡è¨ˆç•«åŠä¸‹åˆ—æƒ…å½¢è€…å¤–ï¼Œå¾—ç«‹å³å…¬é–‹æŸ¥è©¢ 
â–¡æ¶‰åŠå°ˆåˆ©æˆ–å…¶ä»–æ™ºæ…§è²¡ç”¢æ¬Šï¼Œâ–¡ä¸€å¹´â–¡äºŒå¹´å¾Œå¯å…¬é–‹æŸ¥è©¢åŸ·è¡Œå–®ä½ 
 1 
ï® Abstract 
In numerous issues for video compression processing, frame rate up-conversion (FRUC) is 
discussed all the time. Error resilient technique is also important because video streaming over 
networks has become popular in recent years. Multiple Description Coding (MDC) is a strong way 
dealing with erroneous transmission in various network environments.  
In the first year of the project, an adaptive auto-regressive model (AR model) is proposed for 
FRUC. In a conventional AR model, each pixel in the to-be-interpolated frame is modeled as a 
linear combination of temporal neighborhood and spatial neighborhood. This project proposed a 
temporal AR model (called TAR) utilizing temporal neighborhood; and a spatial AR model (called 
SAR) utilizing spatial neighborhood. We also proposed a mechanism which selects TAR or SAR 
adaptively according to motion information in the video sequence. By selecting appropriate AR 
model, unnecessary variables can be eliminated from regression process. Compared to STAR model 
[2] which utilizes joint temporal-spatial neighborhood for each pixel, computational cost can be 
greatly reduced with the proposed method. In addition, the experiment results show that visual 
quality can also be improved.  
  In addition, an AR model with variable training window sizes is proposed for FRUC. In 
conventional auto-regressive model, fixed training window size is adopted within a video sequence. 
Therefore, it may result in worse interpolated frames. We proposed a scheme which selects the best 
training window sizes in the encoder side. The best training window sizes are encoded into a binary 
file and transferred to the decoder side. When the decoder side performs FRUC, this binary file 
provides better training window sizes to improve the quality of the interpolated frames. The 
experiments show that, with variable training window sizes, auto-regression model can achieve 
better interpolated frame quality Althought it has more computational cost in the encoder side. 
In the second year of the project, we design an auto-regressive model with various training 
window sizes for multiple description coding (MDC). Our approach utilizes the best window size 
information to enhance the error resilience of multiple description coding. In our MDC structure, we 
encode a video stream in two descriptions: one contains all odd frames, and the other contains all 
even frames. Both are encoded standard H.264/AVC coder. Besides, the descriptor with all odd 
frames will also carry the best window size maps for all the even frames. Similarly, the descriptor 
with all even frames will also carry the best window size maps for all the odd frames. In the decoder 
side, when there is any packet loss, we conceal the missing frame by using auto-regressive model 
and corresponding training window sizes. 
According the experimental results, the proposed method is efficient and outperforms other 
method in objective and subjective quality. 
  
 
Keywords: Frame rate up-conversion, adaptive auto-regressive model, training window, error 
concealment, multiple description coding. 
 
 3 
ï® ç¬¬ä¸€å¹´è¨ˆç•«å ±å‘Š 
1. æ–‡ç»æ¢è¨/Related Work 
1.1 MC-FRUC 
MC-FRUC can achieve better visual quality than frame average by exploiting the motion 
redundancy between frames. The Figure 1-1 shows the overall architecture of bi-direction motion 
compensated interpolation. 
 
Figure 1-1 : Bi-directional motion estimation diagram. Each block is assumed to be 
experienced a translational motion. 
 First the to-be-interpolated frame is divided into non-overlapping blocks. For each block, the 
bi-direction motion search is performed. The bi-direction motion search will find a motion vector v 
for block B(i, j) by minimizing the bi-directional sum of square error (SBSE) in the search window.  
The bi-directional motion search can be interpreted by SBSE[B(i, j),ğ‘£ğ‘£] = ï¿½ (ğ¹ğ¹ğ‘¡ğ‘¡âˆ’1[ğ‘ ğ‘  âˆ’ ğ‘£ğ‘£] âˆ’ ğ¹ğ¹ğ‘¡ğ‘¡+1[ğ‘ ğ‘  + ğ‘£ğ‘£])2
ğ‘ ğ‘ âˆˆğµğµ(ğ‘–ğ‘–,ğ‘—ğ‘—)  
( 1 ) 
ğ‘£ğ‘£ğ‘–ğ‘–,ğ‘—ğ‘— = ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
ğ‘£ğ‘£
{SBSE[B(i, j),ğ‘£ğ‘£]} 
( 2 ) 
, where S is a 2-D vector representing a pixel location, the ğ¹ğ¹ğ‘¡ğ‘¡âˆ’1, ğ¹ğ¹ğ‘¡ğ‘¡, and ğ¹ğ¹ğ‘¡ğ‘¡+1denote the previous, 
to-be-interpolated, the following frames, respectively. The ğ‘£ğ‘£ğ‘–ğ‘–,ğ‘—ğ‘— represents the bi-direction motion 
searchâ€™s result for block B(i,j) in to-be-interpolated frame ğ¹ğ¹ğ‘¡ğ‘¡. After the motion search, we then use 
the motion information to interpolate the block. Also, to-be-interpolated block Bï¿½(i, j) in ğ¹ğ¹ğ‘¡ğ‘¡ is given 
by (3). 
 5 
spatial-temporal support order (support order, for short). When L is set to 1, the pixel is modeled as 
the weighted sum of 9 pixels in previous frame, 9 pixels in following frame, and 4 pixels in current 
frame. The (k, l) represents the pixel location within the training window. The (u, v) represents 
looping index for each element in spatial-temporal neighborhood, called support region. The optimal 
solution for weighting vector is, the one that minimizes the distortion  Îµ between training 
windows  Rï¿½tâˆ’1 and Rtâˆ’1 to have best fitting weighting vector. 
Îµ = ğ¸ğ¸ï¿½ Rtâˆ’1 âˆ’ Rï¿½tâˆ’1ï¿½ = ï¿½ ï¿½ ğ¸ğ¸ ï¿½ï¿½Rtâˆ’1(ğ‘˜ğ‘˜, ğ‘™ğ‘™) âˆ’ Rï¿½tâˆ’1(ğ‘˜ğ‘˜, ğ‘™ğ‘™)ï¿½2ï¿½ğ‘Šğ‘Šğ‘Šğ‘Š
ğ‘™ğ‘™=0
ğ‘Šğ‘Šğ‘Šğ‘Š
ğ‘˜ğ‘˜=0
 
( 5 ) 
Since the actual pixel values in the to-be-interpolated frame are not available in FRUC (In (5), 
for example, Rtâˆ’1), equation (5) canâ€™t be used for deriving correct weighting coefficients. An 
iterative method, called self-feedback weight training loop algorithm was proposed with STAR 
model to deal with such issue. The self-feedback weight training loop consists of two parts. The 
pixels in training windows Rï¿½tâˆ’1 and Rï¿½t+1 are first interpolated by using their spatial-temporal 
neighborhood with the weighting vector ğ‘¤ğ‘¤ï¿½ï¿½âƒ— , which consists of each element of Wp, Wf, and Ws, 
rewritten in 1-D manner. Then, the pixels in Rï¿½tâˆ’1 and Rï¿½t+1 are used to approximate the training 
window Rï¿½t using the same weighting vector ğ‘¤ğ‘¤ï¿½ï¿½âƒ— , as illustrated in Figure 1-3 and the equation (6) 
below.  
 
Figure 1-3 : Self-feedback algorithm diagram Rï¿½t( k, l ) =  ï¿½ ï¿½ Rï¿½tâˆ’1(k + u, l + v)
v) â‰¤L Ã—  Wp(u, v)âˆ’Lâ‰¤(u, + ï¿½ ï¿½ Rï¿½t+1(k + u, l + v)v) â‰¤L Ã—  Wf(u, v)âˆ’Lâ‰¤(u,+ ï¿½ ï¿½ Rï¿½t(k + u, l + v){v=0,âˆ’Lâ‰¤u<0} Ã—  Ws(u, v){v<0,âˆ’ğ¿ğ¿â‰¤ğ‘¢ğ‘¢â‰¤ğ¿ğ¿}âˆª  
( 6 ) 
After Rï¿½tâˆ’1, Rï¿½t, Rï¿½t+1 have been interpolated, the jointly distortion is defined as follows: 
D(i) =  ï¿½ï¿½E ï¿½ï¿½Rï¿½tâˆ’1i+1 (k, l) âˆ’  Rï¿½tâˆ’1i (k, l)ï¿½2ï¿½Wy
l=0
Wx
k=0
+ ï¿½ï¿½E ï¿½ï¿½Rï¿½t+1i+1 (k, l) âˆ’  Rï¿½t+1i (k, l)ï¿½2ï¿½ + ï¿½ï¿½ E ï¿½ï¿½Rï¿½ti+1(k, l) âˆ’ Rt(k, l)ï¿½2ï¿½Wy
l=0
Wx
k=0
Wy
l=0
Wx
k=0
 
( 7 ) 
 7 
threshold, maximum iteration times (iMAX) â€¦etc. 
Step 2: Use Bi-MCIâ€™s result as initial value for training windows Rï¿½ğ‘¡ğ‘¡âˆ’10  and Rï¿½ğ‘¡ğ‘¡+10  
Step 3: Initialize iteration index 
Step 4&5: Use formulas mentioned before to construct corresponding matrices and vector for 
least square method. After least square method performed, the new weighting vector is obtained. 
Then the D(i) from (7) is calculated. 
Step 6: Test if D(i) is less than predefined threshold or not. If it does, then the procedure is 
done. Else, increase the iteration index, write back the new training windowâ€™s result as next 
iterationâ€™s initial value and loop again. 
2. ç ”ç©¶æ–¹æ³•ä¸€/TAR-model and SAR model 
 
Figure 2-1 : ATAR diagram. 
In the proposed TAR model, each pixel in to-be-interpolated frame t-1 is modeled as linear 
combination of temporal neighborhood, and all pixels in the same training window will share the 
same weighting coefficients. Rï¿½tâˆ’1( k, l ) =  ï¿½ ï¿½ Rtâˆ’2(k + u, l + v)
v) â‰¤L Ã—  Wp(u, v)âˆ’Lâ‰¤(u, + ï¿½ ï¿½ Rt(k + u, l + v)v) â‰¤L Ã—  Wf(u, v)âˆ’Lâ‰¤(u,  
( 10 ) 
The Rï¿½tâˆ’1  means the training window in frame t-1, and Wp, Wf  represents weighting 
coefficients in previous temporal neighborhood and following temporal neighborhood, respectively. 
The ( k, l )  represents pixel location in training window, and (u, v)  is looping index for each 
temporal neighborhood support region. Assuming that support order is 1, the length of weighting 
vector will be 18 as illustrated in figure 2-1. 
Self-feedback weight training algorithm similar to that used in STAR model is also applied in 
the proposed TAR model. Since only temporal neighborhood is used, the formula (6) is modified as 
follows for the approximated pixel in training window Rï¿½t in the proposed TAR. 
 9 
MAğ‘šğ‘šğ‘£ğ‘£ = (Absolute MVï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ğ‘Šğ‘Š)2 + (Absolute MVï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ğ‘Šğ‘Š)2 
( 14 ) 
where MAğ‘šğ‘šğ‘£ğ‘£ represents the mean of motion vectorâ€™s magnitude in training window. When MAğ‘šğ‘šğ‘£ğ‘£ 
is larger than a predefined threshold Î´, we adopt the adaptive SAR (ASAR) as our regression-based 
FRUC. Otherwise, adopt adaptive TAR (ATAR). 
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘™ğ‘™ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ğ‘ğ‘œğ‘œğ‘ğ‘ ğ‘ğ‘ğ‘œğ‘œğ‘šğ‘šğ‘ ğ‘ ğ‘™ğ‘™: ï¿½ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´,               ğ‘ğ‘ğ‘œğ‘œ ğ›¥ğ›¥ < ğ›¿ğ›¿
ğ´ğ´ğ‘†ğ‘†ğ´ğ´ğ´ğ´,            ğ‘œğ‘œğ‘ ğ‘ â„ğ‘ ğ‘ ğ‘ğ‘ğ‘¤ğ‘¤ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘  
( 15 ) 
 After defining the selection criterion, we have to verify the validity of it. 
 
Figure 2-3 : The validity of selection criterion of the test sequence Foreman_QCIF. 
The figure 2-3 shows  ğ‘†ğ‘†ğ‘†ğ‘†ğ¸ğ¸ğ‘¡ğ‘¡âˆ’ğ‘ ğ‘  and  MAğ‘šğ‘šğ‘£ğ‘£ for all the training windows in Foreman_QCIF 
sequence, where the training windows are sorted in an ascending order of MAğ‘šğ‘šğ‘£ğ‘£, defined in 
formula (14). In figure 2-3, the left coordinate is SSEt-sâ€™s value, and the right coordinate is the value 
of MAğ‘šğ‘šğ‘£ğ‘£. Let ğ‘†ğ‘†ğ‘†ğ‘†ğ¸ğ¸ğ‘¡ğ‘¡âˆ’ğ‘ ğ‘  denote the difference between TARâ€™s SSE and SARâ€™s SSE, where SSE 
means the sum of square error. Then, ğ‘†ğ‘†ğ‘†ğ‘†ğ¸ğ¸ğ‘¡ğ‘¡âˆ’ğ‘ ğ‘  can be formulated as (16) below. 
ğ‘†ğ‘†ğ‘†ğ‘†ğ¸ğ¸ğ‘¡ğ‘¡âˆ’ğ‘ ğ‘  = ï¿½ ï¿½ ï¿½ ï¿½ï¿½Rtâˆ’1(ğ‘˜ğ‘˜, ğ‘™ğ‘™) âˆ’ Rï¿½tâˆ’1ğ‘‡ğ‘‡ğ´ğ´ğ´ğ´(ğ‘˜ğ‘˜, ğ‘™ğ‘™)ï¿½2 + ï¿½Rt+1(ğ‘˜ğ‘˜, ğ‘™ğ‘™) âˆ’ Rï¿½t+1ğ‘‡ğ‘‡ğ´ğ´ğ´ğ´(ğ‘˜ğ‘˜, ğ‘™ğ‘™)ï¿½2ğ‘Šğ‘Šğ‘Šğ‘Š
ğ‘™ğ‘™=0
ğ‘Šğ‘Šğ‘Šğ‘Š
ğ‘˜ğ‘˜=0+ ï¿½Rt(ğ‘˜ğ‘˜, ğ‘™ğ‘™) âˆ’ Rï¿½tğ‘‡ğ‘‡ğ´ğ´ğ´ğ´(ğ‘˜ğ‘˜, ğ‘™ğ‘™)ï¿½2 ï¿½ ï¿½
âˆ’ ï¿½ ï¿½ ï¿½ ï¿½ï¿½Rtâˆ’1(ğ‘˜ğ‘˜, ğ‘™ğ‘™) âˆ’ Rï¿½tâˆ’1ğ‘†ğ‘†ğ´ğ´ğ´ğ´(ğ‘˜ğ‘˜, ğ‘™ğ‘™)ï¿½2 + ï¿½Rt+1(ğ‘˜ğ‘˜, ğ‘™ğ‘™) âˆ’ Rï¿½t+1ğ‘†ğ‘†ğ´ğ´ğ´ğ´(ğ‘˜ğ‘˜, ğ‘™ğ‘™)ï¿½2ğ‘Šğ‘Šğ‘Šğ‘Š
ğ‘™ğ‘™=0
ğ‘Šğ‘Šğ‘Šğ‘Š
ğ‘˜ğ‘˜=0+ ï¿½Rt(ğ‘˜ğ‘˜, ğ‘™ğ‘™) âˆ’ Rï¿½tğ‘†ğ‘†ğ´ğ´ğ´ğ´(ğ‘˜ğ‘˜, ğ‘™ğ‘™)ï¿½2ï¿½ ï¿½ 
( 16 ) 
Note that the calculation of ğ‘†ğ‘†ğ‘†ğ‘†ğ¸ğ¸ğ‘¡ğ‘¡âˆ’ğ‘ ğ‘  in figure 2-3 is based on available to-be-interpolated frames, 
that is, the ground truth if the to-be-interpolated frame is known. Rtâˆ’1, Rt, Rt+1 in formula (16) 
denote  the ground truth of the  training windows in to-be-interpolated frames t-1, t, and t+1 
0
2
4
6
8
10
12
14
16
18
20
-4000
-3000
-2000
-1000
0
1000
2000
3000
4000
1 53 10
5
15
7
20
9
26
1
31
3
36
5
41
7
46
9
52
1
57
3
62
5
67
7
72
9
78
1
83
3
88
5
93
7
98
9
10
41
10
93
11
45
11
97
12
49
SSEt-s
MAmv
 11 
encoding architecture. 
 
Figure 3-1 Encoder architecture for the proposed MSTAR model with variable training 
window size 
First the source images are encoded by standard H.264 encoder as well as STAR model 
implemented with four different training window sizes, four by four, eight by eight, sixteen by 
sixteen and thirty-two by thirty-two. Second, to obtain the best training window sizes, the PSNR 
information is generated by comparing the difference between these interpolated frames with varied 
training window sizes and the original source frames which exist in the encoder side. Figure 3-1 
illustrates the MSTAR model implemented with four training window sizes. In this example, we 
divide the frame in foreman sequence by using four training window sizes.  
 
 
Figure 3-1 STAR model implemented with four different training window sizes 
 To determine the best training window sizes, we adopt quadtree-based algorithm as shown in 
 13 
  
Figure 3-3 Quadtree-based algorithm of selecting training window sizes 
 
 15 
Maximum iteration times 2, 4, 6 
The jointly distortion threshold 50 
Support order 1 
Table 1 Parameters for regression model 
Four test sequences: Foreman, News, Mobile and Coastguard with CIF (352x288) resolution are 
used for performance evaluation. Five quantization parameters of H.246 encoder, 26, 28, 30, 32 and 
34, are used. 
4.2 Simulation of TAR and SAR 
We will examine the subjective and objective visual quality in this section. The figure 4-1 shows 
frame by frame PSNR of test sequence Foreman_QCIF for different methods. Horizontal coordinate 
is to-be-interpolated frame index, and the vertical coordinate is PSNR (dB). FA represents Frame 
Average method; MCI represents motion compensated interpolation with bi-directional motion 
estimation [1] (Bi-MCI, here we use MCI for short); and STAR represents the spatial-temporal AR 
model [2]. The Proposed_S represents the proposed SAR, Proposed_T represents the proposed TAR, 
and the Proposed_AST represents the method with adaptive selection between SAR and TAR. 
Proposed_UB is the performance upper bound of the proposed adaptive schema because it selects 
the best AR model (SAR or TAR) according to ground truth of the to-be-interpolated frames. 
 
Figure 4-5 : Frame by frame PSNR of Foreman_QCIF 
In FA, each pixel is interpolated by the co-located pixels in temporal neighborhood with the 
same weight (That is, 0.5). Since the FA didnâ€™t use the motion information from video sequence, it 
has the best computation efficiency. But the visual quality of the frames interpolated by using FA is 
not acceptable, especially in large motion part of the video. 
The MCI produces better visual quality than FA since it exploits the motion information in 
video sequence. However, the performance of MCI-like methods for strongly depends on the 
27.5
30
32.5
35
37.5
40
42.5
45
47.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14
FA
MCI
STAR
Proposed_S
Proposed_T
Proposed_AST
Proposed_UB
 17 
 Figure 4-7 : Frame by frame PSNR of Football_CIF 
The Football_CIF sequence is used to test if the model is able to handle the sequence with 
large motion or not. In figure 4-3, it is observed that all the regression-based FRUC algorithms 
performed closely with no significant difference between them (less than 0.1dB). Even though the 
proposed methods no significantly gain in large-motion sequences, we still have the advantage in 
reduced computation loading. The table 2 shows the performance results of various methods, where 
maximum support 1 and iteration 1 timeâ€™s result in QCIF and CIF. 
Sequence Name FA MCI 4x4 MCI 8x8 STAR Proposed_S Proposed_T Proposed_AST Proposed_UB
Akiyo_QCIF 50.143 49.796 50.641 51.393 50.641 51.742 51.742 52.002
Coastguarad_QCIF 33.873 34.805 37.328 39.105 37.356 39.053 39.064 40.098
Foreman_QCIF 34.426 37.364 37.704 38.514 37.736 39.462 39.492 40.256
Mobile_QCIF 32.528 31.568 33.723 35.566 33.707 36.336 36.320 36.680
Akiyo_CIF 46.162 46.613 47.054 47.641 47.055 48.113 48.113 48.350
Football_CIF 19.896 20.399 21.164 21.191 21.263 21.151 21.248 21.646
Foreman_CIF 31.011 34.224 34.694 34.686 34.717 34.763 34.796 35.553
Mobile_CIF 24.830 27.218 27.512 29.092 27.477 29.932 29.873 30.234
Mother_and daughter_CIF 43.317 43.646 44.292 44.498 44.308 44.543 44.561 44.974
News_CIF 38.138 38.146 37.937 38.306 37.939 38.712 38.712 39.102  
Table 2 : PSNR table of FRUC algorithms. 
The table 2 gives the average PSNR of different sequences using FRUC algorithms. Compared 
to STAR model, the Proposed_AST have better visual quality among all test sequences except 
Coastguard_QCIF. In this experiment result, the average gain of Proposed_AST to STAR model is 
0.39dB, and the maximum average gain is 0.97dB for Foreman_QCIF. The following table shows 
the performance results of AR-based methods. The maximum support order is 6 for all methods and 
maximum iteration time is 5 for STAR, and 2 for proposed methods. 
18.5
19.5
20.5
21.5
22.5
23.5
24.5
25.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14
FA
MCI
STAR
Proposed_S
Proposed_T
Proposed_AST
Proposed_UB
 19 
happened around the ear, mouth, and helmet. Figure 4-4 (b) is MCI8x8 in the same frame. Blurring 
effect is eliminated for motion compensation. Still, the artifact around the mouth occurred because 
its discontinuity of adjacent blockâ€™s motion vectors. 
The figure 4-4 (c) is STAR modelâ€™s interpolation result, for maximum iteration 1 and support 
order is 1. It alleviated the artifact around the mouth a little, and improved overall visual quality. But 
it also required much computation cost than proposed method. And a little blurring effect occurred 
at the edge of the helmet, since it may contain too many unreliable spatial-temporal neighborhoods 
in moving filed. The figure 4-4 (d) is proposed method Proposed_ASTâ€™s interpolation result. 
Though the artifacts around the mouth are not totally alleviated, we improved the blurring effect at 
the edge of the helmet. 
 
Figure 4-9 : Mobile_QCIF PSNR vs. iteration times with maximum support order 3 
The above figure shows the PSNR of test sequence Mobile_QCIF with different iteration times. 
Experimental results show that these two different methods approach to different convergence point. 
Since the self-feedbackâ€™s regression target is not the same pixel value in these two methods, they 
will not converge to the same visual quality level. This figure also shows that the self-feedback loop 
algorithm can produce better PSNR through different maximum iteration times.  
Since we use less number of variables in the proposed AR model, the LSMâ€™s computation 
loading can be significantly reduced, comparing to STAR model. The time complexity to compute 
the matrix inverse is ÎŸ(ğ‘ğ‘4), where n is equal to the number of variables in AR model. Assuming that 
support order is set to 1, the STAR model will have 22 variables for LSM; while our proposed 
method will use only 17 variables in average. This is because that TAR uses 18 variables, SAR uses 
8 variables, and the ratio between TAR and SAR selected in our method is about 9:1. Following 
table shows the ratio between TAR and SAR selected in the proposed method, where the selection 
criteria threshold is set to 4.  
  
34
34.5
35
35.5
36
36.5
37
37.5
38
38.5
1 2 3 4 5 6 7 8 9 10
STAR
Proposed_AST
 21 
is set to 1 in this table. The xxx_IT1, xxx_IT2, xxx_IT5 means the xxx AR model is performed 
iteratively once, two times, and five times, respectively. The percentages in the table show that the 
proposed model consume only 36% to 75% clocks compared to STAR model for iteration once. 
Since STAR_IT5 (iteration 5 times) and Proposed_AST_IT2 (iteration two times) have similar 
visual performance, we also compare their execution times in this table and the results show that the 
proposed model consume only 14% to 36% clocks, compared to STAR model. The proposed model 
can save up to 86% clocks in Football_CIF, because it adaptively chooses SAR about 72% in whole 
process, the average number 10.8 variables for LSM. 
 
4-3 Simulation of multiple training windows sizes 
4-3-1 Frame-by-frame distortion performance observation 
We will examine the subjective and objective visual quality in this section. The Figure 4-6 shows 
frame by frame PSNR of test sequence Mobile_CIF for different methods. Horizontal axis denotes 
to-be-interpolated frame index, and the vertical axis is PSNR (dB). The STAR curve represents the 
spatial-temporal AR model [2]. The Proposed MSTAR curve represents the proposed 
spatial-temporal AR model with variable training window size. 
 
Figure 4-6 Frame by frame PSNR of Mobile_CIF 
 The parameters of regression-based methods (STAR, Proposed) in fare set as following: 
maximum iteration time = 6 and maximum support order = 1. In this experimental result, the gain of 
MSATR to STAR model is up to 0.46dB. 
25.5
26
26.5
27
27.5
28
28.5
29
29.5
30
0 5 10 15 20 25 30 35 40 45 50
PS
N
R(
dB
) 
frame no. 
mobile 
MSTAR
STAR
 23 
 
Figure 4-8 The relationship between bitrate and PSNR in Mobile_CIF sequence. 
  
Figure 4-9 The relationship between bitrate and PSNR in Foreman_CIF sequence. 
QP26 
QP28 
QP30 
QP32 
QP34 
25.5
26
26.5
27
27.5
28
28.5
500 1000 1500 2000 2500 3000
PS
N
R(
dB
) 
bit rate(KB) 
mobile-CIF 
MSTAR-it6
STAR-it6
MSTAR-it4
STAR-it4
MSTAR-it2
STAR-it2
QP26 
QP28 
QP30 
QP32 
QP34 
30.5
31
31.5
32
32.5
33
33.5
100 200 300 400 500 600 700 800
PS
N
R(
dB
) 
bit rate(KB) 
foreman-CIF 
MSTAR-it6
STAR-it6
MSTAR-it4
STAR-it4
MSTAR-it2
STAR-it2
 25 
 Figure 11 Percentage of each training window size for each sequence 
4-3-3 Subjective Quality 
This section observes the subjective quality of the interpolated frames using STAR model and 
Proposed MSTAR method, respectively. 
     
Figure 4-13 The 2th interpolated frame in Mobile_CIF. (a) STAR model (b) Proposed MSTAR 
method 
 Figure 4-13 and Figure 4-14 illustrate the interpolated results for 2th frame of Mobile_CIF and 
96th frame of News_CIF with maximum iteration 6 and support order is 1. Figure 4-13 (a) and 
Figure 4-14 (a) are STAR modelâ€™s interpolation result. The blurring is happened in the marked 
window with color red. Figure 4-13 (b) and Figure 4-14 (b) are proposed methodâ€™s interpolation 
results. The blurring effect in the marked window is improved. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
mobile foreman coastguard news
%
 
32x32
16x16
8x8
4x4
(a) (b) 
 27 
ï® ç¬¬äºŒå¹´è¨ˆç•«å ±å‘Š 
1. æ–‡ç»æ¢è¨/Introduction and Related Works 
Watch the video stream in any device over network is extremely popular. In video transmission, 
video data try to reduce the bit rate for transmission over communication channels. A highly 
compressed video data was usually adopted. Due to the unreliable communication environments, the 
data packet may be lost during the transmission. The packet lost lead to video canâ€™t decode.in 
addition because of motion compensation, this error may propagate to succeeding frames. There are 
many techniques that try to overcome these problems. Multiple Description Coding (MDC) [18]is 
one of them. It generates different encoded versions for the same source. The version called 
description. Each description can decode and transmitted independently. Generally, all the 
descriptions are central decoding, the high quality video is reconstructed. While only one or few 
description are available at the decoder. The receiver still be able to retrieve video but with penalty. 
In recent years there have been many MDC techniques developed for different purpose. In [19], the 
input sequences is split in temporal domain, [20] in spatial domain and [21] in frequency domain. In 
this paper, we adopt one of the temporal-MDC structure named even and odd MDC structure. It spilt 
the original sequences into two descriptors. One description has all odd frames and the other has all 
even frames. 
 
1.1 Motivation  
In STAR model, fixed training window sizes are used. In our experiment, we have observed over 
50% regions use different training window sizes can get better quality and performances. In the 
proposed method, we design a STAR model using various training window sizes. The best training 
windows sizes of each region was found out by the method 
Multiple description coding providing a way that deal the date losses. Each descriptor can decode 
independently. We consider MDC with STAR model effect improve the error concealment. 
The multiple training window sizes are effective get higher quality than fixed sizes. And encode the 
selected training windows sizes is not a difficult issue. Based on these reasons, first, we generate the 
windows sizes map that including all selected train windows sizes in encoder side.  
It is encode to a binary file. Second, construct MDC structure, then sent the video stream and 
training window sizes information to decoder side. When a frame is lost, the frame was 
reconstructed by STAR model and training window sizes of missing frame. 
2. ç ”ç©¶æ–¹æ³•/Proposed System 
2.1 Determine window sizes 
   The proposed method utilizes various training window sizes, which is based on the STAR model 
with fixed training windows size. We call various training window sizes STAR (VSTAR). 
 29 
descriptors. One description has all odd frames and the other has all even frames. The odd frame 
description should include even frameâ€™s window sizes map (WSM). The even frame description 
include odd frameâ€™s. Figure 2-2 shows the architecture. 
 
Figure 2-13 architecture of video stream 
When a frame is lost, decoder utilizes STAR model and the windows sizes map(WMP) of missing 
frame to conceal the missing frame. 
In encoder, generate WSM of frame use VSTAR encoder. Then add the WSM to corresponding 
descriptor. Figure 2-3 show the flowchart of encoder. 
 31 
Non â€“ available Available Frame t+1 : Motion copy 
Frame t : STAR use t-3,t-1,t+1, W32 
Non - available Non - available 
Table 1 decoder strategy under different packet loss circumstances 
  
In this case, the frame was continuous loss (i.e. lose frame t and t+1 both). It was not suitable for 
conceal frame T with window sizes map of frame T. Because the frame T+1 also losing. The 
situation is of encoder and decoder were different already. In encoder, the frame T+1 have best 
quality. But in decoder, the frame T+1 are break. Therefore we donâ€™t use the window sizes map, set 
all this frameâ€™s window sizes was 32. that can get better quality. 
 
Figure 2-15 the flowchart of decoder strategy when packet loss 
 
The Figure 2-4 shows decoder trying his best to get the best quality reference frame. 
 
 33 
 
(B) Foreman sequence at 3% packet lose rates 
 
(C) Foreman sequence at 5% packet lose rates 
QP_20 
QP_22 
QP_24 
QP_26 
QP_28 
32
32.5
33
33.5
34
34.5
35
35.5
36
200000 400000 600000 800000 1000000
PS
N
R(
dB
) 
bitrate(byte) 
foreman (loss rates 3%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
QP_20 
QP_22 
QP_24 
QP_26 
QP_28 
31.5
32
32.5
33
33.5
34
34.5
35
35.5
200000 400000 600000 800000 1000000
PS
N
R(
dB
) 
bitrate(byte) 
foreman (loss rates 5%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
 35 
   
(B) News sequence at 3% packet loss rates 
 
(C) News sequence at 5% packet loss rates 
  
QP_20 QP_22 
QP_24 
QP_26 
QP_28 
34
34.5
35
35.5
36
36.5
37
37.5
38
100000 150000 200000 250000 300000 350000 400000 450000 500000
PS
N
R(
dB
) 
bitrate(byte) 
News (loss rates 3%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
QP_20 QP_22 
QP_24 
QP_26 
QP_28 
34
34.5
35
35.5
36
36.5
37
37.5
38
100000 150000 200000 250000 300000 350000 400000 450000 500000
PS
N
R(
dB
) 
biterate(byte) 
News (loss rates 5%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
 37 
 
(B) Football sequence at 3% packet loss rates 
 
 
(C) Football sequence at 5% packet loss rates 
QP_20 
QP_22 
QP_24 QP_26 
QP_28 
27.6
27.8
28
28.2
28.4
28.6
28.8
29
500000 1000000 1500000 2000000
PS
N
R(
dB
) 
bitrate(byte) 
football (loss rates 3 %) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
QP_20 
QP_22 
QP_24 QP_26 QP_28 
27
27.2
27.4
27.6
27.8
28
28.2
28.4
28.6
28.8
500000 1000000 1500000 2000000
PS
N
R(
dB
) 
bitrate(byte) 
football (loss rates 5%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
 39 
 
(B) Coastguard sequence at 3% packet loss rates 
 
 
(C) Coastguard sequence at 5% packet loss rates 
 
QP_20 
QP_22 QP_24 
QP_26 
QP_28 
27.5
28
28.5
29
29.5
30
30.5
31
31.5
500000 1000000 1500000 2000000 2500000
PS
N
R(
dB
) 
bitrate(byte) 
Coastguard (loss rates 3%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
QP_20 
QP_22 QP_24 
QP_26 
QP_28 
27.5
28
28.5
29
29.5
30
30.5
31
31.5
500000 1000000 1500000 2000000 2500000
PS
N
R(
dB
) 
bitrate(byte) 
Coastguard (loss rates 5%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
 41 
 Figure 3-20 Frame by frame performance of Coastguard sequence  
3-3 Subjective Quality 
Figure 3-6 shows the subjective visual quality comparison for the 57th frame on the Coastguard 
sequence with Qp20, support order 1 and maximum iteration 6. 
We have observed the visual quality has obviously improved. Untidy area and boundary effect have 
been removed. 
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
1 13 25 37 49 61 73 85 97
PS
N
R(
dB
) 
Coastguard : Qp24 ,it 6 
WSM
W32
MCI
Frame NO 
 43 
  
Figure 3-22 the 11th conceal frame in football (A) WSM (B) MCI (C) No error 
Figure 3-7 illustrates the comparison of WSM and MCI with football sequence, Qp22 and iteration 
time 6. The blurring effects occur on the NO.97 player and some object. The blurring effects are 
improved by WSM. 
 45 
[12] B.-D. Choi, J.-W. Han, C.-S. Kim and S.-J. Ko, "Frame rate up-conversion using perspective 
transform," Consumer Electronics, IEEE Transactions on, vol. 52, pp. 975-982, 2006. 
[13] T. Thaipanich, P.-H. Wu and C.-C. Kuo, "Low complexity algorithm for robust video frame rate 
up-conversion (FRUC) technique," Consumer Electronics, IEEE Transactions on, vol. 55, pp. 220-228, 
2009. 
[14] S.-J. Kang, D.-G. Yoo, S.-K. Lee and Y. Kim, "Multiframe-based bilateral motion estimation with 
emphasis on stationary caption processing for frame rate up-conversion," Consumer Electronics, IEEE 
Transactions on, vol. 54, no. 4, pp. 1830-1838, November 2008 
[15] G. Dane and T. Nguyen, "Optimal temporal interpolation filter for motion-compensated frame rate up 
conversion," Image Processing, IEEE Transactions on, vol. 15, no. 4, pp. 978-991, April 2006. 
[16] Y. Zhang, D. Zhao, S. Ma, R. Wang and W. Gao, "A Motion-Aligned Auto-Regressive Model for 
Frame Rate Up Conversion," Image Processing, IEEE Transactions on, vol. 19, no. 5, pp. 1248-1258, 
MAY 2010 
[17] N. Jacobson, Y.-L. Lee, V. Mahadevan, N. Vasconcelos and T. Nguyen, "A Novel Approach to FRUC 
Using Discriminant Saliency and Frame Segmentation," Image Processing, IEEE Transactions on, vol. 
19, no. 11, pp. 2924-2934, November 2010. 
 
ç¬¬äºŒå¹´ 
 
[18]  V. K. Goyal, "Multiple description coding: Compression meets the network," IEEE Signal Process., vol. 
18, no. 5, pp. 74-93, Sep. 2001.  
[19]  A. Reibman, H. Jafarkhani, Y. Wang and M. Orchard, "Multiple description video using rate-distortion 
splitting," IEEE International Conference on Image Processing, pp. 978-981, Oct. 2001.  
[20]  N. Franchi, M. Fumagalli, R. Lancini and S. Tubaro, "Multiple Description Video Coding for Scalable 
and Robust Transmission Over IP," IEEE Transactions on Circuits and Systems for Video Technology, 
vol. 15, no. 3, March 2005.  
[21]  S. Adedoyin, W. A. C. Fernando, H. A. Karim, C. T. E. R. Hewage, A. M. Kondoz, "Scalable Multiple 
Description Coding with Side Information Using Motion Interpolation," Consumer Electronics, IEEE 
Transactions on, vol. 4, no. 54, Nov. 2008.  
 
 
 
 
 
 
 47 
ç¬¬äºŒå¹´ 
ï¬ å®Œæˆç·¨ç¢¼ç«¯ä¹‹ AR model ä¹‹ç ”ç©¶èˆ‡è¨­è¨ˆã€‚ 
æˆ‘å€‘åœ¨ç·¨ç¢¼ç«¯é å…ˆæ ¹æ“š AR modelåšå‡ºçš„ interpolate frame å’Œ original frameè¨ˆç®—å‡º
æœ€å¥½çš„training window å¤§å°ï¼Œæ¥è‘—æŠŠé€™å€‹è³‡è¨Šå„²å­˜èµ·ä¾†ä¸¦åšå£“ç¸®ï¼Œå«åštraining window 
sizes mapï¼Œä¸¦é€åˆ°è§£ç¢¼ç«¯ã€‚ 
ï¬ å®Œæˆå„ç¨®ç”¨ä¾†æ¸›å°‘æ¬Šé‡å‘é‡çš„æ–¹æ³•ä¹‹ç ”ç©¶ï¼ŒåŒ…æ‹¬ 
ï‚² å° training windowå¤§å°å°æ¬Šé‡å‘é‡è³‡æ–™é‡ï¼Œä»¥åŠå½±éŸ¿ç•«é¢å“è³ªä¹‹ç ”ç©¶ã€‚ 
å¯¦é©—çµæœé¡¯ç¤ºï¼Œåˆ©ç”¨å¤šé‡çš„ trainging windowå¤§å°ï¼Œç›¸è¼ƒæ–¼åŸå…ˆå›ºå®šå¤§å°çš„æ–¹
æ³•ï¼Œç¢ºå¯¦èƒ½å¤ å¾—åˆ°è¼ƒå¥½çš„å“è³ªã€‚ 
ï‚² å°æ¬Šé‡å‘é‡è¡¨ç¤ºæ³•çš„ç²¾ç¢ºåº¦åŠè³‡æ–™é‡ä¹‹ç ”ç©¶ã€‚ 
æˆ‘å€‘åœ¨ç·¨ç¢¼ç«¯å¾—åˆ° training window sizes mapï¼Œä¸¦åˆ©ç”¨ quad treeçš„çµæ§‹ä¾†é€²
è¡Œç·¨ç¢¼ã€‚ç”±æ–¼å¯ä»¥åœ¨ç·¨ç¢¼ç«¯ç²å¾—åŸå§‹è³‡æ–™ï¼Œå› æ­¤æˆ‘å€‘å¯ä»¥å¾—åˆ°æœ€ç²¾ç¢ºçš„å¤§å°ï¼Œ 
ï‚² å°æ–¼çµåˆå…¶ä»–æ–¹æ³•(å¦‚ FC, MCI)æ‰€èƒ½æ¸›å°‘çš„æ¬Šé‡å‘é‡è³‡æ–™é‡ä¹‹ç ”ç©¶ã€‚ 
æˆ‘å€‘å‚³é€ training window sizes mapç•¶ä½œ MDCçš„ redundant informationã€‚è€Œ
ç›¸è¼ƒæ–¼æ¬Šé‡å‘é‡è³‡æ–™ï¼Œtraining window sizes map çš„è³‡æ–™é‡æ˜¯é å°æ–¼çš„ã€‚è€Œ
ä¸” training window sizes mapä¹Ÿç›¸å°çš„è¼ƒå®¹æ˜“ç·¨ç¢¼ï¼Œè€Œå£“ç¸®æ•ˆç‡ä¹Ÿå¾ˆå¥½ã€‚ 
ï¬ å®Œæˆæ¤åŸºæ–¼ AR model çš„å¤šé‡æè¿°ç·¨ç¢¼æ³•(ç¨± AR-based MDC)ä¹‹ç ”ç©¶èˆ‡è¨­è¨ˆã€‚ 
æˆ‘å€‘åœ¨è§£ç¢¼ç«¯çš„éƒ¨åˆ†ä½¿ç”¨ training windows size mapåŠ å¼· MDCçš„å®¹éŒ¯èƒ½åŠ›ï¼Œç•¶æœ‰ frame
éºå¤±æ™‚ï¼Œæˆ‘å€‘å¯ä»¥è—‰ç”±å…¶ä»– descriptoræ”¶åˆ°çš„ç›¸é„° frameï¼Œé…åˆå°æ‡‰çš„ training window 
sizes mapå’Œ STAR modelåšè£œæ­£ã€‚ 
ï¬ å®Œæˆå°æœ¬è¨ˆç•«æ‰€è¨­è¨ˆ AR-based MDCçš„å¯¦ä½œã€‚ 
æˆ‘å€‘å°‡å‚³é€ weight factor æ”¹æˆ training window sizes mapï¼Œé€™æ¨£å¯ä»¥ä½¿è³‡æ–™é‡è®Šå°ï¼Œ
ä¹Ÿæœ‰ç›Šæ–¼å£“ç¸®ï¼Œå“è³ªä¹Ÿèƒ½ç¶­æŒã€‚æˆ‘å€‘é‚„æ˜¯èƒ½ç²å¾—é æœŸçš„çµæœã€‚ 
ï¬ å®Œæˆå°å…¶å®ƒ MDCæ–¹æ³•çš„å¯¦ä½œï¼Œä»¥åŠå°å„ç¨® MDCæ–¹æ³•çš„å¯¦é©—(ä¸åŒè¦–è¨Šè³‡æ–™ã€å°åŒ…éºå¤±ç‡ã€
å°åŒ… loss patternç­‰) èˆ‡æ•ˆèƒ½çš„åˆ†ææ¯”è¼ƒã€‚æˆ‘å€‘åˆ©ç”¨æ¨¡æ“¬çš„æ–¹æ³•ä¾†é€²è¡Œå¯¦é©—ï¼Œä½¿ç”¨äº† 4
ç¨®æ€§è³ªä¸åŒçš„è¦–è¨Šè³‡æ–™ï¼Œä¹Ÿæ¡ç”¨äº† 4ç¨®ä¸åŒçš„å°åŒ…éºå¤±ç‡å’Œå¤šç¨® loss patternï¼Œè€Œå¯¦é©—
çµæœä¹Ÿé¡¯ç¤ºï¼Œç„¡è«–åœ¨ç”šéº¼æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘ç¢ºå¯¦çš„åŠ å¼·äº† MDC çš„å“è³ªå’Œå®¹éŒ¯èƒ½åŠ›ã€‚æœ‰æ•ˆçš„çµ
åˆäº† AR-Modelå’Œ MDCã€‚ 
ï¬ è«–æ–‡ç™¼è¡¨: æœ¬è¨ˆç•«æœ‰è¡ç”Ÿå‡ºå¦ä¸€ç ”ç©¶æˆæœå·²ç™¼è¡¨æ–¼é‡è¦ä¹‹åœ‹éš›æœƒè­°ICIP2013ä¸­ï¼Œè«–æ–‡é¡Œ
ç›®ç‚º :â€œError resilient coding using multiple reference framesâ€ã€‚å…¶ä»–æˆæœå‰‡
å°šåœ¨æ’°å¯«ä¸­ã€‚ 
 49 
 ä¸€ã€åƒåŠ æœƒè­°ç¶“é 
ç”±æ–¼æœ¬äººç›®å‰é›¢æ ¡ç ”ä¿®ï¼Œåœ¨ç¾åœ‹èˆ‡ USCå¤§å­¸çš„æ•™æˆåšè¦–è¨Šæ–¹é¢ç ”ç©¶ï¼Œå› æ­¤ï¼Œæ­¤æ¬¡
å‡ºå¸­åœ¨å¢¨çˆ¾æœ¬çš„æœƒè­°æ˜¯å¾ç¾åœ‹åŠ å·å‡ºç™¼ï¼Œé£›è¡Œäº† 16å°æ™‚æ‰åˆ°é”ã€‚ ç”±æ–¼ç¶“è²»ä¸è¶³,æœ¬
äººæ²’æœ‰åƒåŠ æœƒè­°ç¬¬ä¸€å¤©çš„ä»»ä½• Tutorialã€‚ å› æ­¤ï¼Œæ‰€æ­ä¹˜çš„é£›æ©Ÿæ˜¯åœ¨æœƒè­°çš„ç¬¬äºŒå¤©(9/16)
ä¸€æ—©æ‰æŠµé”å¢¨çˆ¾æœ¬ï¼Œä¸¦ç›´æ¥å‰å¾€æœƒå ´è¨»å†ŠåƒåŠ  technical programã€‚ 
    æ­¤æ¬¡çš„æœƒè­°æœ‰å…«å ´ lecture sessions åœ¨äºŒæ¨“ä¸åŒæœƒè­°å®¤åŒæ™‚é€²è¡Œï¼ŒåŒæ™‚é–“ä¹Ÿæœ‰å…«å€‹ 
poster sessions é›†ä¸­åœ¨ä¸€æ¨“å¤§å»³é€²è¡Œã€‚å› æ­¤ï¼Œåªèƒ½å¾å…¶ä¸­æŒ‘é¸æœ‰èˆˆè¶£çš„ä¸»é¡ŒåƒåŠ ã€‚æˆ‘
ä¸»è¦åƒåŠ çš„ HEVC1ã€Error resilient video transmissionã€Video quality assessmentç­‰å ´æ¬¡
çš„ lecture sessionï¼Œä»¥åŠå¤§å¤šæ•¸çš„ post sessionã€‚å› ç‚ºå…«å€‹ post sessions é›†ä¸­åœ¨ä¸€èµ·ï¼Œå¯
ä»¥åœ¨è¼ƒçŸ­æ™‚é–“å…§å¿«é€Ÿç€è¦½çœ¾å¤šç¯‡è«–æ–‡ï¼Œä¸”å¯ä»¥å’Œä½œè€…é¢å°é¢è¨è«–ï¼Œå› æ­¤ï¼Œæœ¬äººèŠ±è¼ƒå¤š
çš„æ™‚é–“åœ¨ post sessionsã€‚æ­¤æ¬¡æœƒè­°å…±æ¥å—äº†ä¹ç™¾å¤šç¯‡è«–æ–‡ï¼Œ posterå äº†ä¸€åŠï¼Œæ•¸é‡å¾ˆ
å¤šã€‚é™¤äº†å¿«é€Ÿç€è¦½æ¯ç¯‡è«–æ–‡å¤–ï¼Œæœ¬äººå°æ–¼ quality assessment ã€visual saliency 
modelingã€high- dimensional video codingã€3D video codingã€HEVC é€™äº›æ–¹é¢çš„ papers
æœƒèŠ±è¼ƒå¤šæ™‚é–“å»äº†è§£ä½œè€…çš„æƒ³æ³•ã€‚å°æ–¼æœ‰èˆˆè¶£ä½†æ²’ç ”ç©¶éçš„ä¸»é¡Œï¼Œé€éè·Ÿä½œè€…çš„è¨è«–
èƒ½å¿«é€Ÿå°æ­¤é ˜åŸŸæœ‰æ‰€ç­è§£ã€‚ 
    æœ¬äººæ‰€ç™¼è¡¨çš„è«–æ–‡æ˜¯åœ¨æœƒè­°ç¬¬ä¸‰å¤©ä¸‹åˆçš„ post sessionï¼Œä¾†ç€è¦½å¾Œæœ‰èˆˆè¶£ä¸¦èˆ‡æœ¬
äººè¨è«–çš„ç´„åä¾†å€‹äººï¼Œä¸ç®—å¤šï¼Œå› æ­¤æˆ‘å€‘æœ‰å……è£•çš„æ™‚é–“äº¤æ›æ„è¦‹ã€‚æœ‰äººè¦ºå¾—ä»–æ‰€åšçš„
æŸæ–¹é¢ç ”ç©¶å¯ä»¥çµåˆæœ¬äººçš„æ–¹æ³•è©¦è©¦ï¼Œå› æ­¤çµ¦äº†äº›å»ºè­°ï¼›ä¹Ÿæœ‰äººæåˆ°ä»–è®€éæœ¬äººå…¶ä»–
å¤šç¯‡è«–æ–‡ï¼Œå¾ˆé«˜èˆˆå¯è·Ÿæˆ‘è¨è«–ï¼›å¤§éƒ¨åˆ†çš„äººå‰‡æ˜¯é‡å°æœ¬äººè«–æ–‡ä¸­æ‰€æçš„æ–¹æ³•åŠå¯¦é©—ï¼Œ
è©¢å•æ›´è©³ç´°çš„ä½œæ³•ã€‚ 
 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
ç”±æ–¼æœ€è¿‘å°æ–°çš„é ˜åŸŸ visual quality assessmentæœ‰ç ”ç©¶èˆˆè¶£ï¼Œå°æ–¼è©²é ˜åŸŸç›¸é—œçš„èƒŒæ™¯çŸ¥
è­˜å¸Œæœ›èƒ½è¿…é€Ÿç´¯ç©ï¼Œè€Œæ­¤æ¬¡åƒåŠ ç ”è¨æœƒå‰›å¥½æ˜¯æœ€ä½³æ™‚æ©Ÿã€‚åœ¨ post sessionä¸­ï¼Œæœ‰å¾ˆå¤šç¯‡
ç›¸é—œçš„è«–æ–‡ï¼Œè€Œé€éè·Ÿä½œè€…é¢å°é¢çš„è¨è«–ï¼Œæ›´è®“æˆ‘èƒ½å¿«é€ŸæŒæ¡è«–æ–‡ä¸­çš„é—œéµæŠ€è¡“æˆ–é‡
è¦è§€å¿µï¼Œå°æ–¼ä¸€å€‹æ–°é ˜åŸŸç›¸é—œçŸ¥è­˜çš„è¿…é€Ÿç´¯ç©ï¼Œè¦ºå¾—é€™æ˜¯æœ¬æ¬¡æœ€å¤§çš„æ”¶ç©«ã€‚ 
 
ä¸‰ã€è€ƒå¯Ÿåƒè§€æ´»å‹•(ç„¡æ˜¯é …æ´»å‹•è€…çœç•¥) 
ç„¡ 
 
å››ã€å»ºè­° 
å»ºè­°æ ¸ç™¼å¤šä¸€äº›å‡ºå¸­åœ‹éš›æœƒè­°ä¹‹ç¶“è²»ã€‚ç›®å‰æ ¸ç™¼çš„ç¶“è²»é€£åƒèˆ‡ä¸€æ¬¡åœ‹éš›æœƒè­°(ä¸”åª
å¾…ä¸‰å¤©)éƒ½ä¸å¤ ï¼Œå› æ­¤é€£ Tutorialéƒ½ç„¡æ³•å ±ååƒåŠ ï¼Œè¦ºå¾—æœ‰äº›å¯æƒœï¼Œå› ç‚ºæ­é£›æ©Ÿçš„
æ™‚é–“å¾ˆé•·ï¼Œç„¡æ³•åƒèˆ‡å®Œæ•´çš„æœƒè­°ï¼Œä¹Ÿç„¡æ³•å¤šç•™ä¸€å…©å¤©åƒè§€è©²åŸå¸‚ï¼Œè¦ºå¾—æœ‰é»éºæ†¾ã€‚     
 
äº”ã€æ”œå›è³‡æ–™åç¨±åŠå…§å®¹ 
  ä¸€æœ¬ ICIP2013 çš„ Program Guide.  
ä¸€å€‹ USB è£¡æœ‰æœƒè­°è«–æ–‡é›†çš„å…¨æ–‡é›»å­æª”ã€‚ 
 
 
 
å…­ã€å…¶ä»– 
 
 
è¡¨ Y04 
100ï¦ï¨å°ˆé¡Œç ”ç©¶è¨ˆç•«ç ”ç©¶æˆæœå½™æ•´è¡¨ 
è¨ˆç•«ä¸»æŒäººï¼šè”¡æ–‡éŒ¦ è¨ˆç•«ç·¨è™Ÿï¼š100-2628-E-009-026-MY2 
è¨ˆç•«åç¨±ï¼šç ”ç©¶è‡ªå‹•è¿´æ­¸æŠ€è¡“åœ¨æå‡å½±ç‰‡æ’­æ”¾é€Ÿï¥¡èˆ‡å¤šé‡æè¿°è¦–è¨Šç·¨ç¢¼ä¸Šä¹‹æ‡‰ç”¨ 
ï¥¾åŒ– 
æˆæœé …ç›® å¯¦éš›å·²é”æˆ
ï¥©ï¼ˆè¢«æ¥å—
æˆ–å·²ç™¼è¡¨ï¼‰
é æœŸç¸½é”æˆ
ï¥©(å«å¯¦éš›å·²
é”æˆï¥©) 
æœ¬è¨ˆç•«å¯¦
éš›è²¢ç»ç™¾
åˆ†æ¯” 
å–®ä½ 
å‚™ è¨» ï¼ˆ è³ª åŒ– ï¥¯
æ˜ï¼šå¦‚ï¥©å€‹è¨ˆç•«
å…±åŒæˆæœã€æˆæœ
ï¦œ ç‚º è©² æœŸ åˆŠ ä¹‹
å° é¢ æ•… äº‹ ...
ç­‰ï¼‰ 
æœŸåˆŠï¥æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒï¥æ–‡ 0 0 100% 
ç¯‡ 
 
ï¥æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100%   
ç”³è«‹ä¸­ä»¶ï¥© 0 0 100%  å°ˆï§ å·²ç²å¾—ä»¶ï¥© 0 0 100% ä»¶  
ä»¶ï¥© 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šï§ï¤Š 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 7 8 88%  
åšå£«ç”Ÿ 2 0 200%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å…§ 
ï¥«èˆ‡è¨ˆç•«äººï¦Š 
ï¼ˆæœ¬åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ï§¤ 0 0 100% 
äººæ¬¡ 
 
æœŸåˆŠï¥æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒï¥æ–‡ 2 2 100% 
ç¯‡ one in ICIP2012, 
and one in 
ICIP2013 
ï¥æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100% ç« /æœ¬  
ç”³è«‹ä¸­ä»¶ï¥© 0 0 100%  å°ˆï§ å·²ç²å¾—ä»¶ï¥© 0 0 100% ä»¶  
ä»¶ï¥© 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šï§ï¤Š 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å¤– 
ï¥«èˆ‡è¨ˆç•«äººï¦Š 
ï¼ˆå¤–åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ï§¤ 0 0 100% 
äººæ¬¡ 
 
