 1 
ï€  
Abstractâ€” Given the rapid expansion of car ownership 
worldwide, vehicle safety is an increasingly critical issue in the 
automobile industry. The reduced cost of cameras and optical 
devices has made it economically feasible to deploy front-mounted 
intelligent systems for visual-based event detection for forward 
collision avoidance and mitigation. While driving at night, vehicles 
in front are generally visible by their tail and brake lights. The 
brake lights are particularly important because they signal 
deceleration and potential collision. Therefore, in this project, we 
propose a novel visual-based approach, based on the Nakagami-m 
distribution, for detecting brake lights at night by analyzing the 
tail lights. Rather than using the knowledge of the heuristic 
features, such as the symmetry, position and size of the rear-facing 
vehicle, we focus on finding the invariant features to model brake 
light scattering by Nakagami imaging and therefore conduct the 
detection process in a part-based manner. Experiments on an 
extensive dataset show that our proposed system can effectively 
detect vehicle braking under different lighting and traffic 
conditions, and thus demonstrates its feasibility in real-world 
environments. 
 
Index Termsâ€”Brake light detection, Nakagami imaging 
 
I. INTRODUCTION 
IVEN the rapid expansion of car ownership worldwide, 
vehicle safety is an increasingly critical issue in the 
automobile industry. The reduced cost of cameras and 
optical devices has made it economically feasible to deploy 
front-mounted intelligent systems for visual-based event 
detection for forward collision avoidance and mitigation. While 
driving at night, vehicles in front are generally visible by their 
tail and brake lights. The brake lights are particularly important 
because they signal deceleration and potential collision. 
In related works, researchers have focused on detecting 
vehicles [1-4], traffic lights [5-7], tail lights of vehicles in front 
[8-10] and potential collisions at night [11]. To detect vehicles 
at night, Chen et al. [1] performed bright object segmentation 
and verified the target segmented regions by spatial clustering 
based on shape, texture, and relative position, i.e., the 
symmetric properties. Color is used for detecting most tail lights, 
and allows for vehicles to be identified by a rule-based 
approach.  
 
 
 
 
  Park and Jeong [5] used color features to extract candidate 
regions, using statistical features to select candidate pixels from 
signal lights from grabbed images. The remaining regions are 
examined for circular shapes since many light sources are 
generally perceived as being circular in shape. In [8], Oâ€™Malley 
et al. proposed a novel image processing system to detect and 
track vehicle rear-lamp pairs in forward-facing color video. 
First, they suggested a camera-configuration process to 
optimize the appearance of rear lamps for segmentation. 
Rear-facing lamps are segmented from low-exposure 
forward-facing color video using a red-color threshold. The 
color threshold is directly derived from automotive regulations 
and adapted for real-world conditions in the 
hueâ€“saturationâ€“value (HSV) color space. Lamps are then paired 
using color cross-correlation symmetry analysis and tracked 
using Kalman filtering. Due to the symmetrical characteristic of 
tail lights, tail lights detection is difficult to perform in a 
part-based manner.  
  In [11], Thammakaroon and Tangamchit developed a 
predictive brake warning approach based on distance estimated 
from analysis of tail lights in front. To localize the regions of 
interest, the red color of the rings is detected based on the RGB 
color model. Brake light distribution was calculated from 50 
collected samples to determine the threshold for each color 
channel, and three candidate Regions of Interest (ROIs) from 
RGB channels were verified by computing the intersection. The 
intersected ROI(s) are regarded as the tail lights and their size is 
used to determine the distance between the vehicle in front and 
the camera. In [6], Yelal et al. proposed detecting signal lights 
by using the luminous values of the glowing points combined 
with color values present in the image frame as features for 
real-time analysis. Lab color models were used to extract the 
luminous values in the image frame, and contour tracking was 
used to detect the shape of the signal lights. The image frames 
extracted from the real time video were subjected to regional 
segmentation to extract the ROI by analyzing the position of the 
signal lights in an image frame based on road tracking. This 
road tracking technique uses the edges of a road as reference 
lines, allowing the image frame to be segmented into different 
regions. 
  In [7], Gong et al. proposed an approach for the recognition 
and tracking of traffic lights. First, the candidate region of the 
traffic light is extracted using the threshold segmentation 
method and the morphological operation. Then, the recognition 
algorithm of the traffic light is employed based on machine 
Nighttime Brake-Light Detection by 
Nakagami Imaging 
Duan-Yu Chen, Yu-Hao Lin and Yang-Jie Peng 
G 
 3 
Prior to model scattering property, a simple step function )(uT  
is first applied to the color intensity image 
iC  to reduce the 
noise generated from non-tail lights, and is defined as 
ïƒ®
ïƒ­
ïƒ¬ ï‚³
ï€½
otherwise
uif
uT
u
,0
,1
)(
ï± .                          (2) 
Using Eq.(2), the color intensity image 
iC  is filtered by 
)( iiT CTCU ï‚´ï€½ ,                                    (3) 
Fig.2(c) shows the image TU  obtained by Eq.(3). It is 
obvious that preprocessing using the step function can raise the 
contrast in the intensity image iC , especially the contrast 
between the tail light regions and the background. 
 
 
Fig.2. (a) Original input image; (b) color intensity image 
iC ; (c) 
resulted image 
TU  after preprocessing using Eq.(3). 
 
The color intensity distribution in Fig.3 shows that we can 
approximately separate the tail lights from non-tail lights. 
However, the distributions of brake lights and tail lights are 
highly similar and thus difficult to distinguish. To overcome this 
problem, we propose an approach that can discriminate brake 
lights from tail lights based on Nakagami imaging. Section 2.2 
introduces the modeling of tail lights by Nakagami distribution 
in detail. 
 
 
Fig.3. Color intensity histogram of tail light (red), brake light (blue) 
and non-tail light (green). 
B. Modeling Tail Lights by Nakagami Distribution 
The Nakagami statistical model was initially used to analyze 
returned radar echoes and be applied to ultrasound by using two 
associated parameters, namely the m-parameter and the scaling 
parameter [13-15]. The Nakagami statistical model is briefly 
introduced as follows. The probability density function of the 
Nakagami distribution is defined as 
0,exp
)(
2
)( 2
12
ï‚³ïƒ·
ïƒ¸
ïƒ¶
ïƒ§
ïƒ¨
ïƒ¦
ï—
ï€­
ï—ï‡
ï€½
ï€­
rr
m
m
rm
rf
m
mm
,               (4) 
where )(ïƒ—ï‡  is the gamma function. The symbol r  indicates 
possible values of the random variable R  in the region of 
interest. The scaling parameter ï—  and the Nakagami parameter 
m  associated with the Nakagami distribution can be obtained 
respectively from Eqs.(5) and (6). 
)( 2REï€½ï— ,                                         (5) 
])[( 22
2
ï—ï€­
ï—
ï€½
RE
m ,                                   (6) 
where )(ïƒ—E  denotes the statistical mean. The Nakagami 
parameter m is a shape parameter determined by the probability 
distribution function of the random variable R . 
From our collected 500 images of tail lights, the statistics of 
the variable R  change from a pre-Rayleigh distribution to a 
Rayleigh distribution as m varies from 0 to 1, and is a 
post-Rayleigh distribution when m is larger than 1, as shown in 
Fig.4. This property makes the Nakagami distribution an 
appropriate model for modeling the variable R  of the ROI in 
the image 
TU  obtained by Eq.(3). 
 
 
Fig.4. Nakagami distributions obtained using different Nakagami 
parameters. 
 
  Therefore, to detect one or more brake lights present in the 
image 
TU , the Nakagami imaging can be employed. The 
Nakagami image is obtained based on the Nakagami parameter 
map, and thus can be obtained by Eq.(6). However, a scanning 
window 
nW  with size nÃ—n pixels should be defined first to 
estimate the local Nakagami parameter Lm , which is assigned 
as the new pixel located in the center of the window. Therefore, 
the variable R is the pixel set of the ROI selected by the Wn, i.e., 
2,...,2,1, niRpi ï€½ïƒ , where ip  is the pixel in the square window 
nW . Based on sliding the window Wn in the image TU , each 
local Nakagami parameter 
Lm  in regions of interest is 
computed, thus obtaining the Nakagami image. The process of 
Nakagami imaging is demonstrated in Fig.5. 
 
(a) (b) (c) 
 5 
the Nakagami image is relatively high; meanwhile, the region 
covering the part of the tail light and the scattering regions is 
highly similar in value to the Nakagami image. Fig.8(b) shows 
that mL correlates positively to the degree of similarity.  
Therefore, the detection of the brake lights can be conducted 
by using the value of the density 
RD  and the consistency rS  
obtained from Nakagami image. However, to reliably detect 
brake lights, the threshold for these measures should be 
determined adaptively, and is described as follows. 
C. Adaptive Decision Making for Brake Light Detection 
To adaptively determine the threshold, a randomly generated 
dataset is used to simulate brake lights behavior. Each data set 
consists of nÃ—n random values 2,...,2,1, niRp rri ï€½ïƒ , where 
10 ï‚£ï‚£ rip . To simulate brake lights behavior, we set rip  such 
that 
RD â‰¥ 93%. After using the data set to model the tail lights 
by the Nakagami distribution, the mean 
Lm  with its standard 
deviation is shown in Fig.9, which shows the distribution for the 
local Nakagami parameter 
Lm  when given various sizes of the 
scanning window Wn. As Wn increases, the mean value stabilizes 
while the standard deviation among the estimated 
Lm  in the 
dataset decreases. 
 
 
Fig. 9. Statistics for 
Lm  using a randomly generated dataset with 
2n  
number of random variables. 
 
From Fig.9, the ideal size of scanning window Wn is between 
17 and 19. Furthermore, the varied distance between the tail 
lights in front and the camera is also critical since the tail light 
signal would attenuate with increased distance. Therefore, the 
threshold 
MT  used to detect the brake lights should adapt to 
distance. The distance between the tail lights and the camera can 
be approximately estimated based on the computation of the 
vanishing point by lane line detection [16]. The horizontal line 
going through the vanishing point, as shown in Fig.10, is thus 
used to evaluate the distance to the tail lights in front.  
 
 
Fig.10. Red dot denotes the vanishing point and the horizontal line 
going through the point is used for distance evaluation. 
 
However, at night the lane lines are usually too indistinct, as 
demonstrated in Fig.11(a). We thus enhance the specific edge 
directions by 
255)(
255)(
)(
,255
),(
,0
ï€¾ï€«
ï‚£ï€«ï‚£
ï€¼ï€«
ïƒ¯
ïƒ®
ïƒ¯
ïƒ­
ïƒ¬
ï€«ï€½
yx
yx
yx
yxedge
GGif
GGif
GGif
GGE ï±
ï­
,             (10) 
where 
xG  and yG  are the horizontal and vertical gradients 
calculated by the gray scale image and ï­  is a threshold. An 
example of the enhanced intensity image is shown in Fig.11(b) 
which indicates that the lane lines can be enhanced using 
Eq.(10). For lane line detection, the Hough Transformation [17] 
is employed, with the detection result demonstrated in 
Fig.11(c).  
 
   
(a)        (b)               (c) 
Fig. 11. (a) Original video image; (b) enhanced intensity image 
edgeE  
using Eq.(10); (c) result of the detection of the vanishing point and 
lane lines. 
 
Finally, to adaptively determine the threshold 
MT  according 
to the evaluated distance, a set of 500 tail lights is collected at 
distinct distances between the vehicles and the camera and used 
to approximate the signal variations using curve fitting [18-19].  
Sometimes, the chosen curve passes through the data points, but 
at other points the curve approaches the data points without 
passing through them. In most cases, we use the least square 
curve fitting to minimize the square error of the data points. We 
can thus obtain an adaptive threshold 
MT  according to the 
distance 
disD  by 
ïƒ¯
ïƒ®
ïƒ¯
ïƒ­
ïƒ¬
ï€¼
ï‚³
ï€«ï€«ï€½
ddis
ddis
disdisM
LD
otherwise
HD
if
if
cbDaDT
,
,
,
5.1
20
2 ,                      (11) 
where the weighting set {a, b, c} is evaluated in the training 
dataset, and 
disD  is the distance between the candidate regions 
to the horizontal line. 
dH  and dL  are the upper and lower 
bounds of the distance, respectively. 
 7 
 
Fig. 13. Pink and blue curves denote histograms respectively obtained 
from the brake light and non-brake light datasets. The adaptive 
threshold 
MT  obtained by Eq.(11) can be used to successfully 
differentiate the brake lights from non-brake lights. 
 
Figure 14 shows some detected results for qualitative 
evaluations. The first column shows the original video frame, 
while the second column denotes the Nakagami image, and the 
third column introduces the criterion used to determine braking 
or non-braking. In this test sequence, the vehicles are further 
from the camera. From Fig.14(c), we can see that the value of 
Lm  would be larger than MT  when brake lights appear in the 
video frame. Examples can be observed from Fig.14(5.c) to 
Fig.14(10.c). In addition, the tail lights of non-braking vehicles 
and street lights have relatively smaller values of Nakagami 
parameter 
Lm  and, accordingly, would not be regarded as brake 
lights. 
 
(1)    
(2)    
(3)    
(4)    
(5)    
(6)    
(7)    
(8)    
(9)    
(10)    
(a)              (b)             (c) 
Fig. 14. (a) Frames shown from the status of non-braking to braking; 
(b) Nakagami image (
Lm ); (c) histograms from non-braking to 
braking. 
 
Figure 15 shows a sequence in which the vehicle is at a greater 
distance from the camera. Braking events from Fig.15(a.7) to 
Fig.15(a.10) are all successfully detected. Many non-brake 
lights around the vehicle in the center are all correctly filtered 
out. 
(1)    
(2)    
 9 
non-braking to braking; (b) Nakagami image (
Lm ); (c) histograms 
from non-braking to braking. 
 
Figure 17 shows detection of brake lights under partially 
occluded conditions where the value of the local Nakagami 
parameter 
Lm  is still higher than the adaptive threshold MT  and 
thus the braking event can be detected. For the effect of 
attenuation on taillights, the reduced signal amplitude can affect 
the quality of the target produced. Therefore, to discuss the 
attenuation effects in taillights, we formulate the relationship 
between the value of 
Lm  and the distance from the camera to the 
vehicle. In Fig.18, the red curve denotes 
Lm  obtained from 
braking events and the green curve represents 
Lm  from 
non-braking events. When the distance from the camera to the 
vehicle in front exceeds 30 meters, it is difficult to discriminate 
brake-lights from non-brake lights in this signal level, thus 
increasing the difficulty of detecting the braking event. 
 
(1)    
(2)    
(3)    
(a)     (b)     (c) 
Fig. 17. Partially occluded tail lights: (a) Frames show braking; (b) 
Nakagami image (
Lm ); (c) histograms show that braking can be 
detected. 
 
 
Fig. 18. Relationship between 
Lm  and the distance from the vehicle to 
the camera. Differentiating tail lights of vehicles more than 30 meters 
away is still challenging during the detection of braking events. The 
red and green curves are the values of 
Lm  respectively obtained from 
braking events and non-braking events. 
 
B. Quantitative Performance Evaluation 
For performance comparison, although the proposed 
approach is the first to perform braking event detection, 
particularly in a part-based manner, the approach proposed in 
[11] is selected as a basis for comparison. In [11], the red color 
of the rings is detected using the RGB color model. This red 
color is regarded as the tail lights and their size is used to 
determine the distance between the vehicle and the camera. 
Figure 17 demonstrates the detection results. In Figs. 19(b) and 
19(d), the rings of the red light of the original frame are detected. 
However, the scattering property makes it difficult for the 
approach proposed in [11] to correctly gauge the distance since 
larger ring around the tail lights would appear when braking 
lights triggered. The quantitative evaluation of our proposed 
approach is shown in Table 2. The overall detection rate for 
braking events in real highway environments reached 87%, 
which is quite promising. Table 3 presents the detection rates 
obtained by using the method in [11], indicating the difficulty in 
differentiating between braking and non-braking lights based on 
the detection of rings of red lights.  
In addition, to evaluate the system performance under 
different road and traffic conditions, an extensive dataset 
demonstrated in Fig. 20 is tested. The detection rate and false 
alarm rate are presented in Table 4. With the color filter applied, 
the performance can be significantly improved. The system 
performs worse under urban area since some environmental 
lighting sources, such as street lights as demonstrated in 
Fig.20(a), would have similar characteristics under our 
regulations of red light colors and adaptive thresholding of 
Nakagami image. Considering the effect to the proposed system 
under distinct weathers, it is worth noting that under raining 
weather conditions (Figs. 20(f)-20(g)) our system still works 
well with above 80% detection rate. Comparing to the condition 
of wet road after rain, the performance degrades with relatively 
lower detection rate and higher false alarm rate. The sort of false 
alarms shown in Fig.21(a) is alerted since large scattering is 
detected from the tail light reflection on a wet road. However, 
under distinct traffic and road conditions including highway and 
urban areas, on average the detection rate achieves 79%, which 
means that the proposed system is practical for real world traffic 
environments.  
Considering the computation cost, the system has been 
implemented in mobile devices for evaluation as introduced in 
Fig. 22. The brake light is detected and is marked by a green 
enclosing bounding box, which is determined by the extent of a 
connected component covered by the scattering region of a tail 
light. Our system can perform brake light detection for a video 
frame in 0.132 seconds and the frame rate is about 8 fps on 
Android 3.0 operating system with 1GHz CPU. That means that 
the proposed system can be performed in near real-time on 
mobile devices. 
 11 
size of rear-facing vehicles, we focus on finding the invariant 
features modeling brake light scattering by Nakagami imaging, 
and are thus able to conduct detection process in a part-based 
manner. Experiments from an extensive dataset have shown that 
our proposed system can effectively detect vehicle braking 
under different lighting and traffic conditions, particularly with 
a detection rate of about 87% in highway and on average 79% in 
both highway and urban areas, thus demonstrating its feasibility 
in real-world environments. 
REFERENCES 
[1] Y. L. Chen, Y. H. Chen, C. J. Chen, and B. F. Wu, â€œNighttime vehicle 
detection for driver assistance and autonomous vehicles,â€ Proc. IEEE 
ICPR, pp. 687-690, 2006. 
[2] J. H. Park, C. S. Jeong, â€œReal-time Signal Light Detection,â€ Proc. 
International Conf. on Future Generation Communication and 
Networking Symposia, 2008. 
[3] R. Oâ€™Malley, E. Jones, M. Glavin, "Rear-Lamp Vehicle Detection and 
Tracking in Low Exposure Color Video for Night Conditions," IEEE 
Trans. on Intelligent Transportation Systems, vol. 11, no. 2, June 2010. 
[4] P. Thammakaroon and P. Tangamchit, â€œPredictive Brake Warning at 
Night using Taillight Characteristic,â€ Proc. IEEE International 
Symposium on Industrial Electronics, 2009. 
[5] M. R. Yelal, S. Sasi, G. R. Shaffer and A.K. Kumar, â€œColor-based Signal 
light Tracking in Real-time Video,â€ Proc. IEEE International Conference 
on Video and Signal Based Surveillance, 2006. 
[6] J. W. Gong, Y. H. Jiang, G. M. Xiong, C. H. Guan, G. Tao, and H. Y. 
Chen, â€œThe Recognition and Tracking of Traffic Lights Based on Color 
Segmentation and CAMSHIFT for Intelligent Vehicles,â€ Proc. IEEE 
Intelligent Vehicles Symposium, 2010. 
[7] T. Schamm, C. von Carlowitz, J. M. Zollner, â€œOn-Road Vehicle 
Detection During Dusk and at Night,â€ Proc. IEEE Intelligent Vehicles 
Symposium, 2010. 
[8] Z. Sun, G. Bebis, R. Miller, â€œMonocular Precrash Vehicle Detection: 
Features and Classifiers,â€ Proc. IEEE Transactions on Image Processing, 
2006. 
[9] S. Y. Kim, S. Y. Oh, J. K. Kang, Y. W. Ryu, K. S. Kim,   S. C. Park, 
and  K. H. Park, â€œFront and Rear Vehicle Detection and Tracking in the 
Day and Night Times Using Vision and Sonar Sensor Fusion,â€ Proc. 
IEEE/RSJ International Conference on Intelligent Robots and Systems, 
2005. 
[10] C. C. Wang, S. S. Huang, and L. C. Fu, â€œDriver Assistance System for 
Lane Detection and Vehicle Recognition with Night Vision,â€ Proc. 
IEEE/RSJ International Conference on Intelligent Robots and Systems, 
2005. 
[11]  J. C. Stover, â€œOptical Scattering: Measurement and Analysis,â€ SPIE 
Optical Engineering Press. ISBN 0-8194-1934-6, 1995. 
[12] M. Nakagami, â€œThe M-Distributionâ€“A General Formula of Intensity 
Distribution in Rapid Fading,â€ in Statistical Methods on Radio Wave 
Propagation, W. C. Hoffman, Ed. New York: Pergamon Press, 1960. 
[13] P. M. Shankar, â€œA General Statistical Model for Ultrasonic 
Backscattering from Tissues,â€ IEEE Trans. Ultrason. Ferroelec. Freq. 
Contr., vol. 47, pp. 727-736, 2000. 
[14] U. Charash, â€œReception through Nakagami fading multipath channels 
with random delays,â€ Proc. IEEE Transactions on Communications, vol. 
27, pp. 657-670, Apr. 1979. 
[15] S. P. Tseng, Y. S. Liao, C. H. Yeh and L. K. Huang, â€œA DSP-based Lane 
Recognition Method for the Lane Departure Warning System of Smart 
Vehicles,â€ Proc. International Conference on NSC, pp. 823-828, 2009. 
[16] B. M. John and N. Donald, â€œApplication of the Hough Transform to Lane 
detection and Following on High Speed Roads,â€ signal & system Group, 
Department of Computer Science, National University of Ireland, pp. 1-9, 
1999. 
[17] G. S. Xu, â€œSub-pixel Edge Detection Based on Curve Fitting,â€ Proc. IEEE 
International Conference on Information and Computing Science, 2009. 
[18] W. Shen, L. Wu and L. Tu, â€œA curve fitting based image segmentation 
method,â€ Proc. International Conference on Computer Science and 
Information Technology, 2009.  
[19] H. Takahashi, D. Ukishima, K. Kawamoto, K. Hirota, "A Study on 
Predicting Hazard Factors for Safe Driving," IEEE Trans. on Industrial 
Electronics, vol. 54, no. 2, pp. 781-789, April 2007. 
[20] H. Mori, M. Charkari, and T. Matsushita, â€œOn-line vehicle and pedestrian 
detections based on sign pattern,â€ IEEE Tran. on Industrial Electronics, 
Vol.41, No.4, 1994. 
 
 
Duan-Yu Chen received the BS degree in computer 
science and information engineering from National 
Chaio-Tung University, Taiwan in 1996, the MS degree in 
computer science from National Sun Yat-Sen University, 
Taiwan in 1998, and the Ph.D. degree in computer science 
and information engineering from National Chiao-Tung 
University, Taiwan in 2004. He was a Postdoctoral 
Research Fellow with Academia Sinica, Taipei, Taiwan 
during 2004-2008. He is currently an Assistant Professor 
of the Department of Electrical Engineering, Yuan Ze University, Taiwan. His 
research interests include computer vision, video signal processing, 
content-based video indexing and retrieval, and multimedia information 
system. He received the Young Scholar Research Award presented by Yuan Ze 
University, Taiwan 2012. 
 
Yu-Hao Lin received the BS degree in electronic 
engineering from Huafan University, Taiwan in 2009. He 
received the master degree in the Department of Electrical 
Engineering, Yuan Ze University, Taiwan in 2011. His 
research interests include computer vision, video signal 
processing, and video surveillance system. 
 
 
 
Yang-Jie Peng received the BS degree in electrical 
engineering from Yuan Ze University, Taiwan in 2011. 
He is now pursuing the master degree in the Department 
of Electrical Engineering, Yuan Ze University, Taiwan. 
His research interests include computer vision, video 
signal processing, and video surveillance system. 
Efficient Co-Salient Video Object Detection Based on Preattentive Processing 
 
Duan-Yu Chen and Chung-You Lin 
Department of Electrical Engineering, Yuan-Ze University, Taiwan 
dychen@saturn.yzu.edu.tw, s994633@mail.yzu.edu.tw 
 
 
ABSTRACT 
 
Automatic video annotation is a critical step for content- 
based video retrieval and browsing. Detecting the focus of 
interest such as co-occurring objects in video frames 
automatically can benefit the tedious manual labeling 
process. However, detecting the co-occurring objects that is 
visually salient in video sequences is a challenging task. In 
this paper, in order to detect co-salient video objects 
efficiently while maintaining the correctness, we first use 
the preattentive scheme to locate the co-saliency in a pair of 
video frames and then measure the similarity based on 
KL-divergence. Finally we improve the correctness of the 
matching across all video frames using our proposed 
filtering scheme. As a result, we are able to describe the 
significant parts of a video sequence based on the detection 
of co-occurring video objects. Our experiment results show 
that the proposed co-salient video objects modeling achieves 
high precision value about 85% and reveals its robustness 
and feasibility in video. 
 
Keywordsâ€” Co-saliency, video object 
 
 
1. INTRODUCTION 
 
Methods for annotating video content as well as related 
video retrieval techniques have attracted a great deal of 
attention in recent years. Since video content contains much 
richer information than other types of media, the annotated 
content can be used in many fields, such as video 
surveillance systems and entertainment applications. Among 
the different types of video annotation approaches, detecting 
visual salience in a video is considered a good way to 
understand the videoâ€™s content. Moreover, detecting visual 
saliency successfully can substantially reduce the 
computational complexity of the video annotation process. 
According to a study conducted by cognitive psychologists 
[17], the human visual system picks salient features from a 
scene. Psychologists believe this process emphasizes the 
salient parts of a scene and, at the same time, disregards 
irrelevant information. However, this raises the question: 
What parts of a scene should be considered â€œsalientâ€, 
especially for the scene in videos? To address this question, 
several visual saliency (or attention) models have been 
proposed in the last decade [1-5, 16, 18]. In videos, salient 
parts may depend on the contextual information in 
successive frames. Therefore, detecting the co-occurring 
objects across video frames would make the saliency 
process more specific. Therefore, in this work, co-salient 
video objects are the target that we aim to locate within each 
video clips.  
 
In the literature [6-9, 10-11], the co-saliency problem in 
an image pair might be solved using some unsupervised 
algorithms to find the common parts among all possible 
correspondences. Previous approaches in the detection of 
co-salient objects generally would first extract the features 
in the regions of interest, e.g., SIFT [12], and then perform 
feature matching accordingly. However, the computation 
cost makes the process unpractical to be conducted in 
videos.  
To speed up the process of the co-saliency computing in 
image pairs, Goldberger et al. [13] proposed two methods that 
approximats the KL-divergence between mixture densities. 
The first (match-based) method can be applied to any 
mixture density while the second (unscented) is tailored for 
mixtures of Gaussian densities. The efficiency and the 
performance of these methods were demonstrated on image 
retrieval tasks on a large database. In all the experiments 
conducted, the unscented approximation achieves the best 
results, results that are very close to large sample 
Monte-Carlo based ground truth. However, the KL-match 
based approximation is faster but less accurate than the 
unscented based method. 
Therefore, in this paper, in order to detect co-salient video 
objects efficiently while maintaining the correctness, we 
first use the preattentive scheme [3, 15-16] to locate the 
co-saliency in a pair of video frames and then measure the 
similarity based on KL-divergence, and finally improve the 
correctness of the matching across all video frames using 
our proposed filtering scheme. As a result, we are able to 
describe the significant parts of a video sequence based on 
the detection of co-occurring video objects.  
The remainder of this paper is organized as follows. In 
the next section, we introduce the proposed co-saliency 
modeling. Section 3 details the experimental results. Then, 
in Section 4, some concluding remarks are drawn.  
 
2. CO-SALIENT VIDEO OBJECTS DETECTION 
is covered by some patches with small value of LK c . In 
Fig.2(b), co-occurring video objects can be addressed in 
co-salient map using the proposed approach. However, to 
find major co-salient objects across consecutive frames, a 
measure that can indicate the degree of similarity between 
co-salient objects is needed. The detail of this measure is 
described as follows.  
2.4. Similarity Measure between Co-salient Objects 
For co-saliency object detection, )||( qpLK c  is used to 
select the patches that are co-salient in a frame pair. 
Considering a frame pair that is of certain co-saliency, 
)||( qpLK c would generally be similar to each other. 
Therefore, based on the difference of LK c  for each 
updating and the convergence rate, the similarity between 
co-salient objects can be estimated.  
To compute the difference between co-salient objects, we 
selectÄ¡Î² patches that are of the smaller )||( qpLK c  and the 
saliency reflected by the Î² patches is defined by 
Â¦ Â¹Â¸Â·Â©Â¨Â§  cÂ pii KLLKÎ²pKL eS 11 of aluessmallest v  the .     (6) 
Accordingly the distance between two co-salient objects is 
defined as 
   )()(, qSpSqpD KLKLKL  .                 (7) 
To model the difference of convergence rate between 
co-salient objects, the difference of LK c  in iteration t and 
t-1 is modeled by  Â¦Â¦  cc )1()(1 tptpptd iii LKLKS W ,           (8) 
where Ï„ is the number iterations. Notice that co-salient 
objects in a frame pair would have small value of Sd if they 
are similar in appearance. An example of computing 
similarity between co-salient objects is illustrated in Fig.2(c) 
and Fig.2(f). As the case of Fig.2(c), it is clear that real 
co-salient objects can be addressed using our measure and 
the distinct ones shown in Fig.2(f) can be eliminated.  
3. EXPERIMENTAL RESULTS 
 
To evaluate the performance of the proposed visual saliency 
model, we conducted experiments on different kinds of 
videos of size 320Ã—240 downloaded from Youtube. Some 
sample frames of the dataset are shown in Fig.3. In order to 
illustrate the qualitative evaluation of our proposed scheme, 
some detected co-salient objects are demonstrated. In 
Figs.3(b)(c), six most similar co-salient objects 
corresponding to source image are shown, i.e., from rank 1 
to rank 6. The co-salient map for each frame pair is also 
shown next to its original frame to indicate where the 
co-salient objects are located. We can see that frames with 
similar co-salient objects corresponding to the source frame 
are detected. Particularly in Fig.3(c), co-salient objects, the 
roof of the hotel, of distinct size across different video 
frames can be correctly detected.  
 
     (a) 
 
Source image 
 
      ; =25.77 
 
Rank1( =0; )    Rank2( =37.92; .55)
 
Rank3( =31.95; )  Rank4( =68.64; ) 
  
 
Rank5( =24.59; )  Rank6( =20.74; ) 
(b) 
 
Source image 
 
      ; =24.35 
 
Rank1( =0; )Rank2( =70.98; .10) 
 
 
Dear DMS 2011 Author Duan-Yu Chen: 
 
The DMS 2011 program committee is pleased to inform you that your paper is 
accepted as a REGULAR paper of the 2011 International Conference on 
Distributed Multimedia Systems (DMS 2011). All papers accepted for 
presentation in the DMS 2011 Conference will be published in the DMS 2011 Proceedings 
in one volume. Papers submitted to the conference and associated workshops are 
subject to the same rigorous review and therefore are equal in quality. The page limit 
of a REGULAR paper is SIX pages, but you can purchase two extra pages so 
that the absolute page limit is EIGHT pages. Your presentation time is 
TWENTY FIVE plus FIVE minutes for Q&A. A computer-controlled projector will be 
provided for your presentation. 
 
All the information related to paper submission and registration can be found at: 
http://www.ksi.edu/seke/dms11.html 
 
In particular, you will find the following information useful: 
(a) AUTHOR GUIDELINES/INSTRUCTIONS FOR PROCEEDINGS MANUSCRIPTS 
http://www.ksi.edu/seke/dms11author.html 
 
(b) DMS 2011 CONFERENCE ONLINE REGISTRATION FORM 
https://earth.ksi.edu/ConferenceRegistration/Default.aspx?id=9 
 
(c) KSI ONLINE COPYRIGHT FORM 
https://earth.ksi.edu/ConferenceRegistration/Default.aspx?id=9 
 
(d) LATEX STYLE FILE FOR LATEX USERS TO PREPARE PAPER 
http://www.ksi.edu/seke/sk00latex.html 
 
(e) FREQUENTLY ASKED QUESTIONS AND ANSWERS 
http://www.ksi.edu/seke/sk00faq.html 
 
The reviews of your paper will be attached to this letter. If you submitted 
several papers and wonder which ones are accepted and which ones not 
accepted, the review file contains such information. Please note the July 11, 
2011 deadline for electronic submission of papers and prepayment of 
registration fee. This is a firm, fixed deadline, and late papers may not be 
included in the proceedings. We are sorry that registration online, by fax or 
Processing 
 
1. Relevant to DMS 2011 
 
2. Paper Technically CORRECT 
 
3. Originality : Neutral 
 
4. Impact : Neutral 
 
5. Presentation : Neutral 
 
6. Overall rating : Neutral 
 
7. Review Confidence : 3 
 
8. This paper SHOULD NOT be considered for journal publication 
 
9. Paper Summary : 
This paper shows a method to detect co-salient video objects 
efficiently while maintaining the correctness. First, the method uses the 
preattentive scheme to locate the co-saliency in a pair of video 
frames. Second, it then measures the similarity based on 
KL-divergence. Finally it improves the correctness of the 
matching across all video frames using a proposed filtering 
scheme. 
 
 
10. Comments to the authors : 
 
The proposed method and the result of the experiment MAY BE 
interesting. However, I cannot judge whether the proposed 
method is practical or not. One reason is that the number of 
the data you used in the experiment was limited. Another reason 
is that the condition of the experiment is not clear. At least, 
write the type of CPU you used to measure the computational 
performance. This will help the reader judge the impact of "For 
the computation cost, in average we need 0.6348 sec for each 
2. Paper Technically CORRECT 
 
3. Originality : Neutral 
 
4. Impact : Accept 
 
5. Presentation : Accept 
 
6. Overall rating : Accept 
 
7. Review Confidence : 8 
 
8. This paper SHOULD be considered for journal publication 
 
9. Paper Summary : 
This paper describes a technique for matching salient objects across frames of a 
video with the idea of using the information in further steps of labeling. 
Measures taken from the literature are combined in a novel way. 
 
10. Comments to the authors : 
 
The results look promising, though I would have been interested in explanations 
for some of the odd-looking results (and more analysis overall besides just a 
percent correct). I am not sure whether 83% matching will be sufficient for the 
applications mentioned in the introduction. The paper is fairly well-written. 
If possible, avoid splitting up the source image from the results in the first 
result figure. 
 
-------------------------------------------------------------------------------- 
1. Relevant to DMS 2011 
 
2. Paper Technically CORRECT 
 
3. Originality : Weak Reject 
 
4. Impact : Weak Accept 
 
5. Presentation : Accept 
more analysis of the results (over the percentage). 
- The paper layout should be improved (for example, fig.3 is split over two pages). 
 
 
-------------------------------------------------------------------------------- 
 
100ï¦ï¨å°ˆé¡Œç ”ç©¶è¨ˆç•«ç ”ç©¶æˆæœå½™æ•´è¡¨ 
è¨ˆç•«ä¸»æŒäººï¼šé™³æ•¦è£• è¨ˆç•«ç·¨è™Ÿï¼š100-2221-E-155-074- 
è¨ˆç•«åç¨±ï¼šä»¥è¦–è¦ºç‚ºåŸºç¤ä¹‹æ™ºæ…§å‹å¤œé–“ï¤‚è¼›ç…ï¤‚äº‹ä»¶åµæ¸¬æŠ€è¡“ 
ï¥¾åŒ– 
æˆæœé …ç›® å¯¦éš›å·²é”æˆ
ï¥©ï¼ˆè¢«æ¥å—
æˆ–å·²ç™¼è¡¨ï¼‰
é æœŸç¸½é”æˆ
ï¥©(å«å¯¦éš›å·²
é”æˆï¥©) 
æœ¬è¨ˆç•«å¯¦
éš›è²¢ç»ç™¾
åˆ†æ¯” 
å–®ä½ 
å‚™ è¨» ï¼ˆ è³ª åŒ– ï¥¯
æ˜ï¼šå¦‚ï¥©å€‹è¨ˆç•«
å…±åŒæˆæœã€æˆæœ
ï¦œ ç‚º è©² æœŸ åˆŠ ä¹‹
å° é¢ æ•… äº‹ ...
ç­‰ï¼‰ 
æœŸåˆŠï¥æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒï¥æ–‡ 0 0 100% 
ç¯‡ 
 
ï¥æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100%   
ç”³è«‹ä¸­ä»¶ï¥© 0 0 100%  å°ˆï§ å·²ç²å¾—ä»¶ï¥© 0 0 100% ä»¶  
ä»¶ï¥© 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šï§ï¤Š 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å…§ 
ï¥«èˆ‡è¨ˆç•«äººï¦Š 
ï¼ˆæœ¬åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ï§¤ 0 0 100% 
äººæ¬¡ 
 
æœŸåˆŠï¥æ–‡ 3 2 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒï¥æ–‡ 3 2 100% 
ç¯‡ 
 
ï¥æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100% ç« /æœ¬  
ç”³è«‹ä¸­ä»¶ï¥© 0 0 100%  å°ˆï§ å·²ç²å¾—ä»¶ï¥© 0 0 100% ä»¶  
ä»¶ï¥© 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šï§ï¤Š 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 4 4 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å¤– 
ï¥«èˆ‡è¨ˆç•«äººï¦Š 
ï¼ˆå¤–åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ï§¤ 0 0 100% 
äººæ¬¡ 
 
