Z`Å
ÃŒÃŸ@~Â‚Ã³<\ÃÅ]P*Â»Â‘Ã€@Â†'ÂŒÂ”ÂŒbÂ°3@DÂºs,
AÂ¡Ã›D.@DÂº (PQCrypto 2008)I	Ã‘JÂ§Ã£n7FÃ•ÂºÃˆÂÃBÃ„
Ã—Ã?Â, &Ã†ÃPÃ3Ã²ÃÃÂ¼[Ã•SAGE ÅÃ£8Â›Ã™&Ã†J
@Ã FaugÃ¨re ÃÂ?, Ã´ÂµÃEyvÂ«Ã—PÃÃ™, F4-F5-XL Â‰Ã•Â°, PÂ°Ã±Ã’Â¼
Ã³` &Ã†Ã´"D%Â§ ÃÂ¸Ã , ÃªGÂ|| 1 `|Ã¬Ã` , Â¸Ã nVidia
GTX 295 ÅÂŒ 48 ÂÃ³ÃÃ«g]PyF2
n"C: %Â§ , 9Â2ÃÃ›DÃ™, Â‚Ã³Ã“Ã›., Å]Ã 
Abstract
During this year, I have conducted further investigation on system-solving involved in
cryptanalysis, including design and implementation. Some results had been published
at conferences like PQCrypto 2008. Some other results have been submitted to other
workshops, and/or being prepared/revised for submission.
Through a series of tests, we have compiled software that will be included into the
SAGE open source software project. We have also veriï¬ed the conjecture of FaugÃ¨re that
on generic systems our improvement of F4-F5-XL algorithms cannot escape exponential
time, but in this we have used special hardware such as Graphics Processing Units, and
today we can solve 48 cubic equations in 48 variables in under 1 hour on a single nVidia
GTX 295 video card in F2.
Keywords:, graphic processing units (GPU), multivariate public-key cryptosystems
(MPKC), algebraic cryptanalysis, system-solving
2
Fast Exhaustive Search for Quadratic Equations over F2
November 1, 2009
The question discussed here is the following: how to solve systems of u quadratic equations (f1, . . . fu)
in n variables (x1, . . . , xn) over F2 by exhaustive search?
Each equation can be seen as polynomial of total degree at most two over F2[x1, . . . , xn], and taken
together, the u polynomials can be considered as a function: f : Fn
2
â†’ F
u
2
.
Let us first note that there are exactly n(n + 1)/2 + 1 monomials of degree at most 2 in n variables.
Therefore, evaluating all the polynomials requires at most un(n + 1)/2 additions, and only n(n + 1)/2 + 1
products, if these are precomputed. Here, a â€œproductâ€ means the AND of two bits, and a â€œsumâ€ means the
XOR of two bits. So, naively evaluating all the possible values of the variables takes nearly:
u Â· n2/2 Â· 2n bit operations.
It must be noted that computing the ZDD here amounts to pre-computing the products, then computing
the sums. Storing the fiâ€™s requires u Â· n(n + 1)/2 bits of memory.
n u bits required bytes required bit ops required
32 32 16896 2112 247
48 48 56448 7056 265
52 52 71656 8957 269
56 56 89376 11172 273.5
60 60 109800 13725 278
64 64 133120 16640 284
Figure 1: performance figures of the naive method
1 Better Algorithms
Let us denote by (e1, . . . , en) the canonical basis of the vector space F
n
2
. We consider the derivative âˆ‚f
âˆ‚i
of
our vector of quadratic f over the i-th variable:
âˆ‚f
âˆ‚i
x 7â†’ f(x+ ei) + f(x)
If f is of total degree 2, then âˆ‚f
âˆ‚i
is an affine function. It is easy to isolate the constant part (which is a u-bit
vector), and we call it ci:
ci =
âˆ‚f
âˆ‚i
(0) = f(ei) + f(0)
Now, the function x 7â†’ âˆ‚f
âˆ‚i
(x)+f(ei)+f(0) is linear, and can be represented by a u-by-n matrix Mi. Observe
that for any vector x, we have:
f(x + ei) =
âˆ‚f
âˆ‚i
(x) + f(x)
1
n u bits required bytes required bit ops required
32 32 33792 4224 242
48 48 112896 14112 259
52 52 143312 17914 263.5
56 56 178752 22344 267.5
60 60 219600 27450 272
64 64 266240 33280 276
Fig. 2. performance figures of algorithm 2
According to this lemma, we denote by rk(j) = 2
kâˆ’1 + j Â· 2k (it follows that Î½2(rk(j)) = k). The
idea is to compute zrk(j+1) knowing zrk(j). Fortunately for us, it is easy to deduce xrk(j+1) from
xrk(j).
Lemma 2. xrk(j+1) = xrk(j) + ek + ek+Î½2(j).
So, all in all:
zrk(j+1) = Mk Â·
[
xrk(j) + ek + ek+Î½2(j)
]
= zrk(j) +Mk Â· ek +Mk Â· ek+Î½2(j)
This naturally leads to the definition of:
Î±k,â„“ = Mk Â· ek +Mk Â· ek+â„“
such that
zrk(j+1) = zrk(j) + Î±k,Î½2(j)
This leads to the definition of algorithm 3. Only 3n bit operations (XOR) are required per
iteration, which is n2/6 times better than the naive method, but the bookkeeping overhead is a bit
more important (need to maintain the j[k] counters, and to evaluate Î½2 twice per iteration). The
memory requirements are the same than for algorithm 2, but they could be made smaller (using the
fact that k + â„“ â‰¤ n on line 10, so not all the Î±â€™s actually have to be stored).
Algorithm 3 An even faster enumeration
1: x0 â† 0
2: y0 â† f(0)
3: for k from 1 to n do
4: Initialize z[k]â† 0
5: Initialize j[k]â† 1
6: end for
7: for i from 1 to 2n do
8: k â† Î½2(i)
9: â„“â† Î½2(j[k])
10: z[k]â† z[k] + Î±k,â„“ {Here we have z[k] = Mk Â· xi}
11: yâ† y + z[k] + wk {Here we have y = f(xi)}
12: if y = 0 then
12: Note i carefully
13: end if
14: j[k]â† j[k] + 1
15: end for
3
Secure PRNGs from Specialized Polynomial Maps over Any Fq
Abstract. Berbain, Gilbert, and Patarin presented QUAD, a pseudo random number generator (PRNG)
at Eurocrypt 2006. QUAD (as PRNG and stream cipher) may be proved secure based on an interesting
hardness assumption about the one-wayness of multivariate quadratic polynomial systems over F2.
The original BGP proof only worked for F2 and left a gap to general Fq. We show that the result can
be generalized to any arbitrary ï¬nite ï¬eld Fq, and thus produces a stream cipher with alphabets in Fq.
Further, we generalize the underlying hardness assumption to specialized systems in Fq (including
F2) that can be evaluated more eï¬ƒciently. Barring breakthroughs in the current state-of-the-art for
system-solving, a rough implementation of a provably secure instance of our new PRNG is twice as fast
and takes 1/10 the storage of an instance of QUAD with the same level of provable security.
Recent results on specialization on security are also examined. And we conclude that our ideas are
consistent with these new developments and complement them. This gives a clue that we may build
secure primitives based on specialized polynomial maps which are more eï¬ƒcient.
Keywords: sparse multivariate polynomial map, PRNG, hash function, provable security
1 Introduction
Cryptographers have used multivariate polynomial maps for primitives since Matsumoto-Imai [26] but there
is a dearth of results proving security based on plausible hardness assumptions. Berbain, Gilbert and Patarin
presented a breakthrough in Eurocrypt 2006, when they proposed a PRNG/stream cipher that is provably
secure provided that the class of multivariate quadratic polynomials is probabilistically one way:
Class MQ(q, n,m): For given q, n,m, the classMQ(q, n,m) consists of all systems of m quadratic polyno-
mials in Fq with n variables. To choose a random system S fromMQ(q, n,m), we write each polynomial
Pk(x) as
âˆ‘
1â‰¤iâ‰¤jâ‰¤n aijkjxixj +
âˆ‘
1â‰¤iâ‰¤n bikxk + ck, where every aijk, bik, ck is chosen uniformly in Fq.
Solving S(x) = b for anyMQ system S is known as the multivariate quadratic problem.
It is often claimed that the NP-completeness of this problem [19] is the basis for multivariate public-key
cryptosystems. We could take instead Pi's to be polynomials of degree d instead of quadratic and get
the class of multivariate polynomial systems MP (q, d, n,m). This contains MQ(q, n,m) as a subset,
so solving arbitrary S(x) = b for anyMP system S would be no easier. However, it is not easy to base a
proof on worst-case hardness; the premise used in [7] is the following average-case hardness assumption:
Assumption MQ: Given any k and prime power q, For parameters n,m satisfying m/n = k + o(1), no
probabilistic polynomial-time algorithm can solve (in poly(n)-time) any ï¬xed Îµ > 0 proportion of systems
S drawn from MQ(q, n,m), and a vector b = (b1, b2, . . . , bm) drawn from S(Un), where Un is uniform
distribution over (Fq)
n
such that S(x) = b.
With this premise, [7, Theorem 4] proved the QUAD PRNG secure over F2. However, a looseness factor
in its security argument in the security proof means that provably secure QUAD instances over F2 are not yet
of practical speed. It also does not work for ï¬elds larger than F2. A similar result over any Fq is non-trivial
to prove, which we do here with diï¬€erent and more involved techniques. However, instances of QUAD with
the same-size state over larger ï¬elds are signiï¬cantly less secure [33]. To increase the diï¬ƒculty of solving
a system of nonlinear polynomial equations, we can plausibly change (a) the ï¬eld size q, (b) the number
of variables n, or (c) the degree d of the system (cf. [3, 4, 31]). Each costs time and space (for a reduction
from the MQ problem in Fq case to F2 case, see [30]). Even with a hardware implementation, an increase
in resource consumption is inevitable.
A logical next step is to combine all these approaches but ï¬nd polynomials that are easier to evaluate.
A natural candidate is sparsity in the chosen polynomials. To our knowledge, however, there are no prior
positive results for provable security of specialized polynomial systems, and speciï¬cally sparse ones.
So the questions we are trying to answer are:
 Aumasson-Meier (ICISC 2007) [1] shows that in some cases sparsity in primarily underdeï¬ned  more
variables than equations  systems leads to improved attacks. Results are very intresting and takes more
study but do not apply to overdetermined systems in general.
 Bard-Courtois-Jeï¬€erson [2] tests SAT solvers on uniformly sparse F2 equations, and gives numbers.
 Raddum-Samaev [27, 28] attacks clumped systems (even though the title says sparse). Similarly the
Courtois-Pieprzyk XSL attack [13] requires a lot of structures (i.e., clumping).
2 PRNG Based on Specialized Polynomial Map in F2
This section both provides a recap of past results and extends them to specialized maps over F2. We will
start with deï¬nitions and models, then give the key results on the provable security level.
Computational Distinguishability: Probability distributions D1 and D2 over a ï¬nite set â„¦ are compu-
tationally distinguishable with computing resources R and advantage Â² if there exist a probabilistic
algorithm A which on any input x âˆˆ â„¦ outputs answer 1 (accept) or 0 (reject) using computing resources
at most R and satisï¬es |PrxâˆˆD1 (A(x) = 1)âˆ’ PrxâˆˆD2 (A(x) = 1)| > Â². The above probabilities are not
only taken over x values distributed according to D1 or D2, but also over the random choices that are
used by algorithm A. Algorithm A is called a distinguisher with advantage Â².
If no such algorithm exists, then we say that D1 and D2 are computationally indistinguishable with
advantage Â². If R is not speciï¬ed, we implicitly mean feasible computing resources (e.g., < 280 simple
operations, and reasonable limits [usually polynomially many] in sampling from D1 and D2).
PRNG: Let n < L be two integers and K = Fq be a ï¬nite ï¬eld. The function G : K
n â†’ KL is said to be a
Pseudorandom Number Generator (PRNG) if the probability distribution of the random variable G(x),
where the vector x is uniformly random in Kn, is computationally indistinguishable (wirth distinguisher
resource R) from a uniformly random vector in KL. Usually q = 2 but it is not required.
Linear polynomial maps: A linear polynomial map R : (Fq)
n â†’ (Fq) means R(x) =
âˆ‘n
i=1 aixi, where
x = (x1, x2, . . . , xn), and x1, x2, . . . , xn are variables. If we give these variables values in Fq, by setting
(x1, x2, . . . , xn) = (b1, b2, . . . , bn) for bi âˆˆ Fq, denoted as b, then R(b) =
âˆ‘n
i=1 aibi is an element in Fq.
In the following sections, a random linear polynomial map (or form) has the coeï¬ƒcients ai's randomly
chosen from Fq. Also, when we mention R or R(x) refers to the function but when we write R(b), that
means the value of the function R with input vector b.
Instance from SMP (or MQ): If S is an instance drawn from SMP(q, d, n,m, (Î·2, . . . , Î·d)), then S(x) =
(P1(x), P2(x), . . . , Pm(x)) (x = (x1, x2, . . . , xn) are variables) is a function that maps (Fq)
n â†’ (Fq)
m
and each Pi(x) has the same probability distribution as that mentioned in section 1.2. For example, if
b = (b1, b2, . . . , bn) is a vector in (Fq)
n
, then S(b) = (P1(b), P2(b), . . . , Pm(b)), a value in (Fq)
m
.
Note: Heretofore we will also say SMP(n,m) for short, if no confusion is likely to ensue.
Given any PRNG, there is a standard way to stretch it into an old-fashioned stream cipher (Prop. 1), i.e.
stream cipher without IV and key setup. There are ways to set up an initial state securely, such as in Sec. 3.1.
Thus we concentrate our eï¬€orts on building a PRNG from anyMQ family of map from Fn2 â†’ F
m
2 ; in order,
we need to
1. show that if an instance S drawn fromMQ is not a PRNG, then for a (secretly) given vector b we can
predict, with the help of information from the value of S, S(b), and any linear form R, the value of R(b)
with strictly larger than 1/2 + Â² probability; then
2. use Goldreich-Levin theorem, which states that the value of any linear function R, R(b) is a hardcore bit
of any F
n
2 â†’ F
m
2 one-way function S, S(b), and R. I.e., being able to guess with strictly larger than 1/2+Â²
probability R(b) from S,S(b), and R means that we can invert S(b) with non-negligible probability.
3
2.2 Constructing a PRNG fromMQ (F2 Case)
Proposition 3 ([25], [7]) Suppose there is an algorithm B that given a system S(: Fn2 â†’ F
m
2 ) from
MQ(2, n,m), a random n-bit to one-bit linear form R and the image S(b) of a randomly chosen unknown
b, predicts R(b) with probability at least 12 + Â² over all possible inputs (S,S(b), R) using time T , then there
is an algorithm C that given S and the m-bit image S(b) of a randomly chosen n-bit vector b produces a
preimage of S(b) with probability (over all b and S) at least Â²/2 in time
T â€² =
8n2
Â²2
(
T + log
(
8n
Â²2
)
+
8n
Â²2
TS
)
Note: This is really the Goldreich-Levin theorem of which we omit the proof here. This essentially states
that linear forms are hard-core of any one-way function. In fact, the tighter form [7, Proof of Theorem 3]
(using a fast Walsh transform) can be simply followed word-for-word.
This above result (which only holds for F2) with Prop. 2 shows that any MQ family of maps induces
PRNGs over F2. To get a useful stream cipher, we can combine Props. 13:
Proposition 4 ([25], [7]) If S = (P,Q) is an instance drawn from MQ(2, n, n + r), where P : Fn2 â†’ F
r
2,
Q : Fn2 â†’ F
n
2 are the stream cipher as in Prop. 1, then if we can distinguish between Î» output blocks of the
stream cipher from truely random distribution in T time, we can ï¬nd b from S(b), where b is a randomly
chosen input, with probability at least
Â²
8Î» in time
T â€² =
27n2Î»2
Â²2
(
T + (Î»+ 2)TS + log
(
27nÎ»2
Â²2
)
+ 2
)
+
27nÎ»2
Â²2
TS (1)
Note: Roughly this means that if we let r = n, want to establish a safety level of 280 multiplications, want
L = Î»r = 240 bits between key refreshes, and can accept Â² = 10âˆ’2, then T â€² . 2230/n. All we need now is to
ï¬nd a map from F
n
2 â†’ F
2n
2 which takes this amount of time to invert.
As we see below, unless equation-solving improves greatly for sparse systems, this implies that a handful
of cubic terms added to a QUAD system with n = r = 208, q = 2 can be deemed secure to 280. There is no
sense in going any lower than that, because solving a system with n bit-variables can never take much more
eï¬€ort than 2n times whatever time it takes to evaluate one equation.
3 PRNG Based on SMP in Fq
In Proposition 3, [7] transformed the problem into a variation of Goldreich-Levin theorem (in F2). The
transformation still works in Fq; however, Goldreich-Levin theorm gets stuck in this place. Here we show a
way to extend the main results to Fq, by using a generalization of the Goldreich-Levin hard-core bit theorem.
Proposition 5 ([25] and our contribution) Let K = Fq. Suppose there is an algorithm B that given a
system S(: Kn â†’ Km) from SMP(n,m), a random Kn â†’ K linear form R and the image S(b) of a
randomly chosen unknown b, predicts R(b) with probability at least 1q +Â² over all possible inputs (S,S(b), R)
using time T , then there is an algorithm C that given S and the m-bit image S(b) of a randomly chosen
vector b âˆˆ Kn produces a preimage of S(b) with probability (over all b and S) at least Â²/2 in time
T â€² â‰¤ 210
(nq
Â²5
)
log2
(n
Â²
)
T +
(
1âˆ’
1
q
)2
Â²âˆ’2 TS
Remark [intuition of why argument in F2 cannot be applied in Fq]: If we know that one out of two exclusive
possibilities takes place with probability strictly larger than 50%, then the other one must happen strictly
less often 50%. If we know that one of q possibilities takes place with probability strictly greater than 1/q,
we cannot be sure that another possibility does not occur with even higher possibility. Therefore, we can
only treat this as a case of learning a linear functional with queries to a highly noisy oracle. Due to this
diï¬€erence, the order of Â² in T â€²/T is as high as Â²âˆ’5 in Prop. 5, but only Â²âˆ’2 in Prop. 3.
5
3.3 Intuition of Reconstructing Linear Polynomials
Now we are given some oracle accesses to a function f : Kn â†’ K, where K is a ï¬nite ï¬eld and |K| =
q. We need to ï¬nd all linear polynomials which match f with at least 1q + Â² fraction of inputs x. Let
p(x1, x2, . . . , xn) =
âˆ‘n
1 pixi, and i-th preï¬x of p is
âˆ‘i
1 pjxj . The algorithm runs n rounds, and in the i-th
round, it extends all possible candidates from the (iâˆ’ 1)-th round with all elements in K and screens them,
ï¬ltering out most bad preï¬xes. The pseudocode of the algorithm is presented in Algorithm 2. Since we
want the algorithm to be eï¬ƒcient, we must eï¬ƒciently screen possible preï¬xes from all extensions. We now
introduce a screening algorithm to be called TestPreï¬x.
Algorithm 1 TestPreï¬x(f ,Â²,n,(c1, c2 . . . , ci))[25]
Repeat poly1(
n
Â²
) times:
Pick s = si+1, . . . , sn âˆˆR GF(q)
Let t = poly2 (
n
Â²
)
for k = 1 to t do
Pick r = r1, r2 . . . , ri âˆˆR GF(q)
Ïƒ(k) = f(r, s)âˆ’
Pi
j=1 cjrj
end for
If there is Ïƒ(k) = Ïƒ for at least 1
q
+ Â²
3
fraction of the k's then ouput accept and halt
endRepeat
If all iterations were completed without accepting, then reject
Algorithm 2 Find All Polynomials(f, Â² )[25]
set a candidate queue Q[i] which stores all the candidates (c1, c2, c3, . . . , ci) in the i-th round
for i = 1 to n do
Pick all elements in Q[i]
TestPreï¬x(f ,Â²,n,(c1, c2 . . . , ci, Î±)) for all Î± âˆˆ F
If TestPreï¬x accepts, then push (c1, c2 . . . , ci, Î±) into Q[i + 1] i.e. it is a candidate in the (i + 1)-th round
end for
Supposed we are testing the i-th preï¬x (c1, c2, . . . , ci), we are going to evaluate the quantity of:
Ps(Ïƒ) := Pr
r1,r2...,riâˆˆK
ï£®
ï£°f(r, s) = iâˆ‘
j=1
cjrj + Ïƒ
ï£¹
ï£»
where r = (r1, r2, . . . , ri). The value of Ïƒ can be thought as a guess of
âˆ‘n
i+1 pjsj . For every s, we can estimate
the probability by a sample of several r's, and the error rate can be controlled by the times of sampling. If
such s makes the probability signiï¬cantly larger than 1/q, then we accept. If no such s exists, we reject. The
detailed algorithm is stated in the Algorithm 1: TestPreï¬x.
If a candidate (c1, c2, . . . , ci) passes through the Algorithm 1 for at least one suï¬ƒx s, there is a Ïƒ such
that the estimate of Ps(Ïƒ) is greater than
1
q +
Â²
3 . For a correct candidate (c1, c2, . . . , ci), i.e. (c1, c2, . . . , ci)
is the preï¬x of p = (p1, p2, . . . , pn) which matches f for at least
1
q + Â², and an arbitrary Ïƒ =
âˆ‘n
i+1 pjsj , it
satisï¬es that Es[Ps(Ïƒ)] â‰¥
1
q +Â². By Markov's inequality, for at least Â²/2 fraction of s and some corresponding
Ïƒ, it holds that Ps(Ïƒ) â‰¥
1
q +
Â²
2 . In Algorithm 1, we set
1
q +
Â²
3 as the passing criteria; thus the correct candidate
will pass though the Algorithm 1 with great probability. However, [21, Sec. 4] shows that the total passing
number of candidates in each round is limited. In fact, only a small number of candidates will pass the test.
This maximum (also given by [21, Sec. 4]) number of preï¬xes that pass the test is â‰¤ (1âˆ’ 1q )
2Â²âˆ’2.
7
4 On SMP under Generic Solvers
To verify that SMP (and SRQ of Appendix C) represent one way property, we need to show that
1. generic system-solvers do not run substantially faster on them; and
2. there are no specialized solvers that can take advantage of the sparsity.
Here generic means the ability to handle any multivariate polynomial system with n variables and m
equations in Fq. There are two well-known types of generic methods for solving polynomial systems, both
related to the original Buchberger's algorithm. One is FaugÃ¨re's F4-F5 and the other is XL-derivatives. In
the former, sparsity is quickly lost and tests show that there are little diï¬€erence in timing when solving
SMP (or SRQ) instances. With recent versions of XL [33], the sparsity results in a proportional decrease
in complexity. The eï¬€ect of sparsity on such generic methods should be predictable and not very drastic, as
shown by some testing (cf. Sec. 4.1). We brieï¬‚y describe what is known about XL and F4-F5 in Appendix B.
4.1 Testing the One-Wayness with Generic Solvers
We conducted numerous tests on SMP maps at various degrees and sparsity over the ï¬elds F2, F16, and
F256. For example, Table 1 lists our tests in solving randomMQ(256, n,m) instances where each polynomial
only has n quadratic terms [we call these instances SMQ(256, n,m, n)] with F4 over GF(256). It takes
almost the same time as solving anMQ instance of the same size.
mâˆ’ n DXL Dreg n = 9 n = 10 n = 11 n = 12 n = 13
0 2m m 6.03 46.69 350.38 3322.21 sigmem
1 m âŒˆm+1
2
âŒ‰ 1.19 8.91 53.64 413.34 2535.32
2 âŒˆm+1
2
âŒ‰ âŒˆm+2âˆ’
âˆš
m+2
2
âŒ‰ 0.31 2.20 12.40 88.09 436.10
Table 1. SMQ(256, n,m, n) timing (sec): MAGMA 2.12, 2GB RAM, Athlon64x2 2.2GHz
For XL variants that use sparse solvers as the last step [33] test results (one of which is shown in Table 2)
conï¬rms the natural guess: For SMP instances where the number of non-linear terms is not overly small,
the solution degree of XL is unchanged, and the speed naturally goes down as the number of terms, nearly
in direct proportion (in Tab. 2, should be close to n/4).
n 7 8 9 10 11 12 13
D 5 6 6 7 7 8 8
SMQ(256, n, n + 2, n) 9.34 Â· 10âˆ’2 1.17 Â· 100 4.04 Â· 100 6.02 Â· 101 1.51 Â· 102 2.34 Â· 103 5.97 Â· 103
MQ(256, n, n + 2) 2.06 Â· 10âˆ’1 2.92 Â· 100 1.10 Â· 10 1.81 Â· 102 4.94 Â· 102 8.20 Â· 103 2, 22 Â· 104
ratio 2.20 2.49 2.73 3.00 3.27 3.50 3.72
Table 2. XL/Wiedemann timing (sec) on Core2Quad 2.4GHz, icc, 4-thread OpenMP, 8GB RAM
For F2, there are many special optimizations made for F4 in MAGMA, so we ran tests at various densities
of quadratic terms in version 2.12-20 and 2.13-8. Typical results are given in Fig. 1 (cf. Appendix, samples
labelled sparse non-random are SRQ tests). Most of the time the data points are close to each other. In
some tests they overlap each other so closely that no diï¬€erence in the timing is seen in a diagram.
4.2 A Brief Discussion on Specialization and Security
Since generic system-solvers show no unexpected improvement on our specializations, it remains for us to
check that there are no other big improvements in solving specialized systems for. We list below what we
9
where Q has only quadratic or higher terms and is extremely sparse, we can consider P(x) = y as
Mx = (y+b)+ perturbation, and use known methods for decoding attacks, i.e., solving overdetermined
linear equations with perturbation. However, SMP maps with a moderate number of quadratic terms
will be intractible.
We note that other specialized polynomials can be constructed that are also easier to evaluate such as the
SRQ construction (cf. Appendix C) which also can carry through the same arguments as SMP, so our
process is more general than it looks.
5 Summary of Uses for Specialized Polynomial Systems
All information seems to point to the conclusion that we always use totally random linear terms, no matter
what else we do. With that taken into account, specialized random systems (such as SMP) represent
improvements over generic systems in terms of storage and (likely) speed.
5.1 The Secure Stream Ciphers SPELT
We build a stream cipher called SPELT(q, d, n, r, (Î·2, . . . , Î·d)), which resembles the construction in section 2:
We specify a prime power q (usually a power of 2), positive integers n and r, a degree d. We have update
function Qi = (Qi,1, Qi,2, . . . , Qi,n) : F
n
q â†’ F
n
q and output ï¬lter Pi = (Pi,1, Pi,2, . . . , Pi,r) : F
n
q â†’ F
r
q, for
i âˆˆ {0, 1}. We still do yn = P(xn) [output]; xn+1 = Q(xn) [transition], iterated according to the initial
vector. To repeat, every polynomial here is of degree d. Aï¬ƒne (constant and linear) term or coeï¬ƒcient are
still uniformly random. But terms of each degree are selected according to diï¬€erent densities of terms, such
that the degree-i terms are sparse to the point of having only Î·i terms. The diï¬€erence between Eq. 1
and Eq. 2, which governs the maximum provable security levels we can get, aï¬€ects our parameter choices
quite a bit, as seen below.
By Eq. 2, if L = Î»n lg q is the desired keystream length, the looseness factor T â€²/T is roughly
215q6(L/Â²)5
n4 lg5 q
lg2
(
2qL
Â² lg q
)
If we let q = 16, r = n, want a safety level of T = 280 multiplications, L = 240 bits between key refreshes,
and can accept Â² = 10âˆ’2, then T â€² . 2354/n4. We propose the following instances:
 SPELT using q = 16, d = 3 (cubics), n = r = 160, 20 quadratic and 15 cubic terms per equation. Projected
XL degree is 54, storage requirement is 2184 bytes. T â€² is about 2346 multiplications, which guarantees
& 288 multiplications security. This runs at 6875 cycles/byte.
 SPELT using d = 4 (quartics), n = r = 108, 20 quadratic, 15 cubic, and 10 quartic terms per equation.
Projected XL degree is 65, storage requirement is 2174 bytes. T â€² is about 2339 multiplications guaranteeing
& 281 multiplications security at a preliminary 5541 cycles/byte.
 SPELT using q = 2, n = r = 208, d = 3 (cubics), with 20 cubic terms each equation. Preliminary tests
achieve 11744 cycles/byte. The expected complexity for solving 208 variables and 416 equations is âˆ¼ 2224
(by brute-force trials, which is much faster than XL here), which translates to a 282 proven security level.
5.2 Comparisons: A Case for SPELT
All modern-day microprocessor are capable of doing 64-bit arithmetic at least, and there is a natural way
to implement QUAD that runs very fast over F2, limited only by the ability to stream data. However, as
number of variables goes up, the storage needed for QUAD goes up cubically, and for parameter choices that
are secure, the dataset overï¬‚ows even the massive caches of an Intel Core 2. That is what slows down
QUAD(2, 320, 320)  tests on a borrowed ia64 server shows that it is almost exactly the same speed as
the SPELT(2, 3, 208, 208, [480, 20]). Looking at the numbers, it seems that the idea of specializd polynomials
11
12. N. T. Courtois, A. Klimov, J. Patarin, and A. Shamir. Eï¬ƒcient algorithms for solving overdeï¬ned systems
of multivariate polynomial equations. In Advances in Cryptology  EUROCRYPT 2000, volume 1807 of
Lecture Notes in Computer Science, pages 392407. Bart Preneel, ed., Springer, 2000. Extended Version:
http://www.minrank.org/xlfull.pdf.
13. N. T. Courtois and J. Pieprzyk. Cryptanalysis of block ciphers with overdeï¬ned systems of equations. In Advances
in Cryptology  ASIACRYPT 2002, volume 2501 of Lecture Notes in Computer Science, pages 267287. Yuliang
Zheng, ed., Springer, 2002.
14. C. Diem. The XL-algorithm and a conjecture from commutative algebra. In Advances in Cryptology  ASIA-
CRYPT 2004, volume 3329 of Lecture Notes in Computer Science, pages 323337. Pil Joong Lee, ed., Springer,
2004. ISBN 3-540-23975-8.
15. J. Ding and B.-Y. Yang. Multivariate polynomials for hashing. In Inscrypt, Lecture Notes in Computer Science.
Springer, 2007. to appear, cf. http://eprint.iacr.org/2007/137.
16. R. R. Farashahi, B. Schoenmakers, and A. Sidorenko. Eï¬ƒcient pseudorandom generators based on the ddh
assumption. In Public Key Cryptography, pages 426441, 2007.
17. J.-C. FaugÃ¨re. A new eï¬ƒcient algorithm for computing GrÃ¶bner bases (F4). Journal of Pure and Applied Algebra,
139:6188, June 1999.
18. J.-C. FaugÃ¨re. A new eï¬ƒcient algorithm for computing GrÃ¶bner bases without reduction to zero (F5). In
International Symposium on Symbolic and Algebraic Computation  ISSAC 2002, pages 7583. ACM Press,
July 2002.
19. M. R. Garey and D. S. Johnson. Computers and Intractability  A Guide to the Theory of NP-Completeness.
W.H. Freeman and Company, 1979. ISBN 0-7167-1044-7 or 0-7167-1045-5.
20. R. Gennaro. An improved pseudo-random generator based on the discrete logarithm problem. Journal of
Cryptology, 18:91110, 2000.
21. O. Goldreich, R. Rubinfeld, and M. Sudan. Learning polynomials with queries: The highly noisy case. SIAM
Journal on Discrete Mathematics, 13(4):535570, Nov. 2000.
22. S. Jiang. Eï¬ƒcient primitives from exponentiation in z
p
. In L. M. Batten and R. Safavi-Naini, editors, ACISP,
volume 4058 of Lecture Notes in Computer Science, pages 259270. Springer, 2006.
23. N. Koblitz and A. Menezes. Another look at "provable security" (part 2). In R. Barua and T. Lange, editors,
INDOCRYPT, volume 4329 of Lecture Notes in Computer Science, pages 148175. Springer, 2006.
24. D. Lazard. GrÃ¶bner-bases, Gaussian elimination and resolution of systems of algebraic equations. In EUROCAL
83, volume 162 of Lecture Notes in Computer Science, pages 146156. Springer, March 1983.
25. L. Levin and O. Goldreich. A hard-core predicate for all one-way functions. In D. S. Johnson, editor, 21th ACM
Symposium on the Theory of Computing  STOC'89, pages 2532. ACM Press, 1989.
26. T. Matsumoto and H. Imai. Public quadratic polynomial-tuples for eï¬ƒcient signature veriï¬cation and message-
encryption. In Advances in Cryptology  EUROCRYPT 1988, volume 330 of Lecture Notes in Computer Science,
pages 419545. Christoph G. GÃ¼nther, ed., Springer, 1988.
27. H. Raddum and I. Semaev. New technique for solving sparse equation systems. Cryptology ePrint Archive,
Report 2006/475, 2006. http://eprint.iacr.org/.
28. I. Semaev. On solving sparse algebraic equations over ï¬nite ï¬elds (part ii). Cryptology ePrint Archive, Report
2007/280, 2007. http://eprint.iacr.org/.
29. R. Steinfeld, J. Pieprzyk, and H. Wang. On the provable security of an eï¬ƒcient rsa-based pseudorandom generator.
In X. Lai and K. Chen, editors, ASIACRYPT, volume 4284 of Lecture Notes in Computer Science, pages 194209.
Springer, 2006. formerly ePrint 2006/206.
30. C. Wolf. Multivariate Quadratic Polynomials in Public Key Cryptography. PhD thesis, Katholieke Universiteit
Leuven, 2005. http://eprint.iacr.org/2005/393.
31. B.-Y. Yang and J.-M. Chen. All in the XL family: Theory and practice. In ICISC 2004, volume 3506 of Lecture
Notes in Computer Science, pages 6786. Springer, 2004.
32. B.-Y. Yang and J.-M. Chen. Theoretical analysis of XL over small ï¬elds. In ACISP 2004, volume 3108 of Lecture
Notes in Computer Science, pages 277288. Springer, 2004.
33. B.-Y. Yang, O. C.-H. Chen, D. J. Bernstein, and J.-M. Chen. Analysis of QUAD. In Biryukov [9], pages 290307.
13
The critical parameter is the diï¬€erence between I = dim(spanR), the rank of the space of equations R,
and T . If T âˆ’ I = 0, the original system cannot be satisï¬ed; if T âˆ’ I = 1, then we should ï¬nd a unique
solution (with very high probability). Also, if T âˆ’ I < min(D, qâˆ’ 1), we can reduce to a univariate equation
[12]. We would like to predict D0, the smallest D enabling resolution.
Note: For any pair of indices i, j â‰¤ m, among linear combinations of the multiples of Pj = 0 will be PiPj = 0,
and among linear combinations of the multiples of Pi = 0 will be PiPj = 0 i.e., one dependency in spanR.
In Fq, (Pi)
q = Pi which generates a similar type of dependency.
Proposition 7 ([32]) Denote by [u]s the coeï¬ƒcient of the monomial u in the expansion of s, then:
1. T = [tD]
(1âˆ’ tq)n
(1âˆ’ t)n+1
which reduces to
(
n+D
D
)
when q > D, and
âˆ‘D
j=0
(
n
j
)
when q = 2.
2. If the system is regular up to degree D, i.e., if the relations R(D) has no other dependencies than
the obvious ones generated by PiPj = PjPi and P
q
i = Pi, then
T âˆ’ I = [tD] G(t), where G(t) := G(t;n; d1, d2, . . . , dm) =
(1âˆ’ tq)n
(1âˆ’ t)n+1
mâˆ
j=1
(
1âˆ’ tdj
1âˆ’ tq dj
)
. (3)
3. For overdeï¬ned systems, Eq. 3 cannot hold when D > DXL = min{D : [t
D]G(t) â‰¤ 0}. If Eq. 3 holds
up for every D < DXL and resolves at DXL, we say that the system is q-semiregular. It is generally
believed [3, 14] that for random systems it is overwhelmingly likely that D0 = DXL, and indeed
the system is not q-semiregular with very small probability.
4. When it resolves, XL takes CXL . (c0 + c1 lg T ) Ï„ T
2
multiplications in Fq, using a sparse solver like
Wiedemann [31]. Here Ï„ is the average number of terms per equation.
We cannot describe methods F4-F5 [17, 18], which are just too sophisticated and complex to present
here. Instead, we simply sketch a result that yields their complexities:
Proposition 8 [3] For q-semiregular systems, F4 or F5 operates at the degree
D = Dreg := min
ï£±ï£²
ï£³D : [tD]
ï£«
ï£­ (1âˆ’ tq)n
(1âˆ’ t)n
mâˆ
j=1
(
1âˆ’ tdj
1âˆ’ tq dj
)ï£¶ï£¸ < 0
ï£¼ï£½
ï£¾ ,
and take . (câ€²0 + c
â€²
1 lg TÂ¯ ) TÂ¯
Ï‰
multiplications, where TÂ¯ = [tDreg ] ((1âˆ’ tq)n(1âˆ’ t)âˆ’n) counts the monomials of
degree exactly Dreg, and 2 < Ï‰ â‰¤ 3 is the order of matrix multiplication used.
We do not know what works best under various resource limitations. We take the position of [33], e.g., XL
with a sparse solver represents the best way to solve large and more or less random overdetermined systems
when the size of main memory space is the critical restraint.
C SRQ, a potential candidate for one way function
An SRQ (Sparse Rotated Quadratics) instance is anMQ system specialized so that it is non-sparse but can
be computed with fewer computations than normal quadratics.
Problem SRQ(q, n,m, h): In Fq, solve P1(x) = P2(x) = Â· Â· Â· = Pm(x) = 0. The Pi are quadratics formed
from sequence of rotations, that is Start with P0 = x1x2 + x3x4 + Â· Â· Â·+ xnâˆ’1xn (where n is even), and
obtain successive Pj by performing sparse aï¬ƒne maps on x. I.e., x
(0) := x, x(i) := M (i)x(iâˆ’1)+b(i), yi :=
Pi(x) := P0(x
(i)) + ci, âˆ€i. Matrices M
(i)
are randomly chosen, invertible and sparse with h entries per
row.
15
