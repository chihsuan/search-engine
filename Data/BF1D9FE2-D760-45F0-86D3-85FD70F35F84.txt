ç›®ï¤¿ 
1. ä¸­æ–‡æ‘˜è¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦....1 
2. è‹±æ–‡æ‘˜è¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦2 
3. å‰è¨€â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦3 
4. Certainty-Enhanced Active Learning for Improving 
Imbalanced Data Classificationâ€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦..6 
5. Certainty-Based Active Learning for Pooled Datasetsâ€¦...6 
6. An aggressive margin-based algorithm for incremental 
learningâ€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦.28 
é™„ï¤¿ 1â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦40 
I
2ã€è‹±æ–‡æ‘˜è¦ 
In practical applications, active learning and incremental learning are two kinds of 
useful approaches to decrease the requirement of labeling samples by human experts 
and to periodically update existing classifiers using new datasets. We discuss several 
encountering practical problems, including bias in label prediction, efficiency in 
classifier adjustment in Section 4 (2011 ICDM workshop), 5 (ready to submit), and 6 
(submitted to 2012 PAKDD). 
In active learning algorithms, informative samples are usually queried for true labels 
according to the disagreement of existing hypotheses. However when the dataset has 
skewed class membership, the imbalanced data classification problem is caused in 
active learning. In this project, we design two active learning approaches for 
pool-based and streaming-based datasets, respectively. For each unlabeled sample we 
propose to utilize only local behavior in the certainty-based neighborhood to generate 
the error minimization hypotheses. Notably there is no need to define the sizes of 
neighborhoods in advance. In our analysis and experimental results, it is presented 
that our approaches have the ability of dealing with the imbalanced data classification 
problem in active learning. 
In incremental learning, the classification model is incrementally updated using the 
small datasets. Different with existing methods, our approach updates the current 
classifier according to each sample in the dataset, respectively. The classifier is 
updated by adjusting more than the margin of each sample. Then the new classifier is 
generated by carefully analyzing classifier adjustments caused for labeled samples. In 
details, we formulate simple constrained optimization problems and then the updated 
classifier is the solution derived using Lagrange multipliers. The experimental results 
are shown that our incremental learning approach is suitable to be applied for the 
requirement of frequently adjusting the existing classifiers. 
keywords: active learning, agnostic active learning, imbalanced data 
classification, incremental learning, passive-aggressive online learning 
  
2
classifiers [12,13] could not be quickly adjusted when encountering diverse sample 
distribution. In this project, we are motivated by the simplicity of online 
Passive-Aggressive (PA) algorithm [14]. For using the new labeled sample, PA 
updates the classifier by adjusting more than the margin of this sample. Thus in our 
approach, while a sample is used for updating and its sign is incorrectly predicted, the 
classifier adjustment is aggressively achieved within the margin. And similar with 
other incremental learning methods, our updated classifier shall correct the prediction 
mistakes of the current classifier as many as possible. In details, we formulate the 
simple constrained optimization problem for each sample and then the candidate 
updated classifier is the solution derived using Lagrange multipliers. Particularly the 
selected new classifier, updated by the suitable margin, shall obtain the best 
classification accuracy on the collected dataset. This selection strategy is able to avoid 
the new classifier being extremely specific to the previous one. Moreover our 
incremental approach does not maintain previously seen samples so the updated 
classifier could flexibly adapt the diverse sample distribution. In the following, this 
approach is introduced in Section 6. 
 
Reference 
[1] S. Ertekin, J. Huang, L. Bottou, and L. Giles. Learning on the Border: Active 
Learning in Imbalanced Data Classification. In ACM Conference on Information and 
Knowledge Management, pp. 127-136, Lisboa, Portugal, November 2007. 
[2] M. Li an I.K. Sethi. Confidence-based active learning. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 28(8): 1251-1261, 2006. 
[3] M.F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In 
International Conference on Machine Learning, pp. 65-72, Pennsylvania, USA, June 
2006. 
[4] S. Hanneke. A bound on the Label Complexity of Agnostic Active Learning. In 
International Conference on Machine Learning, pp. 353-360, Oregon, USA, June 
2007. 
[5] A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active 
learning. In International Conference on Machine Learning, pp. 49-56, Montreal, 
Canada, June 2009. 
[6] A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Agnostic Active Learning 
without Constraints. In Neural Information Processing Systems, Vancouver, Canada, 
December 2010. 
[7] Utgoff, P. E.:Incremental Induction of Decision Trees. Journal of Machine 
Learning, vol. 4, pp. 161-186, 1989. 
[8] Mohamed, S., Rubin, D., Marwala, T.: Incremental Learning for Classification of 
4
4ã€Certainty-Enhanced Active Learning for Improving Imbalanced Data 
Classification 
This approach will be published in International Conference on Data Mining 
Workshops (Workshop on Domain Driven Data Mining) , 2011. å¦‚é™„ï¤¿ 1. 
 
5ã€Certainty-Based Active Learning for Pooled Datasets 
 
I. INTRODUCTION 
Since labeling data by human labors is a time-consuming job, many active 
learning approaches [1], [2], [3], [4], [5], [6], [7] have been proposed to decrease the 
requirement of labeled samples. The framework of active learning is to identify 
informative unlabeled data and then query their true labels from human experts or 
oracles. Relatively, passive learners obtain all data labels in advance. The objective of 
active learning is to query only a small number of unlabeled data and perform as well 
as passive learners. While the noisy oracles exist (agnostic cases) and all unlabeled 
data are collected in a pool, several approaches [1], [2], [8] have been designed to 
query unlabeled samples for their true labels iteratively depending on existing 
hypotheses. Generally those queried samples are near the current class boundary 
(current best hypothesis), or their predicted labels are disagreed among existing 
hypotheses. However there are three challenges worth to be concerned for practical 
applications: 
 
z Practical use. A2, [8], [9] is designed to maintain a candidate set of hypotheses 
and query the true label of the unlabeled sample of which predicted labels are 
disagreed by any two existing hypotheses. Thus A2 uses the theoretical analysis 
to guarantee the hypothesis that potentially is the optimal hypothesis would not 
be eliminated in the active learning process. And it is theoretically shown that 
they have polynomial improvement over passive learners on required labeled 
samples. However we observe that it is difficult to properly define the hypothesis 
set in practical applications. And the queried samples, of which predicted labels 
are disagreed by any two existing hypotheses, should be decided using specific 
importance measurements.  
z Theory-guarantee. In some approaches like [1], [2], unlabeled samples are 
queried by probabilities that are measured using their distance from the class 
boundary or by the diversity of their predicted labels. Those measurements are 
properly designed and effective for real-world datasets, but they are not derived 
from learning theories and could not obtain theoretical bounds. 
6
hypotheses and has the ability of dealing with the imbalanced data classification 
problem in active learning. Specifically, our contributions in this paper include:  
 
z Determination of the query probability. In the data pool, the importance of an 
unlabeled sample is measured depending on two specific hypotheses without 
maintaining the candidate hypothesis set. Also this importance is theoretically 
interpreted as a query probability. That is, the proposed approach achieves the 
purposes of practical use and the guarantee of theoretical bounds.  
z Utilization of the local classification behavior. Rather than the entire skewed 
dataset, only labeled samples in the neighborhood are used for generating the 
two error minimization hypotheses. Those hypotheses are specifically trained 
depending on the local behavior and without being affected by irrelevant data. In 
this paper, while hypotheses are generated in active learning, the problem of 
imbalanced data classification is effectively handled by our proposed approach. 
z Exploration of the specific neighborhood. Generally previous research works 
[23], [25], [27] that utilize local behavior for learning problems have to define 
the neighborhood in advance. In this paper, for each unlabeled sample, we 
extract the distribution of its labeled neighbors to explore the neighborhoods 
with different sizes. The neighborhood is decided for gaining the potential query 
probability (as large/small as possible). There is no need to define the 
neighborhood size before our active learning approach is executed.   
 
In our proposed approach CBAL, two possible problems might be concerned: 
time consumed in exploring the neighborhoods, and overfitting in selecting samples 
and in label prediction. At first, for each unlabeled sample, we explore its 
neighborhood where the potential query probability could be gained as large/small as 
possible. In term of the efficiency for finding the optimal solution, our approach is to 
find a local optimal one by incrementally exploring the neighborhoods using the 
log-uniform sizes and the binary-search strategy. Secondly, rather than depending on 
amounts or distances among neighbors, the sample distribution on the neighborhood 
is carefully observed by the separating hypotheses. In this paper, we utilize the local 
behavior of each unlabeled sample to determine its query probability and the 
predicted label. Particularly, we design a hypothesis-based strategy which depends on 
the difference between two error minimization hypotheses to explore the 
neighborhoods. So that our approach could gain the well-informed local behavior 
which has the ability to carefully handle the overfitting problem. Moreover, in this 
paper, our experimental results show that these two problems mentioned above are 
properly handled in the proposed approach. 
8
II. RELATED WORK
A. Imbalanced Data Classification
While classifiers are built by the imbalanced datasets, minority classes are usually over-
whelmed by majority classes [10]. Several approaches, like boosting algorithm [16], cost-
sensitive Support Vector Machines (SVM) [17], one-class SVM [18], neural network [19],
oversampling [20], and undersampling [21], have been proposed to deal with these classifica-
tion problems. Recently there are some research results [22], [23], [24] to utilize additional
local behaviors of specific neighbors for building classification models. Global classification
models are partially corrected by these local behaviors so that minority samples are able
to be explicitly recognized. Nevertheless, if the global models are seriously affected by the
imbalanced datasets, those additional local behavior might not be effective for the model
correction. Hence in [25], [26], [27], all classifiers are specifically trained based on the
neighbors. Rather than the entire labeled dataset, only some relevant labeled samples are
used for constructing classifiers. That is the typical framework of some research work [25],
[26], [28], [29] which assumes the local behavior in the specific area is as effective as the
global one when dealing with the learning problems. Given an unlabeled sample, relevant
labeled samples are identified and then their class membership is used for its label prediction.
The specific classifier is built by these relevant class membership without being affected by
irrelevant samples. However most of those approaches that use local behavior should define
the size of neighborhoods in advance. In this paper, the idea of utilizing local behavior is
adopted in our active learning method to solve the imbalanced data classification problem for
pooled datasets, and there is no need to predefine the size of each neighborhood.
B. Active Learning
Generally, active learning is to query a small number of informative unlabeled samples and
expected to perform as well as passive learners. Those informative samples usually exist near
the class boundary using which it is ambiguous to separate them [1], [2]. Or those samples
receive the predicted labels that are disagreed among designate hypotheses so as to eliminate
inconsistent hypotheses [4], [12], [13], [31]. In order to deal with the agnostic cases in active
learning, [30] assigns penalties values to the experts/oracles to improve the quality of labeling
results. Additionally several algorithms, like [9], [8], [11], [14], [15], have focused on dealing
with noisy queried samples through theory-based strategies. Those approaches fall into two
main categories in terms of input samples. The first category is that unlabeled samples are
10
Theorem 2. With probability at least 1âˆ’ Î´, the expected number of labels queried by IWAL-
ERM after n iterations is at most
1 + Î¸Â· 2err(hâˆ—)Â· (nâˆ’ 1) +O(Î¸Â·
âˆš
C0nlogn+ Î¸Â· log3n),
where Î¸ is the disagreement coefficient.
III. THE PROPOSED METHOD
In this section, for each unlabeled sample we propose a strategy to explore the suitable
neighborhood within which a query probability is determined based on some learning theories.
A. Determination of Query Probabilities
For practical purposes, IWAL-ERM [11] is proposed to generate only two specific hypothe-
ses for each streaming unlabeled sample to handle the agnostic cases without maintaining a
candidate set of hypotheses. Similarly in this paper, at each round the two hypotheses hk and
hâ€²k are generated to assign a query probability for each unlabeled sample xk in the pool: hk
is the existing best hypothesis (current class boundary), and hâ€²k is the best hypothesis among
a set of hypotheses which disagree with hk on xkâ€™s predicted label. Then Gk is the difference
between the empirical errors of hk and hâ€²k, and xkâ€™s query probability Pk is determined based
on Theorem 1 and the rejection threshold (presented in Alg. 3).
B. Exploration of Certainty-Based Neighborhoods
The most important is that, we assumes local behavior in a well-informed area is as effective
as the global one in dealing with the learning problems. Rather than the entire dataset, only
labeled samples in the neighborhood of the unlabeled sample are used to generate those two
specific hypotheses. Consequently, the large/small difference between the empirical errors of
the two hypotheses means this unlabeled sample locates on the certain/uncertain area in this
specific neighborhood. An example is illustrated in Fig. 1. x1 locates on the certain area in its
neighborhood and would be assigned a low query probability since x1 obtains large difference
between h1 and hâ€²1. Comparatively x2, that obtains small difference between h2 and h
â€²
2, locates
on the uncertain area in its neighborhood and would be assigned a high query probability.
Moreover in Fig. 1, the certain/uncertain area where x1 and x2 locate in the neighborhood is
similar with certain/uncertain area on the whole data distribution. This example presents the
advantage from our assumption: local behavior is as effective as the global one in used for
12
minSmk âˆˆL Gk
= minSmk âˆˆL err(h
â€²
k, S
m
k )âˆ’ err(hk, Smk )
= minSmk âˆˆL {minhâˆˆHâˆ§h(xk)6=hk(xk) err(h, Smk )
âˆ’minhâˆˆH err(h, Smk )} (3)
In terms of efficiency for finding the optimal Grk in eq. 2 and 3, our approach is to find a
local optimal Gk by incrementally exploring the neighborhoods using the log-uniform sizes,
{m,m + 20,m + 21,m + 22, ...}, and the binary-search strategy. Our proposed method to
explore certainty-based neighborhoods is presented in Algorithm 1. And Gk,m, a value of Gk
based on Smk , is calculated by the function calculate Gk. At first, our method determines the
exploring strategy (the value of flag) based on the comparison between the Gk,m within the
initial neighborhood Smk and Gk,m+1 within the first expanded neighborhood S
m+1
k , at line 3.
Then the neighborhood is expanded with the log-uniform sizes to explore the bound of the
neighborhood size, at line 5 to 7. At last, we use binary-search to find the local optimal Gk,
at line 9 to 14.
C. Certainty-Based Active Learning
After the query probability of each unlabeled sample in the data pool is determined,
our active learning algorithm, Certainty-Based Active Learning (CBAL) in Algorithm 3, is
designed to query the true label of the unlabeled sample that has the greatest query probability.
For each unlabeled sample xk in the data pool U , CBAL explores the specific neighborhood
and measures Gk in each round, at line 6. Then the query probability Pk is determined by
the rejection threshold [11] where C0, c1, and c2 are constant values, at line 7 and 8. At last,
the sample xk that has the greatest Pk is queried and added into the labeled dataset L.
D. Correctness Analysis
In the previous research work [11], it is proven that IWAL-ERM satisfies Pk â‰¥ 1/kk
where k is the sequence number of samples of which probabilities are assigned, and then
its deviation bound for errors of the hypotheses is based on learning theorems and explicitly
stated in Theorem 1. Through Theorem 1, the error of the final hypothesis is bounded based
on the rejection threshold of determining query probabilities. And, the expected number of
requested labels is measured by Theorem 2. Notably, the major difference between our CBAL
14
Algorithm 2: functions
calculate Gk(xk,m, L) begin1
Let Smk = {x|x âˆˆ L âˆ§ d(x, xk) â‰¤ dmk } be a neighborhood within the distance dmk ;2
hk = argmin{err(h, Smk ) : h âˆˆ H}, and3
hâ€²k = argmin{err(h, Smk ) : h âˆˆ H âˆ§ h(xk) 6= hk(xk)} ;
return Gk = err(hâ€²k, S
m
k )âˆ’ err(hk, Smk ) ;4
end5
compare(Gk, G
â€²
k, f lag) begin6
if flag == 1 and Gâ€²k > Gk then return true ;7
else if flag == 1 return false ;8
if flag == âˆ’1 and Gâ€²k â‰¤ Gk then return true ;9
else if flag == âˆ’1 return false ;10
end11
Algorithm 3: Certainty-Based Active Learning
Initialize: L =Ã˜;1
Let U be a set of unlabeled samples ;2
Let m be the initial size of each neighborhood ;3
for r = 1, 2, ..., R do4
for xk âˆˆ U do5
Gk = find Gk(xk, L,m), and6
Pk =
ï£±ï£²ï£³ 1, if Gk â‰¤
âˆš
C0logk
kâˆ’1 +
C0logk
kâˆ’1
s, otherwise.
(= min{1, O( 1
G2k
+ 1
Gk
)Â· C0logk
kâˆ’1 }), where
7
s âˆˆ (0, 1) is the positive solution to the equation
Gk = (
c1âˆš
s
âˆ’ c1 + 1)Â·
âˆš
C0logk
kâˆ’1 + (
c2
s
âˆ’ c2 + 1)Â· C0logkkâˆ’1 ;
Toss a biased coin with Pr(heads) = Pk.8
end9
Find xk âˆˆ U that has the greatest Pk ;10
L = L âˆª {(xk, yk, 1/Pk)} ;11
U = U\{xk} ;12
end13
Return: L14
16
TABLE I
THE DISTRIBUTION OF SAMPLES QUERIED BY EM-IWAL AND CEAL. (MAJ. = MAJORITY CLASS AND MIN. =
MINORITY CLASS)
A2 CBAL
Area maj. min. total maj. min. total
x < 120 0 0 1 2 0 2
120 â‰¤ x < 320 68 0 68 4 0 4
320 â‰¤ x < 520 55 11 66 48 7 55
520 â‰¤ x < 720 37 18 55 61 42 103
720 â‰¤ x < 920 37 24 61 51 22 73
920 â‰¤ x 0 0 0 13 0 13
p(y = 1|x) =
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
100%, if x < 120
90%, if 120 â‰¤ x < 320
80%, if 320 â‰¤ x < 520
70%, if 520 â‰¤ x < 720
60%, if 720 â‰¤ x < 920
0%, if 920 â‰¤ x
(4)
This experiment is used to show the effectiveness of querying informative unlabeled samples
by exploring their certainty-based neighborhoods. Table I demonstrates the distributions of
queried samples of A2 and CEAL when totally 250 samples are queried. Compared with the
samples queried by A2, fewer unlabeled samples located on the area from 120 to 520 are
queried by CBAL since a large part of samples on this area belong to the majority class. That
means fewer certain (majority) samples are selected by our method than those by A2. Also
Table I shows that more unlabeled samples on the area from 520 to 920 are queried by CBAL
because many majority samples and minority ones gather on this area. That is, our CBAL
queries more uncertain samples and less certain samples than A2. Hence, this experimental
result demonstrates that our proposed method enhances the ability of active learning to query
informative samples, especially when handling the skewed dataset.
B. Real-World Datasets
Table II presents document categories of five real-world data collections used in our
experiments. abalone, ecoli, and yeast have low-dimension datasets, and ModApte and 20News
18
 
 

 

 

 

 

 


   

  	 

 

 

 

  


 








 

 








ï¬€
 
ï¬ï¬‚ï¬ƒ
 
ï¬
!"#$%
&'()*+
'
,
-
.'
/
0-
.'
/
-
1'
/
Fig. 2. the performance of active learning approaches on cp+im
 

 

 

 

 

 

 

 


   

  	 

 

 

 

  


 








 

 








ï¬€
 
ï¬ï¬‚ï¬ƒ
 
ï¬
!
"#$$
%
&'()*
&+
,
-
&.
/,
-
&.
,
0
&.
Fig. 3. the performance of active learning approaches on im+pp
In Fig. 2-8, all active learning methods including Random have precise classification results
after parts of unlabeled samples in the pool are queried. Notably, on these separable datasets,
our proposed CBAL, that properly assign a query probability to each unlabeled sample,
slightly improves the classification performance of other approaches. In Fig. 9-11, Random and
A2 have very biased classification performance. Their strategies to select unlabeled samples
are strongly affected by the skewed datasets, 9+16, 10+16, and MIT+VAC, which have the
closest distances between class means (shown in Tab. II). Even so CBAL still gains the
enhanced classification accuracy on these skewed datasets because querying samples within
the certainty-based neighborhoods is deliberately designed to improve the imbalanced data
classification problems in active learning.
20
 
 
 
 


	 


 














ï¬€ï¬ï¬‚
ï¬ƒ
 !
 "
 #
 $
"  #  $          %  

& '
'
	(
	
)*+,
-
+
.
 
/012
3
+/
4 !
5!
Fig. 6. the performance of active learning approaches on ship+silver
 
 
 
 
 
 
 
 
 
 
 
              	  


 








 

 








ï¬€
 
ï¬ï¬‚ï¬ƒ
 
ï¬
!"#$%&
'()*+,
(
-
.
/(
0
1.
/(
0
.
2(
0
Fig. 7. the performance of active learning approaches on misc+x
neighborhood for each unlabeled sample while totally 90 samples are queried. It is found
that, only several iterations of neighborhood exploration are required by our CBAL. Thus the
proposed CBAL is practical to explore the neighborhood to find the local optimal Gk. Notably,
on the low-dimension dataset 9+16, it requires extraordinary sizes of the neighborhoods where
local behavior might be corresponding to global behavior. On this dataset, the advantage of
local behavior is hardly utilized for determining query probabilities so that the classification
accuracy of CBAL on 9+16 is not impressed (see Fig. 9). Hence, this experiment is shown
that CBAL is capable of exploring certainty-based neighborhoods efficiently except while the
local behavior on the dataset is complicated to be recognized.
22
  
 
 
 
 
 

	

 

 












ï¬€ï¬
 
 
 

ï¬‚        ï¬ƒ         

! "
"

#


$%&'
(
&)
 
*+,-.&*
/ï¬€ï¬
0ï¬
Fig. 10. the performance of active learning approaches on 10+16
 

 

 

 

 

 

 

 
	

 

 

 

 

 
	
  
 

  

 







 

 





ï¬€ï¬
 
ï¬‚ï¬ƒ 
!
ï¬‚
"#$%&'(
)*+,-.
*

/
0*
1
2/
0*
1
/
3*
1
Fig. 11. the performance of active learning approaches on MIT+VAC
CBAL still has impressive performance on the quality of queried samples and the classification
accuracy. At last, the efficiency in exploring the neighborhoods is shown that, only several
iterations of neighborhood exploration are needed by CBAL, except while the local behavior
on the dataset is complicated to be recognized.
REFERENCES
[1] S. Ertekin, J. Huang, L. Bottou, and L. Giles. Learning on the Border: Active Learning in Imbalanced Data
Classification. In ACM Conference on Information and Knowledge Management, pp. 127-136, Lisboa, Portugal,
November 2007.
[2] M. Li an I.K. Sethi. Confidence-based active learning. IEEE Transactions on Pattern Analysis and Machine Intelligence,
28(8): 1251-1261, 2006.
24
 
 

 

 

 

 

 

	

 

 

 

 

 

 

 

  	  

 







 

 





ï¬€ï¬
 
ï¬‚ï¬ƒ 
!
ï¬‚
"#$%&'(
)*+,-.
/

0
1
/2
Fig. 14. the performance of active learning approaches with cost-sensitive SVM on MIT+VAC
TABLE III
ANALYSIS OF THE AVERAGE ITERATIONS OF NEIGHBORHOOD EXPLORATION AND THE AVERAGE SIZE OF THE
EXPLORED NEIGHBORHOOD FOR EACH UNLABELED SAMPLE.
Dataset Iterations Sizes
9+16 8.8 54.5
10+16 5.66 18.12
10+4 6.41 18.63
cp+im 5.62 13.28
im+pp 6.68 15.29
CYT+ME1 6.57 26.74
MIT+VAC 5.82 18.68
ship+silver 7.48 34.14
ibm+mac 7.91 74.45
ms+x 7.76 36.46
[16] Y. Sun, M.S. Kamel, A.K.C. Wong, and Y. Wang. Cost-sensitive boosting for classification of imbalanced data. Pattern
Recognition, 40(12): 3358-3378, 2007.
[17] V. Vapnik. Statistical Learning Theory. Wiley, New York, NY, 1998.
[18] L.M. Manevitz and M. Yousef. One-class svms for document classification. Machine Learning Research, 2:139-154,
2002.
[19] N. Japkowicz. Supervised versus unsupervised binary-learning by feedforward neural networks. Machine Learning,
42(1-2): 97-122, 2001.
[20] N.V. Chawla, K.W. Bowyer, L.O. Hall, and W.P. Kegelmeyer. SMOTE: synthetic minority over-sampling technique.
Artificial Intelligence Research, 16(1): 321-357, 2002.
[21] M. Kubat and S. Matwin. Addressing the Curse of Imbalanced Training Sets: One-Sided Selection. In International
Conference on Machine Learning, pp. 179-186, Tennessee, USA, July 1997.
26
I. INTRODUCTION
Requests of analyzing collected period data have been emerged in recent practical ap-
plications that includes network traffic analysis [1], anomaly detection [2], and intrusion
detection [3]. Generally, those applications are implemented for adjusting classifiers/detectors
periodically. Most of incremental learning approaches have been proposed based on decision-
tree [4], neural network [5], [6], and Support Vector Machines (SVM) [3], [7], [8], [9], [10].
Typically they are designed to build the statistic classification model based on the previously
seen samples and to correct its prediction mistakes on new labeled samples. While focusing
on the sample space, SVM generalizes the separating hyperplane based on the whole sample
distribution, and maximizes the margins of the labeled samples (support vectors). The margin
of a sample is the distance between the sample and the separating hyperplane. And SVM is
theoretically proven that the hyperplane is able to well separate samples with different labels.
In [10], an incremental SVM approach was designed to update the hyperplane by solving a
constrained optimization problem based on all collected samples. This approach should solve
a complicated constrained optimization problem since all collected samples are considered
simultaneously. Other approaches [8], [9] identified distinct new samples as support vectors
to adjust SVM hyperplanes without solving the constrained optimization problems. The
advantage is to maintain useful samples that were previously seen as support vectors and to
obtain efficient update steps. They shall have stable abilities of label prediction. But, relatively,
those SVM classifiers could not be quickly adjusted when encountering diverse sample
distribution. Thus in this paper, our approach is to simplify the constrained optimization
problem for update steps and to adapt the diverse sample distribution for classifiers.
At first, rather than incrementally training the SVM classifier based on all collected
samples [10], our approach selects one sampleâ€™s margin as the basis for individual classifier
adjustment. For update steps, we divide a complicated constrained optimization problem into
several simpler ones. Consequently for the update steps, we formulate several optimization
problems with single constraint. Then we are motivated by the simplicity of online Passive-
Aggressive (PA) algorithm [11]. For using the new labeled sample, PA updates the classifier
by adjusting more than the margin of this sample. Thus in our approach, while a sample is
used for updating and its sign is incorrectly predicted, the classifier adjustment is aggressively
achieved within the margin. And similar with other incremental learning methods, our updated
classifier shall correct the prediction mistakes of the current classifier as many as possible.
28
3wt+1 = argminwâˆˆRn
1
2
||w âˆ’ wt||2 s.t. l(w, (xt, yt)) = 0, (1)
where l(w, (xt, yt)) is the hinge loss of wâ€™s prediction on xt.
l(w, (x, y)) =
ï£±ï£²ï£³ 0, y(wÂ·x) â‰¥ 11âˆ’ y(wÂ· x), otherwise (2)
Typically whenever the loss is zero, PA is passive and wt+1 = wt means no classifier
adjustment. And while the loss is positive, wt is aggressively updated by adjusting more than
the margin, yt(wtÂ·xt), and then the constrain l(wt+1, (xt, yt)) = 0 can be satisfied. Then the
Lagrangian of the optimization problem in Eq. (1) is defined as Eq. (3).
L(w, Ï„) =
1
2
||w âˆ’ wt||2 + Ï„(1âˆ’ yt(wÂ·xt)) (3)
Let the partial derivation of l with respect to w be zero and then let the deviation of Ï„
with respect to Ï„ be zero, we have
w = wt + Ï„ytxt
Ï„ =
1âˆ’ yt(wtÂ·xt)
||xt||2
Ultimately the PA update is performed by solving the constrained optimization problem in
Eq. (1). And it is theoretically shown that the aggressive update strategy of PA modifies the
weight vector as less as possible. The effectiveness of PA in solving problems of classification
and regression is formally analyzed in [11]. Based on this well-defined learning model of PA,
several online algorithms [17], [18] have been proposed for adding confidence information
and handling non-separable data..
III. INCREMENTAL PASSIVE-AGGRESSIVE LEARNING ALGORITHM
While each set of labeled period samples comes, the existing classifier shall be periodically
updated for adapting the latest sample distribution. In this paper, we propose an incremental
learning algorithm, named Incremental Passive-Aggressive (IPA). For each labeled sample,
there are two update steps in IPA: 1) to correct the prediction mistakes of the current
classifier, and 2) to aggressively update the current classifier by adjusting more than the
margin. At last, the error minimization classifier on the collected dataset is selected as the
October 5, 2011 DRAFT
30
5Let the partial derivation of l with respect to w be zero,
OwL(w, Ï„) = w âˆ’ wt âˆ’
âˆ‘
xiâˆˆÎºt,xi 6=xk
yixi âˆ’ Ï„ykxk
=> w = wt +
âˆ‘
xiâˆˆÎºt,xi 6=xk
yixi + Ï„ykxk (7)
Then substituting Eq. (7) into Eq. (6), we have
L(Ï„) =
1
2
||
âˆ‘
xiâˆˆÎºt
yixi + Ï„ykxk||2
+
âˆ‘
xiâˆˆÎºt,xi 6=xk
(1âˆ’ yi((wt +
âˆ‘
xiâˆˆÎºt,xi 6=xk
yixi + Ï„ykxk)Â·xi))
+ Ï„(1âˆ’ yk((wt +
âˆ‘
xiâˆˆÎºt,xi 6=xk
yixi + Ï„ykxk)Â· xk)) (8)
At last let the deviation of Eq. (8) with respect to Ï„ be zero,
0 = âˆ’Ï„y2k||xk||2 + (1âˆ’ yk(wtÂ·xk))âˆ’ ykxk
âˆ‘
xiâˆˆÎºt,xi 6=xk
yixi
=> Ï„ =
1âˆ’ yk(wtÂ·xk)âˆ’ ykxk
âˆ‘
xiâˆˆÎºt,xi 6=xk yixi
||xk||2 (9)
Ultimately, the update of the proposed incremental learning algorithm is performed by
solving the constrained optimization in Eq. (4) and the updated classifier is determined
by solving Eq. (5). It is theoretically presented in Eq. (7) and (9) that the update to the
current classifier wt is performed by correcting its prediction mistakes Îºt, and by adjusting it
within the margin when the sample is incorrectly predicted. Overall the proposed algorithm
is presented in Algorithm 1. At each round t, the dataset Kt is collected to update the current
classifier wt. Particularly, the samples of which predicted labels are incorrectly assigned by
wt are identified as Îºt, at line 4-5. Then for each sample xk âˆˆ Kt, the current classifier wt is
individually updated as the candidate classifier wk according to Eq. (7) and (9), at line 7-8.
At last, the classifier wk is selected as wt+1 if it gains the least prediction errors on Kt, at
line 10.
IV. EXPERIMENTS
In this section, our experimental results are presented to show the classification performance
while the classifiers are incrementally updated by the small training sets. For the sake of
comparisons, the online PA and an incremental SVM [9] are also run in our experiments. In
October 5, 2011 DRAFT
32
7TABLE I
9 REAL-WORLD DATASETS: SIZES OF THE CLASSES AND THE SIZE OF FEATURE DIMENSIONS
Dataset Class(size) Class(size) Dimensions
Australian Credit Approval 0(383) 1(307) 14
Connectionist Bench 1(111) 2(97) 60
Statlog (Heart) 1(150) 2(120) 13
Ionosphere b(126) g(225) 34
abalone 10(634) 4(57) 8
Pima Indians Diabetes 0(500) 1(268) 8
ecoli cp(143) im(77) 8
Spamming Bots normal(1560) spamming(150) 5
yeast CYT(463) ME1(44) 9
proposed IPA has better performance than IPA with C0 = 0. That means, it is effective to
correct the previous mistakes to gain the more suitable classifier, especially on the small and
insufficient labeled dataset. At last, it is shown IPA has the similar classification performance
with the incremental SVM. Although the SVM approach is outstanding at classification
accuracy, it is required to concern about its computational complexity. Comparatively, our
update strategy is to solve several simple optimization problems with single constraint in
order to improve the computing efficiency. Moreover, in Fig. 1-3, the proposed approach has
higher accuracy in classification than the incremental SVM at the starting rounds. When only
few labeled samples are seen, the sample distribution is diverse. Thus it is shown that our
approach has an ability of adapting the diverse labeled samples efficiently.
Interestingly in Fig. 5-9, our experimental results on the imbalanced datasets show that
the proposed IPA still obtains the stable and impressive ability in label prediction. That is
because the proposed approach adopts weighted errors for the skewed classes in both of the
update steps and the candidate classifier selection. However the incremental SVM approach
has poor results since it does not focus on handling the imbalanced datasets. In details it is
noted on abalone, ecoli, and Spamming Bots that suitable SVM classifiers are generated at
the latter rounds. That is, those SVM classifiers are well updated while enough knowledge
of the class distribution is accumulated.
October 5, 2011 DRAFT
34
9 









	    



  	 		 	  	 	 	

	 	

	
m
ic
ro
-a
v
er
a
g
e 
a
cc
u
ra
cy
rounds
Heart
Online PA
IPA C=0
IPA C=0.5
Incremental SVM
Fig. 3. Classification results of incremental learning approaches on heart
 





er
a
g
e
 a
c
cu
ra
cy
Ionosphere
Online PA
IPA C=0




 	  

 

    	   

  


m
ic
ro
-a
v
e
rounds
IPA C=0.5
Incremental SVM
Fig. 4. Classification results of incremental learning approaches on Ionosphere
resulted classifier. And our approach is able to adapt the diverse labeled samples efficiently.
It is also presented the proposed approach has the similar classification results with other
incremental methods. Furthermore on some real-world datasets, our approach has the ability of
handling the problem of skewed classes. Therefore it is presented that the proposed approach
is suitable to be applied for effectively adjusting the existing classifiers using periodically
collected datasets.
REFERENCES
[1] Sena, G. G. and Belzarena, P.: Early traffic classification using support vector machines. In: 5th International Latin
American Networking Conference, pp. 60-66. ACM New York (2009)
[2] Robertson, W. K., Maggi, F., Kruegel, C., Vigna, G.: Effective Anomaly Detection with Scarce Training Data. In: the
Network and Distributed System Security Symposium. ISOC (2010)
[3] Du, H., Teng, S., Yang, M., Zhu, Q.: Intrusion Detection System Based on Improved SVM Incremental Learning. In:
International Conference on Artificial intelligence and Computational intelligence, pp. 23-28. IEEE Press (2009)
October 5, 2011 DRAFT
36
11
 









 	   



    	    

 


m
ic
ro
-a
v
er
a
g
e 
a
cc
u
ra
cy
rounds



 



 





 






ï¬€
ï¬ï¬‚
ï¬ƒ

 
 
!
Fig. 7. Classification results of incremental learning approaches on cp+im
 





er
a
g
e 
a
cc
r
u
a
cy
		



 


 













 



 
ï¬€
ï¬

ï¬‚
ï¬ƒ


  ï¬‚ ï¬ƒ

 

     ï¬‚ ï¬ƒ 

  


m
ic
ro
-a
v
e
rounds


 
ï¬€
ï¬
! 


"#$%&
 
'(
)
Fig. 8. Classification results of incremental learning approaches on bot
37, 277-296 (1999)
[13] Ng, H. T., Goh, W. B., Low, K. L.: Feature selection, perceptron learning, and a usability case study for text
categorization. In: International Conference on Research and development in information retrieval, pp. 67-73. ACM
New York (1997)
[14] Cesa-Bianchi, N., Conconi, A., Gentile, C.: A Second-Order Perceptron Algorithm. J. Computing, 34(3), 640-668
(2005)
[15] Wang, S., San, Y., Wang, S.: An Online Modeling Method Based on Support Vector Machine. In: International
Conference on COmputer Science and Software Engineering, pp. 98-101. IEEE Press (2008)
[16] Sculley, D. and Wachman, G. M.: Relaxed Online SVMs for spam filtering. In: International Conference on Research
and development in information retrieval, pp. 415-422. ACM New York (2007)
[17] Dredze, M., Crammer, K., Pereira, F.: Confidence-Weighted Linear Classification. In: International Conference on
Machine Learning, pp. 264-271. ACM New York (2008)
[18] Crammer, K., Kulesza, A., Dredze, M.: Adaptive Regularization of Weight Vectors. In: Neural Information Processing
Systems. MIT Press, Cambridge, MA (2009)
[19] Lin, P., Yen, T., Fu, J., Yu, C.: Analyzing Anomalous Spamming Activities in a Campus Network. In: TANET. (2011)
October 5, 2011 DRAFT
38
Certainty-Enhanced Active Learning for
Improving Imbalanced Data Classification
JuiHsi Fu
Department of Computer Science
and Information Engineering
National Chung Cheng University
Chiayi, Taiwan, R. O. C.
fjh95p@cs.ccu.edu.tw
SingLing Lee
Department of Computer Science
and Information Engineering
National Chung Cheng University
Chiayi, Taiwan, R. O. C.
singling@cs.ccu.edu.tw
Abstractâ€”In active learning algorithms, informative sam-
ples are usually queried for true labels according to the
disagreement of existing hypotheses. However we observed
that, when the streaming dataset has skewed class member-
ship, the imbalanced data classification problem is caused
in active learning. The Minority class is overwhelmed by the
majority class in generating the hypotheses. In this paper,
for each unlabeled sample we propose to utilize only local
behavior in the certainty-enhanced neighborhood, rather
than the entire dataset, to generate the error minimization
hypotheses. Consequently, our proposed method enhances
the prediction of hypotheses and is able to determine the
query probabilities properly. In our experiments, synthetic
and real-world datasets are used for presenting the effec-
tiveness of our active learning approach. It is shown that the
proposed approach decreases the probability of querying a
certain (majority) sample and has the ability of dealing
with the imbalanced data classification problem in active
learning.
Index Termsâ€”Imbalanced Data Classification, Active
Learning, Lazy Learning, Streaming Datasets, Certainty-
Enhanced Neighborhood
I. INTRODUCTION
Nowadays massive messages are consecutively trans-
fered among applications and large data are streamingly
accessed through the networks. For example, business
emails are sent for official affairs and network pack-
ets are collected for abnormal detection. Generally, it
is useful to design specific handlers like email filters
[1], [2] and abnormal traffic detectors [3] to manage
those collected data. Those handlers are usually trained
by certain supervised learning approaches after enough
streaming datasets are collected and labeled. However
the critical issue is to waste lots of expertsâ€™ effort and
time to label the massive datasets. Several strategies of
unlabeled sample selection, like semi-supervised learning
[4], [5], membership-query [6], and active learning [7],
[8], [9], [10], [11], [12], [13], [14], have been designed to
This work is supported by NSC, Taiwan, R.O.C. under grant no.
NSC 99-2221-E-194-023.
utilize unlabeled samples for advanced usage in generat-
ing classification models. Different with semi-supervised
learning and membership-query, active learners have
access to query true labels of the unlabeled samples.
And, their objective is to query only a small number
of unlabeled samples and perform as well as supervised
learners.
A practical and statistically consistent scheme for
streaming-based active learning, Importance Weighted
Active Learning(IWAL) [12], has been proposed to
determine probabilities of querying streaming samples,
also called query probabilities, based on the rejection
threshold and the disagreement of candidate hypotheses.
The rejection threshold is used to guarantee that IWAL
has the ability of handling agnostic cases where unknown
noises exist in the labeled datasets. The main idea is
to assign a high query probability for the unlabeled
sample that locates on the uncertain area (close to the
existing class boundary). Recently, authors in [13] re-
vised IWAL, named EM-IWAL for simplicity, to find the
error minimization hypotheses for each unlabeled sam-
ple without maintaining the candidate hypotheses. The
learner determines the query probability of an unlabeled
sample according to the difference between the errors
of these hypotheses. It is presented that EM-IWAL is
computationally tractable and effective for active learning
with varied kinds of classifiers. However, we notice that
in EM-IWAL each query probability strongly depends on
the error minimization hypotheses. While the streaming
dataset has skewed probabilities of sample occurrence,
the biased hypotheses, of which prediction is dominated
by majority samples, tend to be inappropriately trained.
The imbalanced data classification problem is caused,
and the minority class is overwhelmed by the majority
class in generating the (biased) hypotheses â„ğ‘˜ and â„â€²ğ‘˜.
The consequence is that, majority samples are frequently
queried and the biased hypothesis is returned for the
resulting classifier. (see our experimental results in Table
III)
40
sion of IWAL, EM-IWAL, that finds error minimization
hypotheses without maintaining the candidate set is pro-
posed. For an unlabeled sample ğ‘¥ğ‘˜, EM-IWAL generates
two error minimization hypotheses: one is the existing
best hypothesis (current class boundary) â„ğ‘˜, and the other
is the best hypothesis â„â€²ğ‘˜ among a set of hypotheses
which disagree with â„ğ‘˜ on ğ‘¥ğ‘˜â€™s predicted label. ğºğ‘˜ is the
difference between empirical errors of â„ğ‘˜ and â„â€²ğ‘˜. Then
the query probability ğ‘ƒğ‘˜ of ğ‘¥ğ‘˜ is decided by ğºğ‘˜ and the
rejection threshold in Theorem 1 where ğ¶0 is a constant
and â„âˆ— is the optimal hypothesis. Notably Theorem 1
guarantees EM-IWAL has the ability of handling agnostic
cases. Also Theorem 1 bounds the value of ğºğ‘¥. That is,
a large value of ğºğ‘¥, which means it is insignificant to
query ğ‘¥ğ‘˜ for discriminating â„ğ‘˜ between â„â€²ğ‘˜, results in a
low ğ‘ƒğ‘˜ for ğ‘¥ğ‘˜. Ultimately, it is theoretically proven by
Theorem 2 that EM-IWAL has polynomial improvement
over supervised learning on required labeled samples.
Theorem 1. For any ğ‘› â‰¥ 1, EM-IWAL satisfies ğ‘ƒğ‘› â‰¥
1/ğ‘›ğ‘› so that the following holds with probability at least
1âˆ’ ğ›¿.
ğ‘’ğ‘Ÿğ‘Ÿ(â„ğ‘›) â‰¤ ğ‘’ğ‘Ÿğ‘Ÿ(â„âˆ—) +
âˆš
2ğ¶0ğ‘™ğ‘œğ‘”ğ‘›
ğ‘›âˆ’ 1 +
2ğ¶0ğ‘™ğ‘œğ‘”ğ‘›
ğ‘›âˆ’ 1 .
Theorem 2. Let 0 â‰¤ ğ›¿ â‰¤ 1. With probability at least
1 âˆ’ ğ›¿, the expected number of labels queried by EM-
IWAL after n iterations is at most
1 + ğœƒâ‹… 2ğ‘’ğ‘Ÿğ‘Ÿ(â„âˆ—)â‹… (ğ‘›âˆ’ 1) +ğ‘‚(ğœƒâ‹…
âˆš
ğ¶0ğ‘›ğ‘™ğ‘œğ‘”ğ‘›+ ğœƒâ‹… ğ‘™ğ‘œğ‘”3ğ‘›),
where ğœƒ is the disagreement coefficient, a quantity first
used by [13].
III. THE PROPOSED METHOD
In this section, our active learning method is explained
how to utilize local information from the neighborhoods
in order to improve the imbalanced data classification
problems. The strategy of finding a proper neighborhood
is proposed after our observation is explicitly presented.
A. Exploration of the Certainty-Enhanced Neighborhood
In the settings of our approach, it is assumed that local
behavior extracted from the specific area/neighborhood
is as effective as global one in dealing with the learn-
ing problems. Our key idea is to define a suitable
neighborhood for each unlabeled sample to utilize only
labeled samples in the neighborhood, rather than the
entire dataset, to determine the query probability. Then
two error minimization hypotheses are generated without
being affected by irrelevant samples. It is noted that,
the large difference between empirical errors of these
two hypotheses means the unlabeled sample locates on
the certain area in the neighborhood. And this unla-
beled sample should be queried in a low probability.
An illustration is presented in Figure 1 where solid
x1
(a) unlabeled sample ğ‘¥1
x2
(b) unlabeled sample ğ‘¥2
Fig. 1. ğ‘¥1 on a certain area in its neighborhood should be queried in
a low probability. Comparatively ğ‘¥2 close to the class boundary in its
neighborhood has to be queried in a high probability.
initial
expanded
x
Fig. 2. The unlabeled sample ğ‘¥â€™s inner neighborhood is expanded as
the outer one because its ğºğ‘¥ is increased.
circles are the neighborhoods. ğ‘¥1 locates on a certain
area in its neighborhood, and undoubtedly ğ‘¥1 should
be queried in a low probability. Comparatively ğ‘¥2 is
close to the class boundary in its neighborhood so ğ‘¥2
has to be queried in a high probability. Generally, the
objective of active learning is to assign a low query
probability for the unlabeled samples on the certain areas.
Hence for each unlabeled sample ğ‘¥, our active learning
algorithm is designed to explore a certainty-enhanced
neighborhood where the large difference, ğºğ‘¥, is mea-
sured by the error minimization hypotheses. An example
is illustrated in Figure 2 where the gray circle instance
is an unlabeled sample ğ‘¥. The inner neighborhood is
initially generated by its three nearest neighbors. Then
the inner neighborhood is expanded as the outer one
because ğºğ‘¥ is increased. That is, a large ğºğ‘¥ for the
unlabeled sample ğ‘¥ on the certain area could be obtained
by exploring the neighborhoods incrementally. In terms
of sample numbers, the probabilities of querying majority
samples is also decreased because they usually gather
on the certain areas in the neighborhoods. Let ğ¿ be the
entire labeled dataset, a collection of sample-label pairs
{(ğ‘¥1, ğ‘¦1), (ğ‘¥2, ğ‘¦2), ...}, and ğ‘†ğ‘šğ‘˜ be ğ‘¥ğ‘˜â€™s neighborhood
that is a set of ğ‘š specific neighbors. ğ‘’ğ‘Ÿğ‘Ÿ(â„ğ‘˜, ğ‘†ğ‘šğ‘˜ ) is the
empirical error of the hypothesis â„ on the dataset ğ‘†ğ‘šğ‘˜ .
Our strategy shown in eq. (1) is to find the largest ğºğ‘˜
which is particularly based on ğ‘†ğ‘šğ‘˜ and is measured by
â„ğ‘˜ and â„â€²ğ‘˜.
ğ‘šğ‘ğ‘¥ğ‘†ğ‘šğ‘˜ âˆˆğ¿ ğºğ‘˜
= ğ‘šğ‘ğ‘¥ğ‘†ğ‘šğ‘˜ âˆˆğ¿ ğ‘’ğ‘Ÿğ‘Ÿ(â„
â€²
ğ‘˜, ğ‘†
ğ‘š
ğ‘˜ )âˆ’ ğ‘’ğ‘Ÿğ‘Ÿ(â„ğ‘˜, ğ‘†ğ‘šğ‘˜ )
= ğ‘šğ‘ğ‘¥ğ‘†ğ‘šğ‘˜ âˆˆğ¿ {ğ‘šğ‘–ğ‘›â„âˆˆğ»âˆ§â„(ğ‘¥ğ‘˜) âˆ•=â„ğ‘˜(ğ‘¥ğ‘˜) ğ‘’ğ‘Ÿğ‘Ÿ(â„, ğ‘†ğ‘šğ‘˜ )
âˆ’ğ‘šğ‘–ğ‘›â„âˆˆğ» ğ‘’ğ‘Ÿğ‘Ÿ(â„, ğ‘†ğ‘šğ‘˜ )} (1)
42
queried samples are those ğ‘š neighbors. Thus, our CEAL
also satisfies ğ‘ƒğ‘˜ â‰¥ 1/ğ‘˜ğ‘˜. Significantly the rejection
threshold of CEAL is also effective for determining query
probabilities. That means, there is only small difference
between errors of the resulting certainty-enhanced clas-
sifier and the best one. At last, our expected number of
requested labels is also bounded by Theorem 2 which
presents a polynomial label complexity improvement
over supervised learning.
IV. EXPERIMENTS
In this section, our experimental results are presented
to show the effectiveness of CEAL in active learning.
For the sake of comparison, EM-IWAL and Random
selection are also run in our experiments. In the following
experiments, the training set is the collection of queried
samples, and the testing set is the whole dataset. The ac-
curacy of a classifier â„ is the macro-average: ğ‘’ğ‘Ÿğ‘Ÿ(â„, ğ‘†) =
{âˆ‘âˆ£ğ‘†âˆ£ğ‘¥âˆˆğ‘† â„(ğ‘¥) âˆ•=ğ‘¦âˆ§ğ‘¦=+1}+{
âˆ‘âˆ£ğ‘†âˆ£
ğ‘¥âˆˆğ‘† â„(ğ‘¥) âˆ•=ğ‘¦âˆ§ğ‘¦=âˆ’1}
2 .
A. Synthetic Data set
The synthetic dataset is a collection of 1-dimension
samples with labels 1 (majority) and 0 (minority). Given
1,000 total samples at coordinates from 0 to 1,000,
the skewed class distribution is presented in eq. (3).
Approximately, there are 800 majority samples and 200
minority samples. On this dataset, our objective is to find
a separating threshold ğ‘¡ such that ğ‘¥ < ğ‘¡ if ğ‘¥â€™s label is 1
and ğ‘¥ > ğ‘¡ if ğ‘¥â€™s label is 0.
ğ‘(ğ‘¦ = 1âˆ£ğ‘¥) =
â§ï£´ï£´ï£´ï£´ï£´ï£´â¨
ï£´ï£´ï£´ï£´ï£´ï£´â©
100%, if ğ‘¥ < 120
90%, if 120 â‰¤ ğ‘¥ < 320
80%, if 320 â‰¤ ğ‘¥ < 520
70%, if 520 â‰¤ ğ‘¥ < 720
60%, if 720 â‰¤ ğ‘¥ < 920
0%, if 920 â‰¤ ğ‘¥
(3)
This experiment is used to show the effectiveness of
decreasing probabilities of querying certain (majority)
samples by exploring their certainty-enhanced neighbor-
hoods. Table I demonstrates the distributions of queried
samples of EM-IWAL and CEAL when they have similar
classification performance. In order to gain the similar
accuracy, 76.34% and 76.33%, and the similar number
of queried samples, 353 and 327, we set ğ¶0 = 0.35
for EM-IWAL, and ğ¶0 = 1 and ğ‘š = 10 for CEAL.
Compared with samples queried by EM-IWAL, fewer
unlabeled samples located on the area from 120 to 720
are queried by CEAL since a large part of samples on
this area belong to the majority class. That means fewer
certain (majority) samples are selected by our method
than those by EM-IWAL. Also Table I shows that more
unlabeled samples on the area from 720 to 920 are
queried by CEAL because many majority samples and
TABLE I
THE DISTRIBUTION OF SAMPLES QUERIED BY EM-IWAL AND
CEAL. (MAJ. = MAJORITY CLASS AND MIN. = MINORITY CLASS)
EM-IWAL CEAL
Area maj. min. total maj. min. total
ğ‘¥ < 120 1 0 1 2 0 2
120 â‰¤ ğ‘¥ < 320 33 0 33 19 0 19
320 â‰¤ ğ‘¥ < 520 136 28 164 77 16 93
520 â‰¤ ğ‘¥ < 720 84 43 127 73 44 117
720 â‰¤ ğ‘¥ < 920 18 10 28 59 37 96
920 â‰¤ ğ‘¥ 0 0 0 0 0 0
TABLE II
5 REAL-WORLD DATASETS: SIZES OF THE CLASSES, THE SIZE OF
FEATURE DIMENSIONS (DIM), AND THE DISTANCE BETWEEN CLASS
MEANS (DIS)
Dataset Class(size) Class(size) Dim Dis
abalone 9(689) 16(67) 8 0.412
abalone 10(634) 16(67) 8 0.189
ecoli cp(143) im(77) 8 0.581
ecoli im(77) pp(52) 8 0.54
yeast CYT(463) ME1(44) 9 0.458
yeast MIT(244) VAC(30) 9 0.213
ModApte ship(286) silver(29) 7,202 6.581
20News ms(300) x(30) 9,476 3.184
20News ibm(300) mac(30) 10,048 3.13
minority ones gather on this area. That is, our CEAL
queries more uncertain samples than EM-IWAL. Hence,
this experimental result demonstrates that our proposed
method enhances the ability of active learning to query
uncertain samples, especially when handling the skewed
dataset.
B. Real-World Datasets
Table II presents document categories of five real-
world data collections used in our experiments. abalone,
ecoli, and yeast have low-dimension datasets, and
ModApte and 20News are collected by high-dimension
datasets. Specially, to cause imbalanced data classifica-
tion on 20News, 300 samples are randomly selected from
ibm an ms, and 30 samples are randomly picked from
mac and x. Moreover, we compare our approach with
Random selection, EM-IWAL, and our opposite method,
named Certainty-Enhanced Active Learning (UCEAL),
which revises CEAL to explore the uncertainty-enhanced
neighborhood for each unlabeled sample. In the setting
of active learning, we set ğ¶0 = 1 for all algorithms
and ğ‘š = 3 for CEAL and UCEAL. libsvm [30] is
used to implement the SVM classifiers as hypotheses. At
each experiment, the performance of an active learning
algorithm is measured based on the number of queried
samples, the percentage of majority samples (mj. %) in
all queried samples, and the classification accuracy of
44
TABLE VII
PERFORMANCE OF CEAL ON SHIP+SILVER WITH DIFFERENT SIZES
OF INITIAL NEIGHBORHOODS
Neighborhood Size (mj%) Accuracy
m=5 110(80.0%) 89.1%
m=7 143(83.3%) 92.9%
m=9 121(82.0%) 87.8%
m=20% 152(84.3%) 96.6%
m=40% 206(87.4%) 98.3%
m=60% 224(88.0%) 98.3%
m=80% 175(86.3%) 94.8%
TABLE VIII
PERFORMANCE OF CEAL ON IBM+MAC WITH DIFFERENT SIZES OF
INITIAL NEIGHBORHOODS
Neighborhood Size (mj%) Accuracy
m=5 134(90.3%) 70.0%
m=7 154(88.4%) 78.2%
m=9 164(89.0%) 78.9%
m=20% 220(88.2%) 91.8%
m=40% 225(88.9%) 90.2%
m=60% 212(88.0%) 91.8%
m=80% 231(88.8%) 91.9%
presents that EM-IWAL and Random have impressive
performance. These two approaches are not affected by
the skewed high-dimension datasets in determining query
probabilities. However, CEAL and UCEAL do not work
very well for selecting informative unlabeled samples on
the high-dimension feature space. It seems that the initial
size of the neighborhood leads to gain an improper query
probability. Typically, classification algorithms require
enough labeled samples to solve complicated problems
which usually have a high-dimension feature space.
The large size of each unlabeled sampleâ€™s neighborhood
should be given in order to model the complicated classi-
fication problem. Thus on these high-dimension datasets,
additional experiments using CEAL with different initial
sizes of neighborhoods are presented in Table VII, VIII
and IX, where ğ‘š = 5, 7, 9 and m=20%,40%,60%,80% of
current queried samples. It is shown that CEAL obtains
similar or better performance than EM-IWAL although
several additional samples are required for generating
the well-informed neighborhood. Hence, CEAL has the
ability of determining a query probability for the high-
dimension unlabeled sample when the initial size of
neighborhoods is properly given.
Therefore, our experimental results are shown that the
certainty-enhanced neighborhood is properly explored by
finding large difference between empirical errors of error
minimization hypotheses. And the percentage of queried
majority samples is decreased since they are usually on
the certain areas and assigned for low query probabilities.
Ultimately, the proposed CEAL utilizes suitable local
behaviors of the neighbors and improves imbalanced
TABLE IX
PERFORMANCE OF CEAL ON MS+X WITH DIFFERENT SIZES OF
INITIAL NEIGHBORHOODS
Neighborhood Size (mj%) Accuracy
m=5 103(94.2%) 59.7%
m=7 107(91.0%) 67.6%
m=9 142(89.5%) 75.5%
m=20% 225(88.9%) 90.3%
m=40% 221(88.7%) 90.3%
m=60% 214(87.4%) 93.4%
m=80% 217(87.1%) 95.2%
TABLE X
ANALYSIS OF THE NUMBER OF NEIGHBORHOOD EXPANSIONS AND
THE SIZE OF THE EXPLORED NEIGHBORHOOD FOR EACH
UNLABELED SAMPLE. (AVG=AVEARGE, STD=STANDARD
DEVIATION)
Dataset Expansions Sizes
AVG STD AVG STD
9+16 2.34 2.72 5.34 2.73
10+16 1.78 1.68 4.78 1.68
cp+im 3.22 5.6 6.22 5.6
im+pp 2.1 2.48 5.1 2.48
CYT+ME1 2.23 3.6 5.23 3.6
MIT+VAC 2.0 2.6 5.0 2.6
ship+silver 1.69 1.65 45.2 22.07
ibm+mac 1.61 1.81 18.02 8.47
ms+x 1.68 1.7 21.81 10.69
data classification problems in streaming-based active
learning.
It is concerned that CEAL is additionally required to
identify nearest neighbors and generate hypotheses in
each neighborhood expansion. Those local information is
able to present specific behavior of samples explicitly, but
actually it consumes computational time to explore those
neighbors and produce hypotheses iteratively. Table X
presents, for each unlabeled sample, the average iteration
of neighborhood expansion and the average size of
the certainty-enhanced neighborhood. CEAL learners are
those that have the best performance on the real-world
datasets in Table III, IV, and VII. It is shown in Table X
that only several iterations of expansion are required to
explore the certainty-enhanced neighborhoods on each
datasets. Significantly as discussed above, large neigh-
borhoods are needed on the high-dimension datasets,
so that performance efficiency could be improved by
carefully handling high-dimension samples in measur-
ing distance among samples and generating classifiers
(hypotheses). Basically, this paper does not focus on
reducing the complexity of high-dimension datasets since
our main strategy is to utilize local behaviors in certainty-
enhanced neighborhoods for improving the imbalanced
data classification problem in active learning.
46
 9
åœ‹ç§‘æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•«é …ä¸‹å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
                                    æ—¥æœŸï¼š 100 ï¦ 9 æœˆ 27 æ—¥ 
ä¸€ã€ ï¥«åŠ æœƒè­°ç¶“é 
2011 ï¦åœ‹éš›é›»æ©Ÿé›»å­å·¥ç¨‹å¸«å­¸æœƒæ¶ˆè²»é›»å­ã€é€šä¿¡å’Œç¶²ï¤·åœ‹éš›å­¸è¡“æœƒè­°(IEEE CECNet) 
æ–¼ 2011 ï¦ 4 æœˆ 16 æ—¥è‡³ 19 æ—¥åœ¨ä¸­åœ‹å¤§ï§“å’¸ï¥Ÿèˆ‰ï¨ˆã€‚æ­¤æœƒè­°ç‚ºæ¶ˆè²»é›»å­ã€é€šä¿¡å’Œç¶²
ï¤·ï¦´åŸŸä¸­æœ€é‡è¦ä¹‹ï¦ï¨å¤§å‹ç ”è¨æœƒï¼Œå„é …æ¶ˆè²»é›»å­ã€é€šä¿¡å’Œç¶²ï¤·ç›¸é—œç ”ç©¶è­°é¡Œã€å‰
ç»ä¹‹ç ”ç©¶æ–¹å‘ä»¥åŠå¯¦éš›æœå‹™èˆ‡æ‡‰ç”¨çš†åœ¨æœƒè­°ä¸­è¢«æ¢è¨ã€‚ç ”è¨æœƒä¸­çš„è¨ï¥å…§å®¹ç›¸ç•¶å»£
æ³›ï¼Œå…±åˆ†ç‚ºå››å¤§ï§ï¼Œæœ‰ Consumer Electronics Technologyã€
CommunicationEngineering and Technologyã€Wireless Communications 
è¨ˆç•«ç·¨è™Ÿ NSC99ï¼2221ï¼Eï¼194ï¼023ï¼ 
è¨ˆç•«åç¨± Designing agnostic active learning algorithms to improve class 
imbalance problems 
å‡ºåœ‹äººå“¡
å§“å å³å»ºä½‘ 
æœå‹™æ©Ÿæ§‹
åŠè·ç¨± 
åœ‹ï§·ä¸­æ­£å¤§å­¸è³‡è¨Šå·¥ç¨‹ç ”ç©¶æ‰€ 
åšå£«å€™é¸äºº 
æœƒè­°æ™‚é–“ 
2011ï¦ 4æœˆ 16æ—¥
è‡³ 
2011ï¦ 4æœˆ 19æ—¥ 
æœƒè­°åœ°é» ä¸­åœ‹ã€å’¸ï¥Ÿ 
æœƒè­°åç¨± 
(ä¸­æ–‡) IEEE æ¶ˆè²»é›»å­ã€é€šä¿¡å’Œç¶²ï¤·åœ‹éš›å­¸è¡“æœƒè­° 
(è‹±æ–‡) International Conference on Consumer Electronics, 
Communications and Networks 
ç™¼è¡¨ï¥æ–‡
é¡Œç›® 
(ä¸­æ–‡) æ–¼ 802.16e ç¶²ï¤·ä¹‹ç”¨æˆ¶ç«™å°ä¿±å¤šé‡ï¦šç·šçš„èƒ½æºç¯€ç´„ 
(è‹±æ–‡)  Energy Saving with Multi-Connections over IEEE 802.16e 
Networks 
 11
Location and Handoff Management" ã€"Capacity, Throughput, Outage and Coverage 
Multimedia in Wireless Networks" ã€"Optical Networks and Systems Emerging 
Wireless/Mobile Applications "ã€"Intelligent Transportation Systems RFID 
Technology and Application" ã€"Service Oriented 
Architectures,ServicePortability Communications Software and Services" ã€
"Mobile Computing Systems "ã€"Agents, Knowledge-Based Technologies 
Bioinformatics Engineering "ã€"Computer Architecture and design Computer 
Networks and Security" ã€"Data Mining and Database Applications Grid 
Computing" ã€"High Performance Networks and Protocols Multimedia and Web 
Services "ã€"Network Reliability and QoS Neural Networks and Intelligent 
Systems" å’Œ"Software Engineering and Agile Development"ç­‰è­°é¡Œã€‚ä¸»è¦å°±æ˜¯æ¢
è¨æ¶ˆè²»æ€§é›»å­æŠ€è¡“ã€é€šè¨Šç¶²ï¤·ã€ç„¡ç·šç¶²ï¤·ï¦´åŸŸåŠé›»è…¦å·¥ç¨‹æŠ€è¡“çš„è­°é¡Œï¼Œæœ¬æ¬¡æœƒè­°æ‰€
æ”¶ï¤¿çš„ï¥æ–‡ç¯‡ï¥©è¿‘ï§‘ç™¾ç¯‡ï¼Œå…§å®¹éå¸¸çš„è±å¯Œã€‚é™¤ï¦ºä¸»è¦æœƒè­°ä¹‹å¤–ï¼Œå¦æœ‰ï¥©å€‹å°å‹
workshops åŒæ™‚èˆ‰ï¨ˆï¼ŒåŠå­¸è¡“æµ·å ±å¼µè²¼æœƒå ´ã€‚èˆ‡æœƒçš„å­¸è€…æœ‰ï¤­è‡ªä¸–ç•Œå„åœ‹å®¶ã€‚å¤§æœƒ
æ¯å¤©ï¨¦æœ‰é‚€è«‹çŸ¥åçš„å°ˆå®¶å­¸è€…é€²ï¨ˆå°ˆé¡Œæ¼”è¬›ï¼Œä¸‰å¤©çš„æœƒè­°åœ¨å°ˆé¡Œæ¼”è¬›ä¹‹å¾Œæœƒé€²ï¨ˆåˆ†
çµ„ï¥æ–‡å ±å‘Šå’Œè¨ï¥ã€‚æ¯å¤©ï¨¦æœ‰ä¸‰å€‹æ™‚æ®µï¼Œå¤šå€‹ session åŒæ™‚é€²ï¨ˆã€‚ï¥«åŠ æœ¬æ¬¡æœƒè­°çš„
ä¸»è¦ç›®çš„æ˜¯ç™¼è¡¨ç”±ç­†è€…ã€æŒ‡å°æ•™æˆï§¡æ–°ï§´åšå£«åŠä½•æ¼¢å½°åšå£«å…±åŒæ’°å¯«ä¹‹ï¥æ–‡â”€ã€Œæ–¼
802.16e ç¶²ï¤·ä¹‹ç”¨æˆ¶ç«™å°ä¿±å¤šé‡ï¦šç·šçš„èƒ½æºç¯€ç´„ã€ã€‚æœ¬æ¬¡ç­†è€…çš„ï¥æ–‡è¢«åˆ†ï§åœ¨å­¸è¡“æµ·
å ±å¼µè²¼ç¾¤ã€‚å¦å¤–ï¼Œæœ¬å±†æœƒè­°ä¸­çš„ Keynote Speech æœ‰ï¥§å°‘æ–°çš„ç ”ç©¶æ–¹å‘è¢«æ‹¿å‡ºï¤­æ¢è¨ï¼Œ
å…¶ä¸­åŒ…æ‹¬ Semantic Networking in Cyber-Societyã€Cloud Computing-Issues& 
Challengeã€Standardization in 4G networksã€Next Decade of Social,economical 
 13
å®šä½è­°é¡Œã€ç¹ï¤·è­°é¡Œã€èªè­‰è­°é¡Œã€å»£æ’­ç¾¤æ’­è­°é¡Œã€æ›æ‰‹è­°é¡Œã€æœå‹™å“è³ªè­°é¡Œç­‰ç­‰ã€‚
å€‹äººè¦ºå¾—ç¶²ï¤·å®‰å…¨åœ¨é€™å€‹éƒ¨åˆ†é¡¯å¾—ç‰¹åˆ¥é‡è¦ï¼Œé™¤ï¦ºå­¸è¡“çš„ï§¤ï¥ä¹‹å¤–ï¼Œç¾åœ¨ä¹Ÿå¾ˆæ³¨é‡
æ˜¯å¦å¯ä»¥å¯¦éš›ç”¨åœ¨æ—¥å¸¸ç”Ÿæ´» 
ä¸­ã€‚å› æ­¤ï¼Œç¾åœ¨è¶Šï¤­è¶Šå¤šå­¸è€…åœ¨å„ç¨®è­°é¡Œï§¨ï¨¦æœƒç‰¹åˆ¥åœ°è€ƒæ…®å®‰å…¨çš„å›  
ç´ ã€‚ 
æœ€å¾Œï¼Œåœ¨æœå‹™èˆ‡æ‡‰ç”¨é€™å€‹ä¸»é¡Œï¼Œä»¥ï¤‚è¼‰ç¶²ï¤·åŠå¤šåª’é«”ç¶²ï¤·ç‚ºä¸»è¦ 
è¨ï¥çš„è­°é¡Œã€‚ç­†è€…ç‰¹åœ°å»ï¦°è½ï¤‚è¼‰ç¶²ï¤·ç›¸é—œçš„ sessionã€‚ç”±æ–¼é›»å­å…ƒä»¶ 
åƒ¹æ ¼æ¼¸æ¼¸ä¸‹ï¨‰ä»¥åŠï¨ˆï¤‚å®‰å…¨å•é¡Œè¶Šï¤­è¶Šå—é‡è¦–ï¼Œå„åœ‹æ”¿åºœç´›ç´›æŠ•å…¥æ™º 
æ…§å‹é‹è¼¸ç³»çµ±(Intelligent Transport Systems, ITS)çš„ç ”ç©¶ã€‚åŒ…æ‹¬ç¾åœ‹çš„ 
ï¦—é‚¦é€šè¨Šå§”å“¡æœƒ(Federal Communications Commission, FCC)å°±åˆ†é… 
75MHz çš„é »å¯¬çµ¦ï¤‚ç”¨ç³»çµ±ä½¿ç”¨ï¼Œé€™æŠ€è¡“è¢«ç¨±ç‚ºå°ˆç”¨çŸ­è·ï§ªé€šä¿¡ 
(Dedicated Short Range Communication, DSRC)ã€‚ï¤‚è¼‰ç¶²ï¤·å¯¦éš›ä¸Šæ˜¯éš¨ 
æ„å‹ç¶²ï¤·çš„ä¸€å€‹è¡ç”Ÿã€‚ï¥§éï¤‚è¼‰ç¶²ï¤·æœ‰ï¦ºä¸€äº›ç‰¹æ€§ï¼šé«˜ç§»å‹•æ€§ã€ç§»å‹• 
æ–¹å‘æ˜¯æœ‰è¦ï¥¡æ€§çš„ã€å¼·èª¿å®‰å…¨æ€§ã€æ“æœ‰å…¨çƒå®šä½ç³»çµ±(GPS)ã€ï¨ˆå‹•è£ 
ç½®çš„æ•´é«”ï¥©ï¥¾å¾ˆé¾å¤§ã€‚å› æ­¤ï¼Œè¨­è¨ˆä¸€å€‹å®‰å…¨ä»¥åŠæœ‰æ•ˆï¥¡çš„è³‡ï¦¾å‚³è¼¸ï¼Œ 
æ˜¯æœ‰å…¶å¿…è¦æ€§å’Œé‡è¦æ€§ã€‚è‡³æ–¼å¤šåª’é«”ç¶²ï¤·è­°é¡Œï¼Œå·²ç¶“æœ‰è¨±å¤šå­¸è€…ï§ç”¨ 
è·¨å±¤è¨­è¨ˆå»é”åˆ°ä¸€å€‹æ¯”è¼ƒå¥½çš„æ•ˆèƒ½ï¼Œç”¨ï¤­æä¾›ä¸€äº›å³æ™‚çš„æœå‹™ 
(real-time service)ã€‚ 
äºŒã€ èˆ‡æœƒå¿ƒå¾— 
ã€Œæ¶ˆè²»é›»å­ã€é€šä¿¡å’Œç¶²ï¤·åœ‹éš›å­¸è¡“æœƒè­°ã€æ˜¯ IEEE çš„æ–°èˆ‰è¾¦çš„æœƒè­°ï¼Œä»Šï¦æ”¶ï¤¿å„ç¨®
æ¶ˆè²»æ€§é›»å­ã€é€šè¨Šç¶²ï¤·åŠç„¡ç·šç¶²ï¤·å…ˆé€²è­°é¡Œçš„ï¥æ–‡ï¼Œå› æ­¤ï¥«èˆ‡é€™å€‹æœƒè­°ï¼Œå¯ä»¥å¸å–
The International Conference on Consumer Electronics, Communications and Networks
ï¼ˆCECNet 2011ï¼‰
www.cecnetconf.org Apr. 16th-18th,2011 Xianningï¼ŒChina
IEEE Catalog Numberï¼ˆPrint Versionï¼‰: CFP1153N-PRT ISBN: 978-1-61284-470-1
Invitation
Mar. 24th,2011
Thank you very much for your paper submission to the International Conference on Consumer
Electronics, Communications and Networks (CECNet 2011) ; we are pleased to inform you that
your paper:
Paper ID: AC95662
Author(s): Chien-YuWu
Paper Title: Energy Saving with Multi-Connections over IEEE 802.16e Networks
has been accepted by CECNet2011. We cordially invite you to participate in CECNet2011 from
April 16th to 18th,2011 in Xianning, China. The conference program consists of keynote speech,
special sessions, oral and poster presentation. All accepted papers will be published by IEEE and
will be submitted to Ei Compendex.
CECNet has successfully invited some outstanding experts to make keynote speeches on the
conference. Welcome to participate in CECNet2011 and we are looking forward to meeting you in
Xianning, China!
For more information, pls visit our website: www.cecnetconf.org
Thanks again for your support!
Best regards!
CECNet2011 Organizing Committee
Xianning University, China
Scheduling
Results
f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12
M1
M2
C1,1
C1,2
C1,3
(a) The results is scheduled by tank-filling algorithm[5]
f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12
M1
M2
Scheduling
Results
C2,1
C2,2
C2,3
(b) The results is optimal solution
Figure 1. The example for tank-filling algorithm[5] and optimal solution with bandwidth requirement (BR) and delay bound (DB).
of each MS such that the energy consumption of all MSs is
minimized and maintains QoS.
A. Problem Formulation
We formulate the energy saving problem as an Integer
Linear Programming (ILP) problem, with the objective
function and constraints shown as Equations (1)-(5). The
decision variable yi, t âˆˆ {0, 1}, is set to one if the MS must
be active on frame t. The variable T is the scheduling
length (time unit is OFDM/OFDMA frame length). The
objective function of the ILP problem is to minimize energy
consumption of all MSs, shown as Equation (1).
minâˆ‘
tâˆˆT
âˆ‘
siâˆˆ Â¯S
yi, t (1)
âˆ™ Frame capacity constraint is represented as Equation
(2). Equation (2) denotes the total bandwidth assigned
to all connections that cannot exceed more than 1Î©,
where Î© denotes the capacity of a frame. Where Ci is
the connection sets that are established by MS si. The
connection ci, j âˆˆCi denotes that the j-th connection is
established by MS si. The variable bi, j is the bandwidth
requirement of connection ci, j. The decision variable
xti, j âˆˆ {0, 1}, is set to one if the connection ci, j is
assigned to active on frame t.
âˆ‘
siâˆˆ Â¯S
âˆ‘
ci, jâˆˆCi
xti, j Ã—bi, j â‰¤Î© , âˆ€ t âˆˆ T (2)
âˆ™ Bandwidth requirement constraint is represented as
Equation (3). On each sleep cycle, the bandwidth is
assigned to connection ci, j must satisfy its QoS require-
ment. In equation (3), the number of sleep cycle on
connection ci, j during scheduling length T is denoted
by T/T Si, j, where the T Si, j is the length of sleep cycle. For
effectively using bandwidth, the amount of bandwidth
assignment has to equal the bandwidth requirement of
connection ci, j during each sleep cycle.
âˆ‘kÃ—T
S
i, j
t=(kâˆ’1)Ã—T Si, j+1
xti, j Ã—bi, j = bi, j
,âˆ€ si âˆˆ Â¯S, âˆ€ ci, j âˆˆCi, and 1 â‰¤ k â‰¤ TT Si, j (3)
âˆ™ Energy consumption constraint is represented as
Equation (4). Equation (4) denotes that even though
multiple connections of an MS are active on a frame
simultaneously, the MS is only active on a frame period.
yi, t â‰¥ xti, j ,âˆ€ si âˆˆ Â¯S, t âˆˆ T, and ci, j âˆˆCi (4)
âˆ™ Sleep cycle constraint is represented as Equation (5).
The sleep cycle must be fixed on the Type II connection.
Hence, we design a sleep cycle constraint to fix the
sleep cycle of each connection. Equation (5) denotes
the two consecutive frames, with the interval being T Si, j,
must be the same as active or must sleep.
xti, j âˆ’ x
(t+T Si, j)
i, j = 0
,âˆ€ si âˆˆ Â¯S, âˆ€ ci, j âˆˆCi, and 1 â‰¤ t â‰¤ T âˆ’T Si, j (5)
B. An Example of Bandwidth Assignment
Assume that the wireless network system has two MSs
{s1,s2}. Each MS si has three connections ci, j, 1 â‰¤ j â‰¤ 3.
The connection ci, j has a data arrival rate Ï„i, j, the arrival data
has a delay bound DBi, j, and the sleep cycle fixed at T Si, j.
Hence, assume that the data arrival rate for each connection
during a frame period as Ï„1,1 = 0.1Î©, Ï„1,2 = 0.1Î©, Ï„1,3 =
0.125Î©, Ï„2,1 = 0.1Î©, Ï„2,2 = 0.1Î©, and Ï„2,3 = 0.125Î©. Then,
assume that the delay bound for each connection as DB1,1 =
2Î“, DB1,2 = 3Î“, DB1,3 = 4Î“, DB2,1 = 2Î“, DB2,2 = 3Î“, and
DB2,3 = 4Î“, where Î“ is the time length of a frame. Then,
the bandwidth requirement is bi, j = Ï„i, j Ã—T Si, j.
10 20 30 40
0
1
2
The number of Mobile Stations
Th
e 
to
ta
l a
ct
iv
e 
ra
tio
 o
f a
ll 
M
Ss
TFA
BRS
Figure 3. The active ratio simulation with high bandwidth requirement
settings, 4Î“, 8Î“, and 16Î“, respectively. Therefore, the sys-
tem establishes a connection with nine combinations of
data arrival rates and transmission delay bounds, such as
(200bits/frame, 4Î“). In the high bandwidth requirement
experiment, the number of MS is ranged from 5 to 45.
The data arrival rate have three settings, 500bits/frame,
1000bits/frame, and 2000bits/frame, respectively. The trans-
mission delay bound also have three settings, 32Î“, 64Î“, and
128Î“, respectively. We consider two performance metrics:
(i) active ratio: the ratio of active frames for the system and
(ii) throughput: the capacity of the wireless network system.
A. Low bandwidth requirement experiment
In Figure 2, we show the bandwidth assignment results
of the TFA, the BRS, and the ILP with regard to active
ratio. The active ratio simulation is shown in Figure 2. The
BRS is almost close to the optimal solution, which is solved
using the ILP. While the number of MS is 20, the total
active ratio of all MSs of TFA is approximately 6.6; the
BRS is approximately 4.1. Hence, we demonstrate the energy
consumption of BRS as being much less than the TFA in the
low bandwidth requirement experiment.
B. High bandwidth requirement experiment
In Figure 3 and Figure 4, we show the bandwidth assign-
ment results of TFA and BRS with regard to active ratio and
throughput. While the number of MS is less than 15, the
active ratio is the same between TFA and BRS. Moreover,
the active ratio of the BRS is bigger than the TFA when the
number is ranged from 15 to 40, as Figure 3 shows; but the
TFA has less throughput, as Figure 4 shows. Since the TFA
has more connections is dropped than the BRS.
V. CONCLUSION AND FUTURE WORK
In this paper, we consider the energy saving problem of
IEEE 802.16e networks. We propose an ILP model and
a bandwidth reservation scheme that not only guarantees
the quality of service for real-time connections but also
minimizes power consumption of MSs. Simulation results
demonstrated that the proposed schemes BRS outperform
previous approaches TFA. The energy saving problem on
10 20 30 40
200
600
1000
1400
The number of Mobile Stations
Th
ro
ug
hp
ut
 (f
ram
es)
TFA
BRS
Figure 4. The throughput simulation with high bandwidth requirement
the WiMAX relay network will be considered in the future.
Regarding the path selection problems, we should select the
next hop with energy conditions considered.
ACKNOWLEDGMENT
This work was supported in part by Taiwan NSC under
grant no. NSC 99-2221-E-274-007 and NSC 99-2221-E-
194-023-. The author like to thank reviewers for their
insightful comments which helped to significantly improve
the paper.
REFERENCES
[1] IEEE Std 802.16e-2005, â€IEEE Standard for Local and
metropolitan area networks Part 16: Air Interface for Fixed
and Mobile Broadband Wireless Access Systems Amendment
2: Physical and Medium Access Control Layers for Combined
Fixed and Mobile Operation in Licensed Bands and Corrigen-
dum 1,â€ Feb. 2006.
[2] IEEE Std 802.16-2009, â€IEEE Standard for Local and
metropolitan area networks Part 16: Air Interface for Broad-
band Wireless Access Systems,â€ May 2009.
[3] T.-C. Chen, J.-C. Chen, and Y.-Y. Chen, â€Maximizing Un-
availability Interval for Energy Saving in IEEE 802.16e
Wireless MANs,â€ IEEE Transactions on Mobile Computing,
vol. 8, no. 4, pp. 475-487, Apr. 2009.
[4] S.-C. Huang, R.-H. Jan, and C. Chen, â€Energy Efficient
Scheduling with QoS Guarantee for IEEE 802.16e Broadband
Wireless Access Networks,â€ in Proc. of the International con-
ference on Wireless Communications and Mobile Computing
(IWCMC), 2007.
[5] J.-J. Chen, J.-M. Liang, and Y.-C. Tseng, â€An Energy Effi-
cient Sleep Scheduling Considering QoS Diversity for IEEE
802.16e Wireless Networksâ€, IEEE ICC, 2010.
[6] W.-H. Liao , W.-M. Yen, â€Power-saving scheduling with a
QoS guarantee in a mobile WiMAX systemâ€, Journal of
Network and Computer Applications, v.32 n.6, p.1144-1152,
November, 2009
[7] S.-L. Tsao and Y.-L. Chen, â€Energy-efficient packet schedul-
ing algorithms for real-time communications in a mobile
WiMAX system,â€Computer Communications, vol. 31, no. 10,
pp. 2350-2359, June 2008.
 7
å°‘ã€‚ 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
è©²æœƒè­°é‡å°æ¶ˆè²»æ€§é›»å­ç”¢å“ã€é€šè¨Šã€ç„¡ç·šé€šè¨Šã€èˆ‡è¨ˆç®—æ©Ÿä¹‹å·¥ç¨‹èˆ‡æŠ€è¡“è­°é¡Œã€‚å…¶ä¸­
æ¶ˆè²»æ€§é›»å­ç”¢å“ä¹‹å»£å‘Šï¨ˆéŠ·ä¸»é¡Œèˆ‡æœ¬äººä¹‹ç ”ç©¶ä¸»é¡Œç›¸ç¬¦ï¼Œï¥«èˆ‡è©²æœƒè­°å¢åŠ å­¸è¡“ç ”ç©¶
ä¹‹åœ‹éš›è§€ã€‚  
ç­†è€…ä¾ç ”è¨æœƒé å®šä¹‹æ™‚é–“ä»¥æµ·å ±æ¼”ç¤ºå‘ˆç¾ã€‚èˆ‡ï¥©ååœ‹éš›å­¸è€…ä»‹ç´¹ä¸¦è¨ï¥ç›®å‰çš„ç ”ç©¶
æˆæœèˆ‡æ–¹å‘ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œäº¦ï¦ºè§£ï¥§åŒç ”ç©¶ï¦´åŸŸå­¸è€…çœ‹å¾…ç­†è€…ç ”ç©¶ä¹‹å•é¡Œçš„æ€è€ƒé»èˆ‡
å…¶æœªï¤­ç™¼å±•ã€‚é€™äº›åœ‹éš›å­¸è€…ï¥§ç´„è€ŒåŒçš„æåŠä¸€å€‹ç›¸åŒçš„å•é¡Œï¼šï¥æ–‡ä¸­è¨­è¨ˆçš„æ‹è³£æ©Ÿ
åˆ¶æ˜¯å¦èƒ½å¯¦éš›é‹ç”¨æ–¼å¯¦åšç³»çµ±ä¸­ã€‚é€™æ˜¯ä¸€å€‹é—œæ–¼å¯¦åšçš„è€ƒæ…®ï¼Œè¡¨ç¤ºé™¤ï¦ºé‡å°ï§¤ï¥çš„
å­¸è¡“ç ”ç©¶ä¹‹å¤–ï¼Œæ‡‰åŠ å…¥å¯¦åšè€ƒï¥¾çš„å› ç´ ã€‚å­¸è¡“ç ”ç©¶ï¥´æœ‰åŠ©æ–¼è§£æ±ºç¾å¯¦ç”Ÿæ´»ä¸­çš„å•é¡Œï¼Œ
æ°‘çœ¾æ‰èƒ½å—æƒ ã€‚æ•…æ–¼æœªï¤­çš„ç ”ç©¶ä¸­ï¼Œèƒ½å¦å¯¦éš›é‹ç”¨æ–¼å¯¦åšç³»çµ±ä¸­æ˜¯ä¸€å€‹å¿…å‚™çš„è€ƒï¥¾
å› ç´ ã€‚ 
ä¸‰ã€è€ƒå¯Ÿï¥«è§€æ´»å‹•(ç„¡æ˜¯é …æ´»å‹•è€…ï¥¶) 
å››ã€å»ºè­° 
ï¥«èˆ‡åœ‹éš›æœƒè­°èƒ½æ¥è§¸ï¥§åŒåœ‹å®¶çš„å­¸è€…ï¼Œé€éæ¼”ç¤ºèˆ‡è§£ï¥¯çš„è¨ï¥èƒ½å¤ æä¾›å­¸ç”Ÿç›¸ç•¶æœ‰
ç”¨çš„å»ºè­°ï¼Œå°æ–¼æœªï¤­çš„ç ”ç©¶èƒ½å¤ çµ¦äºˆæ­£é¢çš„å¹«åŠ©ã€‚åŒæ™‚ï¼Œï¦°è½èˆ‡ï¦ºè§£å…¶å®ƒå­¸è€…ç›®å‰
çš„ç ”ç©¶çµæœæœ‰åŠ©æ–¼æŒæ¡åœ‹éš›ä¸Šç›®å‰çš„ä¸»ï§Šç ”ç©¶ï¼Œä¸¦å¢åŠ åœ‹éš›å­¸è¡“è§€ã€‚æ„Ÿè¬æ•™è‚²éƒ¨è£œ
åŠ©ç­†è€…ï¥«åŠ æœ¬æ¬¡åœ‹éš›æœƒè­°çš„ç›¸é—œè²»ç”¨ã€‚ä¹Ÿå¸Œæœ›æ•™è‚²éƒ¨æœªï¤­èƒ½æä¾›ï¤å¤šå„æ–¹é¢çš„è£œåŠ©
æ¡ˆï¼Œè®“ç ”ç©¶å­¸è€…èƒ½ä»¥ï¤å°‘çš„è² æ“”ï¥«åŠ å­¸è¡“äº¤ï§Šæœƒè­°ã€‚ 
äº”ã€æ”œå›è³‡ï¦¾åç¨±åŠå…§å®¹ 
ç”±ä¸»è¾¦å–®ä½æ”œå›ç‰©å“å¦‚ä¸‹ 
The International Conference on Consumer Electronics, Communications and Networks 
ï¼ˆCECNet 2011ï¼‰ 
 
www.cecnetconf.org    Apr. 16th-18th,2011    Xianningï¼ŒChina 
IEEE Catalog Numberï¼ˆPrint Versionï¼‰: CFP1153N-PRT  ISBN: 978-1-61284-470-1 
 
Acceptance Notification 
Mar. 25, 2011 
 
 
Dear Author, 
 
Congratulations! We are extremely glad to inform you that your paper: 
 
Paper ID: AC46659 
Author: Tsung Chen-Kun,Ho Hann-Jang,Lee Sing-Ling, 
Paper Title: Repeated Sponsored Search Auction with Non-decreasing Bid Values 
 
has been accepted for presentation at the International Conference on Consumer Electronics, 
Communications and Networks (CECNet 2011) . 
 
Please finish all registration procedures before Mar. 31, 2011 by the Registration Instructions in 
attached.  
 
We are grateful for your contribution to CECNet2011. And we are looking forward to meeting 
you in Xianning, China. We also hope that you will contribute your excellent work to future 
CECNet conferences. 
  
For more information, please visit our website: www.cecnetconf.org 
Thanks for your support again! 
Best regards! 
 
 
CECNet2011 Organizing Committee 
 
Xianning University, China 
 
only at the minimum price to win the target position [11], e.g.
one dollar higher than the target bid rather achieving Nash
equilibrium. For the repeated SSA, although bidding at higher
value will not increase the payment in this round, the advertiser
in the next round may pay more money due to choose a higher
bid. For advertiserâ€™s perspective, proposing the minimum price
to winning the target position is more close to the real world
behavior.
Cary et al. propose balance bidding strategy to discuss the
convergence in repeated SSA [6]. If a repeated SSA converges
to a fixed outcome, SEPâ€™s revenue is expectable. However, not
all instances will converge in [6], and SEPâ€™s revenue is hard
to expect. For SEPâ€™s perspective, designing a mechanism with
convergence property is our objective.
III. NDSSA
We follow the auction definition in [6], which is proposed
by Cary et al.
Consider n advertisers {ad1, ad2, . . . , adn} compete for
k advertising slots {sl1, sl2, . . . , slk}, while n > k. Each
advertiser adi has the worth vi when the advertisement is
clicked by the Internet user, vi > v2 > . . . > vn. Each
advertising slot slj has a CTR Î¸j which is a probability that the
Internet user will click on, Î¸1 > Î¸2 > . . . > Î¸k and âˆ€Î¸j < 1.
After proposing bids to SEP, all advertisers are ranked in
the decreasing order of bid value. The clicked advertiser is
charged according to GSP. Therefore, if adi is allocated slj ,
the expected profit of adi is ui(j) = Î¸j(vi âˆ’ pji ), where pji is
adiâ€™s payment in slj .
The modification of the mechanism is as follows. Similar
to English auction, only non-decreasing bids are allowed in
NDSSA. Suppose adi is in slj . Each advertiser will propose
higher bids to compete for previous slot sljâˆ’1 until overbid-
ding or the profit is maximized in the current slot slj . In
other words, if winning the previous slot is benefited and
without overbidding, i.e. ui(j) â‰¥ ui(j âˆ’ 1) and bi â‰¤ vi,
the advertiser will propose the price (bjâˆ’1 + 1) to defeat the
previous advertiser, where bjâˆ’1 is the (j âˆ’ 1)th high ranked
bid. Otherwise, the bid value will not be updated.
When all advertisers propose the bids equal to that in the
last round, we say that NDSSA is converged.
IV. REVENUE ANALYSIS
NDSSA will converge while 1) overbidding or 2) the profit
of current slot is maximized, which is also called locally envy-
free equilibrium [1]. The bid value of condition one will not
be lower than that of condition two. So we focus on envy-free
bids after converging.
The superscripts of â€œNâ€ and â€œVâ€ in the following context
represent the labels about NDSSA and VCG respectively. For
the convenience, we assume that adi is ranked in sli, i.e. b1 >
b2 > . . . > bn.
Theorem 1. SEPâ€™s revenue in NDSSA, denoted by TRN , is
at least 1/Î¸1 times of that in VCG, denoted by TRV , when
the convergence bid set meets the locally envy-free equilibrium
under truthful bidding.
TRN â‰¥ 1
Î¸1
Ã— TRV
Proof: According to the locally envy-free bids in [1], we
get the following inequality.
bNi â‰¥
vNi Î¸iâˆ’1 âˆ’ uNi (i)
Î¸iâˆ’1
=
1
Î¸iâˆ’1
Ã— (vNi (Î¸iâˆ’1 âˆ’ Î¸i) + bNi+1Î¸i) (1)
Similarly, we have bNi+1 â‰¥ 1Î¸iÃ—(vi+1(Î¸iâˆ’Î¸i+1)+bNi+2Î¸i+1).
Replacing bNi+1 in eq. (1), we receive more details about b
N
i .
bNi â‰¥
1
Î¸iâˆ’1
{vNi (Î¸iâˆ’1 âˆ’ Î¸i) + [
1
Î¸i
(vNi+1(Î¸i âˆ’ Î¸i+1)+
bNi+2Î¸i+1)]Î¸i}
bNi â‰¥
1
Î¸iâˆ’1
{vNi (Î¸iâˆ’1 âˆ’ Î¸i) + vNi+1(Î¸i âˆ’ Î¸i+1) + bNi+2Î¸i+1)}
(2)
After repeatedly replacing bid values in eq. (2), we have the
general form of bNi .
bNi â‰¥
1
Î¸iâˆ’1
(vNi (Î¸iâˆ’1 âˆ’ Î¸i) + vNi (Î¸i+1 âˆ’ Î¸i) + . . .+ bNk Î¸k)
=
1
Î¸iâˆ’1
Ã—
kâˆ‘
s=i
vNs (Î¸sâˆ’1 âˆ’ Î¸s) (3)
Since pNi = b
N
i+1, adiâ€™s payment is:
pNi = b
N
i+1
â‰¥ 1
Î¸i
Ã—
k+1âˆ‘
s=i+1
vNs (Î¸sâˆ’1 âˆ’ Î¸s) (4)
When each advertiser bids truthfully, the bid value of
NDSSA and VCG is identical, i.e. bVi = b
N
i . According to
VCGâ€™s payment pVi =
âˆ‘k+1
s=i+1 v
V
s (Î¸sâˆ’1âˆ’Î¸s) and the truthful
bidding assumption bVi = b
N
i , this proof completes.
TRN =
kâˆ‘
i=1
pNi =
k+1âˆ‘
i=2
bNi
â‰¥
k+1âˆ‘
i=2
1
Î¸iâˆ’1
kâˆ‘
s=i
vs(Î¸sâˆ’1 âˆ’ Î¸s)
=
k+1âˆ‘
i=2
1
Î¸iâˆ’1
pViâˆ’1
â‰¥ 1
Î¸1
k+1âˆ‘
i=2
pViâˆ’1 =
1
Î¸1
Ã— TRV
bN2 â‰¥
1
Î¸1
(vV2 (Î¸1 âˆ’ Î¸2) + vV3 (Î¸2 âˆ’ Î¸3) + . . .). (9)
Adding vN3 + v
N
4 + . . . v
N
k to the LHS of eq. (9), the
inequality of bN2 can be rewritten as follows.
vN2 + . . .+ b
N
k â‰¥ bN2
â‰¥ 1
Î¸1
(vV2 (Î¸1 âˆ’ Î¸2) + vV3 (Î¸2 âˆ’ Î¸3) + . . .)
= vV2 (
Î¸1 âˆ’ Î¸2
Î¸1
) + vV3 (
Î¸2 âˆ’ Î¸3
Î¸1
) + . . .
(vN2 âˆ’ vV2 (
Î¸1 âˆ’ Î¸2
Î¸1
)) + (vN3 âˆ’ vV3 (
Î¸2 âˆ’ Î¸3
Î¸1
)) + . . . â‰¥ 0 (10)
The valuation of NDSSA can be replaced by VCG due to
our assumption vN2 = v
V
2 .
vV2 (1âˆ’
Î¸1 âˆ’ Î¸2
Î¸1
) + vV3 (1âˆ’
Î¸2 âˆ’ Î¸3
Î¸1
) + . . . â‰¥ 0 (11)
Each term is positive in eq.(11) because of (Î¸iâˆ’Î¸i+1)/Î¸1 â‰¥
0 and vVi â‰¥ 0. The assumption vN2 = vV2 is correct.
Our assumption does not have any contradiction from ad2
to adk+1. NDSSA consists of k winners, and the valuations
are the same with VCG except for ad1. According to the pi-
geonhole principle [4], ad1â€™s valuation in NDSSA is identical
to that in VCG. No contradiction is in our assumption. Thus,
the advertiser order in NDSSA is equivalent to that in VCG,
and NDSSA is efficient.
V. CONCLUSION
Applying GSP to the repeated SSA may reduce SEPâ€™s
revenue because advertisers may be benefited by proposing
lower bids. We propose NDSSA to improve SEPâ€™s revenue
through applying the ascending biding behavior of English
auction. We proof that SEPâ€™s revenue lower bound in NDSSA
is Î±-VCG, while Î± = 1/Î¸1. In other words, SEP in NDSSA
is more beneficial than in VCG in repeated SSA. Moreover,
NDSSA is efficient.
Similar to the bid increment issue in English auction,
the SEP in NDSSA also confronts the same determination
problem. In this paper, the bid increment can be treated as
a small value. Higher bid increment settings will speed up
the convergence, but SEPâ€™s revenue may be maximized in
lower bid increment settings. This is the tradeoff between
convergence speed and SEPâ€™s revenue. We will focus on
finding the strategy of determining bid increment to balance
convergence speed and SEPâ€™s revenue.
ACKNOWLEDGEMENT
This work was supported in part by Taiwan NSC under grant
no. NSC 99-2221-E-194-023 and NSC 99-2221-E-274-007.
The author would like to thank reviewers for their insightful
comments which helped to significantly improve the paper.
REFERENCES
[1] B. Edelman, M. Ostrovsky, and M. Schwarz, â€œInternet Advertising and
the Generalized Second Price Auction: Selling Billions of Dollars Worth
of Keywords,â€ American Economic Review, Vol. 97(1), pp. 242-259,
March 2007.
[2] H. R. Varian, â€œPosition Auction,â€ International Journal of Industrial
Organization, Vol. 25(6), pp. 1163-1178, December 2007.
[3] A. Wayne, â€œInequalities and Inversions of Order,â€ Scripta Mathematica,
Vol. 12(2), pp. 146-169, 1946.
[4] R. P. Grimaldi, Grimaldi Discrete and Combinatorial Mathematics: An
Applied Introduction, Addison-Wesley Press, 1998.
[5] T. M. Bu, X. Deng, and Q. Qi, Forward looking Nash equilibrium for
keyword auction, Information Processing Letters, Vol. 105, pp. 41-46,
2008.
[6] M. Cary, A. Das, B. Edelman, I. Giotis, K. Heimerl, A. R. Karlin, C.
Mathieu, and M. Schwarz, Greedy Bidding Strategies for Keyword Auc-
tions, Proceedings of the 8th ACM Conference on Electronic Commerce
(EC-07), pp. 262-271, 2007.
[7] A. Ghosh and M. Mahdian, Externalities in Online Advertising, Proceed-
ing of the 17th international conference on World Wide Web (WWW),
pp. 161-168, 2008.
[8] G. Aggarwal, J. Feldman, S. Muthukrishnan, and M. Pal, Sponsored
Search Auctions with Markovian Users, Proceedings of the 4th Interna-
tional Workshop on Internet and Network Economics (WINE), pp. 621-
628, 2008.
[9] G. Aggarwal, A. Goel, and R. Motwani Truthful Auctions for Pricing
Search Keywords, Proceedings of the 7th ACM Conference on Electronic
Commerce (EC-06), pp. 1-7, 2006.
[10] F. Brandt, T. Sandholm, and Y. Shoham, Spiteful Bidding in Sealed-
Bid Auctions, Proceedings of the 20th International Joint Conference on
Artificial intelligence (IJCAI), pp. 1207-1214, 2007.
[11] G. Aggarwal, S. Muthukrishnan, and J. Feldman, Bidding to the Top:
VCG and Equilibria of Position-Based Auctions, Proceedings of the 4th
Workshop on Approximation and Online Algorithms (WAOA), 2006.
[12] G. Linden, C. Meek, and M. Chickering, Keyword Auction Protocol for
Dynamically Adjusting the Number of Advertisements, Web Intelligence
and Agent Systems, Vol. 8, No. 4, pp. 331-341, 2010.
99 å¹´åº¦å°ˆé¡Œç ”ç©¶è¨ˆç•«ç ”ç©¶æˆæœå½™æ•´è¡¨ 
è¨ˆç•«ä¸»æŒäººï¼šææ–°æ— è¨ˆç•«ç·¨è™Ÿï¼š99-2221-E-194-023- 
è¨ˆç•«åç¨±ï¼šä»¥æ”¹å–„ class imbalance problems ç‚ºç›®æ¨™è¨­è¨ˆ agnostic active learning æ¼”ç®—æ³• 
é‡åŒ– 
æˆæœé …ç›® å¯¦éš›å·²é”æˆ
æ•¸ï¼ˆè¢«æ¥å—
æˆ–å·²ç™¼è¡¨ï¼‰
é æœŸç¸½é”æˆ
æ•¸(å«å¯¦éš›å·²
é”æˆæ•¸) 
æœ¬è¨ˆç•«å¯¦
éš›è²¢ç»ç™¾
åˆ†æ¯” 
å–®ä½ 
å‚™ è¨» ï¼ˆ è³ª åŒ– èªª
æ˜ï¼šå¦‚æ•¸å€‹è¨ˆç•«
å…±åŒæˆæœã€æˆæœ
åˆ— ç‚º è©² æœŸ åˆŠ ä¹‹
å° é¢ æ•… äº‹ ...
ç­‰ï¼‰ 
æœŸåˆŠè«–æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒè«–æ–‡ 0 0 100% 
ç¯‡ 
 
è«–æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100%   
ç”³è«‹ä¸­ä»¶æ•¸ 0 0 100%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 100% ä»¶  
ä»¶æ•¸ 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 7 6 100%  
åšå£«ç”Ÿ 4 2 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å…§ 
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆæœ¬åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 0 0 100% 
äººæ¬¡ 
 
æœŸåˆŠè«–æ–‡ 0 2 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒè«–æ–‡ 3 3 100% 
ç¯‡ 
 
è«–æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100% ç« /æœ¬  
ç”³è«‹ä¸­ä»¶æ•¸ 0 0 100%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 100% ä»¶  
ä»¶æ•¸ 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å¤– 
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆå¤–åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 0 0 100% 
äººæ¬¡ 
 
