2 
æ³¢å™¨ã€‚ä½†æ˜¯é€™é …å‡è¨­åœ¨æ—¥å¸¸å ´æ™¯ä¸­æ˜¯
ä¸æˆï§·çš„ï¼Œé€™æœƒä½¿å¾—ç”±é€™äº›æ–¹æ³•æ‰€è¨ˆ
ç®—çš„è¼¸å‡ºå½±åƒç”¢ç”Ÿè«¸å¤šçš„ç‘•ç–µï¼Œå¦‚å…‰
ï§…ï¼ˆhaloï¼‰ç¾è±¡ã€å¤±å»é¡è‰²ï¼ˆgrey washï¼‰ã€
è‰²èª¿åç§»ã€éåº¦å¼·åŒ–ã€é›œè¨Šæ”¾å¤§ç­‰å•
é¡Œã€‚ 
ç”±å‰è¿°è¨è«–åŠæ–‡ç»å›é¡§ï¼Œé¡¯ç¤ºç›´
æ¥å°‡å½©è‰²å½±åƒåˆ†è§£ç‚ºç…§æ˜èˆ‡åå°„å…©æˆ
åˆ†çš„ä½œæ³•æœ‰æ¥µå¤§çš„å›°é›£åº¦ã€‚ç›¸å°çš„ï¼Œ
æˆ‘å€‘æå‡ºåˆ©ç”¨å½±åƒäº®æš—èˆ‡æº–ä½çš„è‡ªå‹•
èª¿æ•´æ©Ÿåˆ¶å°‡å½±åƒæ‰€å«çš„ç…§æ˜æˆåˆ†é™è‡³
æœ€ä½ç”šæˆ–å»é™¤ï¼Œèª¿æ•´å®Œçš„å½±åƒçŒ¶å¦‚æ˜¯
åœ¨ç„¡å½©åº¦çš„ï¼ˆachromaticï¼‰æ¨™æº–ç™½è‰²å…‰
æºå‡å‹»çš„ç…§æ˜ä¸‹æ‰€å–å¾—ï¼Œä½¿å¾—è‰²å½©æ†
å¸¸æ€§èˆ‡å½±åƒå¼·åŒ–çš„é›™é‡ç›®æ¨™èƒ½å¤ åŒæ™‚
é”æˆã€‚æ¼”ç®—æ³•çš„æµç¨‹å¦‚åœ–1æ‰€ç¤ºï¼Œè¼¸å…¥
çš„å½©è‰²å½±åƒå¯ä»¥æ˜¯æœªé å…ˆgamma æ ¡
æ­£çš„ç·šæ€§ç°éšå€¼æˆ–æ˜¯å·²gammaæ ¡æ­£çš„
éç·šæ€§ç°éšå€¼ï¼Œè¼¸å…¥ç°éšç¯„åœç„¡è«–æ˜¯
é«˜å‹•æ…‹ç¯„åœï¼ˆhigh dynamic rangeï¼Œ ä»¥
ä¸‹ç°¡ç¨±HDRï¼‰æˆ–æ˜¯ä½å‹•æ…‹ç¯„åœï¼ˆlow 
dynamic rangeï¼Œä»¥ä¸‹ç°¡ç¨±LDRï¼‰çš†å¯æ¥
å—ã€‚é¦–å…ˆè—‰è‘—è‡ªå‹•èª¿æ•´è¼¸å…¥å½±åƒæ•´é«”
äº®æš—çš„æ©Ÿåˆ¶ï¼Œå°‡ä¸åŒçš„è¼¸å…¥å‹•æ…‹ç¯„åœ
æ˜ å°„åˆ°æ¨™æº–çš„è¼¸å‡ºç¯„åœï¼Œæ­¤æ­¥é©Ÿå¯ä»¥
å»é™¤å‡å‹»ç…§åº¦çš„å½±éŸ¿ã€‚æ¥è‘—å†é€²è¡Œå±€
éƒ¨çš„äº®æš—è‡ªå‹•èª¿æ•´ï¼Œä¸åƒ…å¯é™ä½éå‡
å‹»ç…§æ˜å…‰æºçš„æˆåˆ†ï¼Œäº¦å¯å¢åŠ åæš—å€
èˆ‡åäº®å€çš„æ¸…æ™°åº¦ã€‚å¾å½±åƒå¼·åŒ–çš„è§€
é»æ­¤æ­¥é©Ÿä¸åƒ…å¯ä½¿å½±åƒæ•´é«”çš„æ˜äº®åº¦
èª¿æ•´è‡³é©ç•¶çš„ç¯„åœï¼Œäº¦èƒ½å°‡ä½æ›å…‰çš„
å€åŸŸèª¿äº®åŠéåº¦æ›å…‰çš„å€åŸŸèª¿æš—ï¼Œçª
é¡¯é€™äº›å€åŸŸçš„ç´°ç¯€ã€‚è€Œå¾è‰²å½©æ†å¸¸æ€§
çš„è§’åº¦æª¢è¦–äº®æš—èª¿æ•´æ­¥é©Ÿå¯å°‡ç„¡è«–æ˜¯
HDRæˆ–æ˜¯LDRçš„è¼¸å…¥æ˜ å°„è‡³æœ€ä½³çš„è¼¸
å‡ºç¯„åœï¼Œé”æˆæ‰€è¬‚çš„æ˜åº¦æ†å¸¸æ€§
ï¼ˆlightness constancyï¼‰ï¼Œä½¿å¾—ç‰©é«”çš„æ˜
åº¦ä¸å—ç…§æ˜å…‰æºå¼·å¼±çš„å½±éŸ¿ã€‚Land[4]
ç¶“å¤šæ¬¡çš„å¯¦é©—è­‰å¯¦è‰²å½©æ†å¸¸æ€§æ˜¯åŸºæ–¼
ç´…ã€ç¶ ã€è—ä¸‰é€šé“çš„æ˜åº¦æ†å¸¸æ€§ã€‚æ‰€
ä»¥æœ€å¾Œå†é€éæº–ä½èª¿æ•´æ©Ÿåˆ¶å”èª¿ä¸‰é€š
é“çš„äº®æš—èª¿æ•´ï¼Œä½¿è¼¸å‡ºå½±åƒä¹‹è‰²å½©è¶¨
è¿‘æ†å¸¸æ€§ä¸¦å‘ˆç¾ç²¾ç¢ºçš„å½±åƒå¤–è²Œ
ï¼ˆimage apperanceï¼‰ã€‚ 
 
åœ– 1 åŸºæ–¼äº®æš—èˆ‡æº–ä½è‡ªå‹•èª¿æ•´è‰²å½©æ†å¸¸æ€§æ¼”ç®—æ¶æ§‹ 
 
    ä»¥ä¸‹å…ˆæè¿°å–®ä¸€é€šé“ï¼ˆç°éšå½±åƒï¼‰
çš„å…¨åŸŸèˆ‡å€åŸŸäº®æš—è‡ªå‹•èª¿æ•´æ¼”ç®—æ³•ï¼Œ
ä¸¦ä¾æ“šæ­¤æ–¹æ³•ç™¼å±•å‡ºä¸‰ç¨®åŸºæ–¼äº®æš—èˆ‡
æº–ä½èª¿æ•´çš„è‰²å½©æ†å¸¸æ€§æ©Ÿåˆ¶åŠè‡ªå‹•é¸
æ“‡çš„æº–å‰‡ï¼Œæœ€å¾Œè¨è«–å¦‚ä½•æ¶æ§‹å¯¦é©—æ–¹
æ³•ä»¥ï¥¥å®¢è§€çš„è©•ä¼°èª¿æ•´å¾Œçš„è¼¸å‡ºå½±åƒ
åœ¨è‰²å½©æ†å¸¸æ€§èˆ‡å½±åƒå¼·åŒ–å…©æ–¹é¢çš„é‡
åŒ–è¡¨ç¾ã€‚ 
 
2. å»ºæ§‹å–®ä¸€é€šé“çš„äº®æš—è‡ªå‹•èª¿æ•´æ©Ÿåˆ¶ 
    æˆ‘å€‘å…ˆé‡å°è¼¸å…¥å½©è‰²å½±åƒçš„ä¸€å€‹
é€šé“æˆ–ç°éšæˆåˆ†èªªæ˜å…¶å…¨åŸŸèˆ‡å€åŸŸçš„
äº®æš—è‡ªå‹•èª¿æ•´æ¶æ§‹ã€‚ 
 
2.1å…¨åŸŸäº®æš—è‡ªå‹•èª¿æ•´æ©Ÿåˆ¶ 
    æ­¤æ©Ÿåˆ¶çš„ä¸»è¦ç›®æ¨™æ˜¯ç„¡è«–è¼¸å…¥å½±
åƒæ˜¯ HDRæˆ–æ˜¯ LDRã€æ•´é«”åäº®æˆ–æ•´é«”
åæš—ï¼Œéƒ½èƒ½å°‡å®ƒèª¿æ•´åˆ°é©ç•¶çš„æ˜äº®æº–
ä½ï¼ŒçŒ¶å¦‚é›»å­ç³»çµ±çš„è‡ªå‹•å¢ç›Šæ§åˆ¶ã€‚
æˆ‘å€‘ä»¥æè¿°è¦–ç¶²è†œä¸Šçš„å…‰æ„Ÿæ¸¬é«”éŸ¿æ‡‰
å…¬å¼ï¼Œç¨±ç‚ºMichaelis-Menten function
æˆ– Naka-Rushton functionï¼Œä½œç‚ºç™¼å±•æœ¬
æ¶æ§‹çš„åŸºç¤ï¼Œå…¶å‹å¼å¦‚ä¸‹æ‰€ç¤º 
è¼¸å‡º 
(æ ¡æ­£) 
å½±åƒ 
4 
èµ·å§‹å€¼ç‚º 1ã€‚é¦–å…ˆå®šç¾©ç†æƒ³çš„è¼¸å‡ºå½±åƒ
äº®åº¦å¹³å‡å€¼ç‚º 
nideal k
I
+
=
1
1  ï¼ˆ6ï¼‰ 
è€Œè¼¸å‡ºå½±åƒçš„å¯¦éš›å¹³å‡å€¼ç‚º 
âˆ‘=
yx
gloreal yxIN
I
,
),(1  ï¼ˆ7ï¼‰ 
å…¶ä¸­ Nç‚ºå½±åƒåƒç´ ç¸½æ•¸ã€‚è¨­å®šå¯¦éš›å¹³
å‡å€¼èˆ‡ç†æƒ³å¹³å‡å€¼çš„å·®ç•°æ¯”ä¾‹ç‚º 
ideal
idealreal
I
IIratio âˆ’=  ï¼ˆ8ï¼‰ 
èª¿æ•´åƒæ•¸ kå€¼çš„å¢æ¸›ç›´åˆ°ä¸Šå¼ä¹‹å·®ç•°
æ¯”ä¾‹å°æ–¼é–€æª»å€¼ï¼Œæˆ–éè¿´åˆ° kå€¼çš„ä¸Šä¸‹
é™å€¼ï¼Œk âˆˆ [1,10]ã€‚å°‡æ±‚å¾—çš„åƒæ•¸ä»£å…¥
å…¬å¼ï¼ˆ1ï¼‰å³å¯ç²å¾—è¼¸å‡ºå½±åƒ 
ğ¼ğ‘”ğ‘™ğ‘œï¿½ğ‘¥ï¼Œğ‘¦ï¿½ï¼Œç”±æ–¼æ˜¯å…ˆæ±‚å¾— nå€¼ï¼Œå†ä»¥
æ­¤å€¼è¨ˆç®—å‡º kå€¼ï¼Œè€Œå…©åƒæ•¸æ˜¯äº’ç›¸å½±éŸ¿
çš„ï¼Œå› æ­¤éœ€è¦å°‡è¼¸å‡ºå½±åƒğ¼ğ‘”ğ‘™ğ‘œï¿½ğ‘¥ï¼Œğ‘¦ï¿½é€
éä¸‹å¼é€²è¡Œå¾®èª¿ 
ğ¼ğ‘”(ğ‘¥, ğ‘¦) = ğ· âˆ™ ğ¼ğ‘”ğ‘™ğ‘œ(ğ‘¥,ğ‘¦)1 + ğ· âˆ’ ğ¼ğ‘”ğ‘™ğ‘œ(ğ‘¥, ğ‘¦) ï¼ˆ9ï¼‰ 
å…¶ä¸­ 
ğ· = ï¿½1 âˆ’ ğ¼ğ‘Ÿğ‘’ğ‘ğ‘™ï¿½ğ¾ ï¼ˆ10ï¼‰ 
åƒæ•¸Kåœ¨å¾ŒçºŒçš„è‰²å½©æ†å¸¸æ€§æ©Ÿåˆ¶ä¸­æ±º
å®šå…¶å€¼ã€‚æœ€å¾Œç¶“ä¸Šè¿°å…¨åŸŸäº®æš—èª¿æ•´çš„
è¼¸å‡ºå½±åƒğ¼ğ‘”(ğ‘¥,ğ‘¦)æœƒæ¯”å…ˆå‰èª¿æ•´çš„å½±åƒ
ğ¼ğ‘”ğ‘™ğ‘œ(ğ‘¥,ğ‘¦)ç•¥æš—ï¼Œå› æ­¤ä¸æœƒç”¢ç”Ÿéäº®çš„æƒ…
å½¢ï¼ŒåŒæ™‚ä¹Ÿæ›´èƒ½é…åˆå¾ŒçºŒçš„å€åŸŸäº®æš—
èª¿æ•´ã€‚ 
 
2.2å€åŸŸäº®æš—è‡ªå‹•èª¿æ•´æ©Ÿåˆ¶ 
ç¶“éå…¨åŸŸäº®æš—é©æ‡‰æ©Ÿåˆ¶è™•ç†å¾Œï¼Œ
å½±åƒçš„æ•´é«”æ˜äº®åº¦å·²èª¿æ•´åˆ°ä¸€å€‹é©ç•¶
çš„æº–ä½ï¼Œä½†æ˜¯ä»æœƒæœ‰ä¸€äº›å€åŸŸç´°éƒ¨ç‰¹
å¾µç„¡æ³•å‡¸é¡¯å‡ºä¾†ï¼Œæ‰€ä»¥éœ€å†ç¶“éå€åŸŸ
æ€§äº®æš—èª¿æ•´æ©Ÿåˆ¶ä¹‹è™•ç†ï¼Œä»¥æœŸæœ›é”åˆ°
æ›´ä½³çš„å½±åƒå“è³ªã€‚æ­¤æ©Ÿåˆ¶ä¹‹åŸºæœ¬è§€å¿µ
æ˜¯æ¯ä¸€åƒç´ ä¹‹ç°éšå€¼æœƒå—åˆ°é„°è¿‘åƒç´ 
çš„å¹³å‡ç°éšä¹‹å½±éŸ¿è€Œèª¿æ•´å…¶å¤§å°ã€‚é€š
å¸¸æ˜¯æ¥µæš—å€çš„åƒç´ ç°éšæœƒç²å¾—æå‡ï¼Œ
ç›¸å°çš„ï¼Œæ¥µäº®å€çš„åƒç´ ç°éšå‰‡æœƒé­åˆ°
ä¸‹é™å…¶å…¬å¼å¦‚ä¸‹ 
ğ¼(ğ‘¥, ğ‘¦) = ï¿½ğ¼ğ‘”(ğ‘¥,ğ‘¦)ï¿½ğ‘š(ğ‘¥,ğ‘¦)
ğ‘(ğ‘¥, ğ‘¦)ï¿½ğ¼ğ‘”(ğ‘¥,ğ‘¦)ï¿½ğ‘š(ğ‘¥,ğ‘¦) + ğ‘(ğ‘¥, ğ‘¦) ï¼ˆ11ï¼‰ 
å…¶ä¸­ğ¼ğ‘”ï¿½ğ‘¥ï¼Œğ‘¦ï¿½ç‚ºç¶“å…¨åŸŸäº®æš—èª¿æ•´å¾Œä¹‹
å½±åƒï¼Œä¸¦å°‡å…¶æœ€å¤§å€¼æ­£è¦åŒ–ç‚º 1ã€‚åœ– 3
æè¿°æ­¤å…¬å¼æ‰€ä»£è¡¨çš„å„ç¨®æ›²ç·šï¼Œæ©«å
æ¨™ä»£è¡¨è¼¸å…¥ç°éšï¼Œç¸±åº§æ¨™ç‚ºè¼¸å‡ºç°éš
åœ–ä¸­çš„ä¸‰æ¢æ›²ç·šåˆ†åˆ¥ä»£è¡¨ a > 0çš„æ
å‡æ˜äº®åº¦ï¼Œ a < 0 çš„é™ä½æ˜äº®åº¦ï¼ŒåŠ a 
= 0 çš„ä¸èª¿æ•´æ˜äº®åº¦ã€‚aï¼Œbï¼Œmä»£è¡¨ 
[ ]5.0)0;,(5.05.11),( âˆ’â‹…âˆ’+= yxIIyxm lpg
 
ï¼ˆ12ï¼‰ 
( ) ( ) ( )( ) ( )[ ]yxIIyxI
yxIyxI
yxa
avav
avav
,,
,,
,
1max2
12
+
âˆ’
=  ï¼ˆ13ï¼‰ 
[ ]
[ ]),(),(
),(),(
),(
1max2
2max1
yxIIyxI
yxIIyxI
yxb
avav
avav
+
+
=  ï¼ˆ14ï¼‰ 
å…¶ä¸­ 1max =I ï¼Œ 1avI èˆ‡ 2avI ç‚ºæ›²ç·šåƒæ•¸å®š
ç¾©ç‚ºï¼š 
( ) ( ) 01 ,, cyxIyxI plpav +=  ï¼ˆ15ï¼‰ 
( ) ( )[ ] 0max2 ,, cyxIIyxI
p
lpav +âˆ’=  ï¼ˆ16ï¼‰ 
å…¶ä¸­ lpI ç‚ºå½±åƒç¶“é«˜æ–¯ï¦„æ³¢å™¨ä¹‹è¼¸å‡ºä½
é »å½±åƒï¼Œpç‚ºæ§åˆ¶å„åƒç´ é»çš„æ”¹è®Šäº®æš—
ä¹‹æ¬Šé‡ï¼Œ 0c å®šç¾©æœ€å°é©æ‡‰æº–ä½ï¼Œ gI ç‚º
è¼¸å…¥å½±åƒä¹‹å¹³å‡å€¼ã€‚ 
6 
èƒ½é”åˆ°ä¸éŒ¯çš„è‰²å½©æ†å¸¸æ€§è¡¨ç¾ã€‚ç•¶ç„¶
å¦‚ä½•åˆ¤æ–·å°±æ˜¯é‡é»ç ”ç©¶èª²é¡Œã€‚ 
æœ‰åˆ¥æ–¼å‚³çµ±è‰²å½©æ†å¸¸æ€§æ–¹æ³•å°‡ç°
è‰²ä¸–ç•Œå‡è¨­æˆ–ç™½å€å‡è¨­ç›´æ¥å¥—ç”¨æ–¼ä¼°
æ¸¬ç…§æ˜å…‰æºï¼Œæˆ‘å€‘æ¡å–å°‡é€™äº›å‡è¨­è»Ÿ
æ€§æ¤å…¥åˆ°ä¸‰é€šé“çš„å…¨åŸŸèˆ‡å€åŸŸäº®æš—èª¿
æ•´æ©Ÿåˆ¶ã€‚ä»¥ä¸‹æå‡ºä¸‰ç¨®è‰²å½©æ†å¸¸æ€§æ©Ÿ
åˆ¶ï¼Œåˆ†åˆ¥æ˜¯ä¿æŒåŸå§‹è¼¸å…¥è‰²èª¿çš„æ©Ÿåˆ¶ï¼Œ
åŸºæ–¼ç°è‰²ä¸–ç•Œçš„è‰²å½©æ†å¸¸æ€§æ©Ÿåˆ¶åŠåŸº
æ–¼ç™½å€çš„è‰²å½©æ†å¸¸æ€§æ©Ÿåˆ¶ã€‚æˆ‘å€‘å°‡å¾
å…¨åŸŸã€å€åŸŸåŠæº–ä½ä¸‰å€‹å¾ªåºèª¿æ•´æ­¥é©Ÿ
èªªæ˜æ¯ä¸€ç¨®æ©Ÿåˆ¶ã€‚ 
 
3.1 ä¿æŒåŸå§‹è¼¸å…¥è‰²èª¿çš„æ©Ÿåˆ¶ 
    æ­¤æ©Ÿåˆ¶ä¸»è¦æ˜¯é‡å°å…·æœ‰å–®ä¸€ç¨®æ”¯
é…è‰²çš„è¼¸å…¥å½±åƒé€²è¡Œèª¿æ ¡ï¼Œå› ç‚ºè¼¸å…¥
ä¸¦éåè‰²å½±åƒï¼Œå› æ­¤èª¿æ•´çš„ç›®æ¨™æ˜¯ç¶­
æŒåŸä¾†çš„è‰²å½©æ¯”ä¾‹ã€‚é¦–å…ˆå–å‡ºè¼¸å…¥å½±
åƒğ¼ğ‘–ï¿½ğ‘¥ï¼Œğ‘¦ï¿½, ğ‘– âˆˆ (ğ‘…,ğº,ğµ)ï¼Œçš„ç°éšæˆåˆ†ï¼Œ
ä»¤å…¶ç‚º 
( ) ( )âˆ‘=
i
i x,yIx,yI 3
1  ï¼ˆ20ï¼‰ 
ä¾ç…§å‰è¿°çš„äº®æš—èª¿æ•´æ©Ÿåˆ¶å°æ­¤ç°éšå½±
åƒé€²è¡Œèª¿æ•´ï¼Œå…¶ä¸­åœ¨å…¨åŸŸäº®æš—æœ€å¾Œå¾®
èª¿éç¨‹ä¸­ä½¿ç”¨çš„åƒæ•¸ Kç”±ä¸‹å¼æ±‚å¾— 
( )( )realglo
glo
II
I
K
âˆ’âˆ’
âˆ’
=
15.0
)1(5.0
max,
max,  ï¼ˆ21ï¼‰ 
å…¶ä¸­ğ¼ğ‘”ğ‘™ğ‘œ,ğ‘šğ‘ğ‘¥ç‚ºğ¼ğ‘”ğ‘™ğ‘œ(ğ‘¥,ğ‘¦)ä¹‹æœ€å¤§å€¼ã€‚æœ€
å¾Œçš„æº–ä½èª¿æ•´æ˜¯ä¾æ“šèª¿æ•´å‰å¾Œçš„äº®åº¦
æ¯”ä¾‹å›å¾©åŸå§‹è¼¸å…¥å½±åƒçš„è‰²åº¦ï¼Œæˆ‘å€‘
æå‡ºå¦‚ä¸‹çš„logæ¯”ä¾‹å®ˆè¡¡åŸå‰‡: 
ğ‘™ğ‘œğ‘” ğ¿ğ‘–(ğ‘¥, ğ‘¦)
ğ‘™ğ‘œğ‘” ğ¼ğ‘–(ğ‘¥, ğ‘¦) = ğ‘™ğ‘œğ‘”ğ¼(ğ‘¥, ğ‘¦)ğ‘™ğ‘œğ‘”ğ¼(ğ‘¥, ğ‘¦)    , ğ‘–ğœ–(ğ‘…,ğº,ğµ) (22) 
å…¶ä¸­ğ¿ğ‘–(ğ‘¥, ğ‘¦)ç‚ºè¼¸å‡ºæ ¡æ­£å½±åƒï¼Œ ğ¼ğ‘–(ğ‘¥,ğ‘¦)
ç‚ºåŸå§‹å½©è‰²å½±åƒï¼Œè€Œğ¼(ğ‘¥,ğ‘¦)å‰‡æ˜¯å…¶ç°éš
å½±åƒï¼Œ ğ¼(ğ‘¥,ğ‘¦)æ˜¯ç¶“äº®æš—èª¿æ•´å®Œçš„ç°éš
å½±åƒã€‚æ‰€ä»¥æœ€å¾Œçš„è¼¸å‡ºå½±åƒğ¿ğ‘–(ğ‘¥, ğ‘¦)å¯
ç”±ä¸Šå¼ç¶“æŒ‡æ•¸ expè¨ˆç®—ç²å¾—ã€‚ 
 
3.2åŸºæ–¼ç°è‰²è‰²å½©æ†å¸¸æ€§æ©Ÿåˆ¶ 
æœ¬æ©Ÿåˆ¶çš„ä¸»è¦ç›®æ¨™æ˜¯ä½¿å½±åƒä¸‰é€š
é“çš„å¹³å‡å€¼äº’ç›¸æ‹‰è¿‘ï¼Œå…¶æ¼”ç®—æ³•å¦‚ä¸‹: 
 
A. å…¨åŸŸäº®æš—è‡ªå‹•èª¿æ•´ 
å½±åƒ Rã€Gã€Bä¸‰é€šé“å„è‡ªç¨ï§·èª¿
æ•´å…¶è¼¸å‡ºç¯„åœï¼Œæ‰€ä»¥å…¬å¼ï¼ˆ1ï¼‰ä¿®
æ­£ç‚º 
ğ¼ğ‘”ğ‘™ğ‘œğ‘– = [ğ¼ğ‘–(ğ‘¥,ğ‘¦)]ğ‘›[ğ¼ğ‘–(ğ‘¥,ğ‘¦)]ğ‘› + (ğ‘˜ğ¼ğ‘–)ğ‘› , ğ‘– âˆˆ (ğ‘…,ğº,ğµ) (23) 
å…¶ä¸­åƒæ•¸ğ‘›æ±‚å¾—æ–¹å¼æ˜¯å°‡å½±åƒä¸‰é€šé“
è¦–ç‚ºä¸€é«”ï¼Œæ‰¾å‡ºå…¶æœ€å¤§å€¼èˆ‡æ’é›¶å¹¾ä½•
å¹³å‡å€¼ä¸¦å°‡å…©å€¼ä»£å…¥å…¬å¼ï¼ˆ4ï¼‰ï¼Œè€Œåƒ
æ•¸ğ‘˜å‰‡æ˜¯ 
ğ‘˜ = min{ğ‘˜ğ‘–, ğ‘– âˆˆ (ğ‘…,ğº,ğµ)} (24) 
å…¶ä¸­ğ‘˜ğ‘–æ˜¯ç”±ç´…ã€ç¶ ã€è—ä¸‰é€šé“å„è‡ªæ±‚å¾—
çš„ğ‘˜åƒæ•¸ã€‚å…¬å¼ï¼ˆ23ï¼‰ä»£è¡¨ä¸‰æ¢ä¸åŒçš„
è½‰æ›æ›²ç·šï¼Œä¸¦å¯è¡¨å¼ç‚º 
( )
( )
( ) n
n
i
i
n
i
i
glo
k
I
x,yI
I
x,yI
x,yI
i
+ï£·ï£·
ï£¸
ï£¶
ï£¬ï£¬
ï£­
ï£«
ï£·ï£·
ï£¸
ï£¶
ï£¬ï£¬
ï£­
ï£«
=  (25) 
ç”±æ–¼æ¯ä¸€é€šé“çš„è¼¸å…¥çš†è¢«è©²é€šé“çš„å¹³
å‡å€¼æ­£è¦åŒ–ï¼Œæ‰€ä»¥ä¸Šå¼å¯è¦–ç‚ºç°è‰²ä¸–
ç•Œå‡è¨­çš„è»Ÿæ€§æ¤å…¥ã€‚æœ€å¾Œå…¨åŸŸäº®æš—èª¿
æ•´æ©Ÿåˆ¶çš„è¼¸å‡ºç”±ä¸‹å¼æ±‚å‡º 
ğ¼ğ‘”ğ‘–(ğ‘¥, ğ‘¦) = ğ·ğ‘– âˆ™ ğ¼ğ‘”ğ‘™ğ‘œğ‘–(ğ‘¥, ğ‘¦)1 + ğ·ğ‘– âˆ’ ğ¼ğ‘”ğ‘™ğ‘œğ‘–(ğ‘¥, ğ‘¦) (26) 
å…¶ä¸­ 
ğ·ğ‘– =ï¼ˆ1 âˆ’ ğ¼ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘–ï¼‰ğ¾ (27) 
ä¸Šå¼çš„ğ¼ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘–ç‚ºğ¼ğ‘”ğ‘™ğ‘œğ‘–(ğ‘¥,ğ‘¦)çš„å¹³å‡å€¼ã€‚  
8 
å¯ç²—ç•¥çš„æè¿°è¼¸å…¥å½±åƒæœ‰é‚£äº›ä¸»è¦é¡
è‰²ï¼Œæ¥è‘—åˆ©ç”¨å·²çŸ¥çœŸå¯¦ç…§æ˜å…‰æºæˆåˆ†
çš„å½±åƒè³‡æ–™åº«æ¸¬è©¦æ¯ä¸€ç¨®è‰²å½©æ†å¸¸æ€§
æ©Ÿåˆ¶çš„æ­£ç¢ºæ€§ï¼Œæœ€å¾Œä¾æ“šä¼°æ¸¬èª¤å·®èˆ‡
çµ±è¨ˆæŒ‡æ¨™å»ºï§·é¸æ“‡æœ€ä½³è‰²å½©æ†å¸¸æ€§æ©Ÿ
åˆ¶çš„é¸æ“‡æ–¹æ³•ã€‚è¡¨ 1ç‚ºä¸‰ç¨®è‰²å½©æ†å¸¸
æ€§æ©Ÿåˆ¶çš„ç‰¹æ€§ã€‚ 
 
 
 
è¡¨ 1 ä¸‰ç¨®è‰²å½©æ†å¸¸æ€§æ©Ÿåˆ¶çš„ç‰¹æ€§ 
æ©Ÿåˆ¶ ç‰¹æ€§ 
ä¿ç•™åŸå§‹è‰²
èª¿ 
é©åˆæœ‰æ”¯é…è‰²ï¼Œå…¶ä¸­ä¸€æˆ–äºŒ
å€‹é€šé“å¹³å‡å€¼å¾ˆå¤§ï¼Œæ©Ÿåˆ¶çš„
ç›®çš„æ˜¯ä»¥ä¸æ”¹è®Šé€šé“é¡è‰²æ¯”
ä¾‹çš„æ–¹å¼æå‡å½±åƒäº®åº¦ä¸¦çª
é¡¯ç´°ç¯€ç‰¹å¾µã€‚  
åŸºæ–¼ç°è‰²ä¸–
ç•Œ 
é©åˆåœ¨æœ‰å¾ˆå¼·çš„åè‰²èƒŒæ™¯ï¼Œ
ä½¿å¾—ä¸€æˆ–äºŒé€šé“å¹³å‡å€¼å¾ˆ
å¤§ï¼Œä½†åˆä¸å±¬æ–¼æ”¯é…è‰²ï¼Œæ©Ÿ
åˆ¶çš„ç›®çš„æ˜¯æ‹‰è¿‘ä¸‰é€šé“çš„å¹³
å‡å€¼ã€‚  
åŸºæ–¼ç™½å€ 
é©åˆæœ‰ç™½è‰²å€å¡Šçš„å½±åƒï¼Œæ©Ÿ
åˆ¶çš„ç›®çš„æ˜¯å°‡ä¸‰é€šé“æœ€å¤§çš„
åå°„æ¯”èª¿æ•´ç›¸åŒã€‚  
    è‡ªå‹•é¸æ“‡æ©Ÿåˆ¶å¦‚åœ– 4æ‰€ç¤ºï¼Œæˆ‘å€‘
åˆ©ç”¨è¼¸å…¥å½±åƒå„é€šé“çš„å¹¾ä½•å¹³å‡å€¼èˆ‡
æ¨™æº–å·®é€²è¡Œé¸æ“‡ã€‚é¦–å…ˆéœ€åˆ¤åˆ¥è¼¸å…¥å½±
åƒæ˜¯å¦å«æœ‰å–®ä¸€è‰²èª¿çš„æ”¯é…è‰²ï¼Œå¦‚æœ
æœ‰å‰‡å±¬è©²é¡è‰²çš„å¹¾ä½•å¹³å‡å€¼é å¤§æ–¼å…¶
ä»–é€šé“ï¼ŒåŒæ™‚è©²é€šé“çš„æ¨™æº–å·®æœƒå¾ˆå°ã€‚
ç•¶è¼¸å…¥å½±åƒä¸‰é€šé“çš„å¹¾ä½•å¹³å‡å€¼ï¼Œå…¶
æœ€å¤§å€¼èˆ‡ä¸­é–“å€¼çš„æ¯”ä¾‹å¤§æ–¼ 3ï¼Œè€Œä¸”æœ€
å¤§å€¼é€šé“çš„æ¨™æº–å·®å°æ–¼ 1.2ï¼Œå‰‡è¡¨ç¤ºå½±
åƒå«æœ‰ç´…è‰²ã€ç¶ è‰²æˆ–è—è‰²çš„æ”¯é…è‰²ã€‚
å¦å¤–ï¼Œç•¶æœ€å¤§å¹¾ä½•å¹³å‡å€¼èˆ‡ä¸­é–“å¹¾ä½•
å¹³å‡å€¼çš„æ¯”ä¾‹é›–ç„¶å°æ–¼ 3ï¼Œä½†æ­¤å…©é€šé“
çš„æ¨™æº–å·®ä¹‹å’Œå°æ–¼ 1.2æ™‚ï¼Œè¡¨ç¤ºå½±åƒå«
æœ‰é»ƒè‰²ã€ç´«è‰²æˆ–æ˜¯é’è‰²çš„æ”¯é…è‰²ã€‚åª
è¦æœ‰æ”¯é…è‰²ï¼Œå³é¸æ“‡ä¿ç•™è¼¸å…¥å½±åƒåŸ
å§‹è‰²èª¿çš„æ©Ÿåˆ¶ã€‚ 
  å¦‚æœè¼¸å…¥å½±åƒæœªå«å–®ä¸€æ”¯é…è‰²ï¼Œæ­¤
æ™‚æª¢æŸ¥æœ€å¤§å¹¾ä½•å¹³å‡å€¼èˆ‡æœ€å°å¹¾ä½•å¹³
å‡å€¼çš„æ¯”ä¾‹æ˜¯å¦å¤§æ–¼ 3ï¼Œå¦‚æœæ˜¯ï¼Œå‰‡è¡¨
ç¤ºå½±åƒå…·æœ‰å¾ˆå¼·çš„ç…§æ˜è‰²åï¼Œéœ€ä½¿ç”¨
åŸºæ–¼ç°è‰²ä¸–ç•Œçš„æ©Ÿåˆ¶é€²è¡Œé¡è‰²æ ¡æ­£ï¼Œ
å¦‚æœè©²æ¯”ä¾‹å°æ–¼ 3ï¼Œå‰‡æª¢æŸ¥æœ€å°èˆ‡æœ€å¤§
æ¨™æº–å·®çš„æ¯”å€¼æ˜¯å¦æ¥è¿‘ 1ï¼Œå¦‚æœæ˜¯ï¼Œè¡¨
ç¤ºç¬¦åˆç°è‰²ä¸–ç•Œçš„å‡è¨­ï¼Œå¦‚æœä¸æ˜¯ï¼Œ
å‰‡ä½¿ç”¨åŸºæ–¼ç™½å€çš„æ©Ÿåˆ¶é€²è¡Œè‰²å½©æ†å¸¸
æ€§çš„æ ¡æ­£ã€‚ 
 
åœ– 4 ä¸‰ç¨®è‰²å½©æ†å¸¸æ€§é¸æ“‡æµç¨‹åœ– 
 
3.4ä¼°æ¸¬åŸå§‹å…‰æºè‰²èª¿ 
    æˆ‘å€‘ç¨±è‰²å½©æ†å¸¸æ€§æ©Ÿåˆ¶çš„è¼¸å‡ºå½±
åƒç‚ºæ ¡æ­£å½±åƒï¼Œè¡¨ç¤ºåœ¨æ¨™æº–ç™½å…‰ç…§æ˜
è¨­å®šä¸‹æ‰€èª¿æ ¡çš„çµæœï¼Œé€šå¸¸å‹¿éœ€å†ä¼°
10 
ä¼°æ¸¬èª¤å·®åˆ†ä½ˆçš„å¹³å‡å€¼ï¼Œè€Œè¡¨ 3å‰‡æ˜¯
èª¤å·®åˆ†ä½ˆçš„ä¸­å€¼ã€‚è¡¨ä¸­ä¹Ÿé¡¯ç¤ºä½¿ç”¨å‚³
çµ±çš„ç°è‰²ä¸–ç•Œæ³•èˆ‡ç™½å€æ³•çµæœã€‚å¦å¤–
åœ– 6æè¿°æœ¬æ©Ÿåˆ¶çš„è‡ªå‹•é¸æ“‡æ†å¸¸æ€§æ–¹
æ³•èˆ‡å‚³çµ±æ–¹æ³•çš„å¹³å‡èª¤å·®æ¯”è¼ƒï¼Œè€Œåœ– 7
å‰‡æ˜¯æè¿°æœ¬æ©Ÿåˆ¶çš„ä¸‰ç¨®æ†å¸¸æ€§æ©Ÿåˆ¶èˆ‡
è‡ªå‹•é¸æ“‡çš„å¹³å‡èª¤å·®çµæœçš„æ¯”è¼ƒã€‚å¾
åœ–è¡¨ä¸­æˆ‘å€‘æœ‰ä»¥ä¸‹çš„è§€å¯Ÿï¼š 
(1) è³‡æ–™åº« 1çš„èª¤å·®ç›¸è¼ƒæ–¼å…¶å®ƒè³‡æ–™åº«
çš„èª¤å·®æ˜¯è¼ƒå¤§çš„ï¼ŒåŸå› æ˜¯è³‡æ–™åº« 1
ç‚ºæœª gammaæ ¡æ­£çš„ç·šæ€§å½±åƒã€‚ 
(2) ä½¿ç”¨å¹³å‡å€¼è©•ä¼°å¯¦é©—çµæœèˆ‡ä½¿ç”¨
ä¸­é–“å€¼ä¹‹çµæœä¸¦ä¸ä¸€è‡´ï¼Œé€™æŒ‡å‡ºè©•
ä¼°æº–å‰‡ä»æœ‰æ”¹å–„çš„ç©ºé–“ã€‚ 
(3) é›–ç„¶æœ¬æ©Ÿåˆ¶çš„è‡ªå‹•é¸æ“‡æ³•æ¯”å‚³çµ±
æ–¹æ³•è¡¨ç¾å¥½ï¼Œä½†æ˜¯é¸æ“‡æ³•ä¸¦æœªèƒ½åœ¨
æ¯ä¸€è³‡æ–™åº«éƒ½é”æˆæœ€å°èª¤å·®ï¼Œå› æ­¤
æˆ‘å€‘æå‡ºçš„è‡ªå‹•é¸æ“‡æ–¹æ³•ä»æœ‰æ”¹
é€²çš„å¿…è¦ã€‚ 
(4) åœ– 8ï¼ˆaï¼‰æ˜¯æœ‰è‰²åçš„åŸå§‹å½±åƒï¼Œåœ–
8ï¼ˆbï¼‰æ˜¯ç¶“çœŸå¯¦å…‰æºæ ¡æ­£å¾Œçš„å½±åƒï¼Œ
ç¨±ç‚ºæ¨™æº–å½±åƒï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯èƒŒæ™¯
ç…§æ˜ç«Ÿç„¶æ ¡æ­£ç‚ºåè—çš„è‰²èª¿ï¼Œä¸»è¦
åŸå› æ˜¯åŸå§‹å½±åƒæ˜¯å¤šç…§æ˜å…‰æºå½±
åƒï¼Œè€Œæ‰€è¬‚çš„çœŸå¯¦å…‰æºä¹ƒæ˜¯å°ˆæŒ‡ç…§
å°„åˆ°ç°çƒå€åŸŸçš„å…‰æºè‰²èª¿ï¼Œæ‰€ä»¥ä»¥
æ­¤è™•å…‰æºæ ¡æ­£æ•´å¼µå½±åƒæ‰æœƒç”¢ç”Ÿ
å‰è¿°çš„ç¾è±¡ã€‚è€Œåœ– 8ï¼ˆcï¼‰æ˜¯æœ¬æ©Ÿåˆ¶
ç”¢ç”Ÿçš„çµæœï¼Œæ­¤æ ¡æ­£å½±åƒä¸ä½†åœ¨è¦–
è¦ºä¸Šæ˜¯åœ¨ç™½å…‰ç…§æ˜çš„æˆåƒçµæœï¼Œå…¶
æ˜äº®åº¦èˆ‡æ¸…æ™°åº¦äº¦é å„ªæ–¼åœ– 8ï¼ˆbï¼‰
çš„æ¨™æº–å½±åƒã€‚é€™ä¹Ÿæç¤ºæˆ‘å€‘éœ€è¦ç™¼
å±•ä»¥æ ¡æ­£å½±åƒç‚ºåŸºç¤åº¦é‡å…¶èˆ‡ç™½
å…‰ç…§æ˜é–“çš„å·®ç•°ã€‚ 
 
 
 
 
è¡¨ 2 å„è³‡æ–™åº«çš„åŸå§‹å…‰æºä¼°æ¸¬ã€Œå¹³å‡ã€èª¤å·® 
 
 
è¡¨ 3 å„è³‡æ–™åº«çš„åŸå§‹å…‰æºä¼°æ¸¬ã€Œä¸­å€¼ã€èª¤å·® 
 
 
 
åœ– 6 æœ¬è¨ˆç•«è‡ªå‹•é¸æ“‡æ©Ÿåˆ¶èˆ‡å‚³çµ±æ–¹æ³•çš„ã€Œå¹³å‡ã€
èª¤å·®æ¯”è¼ƒ 
 
åœ– 7 æœ¬è¨ˆç•«çš„ä¸‰ç¨®æ†å¸¸æ€§æ©Ÿåˆ¶èˆ‡è‡ªå‹•é¸æ“‡çš„ 
ã€Œå¹³å‡ã€èª¤å·®æ¯”è¼ƒ 
 
 
 
0
5
10
15
è‡ªå‹•é¸æ“‡
æ©Ÿåˆ¶
å‚³çµ±ç°è‰²
ä¸–ç•Œæ³•
å‚³çµ±ç™½å€
æ³•
0
5
10
15 è‡ªå‹•é¸æ“‡æ©Ÿåˆ¶
åŸºæ–¼ç°è‰²ä¸–ç•Œ
æ³•
åŸºæ–¼ç™½è‰²ä¸–ç•Œ
æ³•
ä¿ç•™åŸå§‹è‰²èª¿
æ³•
12 
[8]K. Barnard, L. Martin, A. Coath, and Brian Funt, 
â€œA comparison of computational color constancy 
algorithms part II: Experiments with image data,â€ 
IEEE Transactions on Image Processing, vol. 11, no. 9, 
pp. 985â€“996, 2002. 
[9]J. van de Weijer, T. Gevers, and A. Gijsenij, 
â€œEdge-based color constancy,â€ IEEE Transactions on 
Image Processing, vol. 16, no. 9, pp. 2207-2214, 
2007. 
[10]A. Gijsenij, T. Gevers , and J. v. de Weijer, 
â€œGeneralized gamut mapping using image derivative 
structures for color constancy,â€ International Journal 
of Computer Vision, vol. 86, no. 2-3, pp. 127-139, 
2008. 
[11]M. Ebner, â€œColor constancy based on local space 
average color,â€ Machine Vision and Applications, vol. 
20, no. 5, pp.283-301, 2009. 
[12]G. Arjan  and T. Gevers, "Color constancy using 
natural image statistics,â€ in Computer Vision and 
Pattern Recognition, CVPR â€™07, pp. 1-8, June 2007. 
[13]F. Gasparini and R. Schettini, â€œColor balancing of 
digital photos using simple image statistics,â€ Pattern 
Recognition, vol. 37, no. 6, 1201â€“1217, 2004. 
[14]S. Bianco, G. Ciocca, C. Cusano, and R. Schettini, 
â€œImproving color constancy using indoorâ€“outdoor 
image classification,â€ IEEE Transactions on Image 
Processing, vol. 17, no. 12, pp. 2381-2392, 2008. 
 
 
 
 
 
 
 
 
 
 
 
[15]K. Barnard, L. Martin, B. Funt, and A. Coath, â€œA 
data set for color researchâ€. Color Research and 
Application, vol. 27, no. 3, pp. 148-152, 2002. 
 [16]P. V. Gehler , C. Rother, A. Blake, T. Minka , and 
T. Sharp, â€œBayesian color constancy revisited,â€ in 
Computer Vision and Pattern Recognition, CVPR â€™08, 
pp. 1-8, June 2008. 
[17]A. Chakrabarti, K. Hirakawa, and T. Zickler, 
â€œComputational color constancy with spatial 
correlations,â€ Tech. Rep. TR-09-10, Harvard Sch. of 
Eng. & App. Sc., 2010. 
[18]F. Ciurea and B. Funt, â€œA large image database 
for color constancy research,â€ in Proc. IS&T/SID 11th 
Color Imaging Conference, pp.160-164, 2003. 
[19]C. Hsin, J. W. Jang, S. J. Shin, and S. H. Chen, â€œA 
no-reference objective image sharpness metric based 
on a filter bank of gaussian derivative wavelets,â€ in 
the 2nd International Conference on Multimedia 
Technology (ICMT2011), pp. 3362-3365, Hangzhou, 
China, July 26-28, 2011. 
[20]C. Hsin, H. N. Le, and S. J. Shin, â€œColor to 
grayscale transform preserving natural order of hues,â€ 
in the 2011 International Conference on Electrical 
Engineering and Informatics, H10-2, Bandung, 
Indonesia, July17-19, 2011. 
 
 
 
 
 
 
 
 
 
 
 
é¤˜ç¯‡æ–‡ç« ã€‚è«–æ–‡ç™¼è¡¨ä»¥å£é ­èˆ‡å£å ±å…©ç¨®æ–¹å¼é€²è¡Œï¼Œç¸½è¨ˆæœ‰ 63å ´æœƒè­°ç¯€æ¬¡(session)ã€‚ 
     å¤§æœƒå®‰æ’äº”ä½åœ‹éš›çŸ¥åå­¸è€…é€²è¡Œ keynote speechã€‚ä¾†è‡ªæ–¼æ„å¤§åˆ©çš„ Montanariæ•™
æˆé¦–å…ˆé‡å°æ™ºæ…§é›»ç¶²æè¿°å…¨çƒç›£æ§æ©Ÿåˆ¶ï¼Œå…¶è­°é¡Œç‚ºâ€Global Monitoring :The Paradigm 
for Asset Management in the Smart Grid Frameworkâ€ï¼Œç¬¬äºŒä½ keynoteæ¼”è¬›è€…æ˜¯é¦¬ä¾†è¥¿äº
å­¸è€…Hamdanæ•™æˆï¼Œå…¶è¬›é¡Œç‚ºâ€MYNDA: An IDSS Generator with Hyperheuristic Attribute 
Reductionâ€ï¼Œä¸»è¦èªªæ˜å…¶ä»¥ç¶²é ç‚ºåŸºç¤çš„MYNDA è³‡æ–™æ¢å‹˜æ¶æ§‹å¯è‡ªå‹•çš„ç”¢ç”Ÿæ‰€éœ€çš„
çŸ¥è­˜æ¨¡å¼ã€‚æ¥ä¸‹ä¾†çš„ keynoteæ¼”è¬›è€…æ˜¯ä¾†è‡ªæ–¼å°å°¼çš„ Bambangæ•™æˆï¼Œå…¶è¬›é¡Œç‚ºâ€Recent 
Progress in Adaptive Nonlinear Active Noise Controlâ€ï¼Œä¸»è¦ç™¼è¡¨å…¶æœ€è¿‘åœ¨é›œè¨Šçš„ä¸»å‹•æ§
åˆ¶ç ”ç©¶æˆæœã€‚ç¬¬å››ä½keynoteæ¼”è¬›è€…æ˜¯ä¾†è‡ªæ–¼æ—¥æœ¬çš„Hikitaæ•™æˆï¼Œå…¶è¬›é¡Œç‚ºâ€Fundamental 
Principles and Application of Diagnosis for GIS using Partial Discharge Measurementsâ€ï¼Œä¸»
è¦æè¿°é‡å°æ–·è·¯å™¨èˆ‡è®Šå£“å™¨ï¼Œåˆ©ç”¨ UHF æŠ€è¡“é‡æ¸¬è¨ºæ–·å…¶éƒ¨ä»½æ”¾é›»ç¾è±¡ã€‚æœ€å¾Œä¸€ä½
keynote æ¼”è¬›è€…æ˜¯å°å°¼å­¸è€… Supriana æ•™æˆï¼Œå…¶è¬›é¡Œæ˜¯â€Direct skeleton Extraction using 
River-Lake Algorithmâ€ï¼Œä¸»è¦æ˜¯é›†çµéå»åœ¨å½±åƒè™•ç†é ˜åŸŸæ‰€ç™¼å±•çš„ 200 é¤˜ç¨®éª¨æ¶
(skeleton)æ“·å–æŠ€è¡“ï¼Œåˆ†æå…¶å„ªç¼ºé»ï¼Œä¸¦æå‡ºä¸€å¥—æ–°çš„æ¼”ç®—æ³•ã€‚ 
æœ¬äººåœ¨ ICEEEI 2011æ‰€ç™¼è¡¨çš„è«–æ–‡å…§å®¹ä¸»è¦æ˜¯ç™¼å±•ä¸€å€‹å…©éšæ®µçš„å½©è‰²å½±åƒè½‰æ›æˆ
ç°éšå½±åƒçš„æ¼”ç®—æ–¹æ³•ã€‚ç”±æ­¤æ–¹æ³•æ‰€ç”¢ç”Ÿçš„ç°éšå½±åƒå¯ç¶­æŒåŸå½©è‰²å½±åƒçš„å½©è‰²å°æ¯”èˆ‡
æ˜åº¦(lightness)ã€‚æ¼”ç®—æ³•çš„ç¬¬ä¸€éšæ®µæ˜¯é€éä¸€å€‹ä¿ç•™è‰²åº¦è‡ªç„¶æ’åºçš„å…¨åŸŸæ˜ å°„æ©Ÿåˆ¶å°‡
è‰²æ¡å½±åƒçš„æ¯ä¸€åƒç´ ç´…ã€ç¶ ã€è—ä¸‰å€¼è½‰æ›ç‚ºé©ç•¶çš„ç°éšå€¼ï¼Œæ­¤æ˜ å°„æ©Ÿåˆ¶ä¸åƒ…ç°¡å–®å¿«
é€Ÿä¸”èƒ½ä¿æŒåŸæœ‰ç‰¹å¾µçš„åˆ†è¾¨æ€§èˆ‡é¡è‰²çš„è‡ªç„¶æ’åºæ€§ã€‚æ¼”ç®—æ³•çš„ç¬¬äºŒéšæ®µå‰‡æ˜¯ç¶“ç”±èª¿
  
 
                                 
 
 
(compressive sensing)æ©Ÿåˆ¶çš„è¦–è¨Šç·¨ç¢¼ç³»çµ±ï¼Œä½¿ç”¨ç‰©ä»¶åµæ¸¬è¾¨è­˜æ–¹ä½çš„ç³»çµ±ç­‰ã€‚æœ¬
äººå°è‰²å½©æ†å¸¸æ€§èˆ‡è‰²å½©ç‰¹å¾µçš„ç ”ç©¶èª²é¡Œç‰¹åˆ¥æ„Ÿèˆˆè¶£ï¼Œå®ƒä¸ä½†å¯ä»¥æ‡‰ç”¨åˆ°æ•¸ä½ç…§ç›¸
æ©Ÿï¼Œäº¦å¯æ‡‰ç”¨æ–¼è¦–è¨Šç”¢æ¥­ï¼Œå¯æ˜¯æ­¤æ¬¡ ICEEI 2011è‘—å¢¨æ–¼é€™äº›èª²é¡Œçš„è«–æ–‡éå¸¸å°‘ï¼Œ
å¯¦å±¬å¯æƒœã€‚ä¸éå¾æ‰€è†è½èˆ‡è§€çœ‹çš„è«–æ–‡äº†è§£ç³»çµ±æ•´åˆï¼Œå­¸ç¿’æ©Ÿåˆ¶åŠè¾¨è­˜æ‡‰ç”¨æ˜¯ç›®
å‰é›»è…¦ç§‘å­¸é ˜åŸŸçš„ç†±é–€ç ”ç©¶ä¸»é¡Œï¼Œæ­¤æ¬¡çš„ç ”è¨æœƒçµ¦äºˆæœ¬äººç›®å‰æ­£ç ”ç©¶çš„ä¸»é¡Œå¾ˆå¤š
é‡è¦çš„å•Ÿç™¼åŠæœªä¾†æ–¹å‘çš„è¨­å®šï¼Œæ”¶ç©«è±ç¢©ã€‚ 
 
ä¸‰ã€æ”œå›è³‡æ–™åç¨±åŠå…§å®¹ 
æ­¤æ¬¡æœƒè­°çµæŸå¾Œï¼Œæœ¬äººå¸¶å› ICEEI 2011è«–æ–‡é›†å…‰ç¢Ÿä¸€ç‰‡ã€è­°ç¨‹æ‰‹å†ŠåŠå€‹äººç­†
è¨˜ã€‚æœ¬äººéå¸¸æ„Ÿè¬åœ‹ç§‘æœƒè£œåŠ©æœ¬äººåƒåŠ æ­¤æ¬¡ç ”è¨æœƒã€‚åœ¨æœƒè­°ä¸­èªè­˜äº†ä¾†è‡ªå„åœ‹çš„
å­¸è€…ï¼Œç¶“ç”±è¨è«–èˆ‡æ„è¦‹äº¤æ›ï¼Œä½¿æˆ‘å—ç›Šè‰¯å¤šï¼ŒåŒæ™‚ä¹ŸåŸ‹ä¸‹æ—¥å¾Œé€²ä¸€æ­¥åˆä½œçš„ç¨®å­ã€‚ 
 
2011 International Conference on Electrical Engineering and Informatics 
17-19 July 2011, Bandung, Indonesia 
 
 
Color to Grayscale Transform Preserving  
Natural Order of Hues 
Chengho Hsin*, Hoai-Nam Le, and Shaw-Jyh Shin 
Department of Communications Engineering, Feng Chia University, Taichung, Taiwan 
*chhsin@fcu.edu.tw 
 
Abstract - Color-to-gray conversion in many applications re-
quires preserving both the visual appearance and feature dis-
crimination of color images. The existing methods are still need 
of improvement in these aspects. A two-stage color-to-gray trans-
form algorithm is proposed in this paper. It produces a grayscale 
image that matches the color contrast and the lightness of a given 
input color imag1e. The first stage process is a functional map-
ping scheme based on the natural order of hues. The mapping is 
simple and fast, yet it preserves feature discrimination and ap-
propriate color order of color images. The second stage of the 
proposed algorithm recovers the lightness of a color image 
through adjusting the local contrast of the first-stage output. It 
not only retains the visual appearance of the color image but also 
compensates the functional mapping for possible loss of color 
information. The proposed method could accurately convert 
various categories of color images to grayscale images, including 
natural images, computational images, and painting images.  
Experimental results show that our method produced percep-
tually accurate and preferred images when they were compared 
with other schemes. 
 
Keywords - Color-to-gray conversion, visual appearance 
I. INTRODUCTION 
Many applications require converting a color image into a 
grayscale representation, such as for reproducing on monoch-
rome devices, for subsequent image analysis, and for aesthetic 
intents. Color-to-grayscale conversion performs a reduction of 
the three-dimensional color data into a single dimension. The 
loss of information is inevitable during the conversion. Hence, 
the goal of the color-to-grayscale conversion is to retain as 
much information from the original color image as possible. 
At the same time, the aim is also to produce the grayscale im-
age with plausible visual appearance. Unfortunately, informa-
tion of isoluminant regions of a color image may be lost in its 
converted grayscale image when using the luminance channel 
(e.g., CIE L*) of a traditional color space. Fig. 1 shows an 
example of three grayscale images converted from an isolu-
minant color image. The lightness component L* of CIE 
L*a*b* color space [1] loses the color information completely. 
The luma component Yâ€™ of NTSC color space captures partial 
color contents. In contrast with these two color spaces, the 
grayscale image obtained by our method preserves all infor-
mation of the color image. 
To retain feature discrimination in color-to-gray conver-
sion, color differences in the input image should be reflected 
 
 
Original Color Image CIE L* 
 
NTSC Yâ€™ The Proposed Method 
Fig. 1 Our method maps isoluminant colors to unique and appropriately or-
dered grayscales. 
as much as possible onto the converted grayscale values. 
Many color-to-gray conversion methods have been proposed, 
which can be divided into two main categories: local mapping 
and global mapping [2].  In a local mapping method, the col-
or-to-gray mapping of pixel values is spatially varying, de-
pending on the local distributions of colors. Although a local 
mapping has advantages in accurately preserving local fea-
tures, but the constancy of color regions could be converted to 
inconstancy if the mapping could not produce same color val-
ues to the same grayscale values. In a global mapping method, 
the same color-to-gray mapping is used for all pixels in the 
input. It consistently maps the same colors to the same grays-
cale values over an image. However, it would be more chal-
lenging to require a global mapping that preserves local fea-
tures at different locations at the same time. 
In this paper, we present a two-stage color-to-gray conver-
sion algorithm that incorporates the merits of the global and 
local mapping schemes. The first stage of the proposed me-
thod performs a global functional mapping that preserves the 
natural hue order of the input color image. The second stage 
of our method is a local mapping scheme and its main objec-
tive is to make up for the possible loss of local feature dis-
crimination and visual appearance in the global mapped 
grayscale image. Experimental results show that the proposed 
method can successfully reproduce the visual appearance of 
color images. Even the isoluminannt color images can be con-
verted to the grayscale images with proper color order and 
H10 - 2
978-1-4577-0751-3/11/$26.00 Â©2011 IEEE
  
To attain global consistency, we adopt a global mapping 
scheme for color to grayscale conversion. In order to retain 
the maximum information of a color input image, we need to 
map not only intensity values of the color image to the grays-
cale resultant image, but also include the hue and saturation of 
color components in each pixel. The goal of this scheme is to 
preserve all details of a color image in its mapped grayscale 
image. Even some details that could not be observed clearly in 
the original color images are also perceptible in our grayscale 
images. This global mapping of a gamma-corrected RGB im-
age is specified by a simple functional form: 
  ),(),(),( yxCyxIyxG ww Î»+=                          (1) 
where ),( yxI w represents a weighted intensity component, 
),( yxCw denotes a weighted chrominance component, and Î» is 
a parameter to control the amount of chromatic variations ap-
plied to the intensity value. The weighted intensity is given by 
            BwGwRwyxI BGRw ++=),(                             (2) 
where ,, GR ww and Bw are weighting coefficients. The weighted 
chrominance is defined by 
),(),()),((),( yxIyxSyxHyxC www Î¸=            (3) 
where ),( yxÎ¸ and ),( yxS are the hue angle and saturation of 
the color value at a pixel ),( yx in HSI color space respective-
ly. The hue mapping function )(â‹…wH is specified by 
( ) 5.0),(cos5.0)),(( ++= Î²Î¸Î¸ yxyxH w                  (4) 
where the parameter Î² determines the order of hues. Let 
GR Î¸Î¸ , , and BÎ¸ be the hue angles of the primary colors R, G, 
and B, respectively. Let CY Î¸Î¸ , , MÎ¸ be the hue angles of the 
complementary colors of R, G, and B, which are yellow, cyan, 
and magenta. Since the warmest hue is usually a red. The 
coolest color is the complementary color to the warmest hue, 
which is a cyan. The natural order of hues is described by the 
order of warm and cool colors. In terms of the hue mapping 
function, the natural order of hues is preserved by the follow-
ing constraint 
).()()()()()( CHBHGHMHYHRH wwwwww >>>>>     (5) 
Define signed distance from hue A to hue B by
)()( BHAHD wwAB âˆ’= . The parameter Î²  is derived by max-
imizing the distances of the six color pairs as follows: 
( ){ }RCMGYBRBGBRG DDDDDD ,,,,,ofminimummaxÎ²        (6) 
We obtained the optimal D344=Î² . Conforming to the natural 
order of hues, the intensity weighting coefficients are assigned 
to the following values: 
                 
6
1and,
3
1,
2
1
=== BGR www                        (7) 
The third criterion of feature preservation is achieved by de-
termining an optimal Î» that decides the appropriate amount of 
chrominance component added to the weighted intensity val-
ue. Hence, both dimished details and exaggerated feature dis-
criminability are avoided in the resulted grayscale image.   
Fig. 2 shows the influence of the parameter Î» .  
 
 
  
Color Image 0=Î»  1=Î»  2=Î»  3=Î»  1.6=Î»  
Fig. 2 Varying the parameter Î»  
 
Color images include natural images, painting images, and 
computational images. A fixed value of the parameter Î»  for 
all kinds of images is incapable of producing satisfied grays-
cale images. Hence, the parameter Î»  should be chosen opti-
mally and automatically. 
We propose to determine an optimal Î»  by maximizing 
the standard deviation (STD) of the Sobel gradient of ),( yxG
in order to preserve the maximum information of color im-
ages. The range of the parameter Î»  is confined to [0, 
0.5/maxCw], where maxCw is the maximum value of Cw. The 
STD of the gradient of the grayscale image ),( yxG is able to 
represent the feature discriminability of the image.  However, 
Color images frequently contain gray regions where the red, 
green, and blue values of pixels are equal.  These gray areas 
may affect the correct measurement of feature discriminability 
by the STD of the gradient of the grayscale image ),( yxG . 
Fig. 3 shows an example of a color image that has a white 
background. The determined optimal Î» is zero. The resulted 
grayscale image is shown in Fig. 3(b). If the parameter Î»  is 
gradually increased in ),( yxG , the differences between the 
white background and the disks (the green and blue disks in 
the color image) are decreasing because the chrominance 
component Cw is add to these disks only. Hence, the gradient 
of the image ),( yxG is also diminishing. We are unable to 
clearly discriminate the color regions by evaluating the STD 
of the gradient of the image ),( yxG in this example. The solu-
tion to this problem will be addressed below. 
  
(a) Color Image (b) Grayscale Image 
Fig. 3 An example of a color image has a white background. The optimal Î» is 
zero. 
Create a color image ''' BGR by setting the gray pixels (R, 
G, and B are equal) of the input color image RGB to zero. 
Compute a temporary weighted intensity image, ),( yxT , as 
                             '
6
1'
3
1'
2
1),( BGRyxT ++=                                (8) 
By adding the chrominance component to this temporary 
weighted intensity, we obtain a temporary grayscale image as 
follows:   
),(),(),( yxCyxTyxB wÎ»+=                       (9) 
  
thors surveyed nearly 20000 human responses and used them 
to evaluate the accuracy and preference of the color-to-
grayscale conversions.  
The proposed method was implemented by using MAT-
LAB 7.1 software which runs on Pentium-IV 3.0 GHz CPU 
with 2.0 GB main memory. The same twenty four color im-
ages in categories of paintings, natural images, and computa-
tional images used in [3] were applied to evaluate the perfor-
mance of the proposed method. Due to the limit of the space, 
only eight test images along with their conversion results are 
shown in Fig. 5. Test images and resultant images of other 
methods are courtesy of ÄŒadÃ­k [3].  
The perceptual accuracy is the level of the closeness of the 
grayscale image and the original color image in appearance 
based on the criteria of the global consistency, the features 
preservation, and the ordering preservation. We compare the 
results of the proposed methods with both the best accuracy 
resultant images in ÄŒadÃ­kâ€™s paper [3] and the results of Kim et 
al. [4], as shown in Fig. 5. We could examine clearly in that 
the visual appearances of our results are better than the ones 
of other methods in most test images. For example, color or-
dering of Image 3 was maintained by the proposed method as 
shown in Fig. 5. The black boats are darker than the blue trees 
in Image 3. This visual appearance is reproduced faithfully by 
the proposed method. Furthermore, the sun is discriminated 
distinctively in our grayscale results. 
VI. CONCLUSIONS 
In this paper, we proposed a two-stage scheme to convert 
color images to grayscale images that can preserve maximum 
information of color images. Our algorithm not only is effec-
tive at preserving feature discrimination but also gives proper 
perceptual hue order of many kinds of image, e.g., painting 
images, computational images, and natural images, etc. These 
images may have various dynamic ranges of chrominance and 
luminance changes or they may contain isoluminant regions. 
ACKNOWLEDGEMENT 
This work has been supported by the National Science 
Council under Grant No. NSC 99-2221-E-0.35-091. 
REFERENCES 
[1] M. Fairchild, Color Appearance Models, Wiley, 2005. 
[2] K. Smith, P. Landes, J. Thollot, and K. Myszkowsky, â€œApparent grey-
scale: A simple and fast conversion to perceptually accurate images and 
video,â€ Computer Graphics Forum (Proc. Euro graphics 2008) 27, 2, 
193â€“200. 
[3] M. ÄŒadÃ­k, â€œPerceptual evaluation of color-to-grayscale image conver-
sions,â€ Computer Graphics Forum (Proc. Pacific Graphics 2008) 27, 7, 
1745â€“1754. 
[4] Y. Kim, C. Jang, J. Demouth, and S. Lee, â€œRobust color-to-gray via 
nonlinear global mapping,â€ ACM Transactions on Graphics (ACM SIG-
GRAPH Asia 2009), vol 28, no. 5.  
[5] R. Bala, and R. Eschbach, â€œSpatial color-to-grayscale transform preserv-
ing chrominance edge information,â€ in Proc. Color Imaging Conference 
2004, 82â€“86. 
[6] A. Gooch, S. Olsen, J. Tumblin, and B. Gooch, â€œColor2gray: salience-
preserving color removal,â€ ACM Trans. Graphics (Proc. SIGGRAPH 
2005) 24, 3, 634â€“639. 
[7] K. Rasche, R. Geist, and J. Westall, â€œRe-coloring images for gamuts of 
lower dimension,â€ Computer Graphics Forum (Proc. Euro graphics 2005) 
24, 3, 423â€“432. 
[8] L. Neumann, M. ÄŒadÃ­k, and A. Nemcsics, â€œAn efficient perception-based 
adaptive color to gray transformation,â€ In Proc. Computational Aesthet-
ics 2007, 73â€“ 80. 
[9] A. Nemcsics, â€œRecent experiments investigating the harmony interval 
based colour space of the coloroid colour system,â€ in AIC 9th Congress 
Rochester, 2001 
[10] M Grundland, and N. Dodgson, â€œDecolorize: Fast, contrast enhancing, 
color to grayscale conversion,â€ Pattern Recognition 40, 11, 2891â€“2896, 
2007. 
[11] Y. Nayatani, â€œSimple estimation methods for the Helmholtz Kohlrausch 
effect,â€ Color Research and App. 22, 6, 385â€“401, 1997. 
 
 
  
åœ‹ç§‘æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•«é …ä¸‹å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
                                   æ—¥æœŸï¼š100å¹´ 08æœˆ 5 æ—¥ 
è¨ˆç•«ç·¨è™Ÿ NSC 99-2221-E035-091 
è¨ˆç•«åç¨± 
é”æˆç²¾ç¢ºçš„å½©è‰²å½±åƒå¤–è²Œï¼šå»ºæ§‹ä¸€å€‹èåˆå½±åƒå¼·åŒ–èˆ‡è‰²å½©æ†å¸¸æ€§çš„æ¼”ç®—æ³• 
Achieving Accurate Color Image Appearance through Fusion of Image 
Enhancement and Color Constancy 
å‡ºåœ‹äººå“¡
å§“å 
è¾›æ­£å’Œ 
æœå‹™æ©Ÿæ§‹
åŠè·ç¨± 
é€¢ç”²å¤§å­¸ 
   å‰¯æ•™æˆ 
æœƒè­°æ™‚é–“ 
100å¹´ 7æœˆ 26æ—¥
è‡³ 
100å¹´ 7æœˆ 28æ—¥ 
æœƒè­°åœ°é» 
ä¸­åœ‹å¤§é™¸æ­å·å¸‚ 
æœƒè­°åç¨± 
(ä¸­æ–‡) ç¬¬äºŒå±†å¤šåª’é«”ç§‘æŠ€åœ‹éš›å­¸è¡“æœƒè­° 
(è‹±æ–‡) The 2nd International Conference on Multimedia Technology 
(ICMT2011) 
ç™¼è¡¨è«–æ–‡
é¡Œç›® 
(ä¸­æ–‡) åŸºæ–¼é«˜æ–¯å¾®åˆ†å°æ³¢æ¿¾æ³¢å™¨çµ„çš„éåƒè€ƒå¼å®¢è§€å½±åƒéŠ³åˆ©åº¦é‡æ¸¬æ©Ÿåˆ¶ 
(è‹±æ–‡) A No-Reference Objective Image Sharpness Metric based on a Filter Bank of 
Gaussian Derivative Wavelets 
ä¸€ã€åƒåŠ æœƒè­°ç¶“é 
ICMT 2011å¤šåª’é«”ç§‘æŠ€åœ‹éš›å­¸è¡“æœƒè­°ä»Šå¹´åœ¨ä¸­åœ‹å¤§é™¸æ­å·å¸‚èˆ‰è¡Œï¼Œæœƒè­°å¾ 7æœˆ 26
æ—¥é–‹å§‹ 7æœˆ 28æ—¥çµæŸï¼Œç‚ºæœŸä¸‰å¤©ã€‚ç ”è¨æœƒä¸»é¡Œæ¶µè“‹å¤šåª’é«”å·¥å…·èˆ‡ç³»çµ±ã€å½±åƒè™•ç†æŠ€
è¡“ã€ç¶²è·¯æ‡‰ç”¨åŠå¤šåª’é«”å¨›æ¨‚æœå‹™ç­‰ã€‚æœƒè­°ç”± IEEEåˆ†æœƒä¸»è¾¦ï¼Œå”è¾¦å–®ä½åŒ…æ‹¬ç¾åœ‹çš„
University of Louisvilleï¼ŒGeorgia State Universityã€ä¸­åœ‹å¤§é™¸çš„å¯§æ³¢å¤§å­¸ã€æµ™æ±Ÿç†å·¥å¤§
å­¸ã€ä¸­åœ‹é›»ä¿¡å¤§å­¸ã€å—äº¬å¤§å­¸ã€ä¸Šæµ·äº¤é€šå¤§å­¸åŠåŒ—äº¬ç†å·¥å­¸é™¢ç­‰ã€‚æœƒè­°çš„å®—æ—¨æ˜¯å¸å¼•
å…¨çƒå„ªç§€çš„å­¸è€…å°ˆå®¶åŠå·¥ç¨‹å¸«ç™¼è¡¨å…¶æœ€æ–°ç ”ç©¶çµæœï¼Œä¸¦èƒ½ç›¸äº’äº¤æµèˆ‡å»ºç«‹åˆä½œå¹³å°ï¼Œ
ç ”ç©¶ç›¸é—œçš„æ¼”è¬›å…§å®¹ä½œä¸€ç°¡è¦ä»‹ç´¹ã€‚ç¬¬ä¸€ä½è¬›è€…æ˜¯ä¾†è‡ªç¾åœ‹ UCLAå¤§å­¸çš„
Terzopoulosæ•™æˆï¼Œç›®å‰æ˜¯ ACMã€IEEEèˆ‡åŠ æ‹¿å¤§çš‡å®¶ç¤¾åœ˜çš„æœƒå£«ï¼Œä¸»è¦ç ”ç©¶é ˜åŸŸ
ç‚ºé›»è…¦åœ–å­¸ï¼Œé›»è…¦è¦–è¦ºã€é†«å­¸å½±åƒã€é›»è…¦è¼”åŠ©è¨­è¨ˆåŠäººå·¥æ™ºæ…§ç­‰é ˜åŸŸï¼Œæ›¾ç²å¾—å¤š
æ¬¡æœ€ä½³è«–æ–‡å¼•ç”¨çã€‚é€™æ¬¡çš„æ¼”è¬›ä¸»é¡Œæ˜¯è™›æ“¬å¯¦æ™¯çš„é›»è…¦è¦–è¦ºï¼Œè¬›è€…é¦–å…ˆèªªæ˜å¾ˆå¤š
çš„é›»è…¦è¦–è¦ºç³»çµ±å—é™æ–¼å¯¦éš›ç’°å¢ƒçš„é™åˆ¶æˆ–å¯¦é©—è¦æ¨¡éæ–¼é¾å¤§ï¼Œç„¡æ³•é€²è¡Œå¯¦éš›çš„å¯¦
é©—ï¼Œå› æ­¤å€¡è­°åœ¨è™›æ“¬å¯¦æ™¯ä¸­æ¸¬è©¦é›»è…¦è¦–è¦ºç³»çµ±çš„è¡¨ç¾åŠå¯è¡Œæ€§ã€‚ä¾‹å¦‚åœ¨å¤§è¦æ¨¡çš„
åˆ†ä½ˆå¼æ™ºæ…§æ”å½±æ©Ÿç¶²è·¯æˆ–å¤§å‹æ™ºæ…§å‹ç›£æ§ç’°å¢ƒçš„æ‡‰ç”¨ï¼Œç›´æ¥æ¸¬è©¦æ‰€è¦ç™¼å±•çš„é›»è…¦
è¦–è¦ºæ¼”ç®—æ¶æ§‹æ˜¯æœ‰å›°é›£çš„ï¼Œè€Œå¦‚æœèƒ½é€éé›»è…¦ç¹ªåœ–æŠ€è¡“å…ˆå»ºç½®è™›æ“¬çš„æ¸¬è©¦ç’°å¢ƒï¼Œ
æ­¤å›°é›£ä¾¿èƒ½è¿åˆƒè€Œè§£ã€‚ä½†å•é¡Œæ˜¯å¦‚ä½•è­‰æ˜åœ¨è™›æ“¬ç’°å¢ƒä¸­å¯è¡Œçš„æ¼”ç®—æ¶æ§‹èƒ½å¤ åœ¨çœŸ
å¯¦ç’°å¢ƒä¸­ä¹Ÿæ˜¯æˆåŠŸçš„ï¼Œä¾¿æ˜¯ä¸€å€‹çˆ­è«–é»ä¹Ÿæ˜¯ä¸€å€‹é‡è¦èª²é¡Œï¼Œé€™å€‹æ¦‚å¿µéå¸¸æœ‰è¶£ä¸”
å¯Œæœ‰å‰µæ„ï¼Œç›¸ä¿¡å¯åœ¨é›»è…¦è¦–è¦ºé ˜åŸŸè£é–‹å‰µä¸€å€‹æ–°çš„ç ”ç©¶åˆ†æ”¯ã€‚ç¬¬äºŒä½è¬›è€…æ˜¯ä¾†è‡ª
ä¸Šæµ·äº¤é€šå¤§å­¸çš„æ¥Šæ•™æˆï¼Œæ¼”è¬›ä¸»é¡Œæ˜¯ä¸‰ç¶­è¦–è¦ºåŒ–æŠ€è¡“èˆ‡æ‡‰ç”¨ï¼Œä¸»è¦ä»‹ç´¹åœ¨å½±åƒåˆ†
å‰²ã€å½±åƒå®šä½åŠä¸‰ç¶­é¡¯åƒç­‰æ–¹é¢çš„æ”¹å–„æŠ€è¡“ï¼Œä¸¦å°‡å…¶æ‡‰ç”¨æ–¼å¤–ç§‘æ‰‹è¡“çš„ä¸‰ç¶­è¦–è¦º
åŒ–ï¼Œä¸‰ç¶­äººè‡‰çš„é‡å»ºèˆ‡è¾¨è­˜åŠç¶²éš›ç¶²è·¯çš„é›»å­å•†å‹™ã€‚ç¬¬ä¸‰ä½è¬›è€…æ˜¯ä¾†è‡ªç¾åœ‹ 
University of Louisvilleçš„ Faragæ•™æˆï¼Œæ¼”è¬›ä¸»é¡Œæ˜¯æ‹“æ’²å®šä½(topological registration)
çš„å…ˆé€²æ–¹æ³•èˆ‡æ‡‰ç”¨ï¼Œé¦–å…ˆé‡å°æ‹“æ’²å®šä½æ–¹æ³•é€²è¡ŒåŸºæœ¬èˆ‡æ•´é«”çš„ä»‹ç´¹ï¼Œä¸¦èªªæ˜å…¶å¯¦
é©—å®¤ç›®å‰åœ¨æ­¤æ–¹é¢çš„ç ”ç©¶çµæœä»¥åŠå…¶åœ¨ç”Ÿé†«æˆåƒã€ç”Ÿç‰©ç‰¹å¾µè¾¨è­˜åŠå…¶å®ƒè³‡æ–™çµæ§‹
çš„æ‡‰ç”¨ã€‚ç¬¬å››ä½è¬›è€…æ˜¯ä¾†è‡ªæ–¼è‹±åœ‹ Loughborough Universityçš„ Schaeferæ•™æˆï¼Œå…¶
æ¼”è¬›ä¸»é¡Œæ˜¯ç€è¦½å¤§å‹å½±åƒè³‡æ–™åº«çš„æ–¹æ³•ï¼Œæ ¸å¿ƒè§€å¿µæ˜¯å¦‚ä½•è‡ªå‹•çš„å°‡ç›¸ä¼¼çš„å½±åƒé›†
                                 
 
 
ä¸‰ã€æ”œå›è³‡æ–™åç¨±åŠå…§å®¹ 
æ­¤æ¬¡æœƒè­°çµæŸå¾Œï¼Œæœ¬äººæ”œå› ICMT2011è«–æ–‡é›†å…‰ç¢Ÿä¸€ç‰‡ï¼Œå¤§æœƒè­°ç¨‹æ‰‹å†ŠåŠå€‹äººç­†
è¨˜ã€‚æœ¬äººéå¸¸æ„Ÿè¬åœ‹ç§‘æœƒèˆ‡é€¢ç”²å¤§å­¸åˆ†åˆ¥è£œåŠ©æœ¬äººåƒåŠ æ­¤æ¬¡ç ”è¨æœƒã€‚åœ¨æœƒè­°ä¸­èª
è­˜äº†ä¾†è‡ªå„åœ‹çš„å­¸è€…ï¼Œç¶“ç”±è¨è«–èˆ‡æ„è¦‹äº¤æµï¼Œä½¿æˆ‘å—ç›Šè‰¯å¤šï¼ŒåŒæ™‚ä¹ŸåŸ‹ä¸‹æ—¥å¾Œé€²
ä¸€æ­¥åˆä½œçš„ç¨®å­ã€‚ 
 
 A No-Reference Objective Image Sharpness Metric 
based on a Filter Bank of Gaussian Derivative 
Wavelets 
 
Chengho Hsin*, Jr-Wei Jang, Shaw-Jyh Shin, Shin-Hsien Chen 
Department of Communications Engineering  
Feng Chia University 
Taichung, Taiwan 
chhsin@fcu.edu.tw* 
 
 
Abstractâ€”The quality of an image depends on various attributes 
such as sharpness (or blurriness), naturalness, colorfulness, and 
contrast, etc. To develop a so-called no-reference objective image 
quality metric by incorporating all attributes of images without 
referring to the original ones is a difficult task. Hence, the 
purpose of this paper focuses on the development of a no-
reference objective image sharpness metric. A high-frequency 
component preserving the full information of a given input image 
is extracted first and then applied to a multi-channel filter bank. 
The proposed method measures image sharpness based on the 
output of this filter bank. A snug frame composed of the 
Gaussian derivative wavelets is applied to construct the filter 
bank. The output of the filter bank not only contains the 
complete information of the input but also manifests prominent 
image features. Experimental results show that the metric 
predicts well in isotropic and uniform blur case. Validated by the 
high correlation between the metric values and the subjective test 
scores, the performance of the proposed metric is comparable to 
that of human subjects. 
Keywords- sharpness metric; blurriness metric 
I.  INTRODUCTION 
Image quality assessment is indispensable to various image 
processing applications such as compression, transmission, 
enhancement, restoration, printing, display, and analysis. 
Although subjective quality assessment is considered to be the 
most accurate and reliable approach it is expensive and time-
consuming and is inappropriate for real-time implementation.  
The development of objective metrics that can precisely 
predict the perceived image quality is in great demand. 
Objective metrics can be categorized into full-reference, 
reduced reference, and no-reference. A reference such as the 
original image is needed for comparison with the processed 
image in the full-reference scheme. Reduced-reference 
objective metrics only require partial information about the 
original image. In many circumstances, the reference image is 
not available for the assessment task. Hence, objective metrics 
using the reference image impose a limitation on their 
applications. On the other hand, a no-reference metric scheme 
computes the perceived visual quality directly from a given 
image without referring to the reference image. It is much more 
useful than the other two categories. Image quality is governed 
by a variety of factors such as sharpness, naturalness, 
colorfulness, contrast, and noise etc. We shall concentrate on 
the work of the no-reference image sharpness (or blurriness) 
metric. Note that the image sharpness metric can also be 
applied to measure blurriness since they are inversely related. 
A general review of no-reference objective image 
sharpness/blurriness metrics were given in [1]. Only recent 
methods with high performance results [1, 2] are described 
here. Metrics [1-3] rely on the determination of the edge width 
to evaluate the amount of blur. These methods first find edge 
pixels using common edge detectors. The start and end 
positions of the edge for each edge pixel are found. The edge 
width is computed as the distance between the end and start 
positions. Marziliano et al. [3] have derived the overall blur 
metric as the average widths of the edges. Ferzli and Karam [1] 
pointed out that the previous method is unable to predict the 
relative amount of blurriness in images with different content. 
Hence, they proposed an image sharpness metric based on the 
notion of just noticeable blur (JNB). The JNB is the minimum 
amount of perceived blurriness around an edge given a contrast 
higher than the just noticeable difference. The probability of 
detecting a blur distortion is determined by the edge width soft 
thresholded with the corresponding JNB. The block-based 
probability summation model is adopted to compute the 
sharpness metric. It was shown in [2] that the results of the 
JNB method do not correlate well with subjective scores for 
images with non-uniform saliency content. To solve this 
problem, Narvekar and Karam [2] developed a sharpness 
metric based on a cumulative probability of blur detection 
(CPBD) to improve the JNB method. In contrast with the edge-
base approach, Vu and Chandler [4] proposed a block-based 
algorithm, named 3S metric, designed to measure the local 
perceived sharpness in an image. For each block, a spectral 
sharpness measure is determined by the slope of the magnitude 
spectrum and a spatial sharpness measure is calculated by the 
total spatial variation. A perceived sharpness map is generated 
by combining the two measures via a weighted geometric mean. 
The overall sharpness of an image is obtained by the maximum 
value of the sharpness map.  
This work has been supported by the National Science Council under Grant 
No. NSC 99-2221-E-035-091 
3362
978-1-61284-774-0/11/$26.00 Â©2011 IEEE
                     âˆ‘âˆ‘
= =
==
M
m
M
n
kk nmbnmbkB
1 1
2
2
)],([),()(             (2) 
We define the global band-pass image contrast of the kth 
channel, C(k), by 
                                      âˆ‘
=
= L
i
iB
kBkC
1
)(
)()(                             (3) 
where Lk â‰¤â‰¤0 and âˆ‘
=
=
L
k
kC
1
1)( . Fig. 4 illustrates the 
behavior of the global band-pass image contrast for three 
different input images which were obtained from the UT 
Austin LIVE database [6]. Figs. 4 (a), (b), and (c) show the 
blurred versions of the Student Sculpture image, Lighthouse 
image, and Church and Capitol image using a circularly 
symmetric 2-D Gaussian function having a standard deviation 
of 0.53, 2.74, and 7.67, respectively. Fig. 4(d) shows the three 
curves of the global band-pass image contrast distributions for 
Figs. 4(a)-(c), where the red, blue, and green curves correspond 
to Fig. 4(a), Fig. 4(b), and Fig. 4(c), respectively. The three 
curves are monotonically decreasing with respect to the 
channel index. We are unable to differentiate different 
blurriness by examining these curves. We shall address this 
issue below. 
 
Figure 2.  Extraction of high-pass component. 
 
Figure 3.  The structure of the multi-channel filter bank. 
C. Channel Weighting 
According to the idea of the matched filtering, the most 
significant channel is determined by channel comparison and 
then it is applied to compute the sharpness metric. We define 
the weighted global band-pass image contrast as 
              Lk
nmh
kCkCW
k
,,1,0,
),(
)()(
2
L==                       (4) 
where ),( nmhk  is the equivalent impulse response of the kth 
band-pass channel. The most significant band-pass channel is 
determined by 
                    { }LkkCWk ,,1,0,)(maxargmax K==             (5)  
The sharpness metric is given by 
              
âªâªâ©
âªâªâ¨
â§
â‰¥
=
=
=
3,)2(
2,)1(
10,)0(
max
max
max
kC
kC
orkC
metric sharp
                               (6) 
The value of sharpmetric is between 0 and 1. The sharper the 
image, the higher the metric is. Equivalently, the blurriness 
metric is defined by sharpblur metricmetric âˆ’=1  Fig. 5 shows the 
three curves that represent the distributions of the weighted 
global band-pass image contrast of Figs. 4 (a)-(c). The red 
curve corresponding to Fig. 4(a) has a peak at maxk  = 1. 
According to (6), the sharpness metric is 0.6995. The blue 
curve corresponding to Fig. 4(b) reaches maximum at maxk  = 2, 
and the sharpness metric of this image is 0.327. Finally the 
green curve corresponding to Fig. 4(c) attains the maximum at 
maxk  = 3, and the sharpness of this image is 0.16. 
               
(a)                                                         (b) 
        
(c)                                                  (d) 
Figure 4.  Three blurred images and their global band-pass image contrast 
distributions. 
 
 
3364
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«è¡ç”Ÿç ”ç™¼æˆæœæ¨å»£è³‡æ–™è¡¨
æ—¥æœŸ:2011/10/30
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«
è¨ˆç•«åç¨±: é”æˆç²¾ç¢ºçš„å½©è‰²å½±åƒå¤–è²Œï¼šå»ºæ§‹ä¸€å€‹èåˆå½±åƒå¼·åŒ–èˆ‡è‰²å½©æ†å¸¸æ€§çš„æ¼”ç®—æ³•
è¨ˆç•«ä¸»æŒäºº: è¾›æ­£å’Œ
è¨ˆç•«ç·¨è™Ÿ: 99-2221-E-035-091- å­¸é–€é ˜åŸŸ: å½±åƒè™•ç†
ç„¡ç ”ç™¼æˆæœæ¨å»£è³‡æ–™
å…¶ä»–æˆæœ 
(ç„¡æ³•ä»¥é‡åŒ–è¡¨é”ä¹‹æˆ
æœå¦‚è¾¦ç†å­¸è¡“æ´»å‹•ã€ç²
å¾—çé …ã€é‡è¦åœ‹éš›åˆ
ä½œã€ç ”ç©¶æˆæœåœ‹éš›å½±éŸ¿
åŠ›åŠå…¶ä»–å”åŠ©ç”¢æ¥­æŠ€
è¡“ç™¼å±•ä¹‹å…·é«”æ•ˆç›Šäº‹
é …ç­‰ï¼Œè«‹ä»¥æ–‡å­—æ•˜è¿°å¡«
åˆ—ã€‚) 
æ­¤æ¬¡ç™¼è¡¨çš„ ICMT åœ‹éš›æœƒè­°è«–æ–‡ç²å¾—å¤§æœƒæœ€ä½³è«–æ–‡çã€‚ 
 æˆæœé …ç›® é‡åŒ– åç¨±æˆ–å…§å®¹æ€§è³ªç°¡è¿° 
æ¸¬é©—å·¥å…·(å«è³ªæ€§èˆ‡é‡æ€§) 0  
èª²ç¨‹/æ¨¡çµ„ 0  
é›»è…¦åŠç¶²è·¯ç³»çµ±æˆ–å·¥å…· 0  
æ•™æ 0  
èˆ‰è¾¦ä¹‹æ´»å‹•/ç«¶è³½ 0  
ç ”è¨æœƒ/å·¥ä½œåŠ 0  
é›»å­å ±ã€ç¶²ç«™ 0  
ç§‘ 
æ•™ 
è™• 
è¨ˆ 
ç•« 
åŠ  
å¡« 
é … 
ç›® è¨ˆç•«æˆæœæ¨å»£ä¹‹åƒèˆ‡ï¼ˆé–±è½ï¼‰äººæ•¸ 0  
 
