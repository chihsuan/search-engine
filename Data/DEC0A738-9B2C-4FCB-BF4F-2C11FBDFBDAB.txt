 2
ç›®ï¤¿ 
 
ä¸€ã€ ä¸­æ–‡æ‘˜è¦               3 
äºŒã€ è‹±æ–‡æ‘˜è¦               4 
ä¸‰ã€ è¨ˆç•«ç·£ç”±èˆ‡ç›®çš„             5 
å››ã€ çµæœèˆ‡è¨ï¥              6 
äº”ã€ è¨ˆç•«æˆæœè‡ªè©•              15 
ï§‘ã€ ï¥«è€ƒæ–‡ç»               15 
ä¸ƒã€ é™„ï¤¿ï¼ˆå·²ç™¼è¡¨ï¥æ–‡ï¼‰            16 
 
 4
äºŒã€è‹±æ–‡æ‘˜è¦ 
 
The current state-of-the-art approach to automatic speech recognition (ASR) is based on the 
data-driven concept, which learns speech by constructing acoustic and language models 
exclusively from corpora. Unfortunately, even the best ASR system nowadays gives much 
higher error rates in some rather simple tasks, compared to human speech recognition. 
Though some specifically-suitable applications do exist, ASR performance is still far away 
from usersâ€™ expectation. Until more recently, many researchers suggest that we should go 
back to bring in the linguistic and phonological knowledge, which has been largely ignored 
during this decade. The next generation ASR should follow a paradigm of integrating both 
knowledge-based and data-driven techniques. To make this paradigm a reality, a shared 
platform for system design and assessment must be set up for bringing in more researchers 
together to this area. The first phase of this collaborative project attempted to set up a shared 
platform for ASR research in Taiwan. Several universities and research organizations joined 
together to carry out this collaborative project so as to promote the fundamental research of 
the next generation ASR. The major research items of the second phase include (1) detection 
of speech attributes and events, (2) event integration and evidence verification, (3) 
knowledge-bases, corpora, models, tools, and the shared platform, (4) output enhancement 
and post processing, and (5) integration of known linguistic and acoustic knowledge into the 
next generation ASR. Under this collaborative mode, the knowledge-bases, corpora, models, 
and tools generated will be opened to public so as to bring in more researchers to the ASR 
area. This gives a decisive chance to promote the research and development of speech 
technology in Taiwan. 
As an indispensable part of the whole collaborative project, this subproject focuses on: 
(1) establishment of phone-labeled speech corpora for benchmark test, (2) study on acoustic 
features for automatic phone segmentation and attribute and event detection, (3) development 
of automatic audio segmentation, (4) development of automatic phone segmentation, (5) 
study on feature selection, and (6) study on classifiers. 
 
Keywords: speech recognition, speech attributes, speech events, automatic labeling, 
automatic audio segmentation, speech corpus 
 
 6
å››ã€çµæœèˆ‡è¨ï¥ 
 
æœ¬è¨ˆç•«çš„ä¸»è¦å·¥ä½œé …ç›®åŒ…æ‹¬ï¼š(1)è‡ªå‹•éŸ³ç´ åˆ†æ®µ(automatic phone segmentation)æŠ€è¡“æ”¹ï¥¼ï¼Œ
ä»¥åŠ é€Ÿèªï¦¾åº«æ¨™è¨˜çš„é€²ï¨ï¼›(2)åœ‹èªèªï¦¾åº«çš„éŸ³ç´ è‡ªå‹•æ¨™è¨˜åŠäººå·¥ä¿®è¨‚ï¼›(3)èªéŸ³äº‹ä»¶åµ
æ¸¬æŠ€è¡“æ”¹ï¥¼ï¼› (4)åŸºæ–¼åµæ¸¬ä¹‹è‡ªå‹•èªéŸ³è¾¨ï§¼ç³»çµ± (detection-based automatic speech 
recognition system)çš„å»ºç½®èˆ‡æ”¹ï¥¼ï¼›(5)éŸ»ï§˜ç‰¹å¾µ(Prosodic Feature)å°èªéŸ³ç‰¹å¾µåµæ¸¬å½±éŸ¿è©•
ä¼°ã€‚ 
 
(1) è‡ªå‹•éŸ³ç´ åˆ†æ®µæŠ€è¡“æ”¹ï¥¼ 
æˆ‘å€‘åœ¨ç¬¬ä¸€éšæ®µä¸‰ï¦æœŸè¨ˆç•«æå‡ºä¸€å€‹åŸºæ–¼HMMå¼·è¿«æ ¡æº–(forced alignment)åŠSVMé‚Šç•Œ
èª¿æ•´çš„ï¥¸éšæ®µè‡ªå‹•éŸ³ç´ åˆ†æ®µæŠ€è¡“ï¼Œæœ¬è¨ˆç•«é‡å°HMMå¼·è¿«æ ¡æº–åŠéŸ³ç´ é‚Šç•Œèª¿æ•´åˆ†åˆ¥æå‡º
æ”¹ï¥¼çš„æŠ€è¡“ã€‚ 
 
A.HMMè‡ªå‹•éŸ³ç´ åˆ†æ®µæŠ€è¡“æ”¹ï¥¼ 
å°± HMM è²å­¸æ¨¡å‹è¨“ï¦–è€Œè¨€ï¼Œæˆ‘å€‘åœ¨ç¬¬ä¸€éšæ®µä¸‰ï¦æœŸè¨ˆç•«æ‰€æå‡ºçš„æœ€å°é‚Šç•Œèª¤å·®
(minimum boundary error, MBE)è¨“ï¦–æ³•ç›¸è¼ƒæ–¼å‚³çµ±çš„ ML è¨“ï¦–æ–¹å¼å·²æœ‰ç›¸ç•¶å¤§çš„çªç ´
[1-2]ã€‚å‡è¨­æœ‰ R ï¤†è¨“ï¦–èªï¦¾çš„ç‰¹å¾µå‘ï¥¾åºï¦œï¼Œå³ ï» ï½ROOO ,..,1ï€½ ã€‚æœ€å°é‚Šç•Œèª¤å·®çš„ç›®æ¨™å‡½
å¼å®šç¾©ç‚ºæ‰€æœ‰èªï¤†æœŸæœ›é‚Šç•Œèª¤å·®ä¹‹ç¸½å’Œï¼Œå³ï¼š 
ïƒ¥ïƒ¥
ï€½ ïƒ
ï€½ R
r
S
r
c
r
i
rr
iMBE rri SSEROSPF
1
),()|(Î¦       (1) 
å…¶ä¸­ riS æ˜¯ä¸€æ¢é‚Šç•Œåºï¦œï¼› rÎ¦ ç‚ºæ‰€æœ‰å¯èƒ½é‚Šç•Œåºï¦œæ‰€å½¢æˆçš„é›†åˆï¼› )| rri OSPï€¨ ç‚º riS åœ¨çµ¦
å®š rO æ™‚çš„äº‹å¾Œæ©Ÿï¥¡ï¼› ),( rcri SSER å‰‡å®šç¾©ç‚º riS èˆ‡å¯¦éš›é‚Šç•Œ rcS çš„èª¤å·®ï¼Œå³åºï¦œä¸­æ¯å€‹éŸ³ç´ 
é‚Šç•Œèª¤å·®ä¹‹å’Œã€‚é¡¯ç„¶åœ°ï¼Œè¦é”åˆ°æœ€å°æ­¤ç›®æ¨™å‡½å¼çš„ç›®çš„ï¼Œé ˆèª¿é«˜é‚Šç•Œèª¤å·®è¼ƒå°ä¹‹é‚Šç•Œåº
ï¦œçš„äº‹å¾Œæ©Ÿï¥¡ï¼Œèª¿ä½é‚Šç•Œèª¤å·®è¼ƒå¤§ä¹‹é‚Šç•Œåºï¦œçš„äº‹å¾Œæ©Ÿï¥¡ï¼Œé€²è€Œä½¿æ•´é«”çš„é‚Šç•Œèª¤å·®æœŸæœ›
å€¼é”åˆ°æœ€å°ã€‚ç„¶è€Œï¼Œæœ€å°åŒ–é‚Šç•Œèª¤å·®çš„æ•´é«”æœŸæœ›å€¼å»æœ‰å¯èƒ½é€ æˆéï¨è¨“ï¦–
(over-training)ï¼Œä½¿æ¨¡å‹ç„¡æ³•æä¾›è¼ƒä½³çš„ä¸€èˆ¬æ€§(generalization)ã€‚ 
æˆ‘å€‘ç”¨ä¸‹é¢çš„ï¦µå­ï¤­ï¥¯æ˜ã€‚å‡è¨­ ï» ï½321 ,, SSSï€½Î¦ åŒ…å«ä¸‰æ¢é‚Šç•Œåºï¦œï¼Œä¸”
5.5),( 1 ï€½cSSER ã€ 3.2),( 2 ï€½cSSER åŠ 7.6),( 1 ï€½cSSER ï¼Œï¥´äº‹å¾Œæ©Ÿï¥¡åˆ†ä½ˆæˆ‘å€‘åªè€ƒæ…®ï€¨ ï€©OSP iA | åŠ ï€¨ ï€©OSP iB | ï¥¸çµ„ï¼Œå‰‡å¾è¡¨ä¸€å¯ï¨Šï¼Œåœ¨æ­¤ï¦µä¸­ï¼Œæœ€å°é‚Šç•Œèª¤å·®è¨“ï¦–æ³•å°‡åå¥½ BP ï¼Œ
å› ç‚º BP çš„é‚Šç•Œèª¤å·®æœŸæœ›å€¼ç‚º3.65å°æ–¼ AP çš„4.36ï¼›ç„¶è€Œï¼Œï¥´æˆ‘å€‘å°‡æ­¤ä¸‰æ¢é‚Šç•Œåºï¦œä»¥é‚Šç•Œ
èª¤å·®å€¼ç”±å°è‡³å¤§æ’ï¦œå¯å¾—åˆ° 312 ,, SSS ï¼Œè€Œä»¥ AP åŠ BP ç”±å¤§è‡³å°æ’ï¦œå¯åˆ†åˆ¥å¾—åˆ° 312 ,, SSS åŠ
132 ,, SSS ï¼Œåå€’æ˜¯ AP çš„æ’åºèˆ‡é‚Šç•Œèª¤å·®è¼ƒç‚ºä¸€è‡´(consistent)ï¼›æ›ï¤†è©±ï¥¯ï¼Œç•¶è€ƒæ…®åˆ°æ’åº
çš„ä¸€è‡´æ€§æ™‚ï¼Œæˆ‘å€‘å°‡åå¥½é¸æ“‡ AP ã€‚ 
 
è¡¨ä¸€ã€æœ€å°é‚Šç•Œèª¤å·®è¨“ï¦–æ³•èˆ‡æœ€å°æ’åºéŒ¯èª¤è¨“ï¦–æ³•çš„æ¯”è¼ƒã€‚ 
 ),( ci SSER )|( OSP iA )|( OSP iB
1S  5.5 0.30 0.20 
2S  2.3 0.45 0.45 
3S  6.7 0.25 0.35 
é‚Šç•Œèª¤å·®æœŸæœ›å€¼ 4.36 3.65 
å°åºéŒ¯èª¤ï¥© 0 1 
 
è€ƒæ…®æ’åºçš„ä¸€è‡´æ€§æ˜¯å¦åˆï§¤ï¼Ÿæ‰¿è¥²ä¸Šï¦µï¼Œï¥´æ¸¬è©¦æ™‚æ‰€ç”¢ç”Ÿçš„åºï¦œé›†åˆÎ¦ä¸­æœªåŒ…å«
2S ï¼Œå°‡å°è‡´ BP æŒ‘é¸å‡º 3S ï¼Œä½¿å¾—é‚Šç•Œèª¤å·®é”åˆ°6.7ï¼Œé«˜æ–¼ä»¥ AP é¸å‡º 1S çš„5.5ã€‚é€™é›–ç„¶åƒ…åƒ…
æ˜¯ä¸€å€‹ç‰¹ï¦µï¼Œå»ï¥¯æ˜ï¦ºè€ƒæ…®æ’åºçš„ä¸€è‡´æ€§å°‡æœ‰åŠ©æ–¼é¸å‡ºè¼ƒå°é‚Šç•Œèª¤å·®çš„é‚Šç•Œåºï¦œã€‚æ•…æˆ‘
 8
å¦å¤–ï¼Œæˆ‘å€‘ä¹Ÿï§ç”¨æ’åºæå‡æ³•(Rank Boost)å¯¦ä½œéç·šæ€§æ’åºæ¨¡å‹ RANKBOOSTKMå’Œ
RANKBOOSTDTï¼Œå®ƒå€‘å…·å‚™å€æ®µç·šæ€§çš„åˆ†ï§èƒ½ï¦Šã€‚ 
å¯¦é©—çµæœå¦‚è¡¨äºŒæ‰€ç¤ºã€‚é‚Šç•Œèª¿æ•´æ¨¡å‹åœ¨ HMM è‡ªå‹•éŸ³ç´ åˆ†æ®µçš„åˆå§‹é‚Šç•Œå·¦å³å„ 5 æ¯«
ç§’ç¯„åœå…§å°‹æ‰¾æœ€ä½³çš„é‚Šç•Œã€‚SVMKMæ˜¯æ¡ç”¨ K-å‡ï¥©åˆ†ç¾¤çš„çµæœï¼Œè€Œ SVMDTå‰‡æ˜¯æ±ºç­–æ¨¹åˆ†
ç¾¤çš„çµæœï¼Œï¥¸ç¨®çš†ä½¿ç”¨éç·šæ€§çš„å¾‘å‘åŸºåº•æ ¸å‡½ï¥©(radial basis kernel function)ï¼Œä¸¦ï§ç”¨åˆ†
ï§æ¨¡å‹ä½œè¨“ï¦–ã€‚LSVMKMå’Œ LSVMDTå‰‡ä½¿ç”¨ç·šæ€§æ”¯å‘æ©Ÿåˆ†ï§æ¨¡å‹ä½œè¨“ï¦–ã€‚æˆ‘å€‘ç™¼ç¾éç·š
æ€§æ”¯å‘æ©Ÿåˆ†ï§å™¨å„ªæ–¼ç·šæ€§æ”¯å‘æ©Ÿã€‚RANKLSVMKMå’Œ RANKLSVMDTä½¿ç”¨ç·šæ€§æ’åºæ”¯å‘
æ©Ÿä½œè¨“ï¦–ã€‚æˆ‘å€‘ç™¼ç¾ç·šæ€§æ’åºæ¨¡å‹å„ªæ–¼ç·šæ€§åˆ†ï§æ¨¡å‹ï¼Œè€Œå’Œéç·šæ€§åˆ†ï§æ¨¡å‹ï¥§ç›¸ä¸Šä¸‹ã€‚
é ç·š æ€§ æ’ åº æ¨¡ å‹ RANKBOOSTKM å’Œ RANKBOOSTDT åˆ å„ª æ–¼ ç·š æ€§ æ’ åº æ¨¡ å‹
RANKLSVMKMå’Œ RANKLSVMDTã€‚ 
 
è¡¨äºŒã€TIMIT èªï¦¾ä¸‹å¹³å‡é‚Šç•Œèª¤å·®(ms)åŠåœ¨ï¥§åŒå®¹å¿èª¤å·®ä¸‹é‚Šç•Œçš„æº–ç¢ºï¥¡(%)ã€‚ 
èª¿æ•´æ–¹å¼ å¹³å‡é‚Šç•Œèª¤å·® (æ¯«ç§’) 
ï¥§åŒå®¹å¿èª¤å·®ä¸‹é‚Šç•Œçš„æº–ç¢ºï¥¡(%) 
â‰¦10ms â‰¦20ms 
æœªèª¿æ•´ 7.14 81.57 93.73 
LSVMKM 6.84 83.51 93.85  
LSVMDT 6.89 83.44 93.79 
SVMKM 6.75 84.00 94.33 
SVMDT 6.83 83.70 94.12 
RANKLSVMKM 6.72 83.89 94.17 
RANKLSVMDT 6.76 83.90 94.01 
RANKBOOSTKM 6.66 84.20 94.14 
RANKBOOSTDT 6.66 84.13 94.11 
 
(2) åœ‹èªèªï¦¾åº«çš„éŸ³ç´ è‡ªå‹•æ¨™è¨˜åŠäººå·¥ä¿®è¨‚ 
æˆ‘å€‘å¾ MATBN å…¬è¦–æ–°èå£èªèªï¦¾åº«é¸å–å°‡è¿‘ 1.5 å°æ™‚çš„èªï¤†ï¼Œï§ç”¨æ‰€æå‡ºçš„è‡ªå‹•éŸ³ç´ åˆ†
æ®µæŠ€è¡“é€²ï¨ˆè‡ªå‹•æ¨™è¨˜ã€‚ç”±æ–¼äººå·¥ä¿®è¨‚éå¸¸è²»ï¦Šè€—æ™‚ï¼Œæˆ‘å€‘åƒ…èƒ½å°±å…¶ä¸­ 20 åˆ†é˜çš„èªï¦¾å®Œ
æˆäººå·¥ä¿®è¨‚ã€‚ 
 
(3) èªéŸ³äº‹ä»¶åµæ¸¬æŠ€è¡“æ”¹ï¥¼ 
åœ¨ç¬¬ä¸€éšæ®µä¸‰ï¦æœŸè¨ˆç•«çš„ç¬¬ä¸‰ï¦ï¼Œæˆ‘å€‘å·²æ¢è¨ä¸‰ç¨®ï¥§åŒèªéŸ³ç‰¹å¾µç³»çµ±çš„éŸ³ç´ è¾¨ï§¼ï¥¡[6]ï¼Œ
ç™¼ç¾æ”¯é…éŸ³éŸ»ç‰¹å¾µç³»çµ±(Government Phonology (GP) feature system)èƒ½æä¾›å¾Œç«¯èªéŸ³äº‹ä»¶
æ•´åˆè¾¨ï§¼æœ€è±å¯Œçš„èªéŸ³è³‡è¨Šï¼Œå¾è€Œé”åˆ°æœ€ä½³çš„è¾¨ï§¼æ•ˆæœã€‚å› æ­¤ï¼Œåœ¨ç¬¬äºŒéšæ®µä¸‰ï¦æœŸè¨ˆç•«
æˆ‘å€‘ä»¥æ”¯é…éŸ³éŸ»ç‰¹å¾µåšç‚ºèªéŸ³äº‹ä»¶åµæ¸¬ä¹‹ä¸»è¦ç ”ç©¶å°è±¡ã€‚æ”¯é…éŸ³éŸ»ç‰¹å¾µç³»çµ±æ˜¯é€éè²å­¸
é »è­œçš„åˆ†æï¼Œå°‡èªéŸ³éŸ³ç´ ä¸­å„ç¨®äº’è£œçš„ç¨ï§·æˆåˆ†æŠ½å–å‡ºï¤­ã€‚æ­¤æ–¹æ³•å…±å–å¾— 11 é …ç¨ï§·æˆ
åˆ†(å³"A", "I", "U", "E", "S", "h", "H", "N" èˆ‡ "a", "i", "u")ï¼Œé€éæ­¤ 11 é …ç¨ï§·æˆåˆ†ä¹‹çµ„åˆ
å¯è¡¨ç¤ºä»»ä½•èªéŸ³éŸ³ç´ ã€‚ 
é‡å° GP èªéŸ³ç‰¹å¾µç³»çµ±ï¼Œæˆ‘å€‘é€²ä¸€æ­¥è¨­è¨ˆï¥¸ç¨®æ–°çš„èªéŸ³äº‹ä»¶åµæ¸¬å™¨æ¶æ§‹ï¼Œä¸¦èˆ‡ç¬¬ä¸€
éšæ®µä¸‰ï¦æœŸè¨ˆç•«æœŸé–“æ¡ç”¨çš„æ¶æ§‹é€²ï¨ˆæ¯”è¼ƒã€‚ä¸‰ç¨®æ¶æ§‹å‡ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥å™¨(multi-layer 
perceptron, MLP)ä½œç‚ºèªéŸ³äº‹ä»¶æ©Ÿï¥¡æ¨¡å‹ä¹‹æ ¸å¿ƒï¼Œåˆ†åˆ¥ç°¡ä»‹å¦‚ä¸‹ï¼š 
 
1ã€ å‚³çµ±èªéŸ³ç‰¹å¾µäº‹ä»¶åµæ¸¬å™¨ï¼š 
æ­¤æ¶æ§‹ç‚ºç¬¬ä¸€éšæ®µä¸‰ï¦æœŸè¨ˆç•«æœŸé–“ä½¿ç”¨ä¹‹èªéŸ³äº‹ä»¶åµæ¸¬å™¨æ¶æ§‹ï¼Œå¯ç•¶ä½œå…¶å®ƒï¥¸ç¨®æ–°
è¨­è¨ˆæ¶æ§‹ä¹‹æ¯”è¼ƒåŸºç¤ã€‚åœ¨æ­¤å¤šå±¤æ„ŸçŸ¥å™¨çš„è¨­è¨ˆä¸Šï¼Œè¼¸å…¥å‘ï¥¾ä½¿ç”¨ 13 ç¶­æ¢…çˆ¾å€’é »è­œä¿‚ï¥©ï¼Œ
è¼¸å‡ºå‰‡ç‚º GP ç³»çµ± 11 å€‹èªéŸ³ç‰¹å¾µäº‹ä»¶ä¹‹äº‹å¾Œæ©Ÿï¥¡æ‰€å½¢æˆä¹‹ 11 ç¶­å‘ï¥¾ã€‚æˆ‘å€‘æ¡ç”¨å¾ªç’°æ¶
 10
è¡¨ä¸‰ã€å‚³çµ±èªéŸ³ç‰¹å¾µäº‹ä»¶åµæ¸¬å™¨æ¶æ§‹ä¸‹ï¼Œæ”¯é…éŸ³éŸ»(GP)èªéŸ³ç‰¹å¾µäº‹ä»¶åµæ¸¬ä¹‹éŸ³æ¡†æ­£ç¢ºï¥¡ã€‚ 
èªéŸ³ç‰¹å¾µ éŸ³æ¡†æ­£ç¢ºï¥¡ èªéŸ³ç‰¹å¾µ 
éŸ³æ¡†æ­£
ç¢ºï¥¡ 
A 85.0 % H 92.6 %
I 89.6 % N 97.3 %
U 86.2 % a 96.1 %
E 85.8 % i 94.2 %
S 90.8 % u 95.3 %
H 94.6 %   
 
è¡¨å››ã€èªéŸ³ç‰¹å¾µæš¨éŸ³ç´ äº‹ä»¶åµæ¸¬å™¨æ¶æ§‹ä¸‹ï¼Œæ”¯é…éŸ³éŸ»(GP)èªéŸ³ç‰¹å¾µäº‹ä»¶åµæ¸¬ä¹‹éŸ³æ¡†æ­£ç¢ºï¥¡ã€‚ 
èªéŸ³
ç‰¹å¾µ 
éŸ³æ¡†æ­£ç¢º
ï¥¡ èªéŸ³ç‰¹å¾µ 
éŸ³æ¡†æ­£ç¢º
ï¥¡ 
A 85.9 % H 93.6 % 
I 89.5 % N 97.3 % 
U 87.0 % a 96.3 % 
E 86.6 % i 94.3 % 
S 91.2 % u 95.7 % 
H 94.9 %   
 
è¡¨äº”ã€åŸºæ–¼é•·æ™‚è³‡è¨Šä¹‹èªéŸ³ç‰¹å¾µæš¨éŸ³ç´ äº‹ä»¶åµæ¸¬å™¨æ¶æ§‹ä¸‹ï¼Œæ”¯é…éŸ³éŸ»(GP)èªéŸ³ç‰¹å¾µäº‹ä»¶åµæ¸¬ä¹‹éŸ³æ¡†æ­£ç¢º
ï¥¡ã€‚ 
èªéŸ³ç‰¹
å¾µ 
éŸ³æ¡†æ­£ç¢º
ï¥¡ èªéŸ³ç‰¹å¾µ 
éŸ³æ¡†æ­£ç¢º
ï¥¡ 
A 88.2 % H 94.1 % 
I 91.0 % N 97.6 % 
U 89.0 % a 96.8 % 
E 88.5 % i 95.2 % 
S 92.4 % u 96.2 % 
h 95.4 %   
 
(4) åŸºæ–¼åµæ¸¬ä¹‹è‡ªå‹•èªéŸ³è¾¨ï§¼ç³»çµ±çš„å»ºç½®èˆ‡æ”¹ï¥¼ 
åœ¨å‰é …èªéŸ³äº‹ä»¶åµæ¸¬ç ”ç©¶å·¥ä½œä¸­ï¼Œæˆ‘å€‘é‡å° GP èªéŸ³ç‰¹å¾µç³»çµ±çš„èªéŸ³äº‹ä»¶åµæ¸¬å™¨æå‡ºï¦º
ï¥¸å€‹æ–°çš„æ¶æ§‹ï¼Œä¸¦æœ‰æ•ˆè€Œç©©å®šçš„æå‡ï¦ºèªéŸ³ç‰¹å¾µäº‹ä»¶åµæ¸¬æ­£ç¢ºï¥¡ã€‚åŸºæ–¼æ­¤ç ”ç©¶æˆæœï¼Œæˆ‘
å€‘é€²ä¸€æ­¥ä»¥æ­¤ï¥¸çµ„æ”¹é€²å¾Œçš„èªéŸ³äº‹ä»¶åµæ¸¬çµæœï¤­è¾¨ï§¼ï¤é«˜å±¤æ¬¡çš„èªè¨€è³‡è¨Šï¼Œä»¥é©—è­‰å‰ç«¯
èªéŸ³äº‹ä»¶åµæ¸¬å°å¾Œç«¯é«˜å±¤æ¬¡è¾¨ï§¼æ•ˆèƒ½çš„å½±éŸ¿ã€‚æˆ‘å€‘é¦–å…ˆå˜—è©¦é€²ï¨ˆéŸ³ç´ çš„è¾¨ï§¼ï¼Œä½œç‚ºï¤é«˜
å±¤æ¬¡èªè¨€è³‡è¨Šè¾¨ï§¼ç ”ç©¶ä¹‹åŸºç¤ã€‚ 
æˆ‘å€‘ä½¿ç”¨æ¢ä»¶éš¨æ©ŸåŸŸ(conditional random fields, CRF)ï¤­æ•´åˆèªéŸ³äº‹ä»¶åµæ¸¬çµæœï¼Œé€²
ï¨ˆéŸ³ç´ è¾¦ï§¼ã€‚å°ï¥¸çµ„æ”¹é€²å¾Œçš„èªéŸ³äº‹ä»¶åµæ¸¬çµæœï¼Œæˆ‘å€‘åˆ†åˆ¥å»ºï§·å°æ‡‰çš„æ¢ä»¶éš¨æ©ŸåŸŸæ¨¡
å‹ï¼Œç”¨ä»¥è§€å¯Ÿï¥§åŒèªéŸ³äº‹ä»¶åµæ¸¬çµæœå°è‡ªå‹•èªéŸ³è¾¨ï§¼æ­£ç¢ºï¥¡çš„å½±éŸ¿ã€‚åœ¨æ¢ä»¶éš¨æ©ŸåŸŸæ¨¡å‹
çš„è¨­å®šä¸Šï¼Œæˆ‘å€‘ä½¿ç”¨äºŒå…ƒå€¼ç‰¹å¾µ(binary-valued feature)ï¼Œå³åƒ…ä»¥å„èªéŸ³äº‹ä»¶æ˜¯å¦ç™¼ç”Ÿåšç‚º
æ¨¡å‹ä¹‹è¼¸å…¥ï¼›åœ¨è¨“ï¦–ä¸Šï¼Œå‚³çµ±ä¸»è¦è¨“ï¦–æ–¹æ³•åˆ†ç‚ºï¥¸ç¨®ï¼šä¸€ã€ä½¿ç”¨ç”±è¨“ï¦–èªï¦¾æä¾›ä¹‹æ­£ç¢º
èªéŸ³äº‹ä»¶ç™¼ç”Ÿä¹‹éŸ³æ¡†æ¨™è¨˜åšç‚ºè¨“ï¦–è³‡ï¦¾(è¨˜ç‚º oracle train, OT)ï¼›äºŒã€ä½¿ç”¨å‰è¿°å¤šå±¤æ„ŸçŸ¥å™¨
(MLP)ç›´æ¥å°è¨“ï¦–èªï¦¾é€²ï¨ˆåµæ¸¬ï¼Œæ‰€ç”¢ç”Ÿå¸¶æœ‰ï¥´å¹²åµæ¸¬éŒ¯èª¤çš„èªéŸ³äº‹ä»¶ç™¼ç”ŸéŸ³æ¡†æ¨™è¨˜åš
ç‚ºè¨“ï¦–è³‡ï¦¾(è¨˜ç‚º detection-based train, DT)ã€‚ 
åœ¨ç¬¬ä¸€éšæ®µä¸‰ï¦æœŸè¨ˆç•«çš„ç¬¬ä¸‰ï¦æˆ‘å€‘å·²ç¶“æ¢è¨éå‰è¿°ï¥¸ç¨®è¨“ï¦–æ–¹æ³•ï¼Œåœ¨ç¬¬äºŒéšæ®µä¸‰
ï¦æœŸè¨ˆç•«æˆ‘å€‘æå‡ºä¸€å€‹æ–°çš„è¨“ï¦–æ–¹æ³•ï¼Œä½¿ç”¨å¤šå±¤æ„ŸçŸ¥å™¨(MLP)å°è¨“ï¦–èªï¦¾é€²ï¨ˆæ™‚é–“é‡æ–°
å°æº–(Realign)ï¼Œæ‰€ç”¢ç”Ÿå¸¶æœ‰èªéŸ³ç‰¹å¾µä¹‹éåŒæ­¥èµ·å§‹è³‡è¨Šã€åŒæ™‚äº¦ä¿æœ‰ä¸€å®šæ­£ç¢ºæ€§ä¹‹èªéŸ³
äº‹ä»¶ç™¼ç”Ÿä¹‹éŸ³æ¡†æ¨™è¨˜åšç‚ºè¨“ï¦–è³‡ï¦¾(è¨˜ç‚º alignment-based train, AT)ã€‚ 
 èªéŸ³ç‰¹å¾µä¹‹éåŒæ­¥èµ·å§‹ç¾è±¡æ˜¯ï¦šçºŒèªéŸ³çš„ç‰¹æ€§ã€‚åœ¨ï¦šçºŒèªéŸ³ä¸­ï¼Œèªè€…ä¸¦éä¾éŸ³ç´ åº
ï¦œé€ä¸€å®Œæ•´ç™¼è²ï¼Œè€Œæ˜¯ï§Šæš¢åœ°å°‡éŸ³ç´ åºï¦œä¹‹ç™¼è²ï¤…ï¦šèµ·ï¤­ï¼Œå½¢æˆèªï§Šã€‚ç•¶èªè€…åœ¨å°ï¦šçºŒ
 12
æ–¹æ³•ä¸­ï¼Œæˆ‘å€‘ä½¿ç”¨èªéŸ³äº‹ä»¶åµæ¸¬å™¨å°‡è¨“ï¦–èªï¦¾æä¾›ä¹‹æ­£ç¢ºèªéŸ³ç‰¹å¾µäº‹ä»¶åœ¨æ™‚é–“ä¸Šé€²ï¨ˆé‡
æ–°æ ¡æº–ã€‚å¦‚åœ–äº”æ‰€ç¤ºï¼Œé‡æ–°æ ¡æº–å¾Œçš„è¨“ï¦–è³‡ï¦¾åœ¨éŸ³ç´ åºï¦œä¹‹é‚Šç•Œé™„è¿‘ä¿æœ‰è‡ªç„¶èªï§Šä¹‹èª
éŸ³ç‰¹å¾µéåŒæ­¥èµ·å§‹ç¾è±¡ï¼ŒåŒæ™‚åœ¨å…¶å®ƒæ™‚é–“é»ä¸Šåˆèƒ½ä¿æŒç¬¦åˆèªéŸ³ç‰¹å¾µç³»çµ±æ‰€å®šç¾©ä¹‹éŸ³ç´ 
èˆ‡èªéŸ³ç‰¹å¾µå°æ‡‰é—œä¿‚ã€‚æˆ‘å€‘é æœŸä½¿ç”¨æ­¤æ–¹æ³•è¨“ï¦–ä¹‹æ¢ä»¶éš¨æ©ŸåŸŸæ¨¡å‹è¼ƒå‰ï¥¸ç¨®å‚³çµ±æ–¹æ³•ï¤
èƒ½å¦¥å–„è™•ï§¤èªéŸ³äº‹ä»¶åµæ¸¬çµæœï¼Œå¾è€Œæå‡æ•´é«”éŸ³ç´ è¾¨ï§¼æ•ˆèƒ½ã€‚ 
 å¯¦ä½œä¸Šï¼Œæˆ‘å€‘ä½¿ç”¨å‰ç¯€æ‰€æå‡ºä¹‹èªéŸ³äº‹ä»¶åµæ¸¬å™¨æ‰€ç”¢ç”Ÿä¹‹èªéŸ³ç‰¹å¾µèˆ‡éŸ³ç´ äº‹ä»¶ä¹‹äº‹
å¾Œæ©Ÿï¥¡æ­é…ç¶­ç‰¹æ¯”(Viterbi)æ¼”ç®—æ³•ï¼Œåˆ†åˆ¥å°éŸ³ç´ åºï¦œèˆ‡èªéŸ³ç‰¹å¾µäº‹ä»¶åºï¦œé€²ï¨ˆæ™‚é–“çš„é‡
æ–°å°æº–ã€‚ 
 åœ¨æ¸¬è©¦æ™‚ï¼Œæ¢ä»¶éš¨æ©ŸåŸŸæ¨¡å‹å‡ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥å™¨æ‰€åµæ¸¬ä¹‹èªéŸ³ç‰¹å¾µäº‹ä»¶éŸ³æ¡†æ¨™è¨˜åšç‚º
è¼¸å…¥ï¼Œè€Œä»¥éŸ³ç´ åºï¦œåšç‚ºè¾¨ï§¼çµæœè¼¸å‡ºã€‚å¯¦é©—èªï¦¾åŒæ¨£ä½¿ç”¨ TIMIT è‹±æ–‡èªï¦¾åº«ï¼Œè¡¨ï§‘ï¦œ
å‡ºä¸‰ç¨®æ¢ä»¶éš¨æ©ŸåŸŸæ¨¡å‹è¨“ï¦–æ–¹å¼ä¸‹åŸºæ–¼åµæ¸¬ä¹‹éŸ³ç´ è¾¨ï§¼çµæœï¼Œå…¶ä¸­æº–ç¢ºï¥¡%(accuracy) = 
æ­£ç¢ºï¥¡%(correction) â€“ æ’å…¥ï¥¡%(insertion rate)ã€‚è¡¨ï§‘åŒæ™‚äº¦ï¦œå‡ºå‚³çµ±åŸºæ–¼ HMM ä¹‹éŸ³ç´ 
è¾¨ï§¼çš„çµæœåšç‚ºæ¯”è¼ƒã€‚ 
 
è¡¨ï§‘ã€ä¸‰ç¨®æ¢ä»¶éš¨æ©ŸåŸŸæ¨¡å‹è¨“ï¦–æ–¹å¼ä¸‹ï¼Œè‡ªç”±éŸ³ç´ è¾¨ï§¼(free phone decoding)çµæœæ¯”è¼ƒã€‚ 
äº‹ä»¶åµæ¸¬å™¨ï§å‹ è¾¨ï§¼ç³»çµ± æ­£ç¢ºï¥¡ 
(%) 
æº–ç¢ºï¥¡
(%) 
åŸºæ–¼ HMM ä¹‹è‡ªå‹•èªéŸ³è¾¨ï§¼ 69.02 63.45 
èªéŸ³ç‰¹å¾µæš¨éŸ³ç´ 
äº‹ä»¶åµæ¸¬å™¨ 
OT CRF 70.55 34.38 
DT CRF 57.30 56.14 
AT CRF 64.87 62.32 
åŸºæ–¼é•·æ™‚è³‡è¨Šä¹‹
èªéŸ³ç‰¹å¾µæš¨éŸ³ç´ 
äº‹ä»¶åµæ¸¬å™¨ 
OT CRF 75.38 47.47 
DT CRF 62.76 61.46 
AT CRF 69.83 66.97 
 
é€éæ¯”è¼ƒè¡¨ï§‘ä¸­å‰ç«¯ä½¿ç”¨ã€ŒèªéŸ³ç‰¹å¾µæš¨éŸ³ç´ äº‹ä»¶åµæ¸¬å™¨ã€èˆ‡ã€ŒåŸºæ–¼é•·æ™‚è³‡è¨Šä¹‹èªéŸ³
ç‰¹å¾µæš¨éŸ³ç´ äº‹ä»¶åµæ¸¬å™¨ã€è¾¨ï§¼ç³»çµ±ä¹‹æ•´é«”è¡¨ç¾ï¼Œæˆ‘å€‘é¦–å…ˆå¯ä»¥ç™¼ç¾å‰ç«¯èªéŸ³äº‹ä»¶åµæ¸¬å™¨
ä¹‹æ•ˆèƒ½å°èªéŸ³è¾¨ï§¼ç³»çµ±æ•´é«”è¡¨ç¾æœ‰ç›¸ç•¶é—œéµä¹‹å½±éŸ¿ã€‚å„˜ç®¡ä½¿ç”¨é•·æ™‚è³‡è¨Šä¹‹èªéŸ³äº‹ä»¶åµæ¸¬
å™¨åƒ…åœ¨èªéŸ³ç‰¹å¾µäº‹ä»¶éŸ³æ¡†æ­£ç¢ºï¥¡ï¥©æ“šä¸Šé€²æ­¥ç´„ 1~2%ï¼Œç„¶è€Œå°æ•´é«”éŸ³ç´ è¾¨ï§¼è€Œè¨€ï¼Œå»èƒ½
é”åˆ° 4~10%çš„æå‡ã€‚æ­¤ä¸€ç™¼ç¾å¯ä»¥åšç‚ºè¨‚å®šå¾ŒçºŒç ”ç©¶æ–¹å‘ä¹‹ï¥«è€ƒã€‚ 
å¾è¡¨ï§‘æˆ‘å€‘ä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œå‚³çµ± OT è¨“ï¦–æ‰€å¾—ä¹‹æ¢ä»¶éš¨æ©ŸåŸŸæ¨¡å‹æ“æœ‰é«˜æ­£ç¢ºï¥¡ï¼Œä½†ç”±
æ–¼ç„¡æ³•å¦¥å–„è™•ï§¤è‡ªç„¶èªéŸ³ä¸­èªéŸ³ç‰¹å¾µä¹‹éåŒæ­¥èµ·å§‹ç¾è±¡ï¼Œå› æ­¤é€ æˆåœ¨éŸ³ç´ é‚Šç•Œè™•çš„éŒ¯èª¤
æ’å…¥(Insertion Errors)å¢åŠ ï¼Œä½¿æº–ç¢ºï¥¡å¤§å¹…ä¸‹ï¤„ã€‚è€Œå‚³çµ± DT è¨“ï¦–æ‰€å¾—ä¹‹æ¢ä»¶éš¨æ©ŸåŸŸæ¨¡å‹
ç”±æ–¼ä½¿ç”¨èˆ‡è¾¨ï§¼ç’°å¢ƒç›¸åŒ¹é…ä¹‹è¨“ï¦–è³‡ï¦¾é€²ï¨ˆè¨“ï¦–ï¼Œå› æ­¤ä¸¦æ²’æœ‰å‡ºç¾ OT è¨“ï¦–æ³•ä¸­å‡ºç¾å¤§
ï¥¾éŒ¯èª¤æ’å…¥æƒ…å½¢ï¼Œä½†ç”±æ–¼è¨“ï¦–èªï¦¾ä¸­å¸¶æœ‰ä¹‹å‰ç«¯äº‹ä»¶åµæ¸¬å™¨éŒ¯èª¤æ¸›å¼±ï¦ºæ¨¡å‹çš„éŸ³ç´ è¾¨ï§¼
é‘‘åˆ¥ï¦Šï¼Œè¾¨ï§¼çµæœå„˜ç®¡è¼ƒ OT è¨“ï¦–æ³•æ‰€å¾—ä¹‹æ¨¡å‹å¥½ï¼Œä»é ï¥§åŠå‚³çµ± HMM è¾¨ï§¼ç³»çµ±ã€‚ 
 ç›¸è¼ƒä¹‹ä¸‹ï¼Œæ–°æå‡ºä¹‹ AT è¨“ï¦–æ³•æ‰€å¾—ä¹‹æ¢ä»¶éš¨æ©ŸåŸŸæ¨¡å‹åœ¨éŸ³ç´ æº–ç¢ºï¥¡ä¸Šæœ‰ç›¸ç•¶ç¨‹ï¨
ä¹‹æå‡ã€‚åœ¨å‰ç«¯ä½¿ç”¨èªéŸ³ç‰¹å¾µæš¨éŸ³ç´ äº‹ä»¶åµæ¸¬å™¨æ™‚ï¼Œä½¿ç”¨ AT è¨“ï¦–æ³•ç›¸è¼ƒ DT è¨“ï¦–æ³•æº–
ç¢ºï¥¡èƒ½çµ•å°æå‡ 6.18%ï¼›è€Œåœ¨å‰ç«¯ä½¿ç”¨åŸºæ–¼é•·æ™‚è³‡è¨Šä¹‹èªéŸ³ç‰¹å¾µæš¨éŸ³ç´ äº‹ä»¶åµæ¸¬å™¨æ™‚ï¼Œ
å‰‡èƒ½æå‡ 5.51%çµ•å°å€¼ï¼Œæ‰€å¾—ä¹‹æº–ç¢ºï¥¡ 66.97%ä¸¦å·²å¤§å¹…è¶…è¶Šå‚³çµ± HMM è¾¨ï§¼ç³»çµ± 63.45%
çš„æº–ç¢ºï¥¡ã€‚ 
 
(5) éŸ»ï§˜ç‰¹å¾µ(Prosodic Feature)å°èªéŸ³ç‰¹å¾µåµæ¸¬å½±éŸ¿è©•ä¼° 
åœ¨å‰ä¸€éšæ®µç ”ç©¶çµæœä¸­ï¼Œæˆ‘å€‘ç™¼ç¾è®“æ¢ä»¶éš¨æ©ŸåŸŸæ¨¡å‹å­¸ç¿’èªéŸ³ç‰¹å¾µåœ¨éŸ³ç´ é‚Šç•Œçš„éåŒæ­¥
èµ·å§‹ç¾è±¡å°éŸ³ç´ è¾¨ï§¼çµæœå½±éŸ¿é‡å¤§ã€‚åŒæ™‚æˆ‘å€‘äº¦ç™¼ç¾ç•¶ä½¿ç”¨é•·æ™‚è²å­¸ç‰¹å¾µè³‡è¨Šæ–¼èªéŸ³äº‹
ä»¶åµæ¸¬ä»¥æ¸›å°‘æ­¤èªéŸ³ç‰¹å¾µéåŒæ­¥èµ·å§‹ç¾è±¡ä¹‹ç¨‹ï¨æ™‚ï¼Œæ•´é«”éŸ³ç´ è¾¨ï§¼æº–ç¢ºï¥¡æœƒå¤§å¹…æå‡ã€‚
 14
MFCC + éŸ³å¼·(I) 60.0098 
MFCC + èªé€Ÿ(R) 59.7697 
MFCC + P + I + R 60.8396 
 
Articulatory Feature Asynchrony Analysis and Compensation in  
Detection-Based ASR 
I-Fan Chen1 and Hsin-Min Wang1,2
1Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan 
2Institute of Information Science, Academia Sinica, Taipei, Taiwan 
{ifanchen, whm}@iis.sinica.edu.tw 
 
Abstract 
This paper investigates the effects of two types of 
imperfection, namely detection errors and articulatory feature 
asynchrony, of the front-end articulatory feature detector on 
the performance of a detection-based ASR system. Based on a 
set of variable-controlled experiments, we find that 
articulatory feature asynchrony is the major issue that should 
be addressed in detection-based ASR. To this end, we propose 
several methods to reduce the asynchrony or the effects of 
asynchrony. The results are quite promising; for example, 
currently, we can achieve 67.67% phone accuracy in the 
TIMIT free phone recognition task with only 11 binary-valued 
articulatory features. 
Index Terms: articulatory feature asynchrony, detection-
based ASR, speech recognition 
1. Introduction 
In recent years, detection-based automatic speech recognition 
(ASR) has become a popular research topic in fields related to 
ASR. By simulating human speech recognition (HSR), 
detection-based ASR systems attempt to reduce the gap 
between HSR and ASR. Basically, the framework of 
detection-based ASR can be divided into two key components: 
a front-end knowledge attribute detector and a backend 
knowledge integrator [1]. The front-end process collects a 
wide variety of speech-related knowledge attributes to form 
knowledge sources; and the backend process integrates the 
attributes into higher-level speech units, such as phones, 
syllables, words, and sentences. If the front-end detector 
targets articulatory features (AFs), two factors may affect the 
system's overall recognition accuracy: the errors produced by 
the detector and the asynchrony of AFs. 
The asynchrony of AFs is a phenomenon caused by 
variations in natural speech production; i.e., the onset times of 
different AFs shift differently in different contextual 
conditions in speech. Because of this phenomenon, it is 
generally believed that one of the reasons why AFs are more 
suitable than phones for ASR is their ability to describe the 
contextual information in speech. A number of studies have 
focused on this phenomenon. For example, in [2], AF 
asynchrony was introduced to the articulatory feature 
recognizer by applying an embedded training technique; while 
in [3], the authors used the pattern of AF asynchrony in the 
AF space to probe for acoustic differences in the onset of 
Dutch singulars and plurals. However, it is still unclear what 
role this phenomenon plays in a phone or word recognition 
task, which maps multiple asynchronous AF streams into a 
single phone or word stream. 
In our previous research [4], we found that the 
imperfection of the front-end articulatory feature detector 
(AFDT) can cause a substantial decline in recognition 
accuracy in detection-based ASR. Here, we investigate the 
effects of two types of the above imperfection, namely AFDT 
errors and AF asynchrony, on the backend integrator's 
performance. To compare the relative importance of the two 
factors, we design a set of variable-controlled experiments. 
The results show that AF asynchrony plays a more important 
role in detection-based ASR than AFDT errors. Based on this 
finding, we propose a number of ways to improve the system 
performance. We believe that our experiment results provide 
further insight into how to design a high-performance 
detection-based ASR system. 
2. Front-end articulatory feature detection 
and AF asynchrony analysis 
2.1. Front-end articulatory feature detection 
The AFDT's detection target is the Government Phonology 
(GP) feature set [5], a phonological feature system in which 
speech sounds are destructed into a set of primes and can be 
represented by fusing these primes structurally. We select the 
GP feature set because the results of our previous study show 
that it is the most effective for building a high-performance 
detection-based ASR system [4]. In this work, the GP feature 
set contains 11 AFs, including 8 AFs (namely "A", "I", "U", 
"E", "S", "h", "H", and "N" ) defined in the original GP feature 
system [5] and 3 AFs ("a", "i", and "u") added by King and 
Taylor [6]. 
In addition to the 11 AFs, our front-end AFDT 
simultaneously detects TIMIT 61 phones for AF asynchrony 
analysis. The objective of phone detection is to remove the 
bias introduced by the inconsistency between human 
alignment and machine alignment. AF asynchrony is defined 
as the shift between a phone's boundary and the boundaries of 
its associated AFs. Thus, the different criteria used for AF 
alignment and phone alignment might lead to inaccurate 
articulatory feature asynchrony measurements. Even though 
the TIMIT corpus provides human aligned phone labels, to 
generate unbiased AF asynchrony data, we need to re-align 
the phone sequence in speech using our front-end AFDT. 
Following the work in [6], we use a single time delay 
recurrent neural network [7] to implement the front-end AFDT. 
The inputs of the neural network are 12 Mel-frequency 
cepstral coefficients (MFCCs) plus energy, which are 
extracted by a 25-ms Hamming-windowed frame with 10-ms 
shifts. The neural network outputs a 72-dimensional vector 
representing 11 GP AFs and 61 TIMIT phones. The value of 
each element in the vector ranges from 0 to 1, which can be 
treated as a posterior probability. The values of the AFs are 
then discretized1 with a threshold to form binary-valued AF 
                                                                 
 
1  Theoretically, CRFs can handle real-valued input; however, we 
discretize the AFDT's results because the CRF++ toolkit used in our 
experiments only supports binary-valued input. Moreover, it is easier 
to define the boundaries of AFs, which are important for asynchrony 
measurement, by using the discrete AF detection results.
recognition performance in detection-based ASR. The derived 
knowledge will help us to further refine our detection-based 
ASR system. 
4. Experiments 
4.1. Experiment setup 
We conduct experiments on the TIMIT acoustic-phonetic 
continuous speech corpus, but the dialect utterances (SA1 and 
SA2) are not used. The database is divided into three parts: a 
training set (3296 utterances), a development set (400 
utterances), and a test set (1344 utterances). The training and 
development sets are subsets of the standard TIMIT training 
set, while the test set is the standard TIMIT test set. In the 
experiments, all the models are trained by the training set, and 
the configurations and parameters associated with the models 
are assigned empirically based on the development set. The 61 
phones defined in TIMIT are used as recognition units; 
however, for the performance evaluation, the recognized 
TIMIT 61-phone results are mapped to the CMU/MIT 39-
phone set. No language models are used in recognition. 
4.2. The baseline detection-based ASR systems 
First, we evaluate the baseline detection-based ASR systems, 
in which the CRF models are trained by oracle data training 
(OT) and detected data training (DT). We consider two test 
conditions: the ideal case and the real case. In the ideal case, 
the test data is set to match the CRF model's training 
conditions. In the OT-trained CRF system, the AFs converted 
from the human phone labels of the testing data are input to 
the CRF model. In this way, we can measure the performance 
upper bound of the CRF-based system. In the real case, the 
inputs to the CRF models are the AFs detected automatically 
by the AFDT. The phone recognition results of these baseline 
systems are shown in Table 1, where Corr (correct rate) and 
Acc (accuracy) are obtained by HTK's HResults tool; and Acc 
= Corr â€“ insertion rate. The second row shows that if there are 
no AFDT errors and AF asynchrony, the OT-trained CRF 
system can achieve a very high phone recognition 
performance. Clearly, the GP11 AF set demonstrates high 
potential for detection-based ASR; therefore, we may consider 
this performance upper bound our ultimate goal in this 
research. In the real case, since the detected AFs inevitably 
contain errors and asynchrony, though with a high correct rate, 
the OT-trained CRF system suffers from serious phone 
insertion errors, as shown in the third row of Table 1. 
Although the performance of the DT-trained CRF system is 
quite stable in terms of the correct rate and accuracy, the 
values are not satisfactory. 
Table 1. The phone recognition results of the oracle data 
trained (OT) CRF model and the detected data trained (DT) 
CRF model. 
Test Data Type System Corr Acc 
Ideal (upper bound) OT CRF 98.31 98.28 
OT CRF 70.55 34.38 Detected (real case) DT CRF 57.30 56.14 
4.3. The AFDT aligned data trained system 
To identify the degrees of AFDT errors and AF asynchrony 
that cause the substantial difference in the performance of the 
OT-trained CRF model between the ideal case and the real 
case, we applied AFDT aligned data training (AT) in the 
experiments. Table 2 shows the ideal and real performances of 
the AT-trained CRF system, where the ideal performance 
(upper bound) is obtained by feeding the individual frame 
alignments of the 11 AF sequences of the test data (obtained 
by the alignment methods described in Section 2.2) to the AT-
trained CRF model. Comparing the upper bound performance 
of the AT-trained CRF model in Table 2 with that of the OT-
trained CRF model in Table 1, it is clear that the introduction 
of AF asynchrony to the backend CRF integrator has a major 
impact on the performance (26.82% drop in the correct rate 
and 27.97% drop in accuracy). The effect of the front-end 
AFDT errors can be measured by the difference between the 
performances of the AT-trained CRF model in the ideal and 
real cases. Specifically, the differences are only 6.62% for the 
correct rate and 7.99% for accuracy. We also compare the real 
case performance of the DT-trained CRF model (the fourth 
row in Table 1) with that of the AT-trained CRF model (the 
third row in Table 2). Clearly, the CRF model trained with AF 
asynchrony information alone outperforms the CRF model 
trained with both AF asynchrony and AFDT errors. The above 
evidences show that AF asynchrony plays a much more 
important role than AFDT errors in detection-based ASR. In 
other words, we can conclude that, to build a high-
performance detection-based ASR system, it is better to use 
AFDT aligned data training for the backend CRF integrator; 
and employ as many methods as possible to reduce or conceal 
AF asynchrony from the front-end AFDT. 
Table 2. The phone recognition results of the AFDT aligned 
data trained (AT) CRF model. 
Test Data Type System Corr Acc 
Ideal (upper bound) AT CRF 71.49 70.31 
Detected (real case) AT CRF 64.87 62.32 
4.4. AF asynchrony compensation 
In this section, we perform several preliminary experiments to 
determine whether AF asynchrony can be reduced or 
concealed. AF asynchrony compensation can be applied in the 
front-end AFDT or the backend CRF integrator. 
4.4.1. AF asynchrony compensation in the AFDT using 
long-term information 
Contextual variation in natural speech is one of the major 
reasons for AF asynchrony [2]. One way to reduce the output 
asynchrony is to let the AFDT learn the contextual variation 
directly. This can be done by introducing long-term 
information in speech to the AFDT. It has already been shown 
that such information is helpful for speech recognition [12]. 
We believe that using long-term speech information in the 
AFDT makes the detection of AFs more stable than using only 
local speech information; hence, the AF boundary shift around 
the phone boundary can be reduced. 
Table 3. The mean AF-phone boundary distances (in frames) 
of GP11 AFs generated by the original AFDT (using MFCC 
as input) and the AFDT using long-term information. 
AFDT sys a A E h H i I N S u U
MFCC 0.94 4.27 1.18 0.94 0.59 1.07 2.44 1.74 0.56 1.88 7.16
Long Term 0.75 3.57 0.94 0.77 0.38 0.91 2.36 1.15 0.43 1.32 4.55
 
Following the approach in [12], we use the Mel-frequency 
filter bank vectors in a 310-ms window jointly as input to 
implement an AFDT with long-term speech information. 
Table 3 shows the statistics of the AF asynchrony of the 
original AFDT and the AFDT with long-term information. 
Clearly, the introduction of long-term information reduces the 
AF asynchrony for all 11 AFs significantly. 
PHONE BOUNDARY REFINEMENT USING
RANKING METHODS
Hung-Yi LOâˆ—â€ , Hsin-Min WANGâˆ—
âˆ—Institute of Information Science, Academia Sinica, Taipei
â€ Department of Computer Science and Information Engineering, National Taiwan University, Taipei
{hungyi, whm}@iis.sinica.edu.tw
Abstractâ€”The HMM/SVM-based two-stage framework has
been widely used for automatic phone alignment. The two-
stage method uses SVM classifiers to refine the hypothesized
boundaries given by the HMM-based Viterbi forced alignment.
However, there are two drawbacks in using the classification
model for detecting the phone boundaries. First, the training data
contains only information about the boundary and far away non-
boundary signal characteristics. Second, the classification model
suffers from the class-imbalanced training problem. To overcome
these drawbacks, we propose using ranking methods to refine
the hypothesized boundaries. We train multiple phone-transition-
dependent rankers by using K-means-based and decision-tree-
based clustering. Both Ranking SVM and RankBoost are
evaluated. The results of experiments on the TIMIT corpus
demonstrate that the proposed ranking method outperforms
the classification method. The best accuracy achieved is 84.20%
within a tolerance of 10 ms. The mean boundary distance is 6.66
ms.
Index Termsâ€”automatic phone segmentation, ranking method,
ranking SVM, RankBoost
I. INTRODUCTION
Annotated speech corpora are indispensable to various ar-
eas of speech research, e.g., speech recognition and speech
synthesis. Phoneme level annotation is especially important
for fundamental speech research. However, the development
of a large high-quality, manually labelled speech corpus
requires lots of human effort, and is time-consuming. To
reduce the human effort and speed up the labelling process,
many attempts have been made to utilize automatic phone
alignment approaches to provide initial phone segmentation
for subsequent manual segmentation and verification [5]â€“[10],
[12].
The most popular method of automatic phone alignment
is to adapt an HMM-based phone recognizer to align a
phoneme transcription with a speech utterance. Empirically,
phone boundaries obtained in this way should contain few
serious errors, since HMMs in general capture acoustic prop-
erties of phones; however, small errors are inevitable because
HMMs are not sensitive enough to detect changes between
adjacent phones. Consequently, researchers have proposed
a HMM/SVM-based two-stage framework [6], which uses
support vector machine (SVM) classifiers to refine the hy-
pothesized boundaries given by the HMM-based Viterbi forced
alignment.
In order to provide training data for the SVM classifier,
current researches [6], [7], [12] use the feature vectors as-
sociated with the true phone boundaries as positive training
samples and the randomly selected feature vectors at least 20
ms away from the true boundaries as negative training samples.
However, using a classification model for detecting the phone
boundaries has two drawbacks. First, the training data contains
only information about the boundary and far away non-
boundary signal characteristics. We expect that the refinement
task can be better modelled by treating the instances extracted
from the true boundaries as high preference instances, the
nearby instances as medium preference instances, and the far
away instances as low preference instances. Second, modelling
the refinement task as a classification problem will suffer
from the class-imbalanced training problem, since the training
data contains a lot of negative instances but only a limited
amount of positive instances. As a result, general classification
algorithms will be biased to predict all instances to be negative
on such a class-imbalanced dataset, since they are learned to
minimize the number of incorrectly classified instances.
Fig. 1. The difference between using a classification model and using a
ranking model for phone boundary refinement.
In this paper, we propose using ranking methods [3],
[4] to refine the hypothesized phone boundaries given by
the HMM-based Viterbi forced alignment. For each true
boundary, we generate a from-high-to-low preference list, i.e.,
the instance extracted from the true boundary is associated
with the highest preference and, for the remaining instances,
the preference degree decreases as the distance to the true
boundary increases. These preference lists are used to train a
ranking model. In our approach, a phone-transition-dependent
ranker is applied to detect the true phone transition boundary
around each hypothesized boundary given by the initial HMM-
is, the frame with the largest score will be selected to replace
the initial boundary.
B. RankBoost
RankBoost finds a highly accurate ranker by combining
many weak rankers, despite that each of them is only moder-
ately accurate. The typical weak ranker is a threshold function:
â„(ğ’™) =
{
1 if ğ’™ğ‘— > ğœƒ
0 if ğ’™ğ‘— â‰¤ ğœƒ, (3)
where ğ’™ğ‘— is the ğ‘—-the element of ğ’™. The RankBoost algorithm
iteratively trains many weak rankers. The training procedure
maintains a weight distribution matrix ğ·ğ‘¡ over ğ’³ Ã—ğ’³ in each
iteration and determines the parameters ğœƒ and ğ‘— in the ranker â„ğ‘¡
to minimize a weighted pairwise mis-ordered error ğ‘Ÿ according
to ğ·ğ‘¡,
ğ‘Ÿ =
âˆ‘
ğ’™ğ‘–,ğ’™ğ‘—
ğ·ğ‘¡(ğ’™ğ‘–,ğ’™ğ‘—)(â„ğ‘¡(ğ’™ğ‘—)âˆ’ â„ğ‘¡(ğ’™ğ‘–)). (4)
After each iteration, the weight is updated by
ğ·ğ‘¡+1(ğ’™ğ‘–,ğ’™ğ‘—) =
ğ·ğ‘¡(ğ’™ğ‘–,ğ’™ğ‘—) exp(ğ›¼ğ‘¡(â„ğ‘¡(ğ’™ğ‘—)âˆ’ â„ğ‘¡(ğ’™ğ‘–)))
ğ‘ğ‘¡
, (5)
where â„ğ‘¡(ğ’™ğ‘–) is the prediction score of ranker â„ğ‘¡ on the
instance ğ’™ğ‘–, {ğ’™ğ‘– â‰» ğ’™ğ‘—} is an ordered pair, ğ‘ğ‘¡ is a normaliza-
tion factor to make ğ·ğ‘¡+1 a distribution. The parameter ğ›¼ğ‘¡ is
calculated by 12 ln(
1+ğ‘Ÿ
1âˆ’ğ‘Ÿ ). The output score for the input vector
ğ’™ is ğ‘“(ğ’™) =
ğ‘‡âˆ‘
ğ‘¡=1
ğ›¼ğ‘¡â„ğ‘¡(ğ’™).
The ranking methods in general require a higher training
time complexity than the classification methods. However,
there is an efficient solution for training RankBoost [3]. Its
time complexity depends on the number of instances but not
on the number of ordered pairs.
C. Generation of Ordered Pairs for Training
The training data for both Ranking SVM and RankBoost
is represented by a set of correct-ordered pairs of instances.
For each true boundary, we first extract feature vectors from
a speech segment centered at it. Let the instances in the
segment be (ğ’™1, . . . ,ğ’™ğ‘‡âˆ’1,ğ’™ğ‘‡ ,ğ’™ğ‘‡+1, . . . ,ğ’™ğ‘ ), where ğ’™ğ‘‡ is
extracted from the true boundary. Then, we generate four
ordered ranking lists for this segment:
1) (ğ’™ğ‘‡ ,ğ’™ğ‘‡âˆ’1, . . . ,ğ’™2,ğ’™1)
2) (ğ’™ğ‘‡ ,ğ’™ğ‘‡+1, . . . ,ğ’™ğ‘âˆ’1,ğ’™ğ‘ )
3) (ğ’™ğ‘‡âˆ’1,ğ’™ğ‘‡âˆ’2, . . . ,ğ’™2,ğ’™1)
4) (ğ’™ğ‘‡+1,ğ’™ğ‘‡+2, . . . ,ğ’™ğ‘âˆ’1,ğ’™ğ‘ )
Each ordered ranking list can produce multiple ordered pairs
by coupling the first instance in the list with every one of the
remaining instances in the same list. For example, the first
list produces {ğ’™ğ‘‡ â‰» ğ’™ğ‘‡âˆ’1}, {ğ’™ğ‘‡ â‰» ğ’™ğ‘‡âˆ’2}, . . . , and {ğ’™ğ‘‡ â‰»
ğ’™1}. Then, the ordered pairs are gathered together to form the
training set. We have tried to generate more ordered ranking
lists for each segment, e.g., the lists starting from ğ’™ğ‘‡âˆ’2 and
ğ’™ğ‘‡+2. However, the computational time increases substantially
but the prediction performance does not improve.
V. PHONE TRANSITION CLUSTERING
Ideally, we should be able to train a ranker or classifier for
each type of phone transition. However, this is not feasible
because the training data is always limited. Maintaining a
balance between the available training data and the modelâ€™s
complexity is critical to the training process. Furthermore,
since many phone transitions have similar acoustic characteris-
tics, we can partition them into clusters so that the training data
can be shared and the phone transitions with little training data
can be covered by the rankers or classifiers of the categories
they belong to. We implement phone transition clustering in
two ways: by K-means clustering and by decision-tree-based
clustering.
A. K-means-based Clustering
The K-means-based phone transition clustering is described
as follows:
1) For each specific phone transition case, we gather all the
augmented vectors associated with the human-labelled
phone boundaries, and compute the mean vector.
2) For each one of the four phone transition classes, namely
sonorant to non-sonorant, sonorant to sonorant, non-
sonorant to sonorant, and non-sonorant to non-sonorant,
we apply the K-means algorithm to cluster the phone tran-
sitions according to their mean vectors. Note that only the
phone transitions with enough instances are considered in
this step. The number of clusters is determined according
to the cross-validation accuracy that the resulting rankers
or classifiers achieve in the training data.
3) We assign the phone transitions, which are ignored in
Step 2 due to sparse instances, to the nearest clusters
according to the distances between their mean vectors
and the cluster centers.
B. Decision-tree-based Clustering
The drawback of K-means clustering is that it can not
cover phone transitions that do not occur in the training
data. In contrast, decision-tree-based clustering can generalize
to unseen phone transitions and take advantage of linguistic
knowledge during clustering. Here, all the questions have the
form â€œIs the left phone of the transition a member of the set
X and the right phone a member of the set Y?â€ The sets X
and Y range from broad phonetic classes, such as sonorant,
stop, and unvoiced classes, to distinct phonemes, such as {r}
and {s}. In total, 397 phonetic sets are used.
VI. EXPERIMENTS
A. Experiment setup
Our experiments were conducted on the TIMIT acoustic-
phonetic continuous speech corpus. TIMIT contains a total of
6,300 sentences, comprised of 10 sentences spoken by each of
630 speakers from 8 major dialect regions in the United States.
The TIMIT suggested training and testing sets contain 462 and
168 speakers, respectively. We discard the dialect sentences
(SA1 and SA2 utterances) and utterances with phones shorter
[10] S. S. Park and N. S. Kim. On using multiple models for automatic
speech segmentation. IEEE Trans. on Audio, Speech, and Language
Processing, 15:2202â€“2212, 2007.
[11] J.-L. Shen, J.-W. Hung, and L.-S. Lee. Robust entropy-based endpoint
detection for speech recognition in noisy environments. In Proc. ICSLP,
1998.
[12] T. G. Toledano, L. A. H. Gomez, and L. V. Grande. Automatic phonetic
segmentation. IEEE Trans. on Speech and Audio Processing, 11:617â€“
625, 2003.
æŠµæ¡ƒåœ’æ©Ÿå ´ã€‚ 
 
äºŒ. èˆ‡æœƒå¿ƒå¾—èˆ‡å»ºè­° 
 
ç”± IEEE Signal Processing Society æ‰€èˆ‰è¾¦çš„è²å­¸èªéŸ³ä¿¡è™Ÿè™•ï§¤åœ‹éš›ç ”è¨æœƒ(ICASSP)æ˜¯åœ‹
éš›ä¸Šé—œæ–¼ä¿¡è™Ÿè™•ï§¤åŠå…¶ç›¸é—œæ‡‰ç”¨ç ”ç©¶è¦æ¨¡æœ€å¤§ï¼Œæœ€è² ç››åçš„å­¸è¡“ç ”è¨æœƒã€‚èªéŸ³è™•ï§¤ç ”ç©¶
ï¦´åŸŸçš„å­¸è€…å‡è¦–å…¶ç‚ºï¦ï¨æœ€é‡è¦çš„ç ”è¨æœƒï¼Œæ‰€æœ‰çŸ¥åå­¸è€…å°ˆå®¶å¹¾ä¹ï¨¦æœƒèˆ‡æœƒç™¼è¡¨æœ€æ–°ç ”
ç©¶æˆæœåŠäº’ç›¸è¨ï¥ã€äº¤æ›ç ”ç©¶å¿ƒå¾—ã€‚æ­¤æ¬¡æœƒè­°å°ç£åœ°å€èˆ‡æœƒäººï¥©å¾ˆå¤šï¼Œé™¤ä¸­ç ”é™¢å¤–ï¼ŒåŒ…
æ‹¬å°å¤§ã€æ¸…å¤§ã€äº¤å¤§ã€æˆå¤§ã€å¸«å¤§ã€ï¥£ç§‘å¤§ç­‰ï¨¦æœ‰å¤šä½å¸«ç”Ÿå‡ºå¸­ã€‚æ ¹æ“šå¤§æœƒçµ±è¨ˆï¼Œå°ç£
åœ°å€æŠ•ç¨¿ï¥æ–‡ï¥©ä½”ç¬¬ 8 ä½ï¼Œåƒ…æ¬¡æ–¼ç¾ã€ä¸­ã€æ³•ã€æ—¥ã€å¾·ã€è‹±å’ŒåŠ æ‹¿å¤§ï¼Œæˆç¸¾ç›¸ç•¶å¯è§€ã€‚
 
ä¸‰. æ”œå›è³‡ï¦¾åç¨±åŠå…§å®¹ 
 
æœƒè­°ï¥æ–‡é›†éš¨èº«ç¢Ÿ 
 
 
 
97 å¹´åº¦å°ˆé¡Œç ”ç©¶è¨ˆç•«ç ”ç©¶æˆæœå½™æ•´è¡¨ 
è¨ˆç•«ä¸»æŒäººï¼šç‹æ–°æ°‘ è¨ˆç•«ç·¨è™Ÿï¼š97-2221-E-001-022-MY3 
è¨ˆç•«åç¨±ï¼šæ–°ä¸–ä»£è‡ªå‹•èªéŸ³è¾¨è­˜æŠ€è¡“ä¹‹ç ”ç©¶ï¼ç¬¬äºŒéšæ®µ--è‡ªå‹•æ¨™éŸ³åŠèªéŸ³è³‡æ–™åº«ç¢ºèª 
é‡åŒ– 
æˆæœé …ç›® å¯¦éš›å·²é”æˆ
æ•¸ï¼ˆè¢«æ¥å—
æˆ–å·²ç™¼è¡¨ï¼‰
é æœŸç¸½é”æˆ
æ•¸(å«å¯¦éš›å·²
é”æˆæ•¸) 
æœ¬è¨ˆç•«å¯¦
éš›è²¢ç»ç™¾
åˆ†æ¯” 
å–®ä½ 
å‚™ è¨» ï¼ˆ è³ª åŒ– èªª
æ˜ï¼šå¦‚æ•¸å€‹è¨ˆç•«
å…±åŒæˆæœã€æˆæœ
åˆ— ç‚º è©² æœŸ åˆŠ ä¹‹
å° é¢ æ•… äº‹ ...
ç­‰ï¼‰ 
æœŸåˆŠè«–æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒè«–æ–‡ 0 0 100% 
ç¯‡ 
 
è«–æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100%   
ç”³è«‹ä¸­ä»¶æ•¸ 0 0 100%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 100% ä»¶  
ä»¶æ•¸ 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 1 1 100%  
åšå£«ç”Ÿ 4 4 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å…§ 
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆæœ¬åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 1 1 100% 
äººæ¬¡ 
 
æœŸåˆŠè«–æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒè«–æ–‡ 3 3 100% 
ç¯‡ 
 
è«–æ–‡è‘—ä½œ 
å°ˆæ›¸ 1 1 100% ç« /æœ¬  
ç”³è«‹ä¸­ä»¶æ•¸ 0 0 100%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 100% ä»¶  
ä»¶æ•¸ 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å¤– 
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆå¤–åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 0 0 100% 
äººæ¬¡ 
 
