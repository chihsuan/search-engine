 2 
è¡Œæ”¿é™¢åœ‹å®¶ç§‘å­¸å§”å“¡æœƒå°ˆé¡Œç ”ç©¶è¨ˆç•«æˆæœå ±å‘Š 
 
ä»¥å½±åƒç‰¹å¾µæ“·å–ç‚ºåŸºç¤çš„ç›¸æ©Ÿä½ç½®ä¼°æ¸¬æ–¹æ³•ä¹‹è¨­è¨ˆèˆ‡å¯¦ç¾ 
 
è¨ˆç•«ç·¨è™Ÿï¼šNSC 99-2221-E-269 -020 
åŸ·è¡ŒæœŸé™ï¼š99 å¹´ 08 æœˆ 01 æ—¥è‡³ 100 å¹´ 07 æœˆ 31 æ—¥ 
ä¸»æŒäººï¼šå¤éƒ­è³¢  é æ±ç§‘æŠ€å¤§å­¸è³‡è¨Šç®¡ç†ç³»å‰¯æ•™æˆ 
 
ä¸€ã€ä¸­æ–‡æ‘˜è¦ 
åŸºæ–¼ç„¡äººé£›è¡Œè¼‰å…·çš„ç©ºæ‹æ‡‰ç”¨èˆ‡è¼”åŠ©é™è½çš„
ç ”ç©¶ä¸Šï¼Œç›¸æ©Ÿä½ç½®çš„ç´€éŒ„æ˜¯ä¸€é …é‡è¦çš„ä¸»é¡Œã€‚æœ¬ç ”
ç©¶çš„ä¸»è¦ç›®çš„ï¼Œæ˜¯å°ç›¸æ©Ÿä½ç½®èˆ‡å½±åƒé–“é€²è¡ŒæŠ•å½±å¹¾
ä½•çš„åˆ†æï¼Œæ‰¾å‡ºèˆ‡ç›¸æ©Ÿä½ç½®ç›¸é—œçš„å½±åƒç‰¹å¾µï¼Œä¸¦ä»¥
å½±åƒç‰¹å¾µæ“·å–çš„æŠ€å·§ï¼Œæ“·å–ç›¸é—œç‰¹å¾µã€‚åŒæ™‚ä»¥æ¨ç®—
æ³•å»ºç«‹å½±åƒç‰¹å¾µè³‡æ–™åº«ï¼Œä¸¦ä»¥æ™ºæ…§å‹å­¸ç¿’æ³•å‰‡é€²è¡Œ
è³‡æ–™åº«å­¸ç¿’ï¼Œä½œç‚ºå¿«é€Ÿä¼°æ¸¬çš„åŸºç¤ã€‚ 
æœ¬ç ”ç©¶è¨ˆç•«æ‰€å®šçš„ç ”ç©¶ç¯„åœï¼Œè¨­å®šç‚ºåœ“å½¢ç›®æ¨™
ç‰©å‡ºç¾åœ¨ç•«é¢ç‰¹å®šå€åŸŸçš„æƒ…æ³ï¼Œä¸¦ä»¥æ­¤æ¢ä»¶è¨­è¨ˆå½±
åƒç‰¹å¾µæ“·å–æ¼”ç®—æ³•èˆ‡ç›¸æ©Ÿä½ç½®ä¼°æ¸¬æ¼”ç®—æ³•ï¼Œç”¨ä»¥æ¨¡
æ“¬æ—‹ç¿¼å‹ç„¡äººé£›è¡Œè¼‰å…·æ¥è¿‘åœæ©Ÿåªæ™‚çš„å½±åƒå®šä½ã€‚ 
ç„¶è€Œåœ¨æŸäº›æƒ…å½¢ä¸‹ï¼Œè³‡æ–™åº«å°‡æœƒå¾ˆå¤§è€Œä¸åˆ©æ–¼
å­¸ç¿’ï¼Œæ‰€ä»¥æˆ‘å€‘é‹ç”¨è³‡æ–™åˆ†å‰²èˆ‡åˆ†é¡æ³•å‰‡ä¾†åŠ ä»¥ç°¡
åŒ–ï¼Œä»¥åŠ é€Ÿè³‡æ–™åº«å­¸ç¿’ã€‚æ­¤å¤–ï¼Œæœ¬è¨ˆç•«è‡ªè¡Œè¨­è¨ˆä¸¦
å»ºç«‹ä¸€å¥—é™„æœ‰å°ºè¦çš„å½±åƒæ¸¬è©¦å¹³å°ï¼Œä»¥é©—è­‰å…¶æº–ç¢º
æ€§ã€‚  
åœ¨æœ¬ç ”ç©¶è¨ˆç•«ä¸­ï¼Œæˆ‘å€‘è¨­è¨ˆäº†æ¨¡æ“¬ç„¡äººé£›è¡Œè¼‰
å…·æ¥è¿‘åœ°é¢ç›®æ¨™ç‰©çš„å½±åƒæ¸¬è©¦å¹³å°ï¼Œç”¨ä»¥é©—è­‰æ¼”ç®—
æ³•ï¼Œä¸¦æ­é…åœ°é¢ç«¯ç›£æ§é›»è…¦çš„é¡¯ç¤ºèˆ‡è³‡æ–™å„²å­˜åŠŸ
èƒ½ï¼Œé©—è­‰æ¼”ç®—æ³•çš„æ­£ç¢ºæ€§ã€‚æœªä¾†å¸Œæœ›ä»¥æœ¬ç ”ç©¶æˆ
æœï¼Œå¯ä»¥è¼”åŠ©é€²è¡Œç„¡äººé£›è¡Œè¼‰å…·ä¹‹ç‰¹å®šå€åŸŸç©ºæ‹æˆ–
è‡ªä¸»å®‰å…¨é™è½ã€‚ 
é—œéµå­—ï¼šå½±åƒç‰¹å¾µæ“·å–ã€æ™ºæ…§å‹å­¸ç¿’ã€è³‡æ–™åˆ†é¡ã€
æ•¸ä½å½±åƒè™•ç† 
 
Abstract 
Computer vision is a very useful and important 
technique for aerial photography and applications of 
UAV (Unmanned aerial vehicle), e.g., auxiliary 
landing. The objective of this project is to estimate 
the position of the UAV for landing. The most 
common landing mark for helicopter is usually 
marked with a circle and/or a letter "H", so as to be 
visible from the air. In this project, the landing mark 
considered is a simple circle, which is the outer shape 
of the commonly used landing mark, for simplicity. 
Its image on the captured picture will be a deformed 
circle which is like an ellipse. The objective of this 
project is to determine the position of the camera by 
an image of the circle. 
In this work, the projective geometry analyzing 
method is proposed for finding out the relationship 
between images and camera positions in the 
particular range. Base on projective geometry 
analyzing method, the image feature database has 
been constructed, and the intelligent learning 
methods are developed for learning the I/O mapping 
of the database. Since the amount of the training 
database is so huge that the training time is 
intolerably long, the data pre-processing procedure is 
proposed for easing the database analysis. On the 
other hand, the image feature extraction algorithm is 
proposed for extracting the important features. The 
camera position estimation procedure has been 
verified on the specially designed plane for 
corroborating its capability. 
Also we have designed an image-base camera 
position system for UAV. The receiver and ground 
station receive the image data and memorize the 
image features. The proposed algorithms in this 
project are coded into the system for UAV position 
estimation. 
By the proposed algorithms, there are more 
information sensed for UAV controlling. When the 
UAV carries out the task of aerial photography or 
landing, it will be much easier. 
Keywords ï¼š Image feature extraction, intelligent 
learning method, data pre-processing procedure, 
digital image processing. 
 
äºŒã€å‰è¨€ 
 è¿‘å¹´ä¾†é›»è…¦è¦–è¦º(computer vision)çš„ç ”ç©¶ååˆ†
èˆˆç››ï¼Œç”±æ–¼æ‡‰ç”¨å±¤é¢ååˆ†å»£æ³›ï¼Œä¸è«–æ˜¯ç©ºæ‹çš„å½±åƒ
æ¥åœ–[1]ã€ç›£æ§ç³»çµ±[2]ã€å„å¼è¼”å…·é–‹ç™¼ã€å·¥æ¥­æª¢æ¸¬
[3]ã€æ©Ÿå™¨äººæ‡‰ç”¨[4]æˆ–ç„¡äººé£›è¡Œè¼‰å…·(UAV)[5][6][7]
ä¸Šï¨¦æœ‰æ‡‰ç”¨æˆåŠŸçš„ä¾‹å­ã€‚ä»¥[4]èˆ‡[6]ç‚ºä¾‹ï¼Œæ©Ÿå™¨äºº
èˆ‡ç„¡äººé£›è¡Œè¼‰å…·çš„é–‹ç™¼éœ€è¦è¨±å¤šå‹å¼çš„æ„Ÿæ¸¬å™¨ä¾†
æ„Ÿæ¸¬å‘¨åœç’°å¢ƒï¼Œä»¥ä½œç‚ºæ§åˆ¶å™¨çš„æ§åˆ¶åƒè€ƒã€‚ä¸€èˆ¬å¸¸
è¦‹ä¸”è¼ƒç‚ºä½åƒ¹ä½çš„å…‰å­¸æ„Ÿæ¸¬å™¨(å¦‚ç´…å¤–ç·šæ„Ÿæ¸¬å™¨)
 4 
å€‹é»ï¼Œå…¶é–“å­˜åœ¨è‘—ä¸€é½Šæ¬¡è½‰æ›é—œä¿‚ï¼Œå¦‚å¼(1)ï¼š 
ïƒ·
ïƒ·
ïƒ¸
ïƒ¶
ïƒ§
ïƒ§
ïƒ¨
ïƒ¦
ïƒº
ïƒº
ïƒ»
ïƒ¹
ïƒª
ïƒª
ïƒ«
ïƒ©
ï€½
ïƒ·
ïƒ·
ïƒ·
ïƒ¸
ïƒ¶
ïƒ§
ïƒ§
ïƒ§
ïƒ¨
ïƒ¦
3
2
1
333231
232221
131211
'
3
'
2
'
1
x
x
x
hhh
hhh
hhh
x
x
x
 (1) 
å¼(1)å¯é€²ä¸€æ­¥è¡¨ç¤ºç‚º HXX ï€½' ï¼Œå…¶ä¸­ H ç‚ºç·šæ€§é½Š
æ¬¡ä¸”éå¥‡ç•°çš„çŸ©é™£ã€‚æ‰€ä»¥ï¼Œåœ¨ä¸€å¹³é¢çš„ä»»æ„é»ï¼Œå­˜
åœ¨è‘—ä¸€å€‹ç·šæ€§çš„æŠ•å½±è½‰æ›çŸ©é™£ H ï¼ŒæŠ•å½±åˆ°å¦ä¸€ç‰¹
å®šçš„å¹³é¢ä¸Šã€‚ç”±æ–¼æŠ•å½±è½‰æ›çŸ©é™£ H æ˜¯æè¿°äºŒç‰¹å®š
å¹³é¢é–“çš„è½‰æ›é—œä¿‚ï¼Œæ‰€ä»¥å°æ–¼ä¸åŒå¹³é¢é–“çš„æŠ•å½±è½‰
æ›ï¼Œæœƒæœ‰ä¸åŒçš„çŸ©é™£ H ã€‚æ­¤ç¾è±¡è§£é‡‹äº†ä¸åŒçš„ç›¸
æ©Ÿä½ç½®æœƒæ‹æ”åˆ°ä¸åŒçš„å½±åƒï¼ŒåŒæ™‚ä¹Ÿèªªæ˜äº†å¹³é¢æŠ•
å½±è½‰æ›çš„å”¯ä¸€æ€§ã€‚ 
 
5.2 ç›®æ¨™ç‰©å½±åƒè½åœ¨ç•«é¢ä¸­å¿ƒç‰¹å¾µåˆ†æèˆ‡å½±åƒç‰¹
å¾µè³‡æ–™åº«å»ºç«‹ 
5.2.1 å½±åƒç‰¹å¾µåˆ†æ 
åœ¨ç›¸æ©Ÿå­˜åœ¨ä¸€å€‹å‚¾è§’çš„ç‹€æ³ä¸‹ï¼Œæ‰€æ‹æ”åˆ°å½±åƒ
æœƒå½¢æˆä¸€å€‹è®Šå½¢çš„æ©¢åœ“ã€‚ç¶“åœ–1çš„å½±åƒç‰¹å¾µåˆ†æï¼Œ
å¯ä»¥æ­¸ç´å‡ºæ¼”ç®—æ³•1ã€‚ 
H
a
b
D
C
E
Ï†
Ïƒ
Ï
Î¸
Î¼
U V
G
O
R
 
åœ–1 ç›¸æ©Ÿä½ç½®èˆ‡ç›®æ¨™ç‰©çš„å¹¾ä½•æŠ•å½± 
 
æ¼”ç®—æ³•1ï¼šæ±‚å¾—ç›®æ¨™å½±åƒç‰¹å¾µä¹‹æ¼”ç®—æ³• 
 
 
5.2.2 å½±åƒç‰¹å¾µåˆ†æå½±åƒç‰¹å¾µè³‡æ–™åº«å»ºç«‹ 
ä»¥é¡ç¥ç¶“ç¶²è·¯[12]å°å‰é¢çš„æ¼”ç®—æ³•æ‰€ç”¢ç”Ÿä¹‹
è³‡æ–™åº«é€²è¡Œå­¸ç¿’ï¼Œå¯ä»¥å¾—åˆ°ç›¸æ©Ÿä½ç½®èˆ‡å½±åƒé–“çš„è½‰
æ›é—œä¿‚ã€‚ç‚ºäº†ä½¿å­¸ç¿’èª¤å·®æœ€å°åŒ–ï¼Œæœ¬è¨ˆç•«å°‡æ¡ç”¨èª¤
å·®å€’å‚³éå­¸ç¿’æ³•å‰‡ï¼ˆ Error back propagation, 
EBPï¼‰ã€‚EBPç‚ºä¸€ç¨®ç›£ç£å¼çš„å­¸ç¿’æ³•å‰‡ï¼Œåˆ©ç”¨å·²çŸ¥
çš„æ¨£æœ¬ä½œç‚ºç¶²è·¯è¼¸å…¥ï¼Œå¯ä»¥é”åˆ°æº–ç¢ºçš„çµæœã€‚ 
 
 
5.2.3 å½±åƒç‰¹å¾µæ“·å–æ¼”ç®—æ³•è¨­è¨ˆ 
    å½±åƒç‰¹å¾µæ“·å–æ³•çš„è¨­è¨ˆï¼Œæ˜¯ç‚ºäº†æ“·å–å½±åƒä¸­æœ‰
ç”¨çš„è³‡è¨Šã€‚åœ¨åœ–1ä¸­ï¼Œèˆ‡ç›¸æ©Ÿä½ç½®æœ‰é—œçš„å½±åƒç‰¹å¾µ
ç‚º aã€bã€ rã€d ã€‚åœ–2å‰‡æ˜¯èªªæ˜å½±åƒè™•ç†èˆ‡å½±åƒç‰¹
å¾µæ“·å–çš„æµç¨‹ã€‚ 
 
Connected components 
labeling algorithm
Basic rectangle 
computing
Major axis and minor 
axis computing
Image Data input
ïƒ¾
ïƒ½
ïƒ¼
ïƒ®
ïƒ­
ïƒ¬
d
r
r
a
b
a
Feature extraction
Thresholding
Filtering
Image processing
The  ratios 
of the Target  
 
åœ–2 å½±åƒè™•ç†èˆ‡å½±åƒç‰¹å¾µæ“·å–æµç¨‹ 
 
5.2.4 å¯¦é©—æ§‹æƒ³ 
ä»¥å–®å¼µç…§ç‰‡ä½œç‚ºå½±åƒè¼¸å…¥ï¼Œä¸¦æ–¼ PC ä¸Šé€²è¡Œå½±
åƒç‰¹å¾µæ“·å–æ¼”ç®—ï¼Œå–å‡ºç›¸é—œçš„å½±åƒç‰¹å¾µã€‚åŒæ™‚åˆ©ç”¨
æœ¬ä¾‹çš„å¯¦é©—æ¨¡æ“¬æµç¨‹å¦‚åœ– 3 æ‰€ç¤ºï¼Œåˆ†ç‚ºäºŒéƒ¨åˆ†ï¼š1. 
ä»¥ Database è¨“ç·´é¡ç¥ç¶“ç¶²è·¯ã€‚2. å°‡å½±åƒä¿¡è™Ÿç¶“ç”±
å½±åƒç‰¹å¾µæ“·å–æ¼”ç®—æ³•é‹ç®—ï¼Œæ“·å–å‡ºå¿…è¦çš„ç‰¹å¾µå€¼ä½œ
ç‚ºé¡ç¥ç¶“ç¶²è·¯çš„è¼¸å…¥ï¼Œé€²è€Œä¼°æ¸¬å‡ºä¸€çµ„å¯èƒ½çš„ç›¸æ©Ÿ
ä½ç½®ã€‚ 
 
Neural network training Neural network 
ïƒ¾
ïƒ½
ïƒ¼
ïƒ®
ïƒ­
ïƒ¬
'
'
'
'
'
'
d
r
r
a
b
a
ï» ï½'' Rï±
Image  processing
Feature extraction Training/checking 
data
 intput data
Output data
ï» ï½Rï± ï» ï½'' Rï±
Position of the camera
Image Data input
{- - -}a  a  r
b  r  d 
{- - -}a    r
b  r  d 
 
åœ–3 å¯¦é©—æ§‹æƒ³èˆ‡é€²è¡Œæµç¨‹ 
 
5.3 ç›®æ¨™ç‰©å½±åƒè½åœ¨ç•«é¢ä¸­å¿ƒå‚ç·šä¸Šç‰¹å¾µåˆ†æèˆ‡å½±
åƒç‰¹å¾µè³‡æ–™åº«å»ºç«‹ 
  
5.3.1 å½±åƒèˆ‡ç›¸æ©Ÿä½ç½®ä¹‹åˆ†æ 
    åœ¨ç›¸æ©Ÿå­˜åœ¨ä¸€å€‹å‚¾è§’çš„ç‹€æ³ä¸‹ï¼Œæ‰€æ‹æ”åˆ°å½±åƒ
æœƒå½¢æˆä¸€å€‹è®Šå½¢çš„é¡æ©¢åœ“ã€‚ç„¶è€Œåœ¨å¯¦éš›çš„ä¾‹å­ä¸­ï¼Œ
å½±åƒå¾ˆå°‘æœƒå‰›å¥½å‡ºç¾åœ¨ç•«é¢çš„ä¸­å¿ƒã€‚å¦‚åœ–4(a)èˆ‡åœ–
4(b)æ‰€ç¤ºï¼Œå¯¦éš›æ‹æ”åˆ°çš„åœ“å½¢ç›®æ¨™ç‰©æœƒå‘ˆç¾æ©¢åœ“çš„
è®Šå½¢ï¼Œç”±å‰è¿°çš„æŠ•å½±å¹¾ä½•åˆ†æï¼Œå¯ä»¥å¾—åˆ°å¦‚åœ–5èˆ‡
åœ–6çš„æ”å½±æ©Ÿä½ç½®èˆ‡å½±åƒæŠ•å½±çš„é—œä¿‚ï¼ŒåŒæ™‚å¯ä»¥å°‡
æ­¤æŠ•å½±é—œä¿‚æ•´ç†ç‚ºæ¼”ç®—æ³•2ã€‚è§€å¯Ÿåœ–5èˆ‡åœ–6ï¼Œå¯ä»¥
ç™¼ç¾é€™æ™‚å€™æƒ…æ³è®Šå¾—è¤‡é›œè¨±å¤šï¼Œé€™æ˜¯å› ç‚ºæ¢ä»¶æ”¾å¯¬
çš„ç·£æ•…ã€‚ç”±åœ–6(b)ä¸­å–å‡ºç›¸å°æ‡‰çš„å½±åƒç‰¹å¾µæœ‰ hã€
k ã€Qã€ r ã€ d ï¼Œä¸¦å®šç¾©Ratioproject= kh ï¼ŒRatioheight 
= dr ï¼ŒRatioangle= rh èˆ‡Ratioposition= k Qå››å€‹ç‰¹å¾µï¼Œå…¶
 6 
 
Data stage 1 Data stage k
Data related to R Data related to Î¸ Data related to A 
 Data base
Data Class2Data Class 1
Correlation of the 
I/O  is low
Data Class nData Class (n-1)
ANFIS 2ANFIS 1 ANFIS nANFIS (n-1)
Correlation of the 
I/O  is high
Data segmentation
Data classification
 
åœ–7 è³‡æ–™åˆ†å‰²èˆ‡è³‡æ–™åˆ†é¡æµç¨‹ 
 
Step(4) å°äºŒå€¼å½±åƒé€²è¡Œé€£é€šæ¨™è¨˜  (Connected 
components labeling algorithm)ï¼Œæ¨™è¨˜å‡ºéœ€è™•ç†çš„ç›®
æ¨™ç‰©å½±åƒã€‚ 
Step(5) æ¡†å‡ºåŸºæœ¬çŸ©å½¢ï¼ŒåŒæ™‚è¨ˆç®—ä¸»å‰¯è»¸åŠå…¶äº¤é»ã€‚ 
Step(6) è¨ˆç®— Ratioproject= kh ï¼ŒRatioheight = dr ï¼Œ
Ratioangle= rh èˆ‡ Ratioposition= k Qã€‚ 
 
Connected components 
labeling algorithm
Major axis and minor 
axis computing
Feature extraction
Image Data input
Thresholding & Filtering Image processing
Binary image
Basic rectangle 
computing
Multiple objects
Image Features
Y
N
 
åœ– 8 å½±åƒè™•ç†èˆ‡å½±åƒç‰¹å¾µæ“·å–æµç¨‹ 
 
5.4 å¯¦é©—çµæœèˆ‡è¨è«– 
5.4.1 ç›¸æ©Ÿæ ¡æ­£çµæœ 
    ç‚ºäº†ç¢ºå®šç›¸æ©Ÿçš„å…‰è»¸æ˜¯ç²¾ç¢ºåœ°å°æº–é™è½æ¨™è¨˜
çš„ä¸­å¿ƒï¼Œæ‰€ä»¥ç ”ç©¶ä¸­æ‰€ä½¿ç”¨çš„webcaméœ€è¦ç¶“éç›¸
æ©Ÿæ ¡æ­£çš„æ­¥é©Ÿã€‚ä¸€èˆ¬ç›¸æ©Ÿçš„é‡å­”æ¨¡å‹å¯ä»¥åˆ©ç”¨ç›¸æ©Ÿ
å…§éƒ¨åƒæ•¸èˆ‡å¤–éƒ¨åƒæ•¸ä¾†æè¿°ï¼Œå…§éƒ¨åƒæ•¸å¯ä»¥å®šç¾©ç‚º
K (ç¨±ç‚ºå››åƒæ•¸æ¨¡å‹) å¦‚(2)æ‰€è¡¨ç¤ºï¼Œå…§éƒ¨åƒæ•¸På¦‚(3)
æ‰€è¡¨ç¤ºã€‚ 
ïƒº
ïƒº
ïƒº
ïƒ»
ïƒ¹
ïƒª
ïƒª
ïƒª
ïƒ«
ïƒ©
ï€½
1
yy
xx
pf
pf
K  (2) 
P= [R| t ] (3) 
å…¶ä¸­Rç‚ºæ—‹è½‰çŸ©é™£ï¼Œtç‚ºå‚³é€çŸ©é™£ã€‚è‡³æ–¼Pçš„æ±‚è§£ï¼Œ
åœ¨æœ¬ç ”ç©¶ä¸­æ˜¯ä»¥ Zhangæ‰€æå‡ºçš„æ ¡æ­£æ–¹æ³•é€²è¡Œç›¸
æ©Ÿæ ¡æ­£ï¼Œå…¶æ ¡æ­£çµæœå¦‚è¡¨ä¸€æ‰€ç¤ºã€‚å¦‚æ­¤å¯ä»¥ç¢ºå®šï¼Œ
å…‰è»¸é€šéå½±åƒçš„åº§æ¨™ç‚º(293.7, 237.91)ã€‚ç¢ºå®šäº†å…‰
è»¸çš„æ–¹å‘ï¼Œæœ‰åŠ©æ–¼æ¸›å°‘å½±åƒç‰¹å¾µæ“·å–æ™‚æ‰€å‡ºç¾çš„èª¤
å·®ã€‚ 
 
è¡¨ä¸€ã€ç›¸æ©Ÿæ ¡æ­£çµæœ 
Focal Length 
Webcam 
fx fy 
688.92 690.84 
Principal point 293.70 237.91 
 
5.4.2 ç›®æ¨™ç‰©å½±åƒè½åœ¨ç•«é¢ä¸­å¿ƒç‰¹å¾µåˆ†æèˆ‡å½±åƒç‰¹
å¾µè³‡æ–™åº«å»ºç«‹ä¹‹å¯¦é©—çµæœ 
å¯¦éš›å½±åƒèˆ‡ç›¸æ©Ÿä½ç½®ç›¸å°æ‡‰çš„ç‰¹å¾µå€¼å¦‚åœ– 1
æ‰€ç¤ºã€‚åœ¨å½±åƒç‰¹å¾µæ“·å–æ¼”ç®—æ³•çš„è¨­è¨ˆä¸Šï¼Œå¯ä»¥é€é
å°ç›®æ¨™ç‰©åœˆå–â€œåŸºæœ¬çŸ©å½¢â€çš„æ–¹æ³•ï¼Œå¿«é€Ÿçš„å–å¾—ã€‚
åŒæ™‚åˆ©ç”¨æ¼”ç®—æ³• 1 å¯ä»¥è¨ˆç®—å‡ºåœ¨æ‹æ”ä½ç½® 20 ~ 
300CMï¼Œè§’åº¦åœ¨ï€­89~89 åº¦é–“æ‰€æœ‰çš„å½±åƒç‰¹å¾µè®Š
åŒ–ï¼Œä¸¦åŒæ™‚å®šç¾© Ratioproject = a / b, Ratioheight = r / d, 
èˆ‡ Ratioangle = a / r ç­‰ä¸‰å€‹èˆ‡ç›¸æ©Ÿä½ç½®ç›¸é—œçš„ç‰¹å¾µ
æ¯”ä¾‹ï¼Œä½œç‚ºæ™ºæ…§å‹å­¸ç¿’æ³•å‰‡çš„è³‡æ–™åº«ã€‚åœ¨æœ¬ç ”ç©¶è¨ˆ
ç•«ä¸­æ‰€æå‡ºçš„äºŒç¨®æ™ºæ…§å‹å­¸ç¿’æ³•å‰‡åˆ†åˆ¥ç‚ºé¡ç¥ç¶“
ç¶²è·¯èˆ‡ ANFISã€‚ 
é¡ç¥ç¶“ç¶²è·¯ä¼°æ¸¬ 
å°æ–¼é¡ç¥ç¶“ç¶²è·¯çš„è¨­è¨ˆï¼Œç‚ºäº†ä½¿å­¸ç¿’èª¤å·®æœ€å°
åŒ–ï¼Œæœ¬ç ”ç©¶æ¡ç”¨èª¤å·®å€’å‚³éå­¸ç¿’æ³•å‰‡ï¼ˆEBPï¼‰ã€‚EBP
ç‚ºä¸€ç¨®ç›£ç£å¼çš„å­¸ç¿’æ³•å‰‡ï¼Œåˆ©ç”¨å·²çŸ¥çš„æ¨£æœ¬ä½œç‚ºç¶²
è·¯è¼¸å…¥ï¼Œå¯ä»¥é”åˆ°æº–ç¢ºçš„çµæœã€‚é¡ç¥ç¶“ç¶²è·¯å­¸ç¿’è¼¸
å‡ºèˆ‡å…¶è¨“ç·´èª¤å·®ï¼Œåˆ†åˆ¥å¦‚åœ– 9 èˆ‡åœ– 10 æ‰€ç¤ºï¼Œé™¤äº†
æ”å½±æ©Ÿè§’åº¦ä½æ–¼ ï€­89åº¦èˆ‡ 89åº¦å…©å€‹æ¥µç«¯çš„ä½ç½®é™„
è¿‘çš„è¨“ç·´æ•ˆæœè¼ƒå·®å¤–ï¼Œå…¶é¤˜çš„è³‡æ–™è¨“é‡èª¤å·®çš†åœ¨
1.0ï‚± é™„è¿‘ï¼Œè¨“ç·´æ•ˆæœä¸éŒ¯ã€‚å­¸ç¿’èª¤å·®å¦‚åœ– 10ï¼Œåœ–
11 ç‚ºå­¸ç¿’æ­·ç¨‹çš„å‡æ–¹å·®ã€‚ç”±åœ– 11 é¡¯ç¤ºï¼Œå‡æ–¹å·®ä¹Ÿ
åœ¨ç–Šä»£è¨“ç·´ 1000 æ¬¡å¾Œå¯é”è¿‘ä¼¼æœ€ä½³ã€‚ 
    å¯¦é©—çµæœé¡¯ç¤ºæ–¼è¡¨äºŒï¼Œæœ¬å¯¦é©—å…±å–å‡º 12 çµ„å¯¦
é©—æ•¸æ“šï¼Œå¯ä»¥ç™¼ç¾èª¤å·®ï¨¦å°æ–¼ 0.1 åº¦ï¼Œæ¨ä¼°ä¹‹æº–ç¢º
åº¦å¾ˆé«˜ï¼Œè¡¨ç¤ºæ¼”ç®—æ³•çš„è¨­è¨ˆæ˜¯æ­£ç¢ºç„¡èª¤çš„ï¼Œä¸ä½†å¯
ä»¥ç¶“ç”±å½±åƒè™•ç†çš„æŠ€å·§ï¼Œå–å‡ºå½±åƒçš„ç‰¹å¾µé•·åº¦çš„æ¯”
ä¾‹ï¼Œè€Œé¡ç¥ç¶“ç¶²è·¯çš„å­¸ç¿’æ•ˆæœä¹Ÿä¸éŒ¯ã€‚ 
 
 8 
 
(a-1) Results of checking data for ANFIS for ï±. 
 
 
(a-2) Results of estimating data for ANFIS for ï± 
 
 
(b-1) Results of checking data for ANFIS for R. 
 
 
(b-2) Results of estimating data for ANFIS for R. 
åœ– 13 ANFIS è¨“ç·´èˆ‡æ¸¬è©¦çµæœ 
 
 
ç”±ä¸Šè¿°äºŒç¨®æ–¹æ³•æ‰€ä¼°æ¸¬çš„çµæœå¯çŸ¥ï¼Œä¼°æ¸¬èª¤å·®
ï¨¦åœ¨è¶³å¤ å°çš„ç¯„åœå…§ï¼Œç„¶è€Œåœ¨è¨ˆç®—æ™‚é–“ä¸Šï¼Œç”±æ–¼
ANFISçš„è¨ˆç®—æ™‚é–“é å°æ–¼é¡ç¥ç¶“ç¶²è·¯ï¼Œæ‰€ä»¥ANFIS
ç›¸å°ä¹‹ä¸‹æ˜¯æ›´é©åˆæœ¬ç ”ç©¶è¨ˆç•«çš„ç›®æ¨™çš„ã€‚ 
 
5.4.3 ç›®æ¨™ç‰©å½±åƒè½åœ¨ç•«é¢ä¸­å¿ƒå‚ç·šä¸Šç‰¹å¾µåˆ†æèˆ‡
å½±åƒç‰¹å¾µè³‡æ–™åº«å»ºç«‹ä¹‹å¯¦é©—çµæœ 
ç¶“ç”±æ¼”ç®—æ³•2å»ºæ§‹å®Œæˆçš„è³‡æ–™åº«ååˆ†é¾å¤§ï¼Œè³‡æ–™æ•¸
é«˜é”æ•¸åè¬ç­†ï¼Œå°‡éœ€è¦å¾ˆä¹…çš„è¨“ç·´æ™‚é–“ï¼Œæ‰€ä»¥å…ˆå°‡
è³‡æ–™åšå‰è™•ç†ï¼Œå°‡é—œè¯åº¦é«˜èˆ‡é—œè¯åº¦ä½çš„è³‡æ–™åˆ†åˆ¥
åšè¨“ç·´ï¼Œå‰‡å¯ä»¥ç¯€çœå¤§é‡çš„è¨“ç·´æ™‚é–“ã€‚å…¶è¨“ç·´çµæœ
ä»¥â€œheight 126~130cmâ€ â€œÎ¸= ï€­62Â° to 62Â°â€æ­¤å€é–“è³‡æ–™
ä½œå±•ç¤ºï¼Œå…¶ANFISè¨“ç·´çµæœå¦‚åœ–14æ‰€ç¤ºã€‚è¨“ç·´å®Œæˆ
å¾Œæ‰€ç”¢ç”Ÿçš„å¾å±¬å‡½æ•¸å¦‚è¡¨ä¸‰æ‰€ç¤ºï¼Œé—œé€£åº¦è¼ƒé«˜çš„è³‡
æ–™å‰‡ä»¥æ•¸é‡è¼ƒå°‘çš„å¾å±¬å‡½æ•¸å¯ä»¥ç²å¾—ä¸éŒ¯çš„å­¸ç¿’
æ•ˆæœï¼Œåä¹‹é—œè¯åº¦ä½çš„è³‡æ–™ï¼Œå‰‡éœ€è¦æ•¸é‡è¼ƒå¤šçš„å¾
å±¬å‡½æ•¸ã€‚å…‰è»¸åœ¨åœ°é¢æŠ•å½±é»åˆ°é™è½æ¨™è¨˜çš„è·é›¢è¨“ç·´
ç„¡äººè¼‰å…·åˆ°é™è½æ¨™è¨˜çš„è·é›¢èˆ‡å…‰è»¸èˆ‡åœ°é¢çš„å¤¾è§’
çš„RMSEï¼Œåˆ†åˆ¥ç‚º 0.641, 0.124 èˆ‡ 0.365ã€‚åœ–15ç‚º
å½±åƒè™•ç†çš„çµæœï¼Œåˆ©ç”¨åŸºæœ¬çŸ©å½¢æ‰€æ¡†å‡ºçš„ç›®æ¨™ç‰©ï¼Œ
è¡¨ä¸‰èˆ‡è¡¨å››æ‰€ç¤ºå‰‡æ˜¯ä»¥åœ–15ç‚ºä¾‹æ‰€ä¼°æ¸¬çš„çµæœã€‚ 
åŸºæ–¼ä¸Šè¿°çš„åˆ†æèˆ‡æ¨¡æ“¬çµæœé¡¯ç¤ºï¼Œç ”ç©¶ä¸­æ‰€æ
å‡ºä»¥æ™ºæ…§å‹å­¸ç¿’æ³•å‰‡ä¾†å­¸ç¿’ 2D å½±åƒèˆ‡ 3D ç©ºé–“ä¸­
çš„ mapping é—œä¿‚ï¼Œå¯ä»¥ç”±å–®ä¸€çš„å½±åƒä¸­å–å‡ºç›®æ¨™ç‰©
ç‰¹å¾µï¼Œä¸¦å¿«é€Ÿçš„ä¼°æ¸¬å‡ºä¸€å€‹èª¤å·®åœ¨å¯æ¥å—çš„ç¯„åœå…§
çš„è¿‘ä¼¼æ‹æ”ä½ç½®ã€‚ 
 
 
(a) å…‰è»¸åœ¨åœ°é¢æŠ•å½±é»åˆ°é™è½æ¨™è¨˜çš„è·é›¢ 
 
 
(b) ç„¡äººè¼‰å…·åˆ°é™è½æ¨™è¨˜çš„è·é›¢ 
 
 
(c) å…‰è»¸èˆ‡åœ°é¢çš„å¤¾è§’ 
åœ– 14 ANFIS è¨“ç·´çµæœ 
 
   
(a) (b) (c) 
åœ– 15 å½±åƒè™•ç†çš„çµæœã€‚ (a) Image input (b) Binary 
image (c) Basic rectangle of image 
 
è¡¨ä¸‰ã€åœ– 15 ä¸­çš„ Image features  
 Ratioproject Ratioheight Ratioangle Ratioposition 
Actually 
Image 
Ratios 
1.1092 1.0247 8.9016 0.0817 
 10 
tracking of constrained robot manipulators, 
American Control Conference, pp.3694-3700. 
[5] é™³ä¿Šå¿—ï¼Œ2006ï¼Œç„¡äººé£›è¡Œè¼‰å…·åœ°é¢ç§»å‹•ç›®æ¨™å½±
åƒè‡ªå‹•é–å®šè¿½è¹¤ç³»çµ±ä¹‹ç ”ç™¼ï¼Œç¢©å£«è«–æ–‡ï¼Œåœ‹ç«‹
æˆåŠŸå¤§å­¸èˆªç©ºå¤ªç©ºå·¥ç¨‹å­¸ç³»ï¼Œå°å—ï¼Œå°ç£ã€‚ 
[6] è˜‡ä»²éµ¬ã€é»ƒæŸå‡±ã€å¼µæ”¿é›„ã€ï§¡ç´€ï¦®ï¼Œ2009ï¼Œè‡ª
ä¸»ç„¡äººç›´å‡æ©Ÿåœ°é¢å°æ§èˆ‡ç›£æ¸¬ç³»çµ±çš„è¨­è¨ˆ
èˆ‡å¯¦ç¾ï¼Œ2009 ä¸­è¯æ°‘åœ‹èˆªç©ºå¤ªç©ºå­¸æœƒ/ä¸­è¯æ°‘
ç”¨èˆªç©ºå­¸æœƒè¯åˆå­¸è¡“ç ”è¨æœƒï¼Œ1-8 é ï¼Œå°ï¥£å¸‚ã€‚ 
[7] è˜‡ä»²éµ¬ã€ï§¡å‰å›ã€é™³é€¸å¼·ã€é€£ç´¹å¸†ï¼Œ2008ï¼Œæ‡‰
ç”¨æ–¼è‡ªä¸»é£›è¡Œç„¡äººç›´å‡æ©Ÿå‚ç›´çˆ¬å‡èˆ‡é™è½
çš„å½±åƒè™•ç†æŠ€è¡“ï¼Œä¸­è¯æ°‘éç¬¬äº”åå±†èˆªå¤ªå­¸
æœƒå­¸è¡“ç ”è¨æœƒï¼Œå°ï¥£ï¼Œå°ç£ã€‚ 
[8] ï§¡å¿—å‚‘ã€é™³å‚³ç”Ÿï¼Œ2006ï¼Œç„¡äººç›´æ˜‡æ©Ÿåœæ‡¸ç©©å®š
æ§åˆ¶ç³»çµ±ï¼Œç¢©å£«è«–æ–‡ï¼Œå…ƒæ™ºå¤§å­¸æ©Ÿæ¢°å·¥ç¨‹å­¸
ç³»ï¼Œä¸­å£¢ï¼Œå°ç£ã€‚ 
[9] J.M. Roberts, P.I Corke and G. Buskey, 2002, 
â€œLow-Cost Flight Control System for a Small 
Autonomous Helicopterâ€, Australasian 
Conference on Robotics and Automation, 
pp.27-29. 
[10] S.B. Kim, S.Y. Lee, J.H. Choi, K.H. Choi and 
B.T. Jang, 2003, â€œA Bimodal Approach for GPS 
and IMU Integration for Land Vehicle 
Applicationsâ€, IEEE Vehicular Technology 
Conference, vol. 4, pp.2750-2753. 
[11] R. Hartley and A. Zisserman, 2003, Multiple 
View Geometry in computer vision, Cambridge 
University Press. 
[12] M.T. Hagan, H.B. Demuth and M. Beale, 2004, 
Neural Network Design, Thomson. 
[13] J.S.R. Jang, 1993, ANFIS: Adaptive network 
based fuzzy inference system, IEEE Trans. On 
System, Man and Cybernetics, Vol. 23, no. 3, 
pp.665-685, May/June. 
 
é™„éŒ„ï¼š 
æœ¬è¨ˆç•«è¡ç”Ÿè‘—ä½œæ¸…å–®ï¼š 
ç ”è¨æœƒè«–æ–‡ï¼š 
1. Lien, S.F., K.H. Hsia and J.P. Su (2010). 
Image-Guided Height Estimation for Unmanned 
Helicopter Landing, Third International 
Symposium on Intelligent Informatics, Dalian 
City, China. (2010.9.5-7, BoHai Pearl Hotel 
Dalian, Dalian, China) 
2. Hsia, K.H., S.F. Lien, and J.P. Su (2010). Camera 
Position Estimation by ANFIS from Incomplete 
Landmark Image, The 18th National Conference 
on Fuzzy Theory and Its Applications, Hualien, 
Taiwan, pp. 67-72. (2010.12.3-4, æ±è¯å¤§å­¸) 
3. Hsia, K.H., S.F. Lien, C.C. Wang, T.N. Lee, and 
J. P. Su (2011).  Camera Position Estimation 
and Feature Extraction from Incomplete Image of 
Landmark, Proceeding of International The 
Sixteenth Symposium on Artificial Life and 
Robotics (AROB 16th '11), Oita, Japan, pp. 
277-280. (2011.1.27-29, B-Con Plaza, Beppu, 
Oita, Japan) 
æœŸåˆŠè«–æ–‡ï¼š 
1. Hsia, K.H., S.F. Lien, C.C. Wang, T.N. Lee, and 
J. P. Su (2011).  Camera Position Estimation 
and Feature Extraction from Incomplete Image of 
Landmark, Artificial Life and Robotics, Vol. 16, 
No. X, pp.XX-XX. (To appear) 
å°ˆæ›¸å°ˆç« ï¼š 
1. Hsia, K.H., S.F. Lien and J.P. Su (2011). Height 
Estimation via Stereo Vision System for 
Unmanned Helicopter Autonomous Landing, in 
Machine Vision (ISBN 979-953-307-677-4), 
Fabio Solari ed., InTech - Open Access Publisher. 
 
 
of Residual Disability (ä½é¢¨éšªçš„å¾®å‰µæ‰‹è¡“ç™²ç™æ²»ç™‚)ã€‚å±±å·æ•™æˆæ˜¯ä¹å·å·¥æ¥­å¤§å­¸å¤§
å­¸é™¢ç”Ÿå‘½é«”å·¥å­¸ç ”ç©¶ç§‘è…¦æƒ…å ±å°ˆæ”»çš„ç‰¹è˜æ•™æˆï¼Œä¹Ÿæ˜¯æ—¥æœ¬ Fuzzy Logic Systems 
Institute (FLSI) çš„ Chairmanï¼Œä»–åŒæ™‚ä¹Ÿæ˜¯ IEEE çš„ Fellowï¼Œå…·æœ‰ç›¸ç•¶å´‡é«˜çš„å­¸è¡“
åœ°ä½ã€‚å±±å·æ•™æˆé™¤äº†å­¸è¡“åœ°ä½å´‡é«˜ä¹‹å¤–ï¼Œä¹Ÿæ˜¯ç©ºæ‰‹é“é»‘å¸¶äº”æ®µçš„åŠŸå¤«é«˜æ‰‹ï¼Œä¹Ÿèƒ½
å½ˆå¥ä¸‰å¼¦ç´å’Œå°ºå…«ç­‰æ—¥æœ¬çš„å‚³çµ±æ¨‚å™¨ï¼ŒçœŸæ˜¯å€¼å¾—è®“äººæ•¬ä»°ã€‚åœ¨å±±å·æ•™æˆçš„æ¼”è¬›ä¸­
æåˆ°ï¼Œç™²ç™æ˜¯ä¸€ç¨®æœƒåå¾©ç™¼ä½œçš„æ…¢æ€§è…¦éƒ¨ç–¾ç—…ï¼Œå…¨ä¸–ç•Œç´„æœ‰ 6 åƒ 8 ç™¾è¬å€‹ç™²ç™æ‚£
è€…ï¼Œå…¶ä¸­ 80%å¯ä»¥ä»¥è—¥ç‰©æ§åˆ¶è€Œèƒ½æœ‰è‰¯å¥½çš„ç”Ÿæ´»å“è³ªï¼Œä½†é‚„æ˜¯æœ‰ 1360 è¬å·¦å³çš„
ç™²ç™æ‚£è€…å› ç‚ºå¾—é¢å°ç™²ç™çš„çªç„¶ç™¼ä½œè€Œä½¿å¾—ç”Ÿæ´»ä½œæ¯ç„¡æ³•æ­£å¸¸ï¼Œè§£æ±ºçš„æ–¹æ³•å°±æ˜¯
è¦ä»¥æ‰‹è¡“é™¤å»æ‰€è¬‚çš„ã€è‡´ç™‡å€ã€ã€‚ä½†è¦æ­£ç¢ºåœ°æ‰¾åˆ°é€™å€‹è‡´ç™‡å€æ˜¯ä¸å¤ªå®¹æ˜“çš„ï¼Œæ‰¾
åˆ°å¾Œè¦åœ¨ä¸æå‚·æ­£å¸¸è…¦éƒ¨çš„ç‹€æ³ä¸‹å»é™¤é€™å€‹è‡´ç™‡å€ä¹Ÿæ˜¯ç›¸ç•¶åœ°å›°é›£ã€‚å±±å·æ•™æˆçš„
ç ”ç©¶åœ˜éšŠåœ¨ç¨ç«‹è¡Œæ”¿æ³•äººæ—¥æœ¬å­¸è¡“æŒ¯èˆˆæœƒ (Japan Society for the Promotion of 
Science, JSPS) çš„æ”¯æŒä¸‹å¾ 2008 å¹´å…«æœˆé–‹å§‹é€²è¡Œè‡´ç™‡å€å®šä½é€™ä¸€æ–¹é¢çš„ç ”ç©¶ï¼Œé€™
å€‹è¨ˆç•«å°‡æŒçºŒåˆ° 2012 å¹´ä¸‰æœˆã€‚åœ¨æ­¤æ¬¡çš„å°ˆé¡Œæ¼”è¬›ä¸­ï¼Œå±±å·æ•™æˆç™¼è¡¨äº†ç›®å‰çš„ç ”
ç©¶æˆæœã€‚ 
ç¬¬äºŒå ´å°ˆé¡Œæ¼”è¬›æ˜¯åœ¨ 9 æœˆ 7 æ—¥ä¸Šåˆ 8:50~9:50 é€²è¡Œï¼Œç”±å¤§é€£ç†å·¥å¤§å­¸ç®¡ç†å­¸
é™¢çš„æ¥Šå¾·ç¦®æ•™æˆä¸»è¬›ï¼Œå¤§è¿ç†å·¥å¤§å­¸ä¿¡æ¯å®‰å…¨ç ”ç©¶ä¸­å¿ƒå¸¸å‹™å‰¯ä¸»ä»»å­”ç¥¥ç¶­æ•™æˆä¸»
æŒï¼Œè¬›é¡Œæ˜¯ã€Internet of Things: Next Generation of Internet?ã€ï¼Œå±¬æ–¼ ISIKM2010
çš„å°ˆé¡Œæ¼”è¬›ã€‚åœ¨é€™å ´æ¼”è¬›ä¸­ï¼Œæ¥Šæ•™æˆä»‹ç´¹äº†ç¶²éš›ç¶²è·¯çš„éå»ã€ç¾åœ¨åŠæœªä¾†çš„å¯èƒ½
ç™¼å±•ã€‚æˆ‘å€‘æ‰€ç™¼è¡¨çš„è«–æ–‡åœ¨ 6 æ—¥ä¸‹åˆçš„ç¬¬ä¸€å€‹å ´æ¬¡ï¼Œåœ¨å››æ¨“çš„ D å ´åœ°é€²è¡Œï¼Œå…±
æœ‰å…­ç¯‡è«–æ–‡åœ¨é‚£å€‹å ´æ¬¡ä¸­ç™¼è¡¨ï¼Œå ´æ¬¡çš„ä¸»é¡Œæ˜¯ï¼šæ™ºæ…§å‹ç³»çµ±ã€‚æœ¬äººä¸Šå°å®£è®€çš„è«–
æ–‡åŸºæœ¬ä¸Šæ˜¯å»¶çºŒå…ˆå‰çš„ç ”ç©¶ï¼Œé€™æ¬¡æ˜¯åˆ©ç”¨å…©å€‹é¡é ­å¾—åˆ°ç›´å‡æ©Ÿé™è½æ¨™è¨˜å½±åƒçš„åƒ
å·®ä¾†å”åŠ©ç„¡äººç›´å‡æ©Ÿè‡ªä¸»é™è½æ™‚çš„é«˜åº¦èˆ‡ä½ç½®ä¹‹åˆ¤æ–·ï¼Œç‚ºæœ¬äººä»Šå¹´åœ‹ç§‘æœƒå°ˆé¡Œç ”
ç©¶è¨ˆç•«çš„éƒ¨ä»½æˆæœï¼Œä¸éå› æœ€çµ‚å®šå‰ç¨¿ï¼Œåœ‹ç§‘æœƒè¨ˆç•«ä»æœªæ ¸å®šï¼Œæ‰€ä»¥ç„¡æ³•åœ¨è«–æ–‡
ä¸­è‡´ä¸Šè¬å¿±ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œç„¡äººç›´å‡æ©Ÿéƒ½æ˜¯é€é GPS ä¾†é€²è¡Œå®šä½ï¼Œåˆ©ç”¨ IMU ä¾†è¼”
åŠ©å®šä½çš„ä¿®æ­£ã€‚ä¸éï¼ŒGPS çš„èª¤å·®é€šééƒ½åœ¨æ•¸å…¬å°ºä»¥ä¸Šï¼Œé«˜åº¦èª¤å·®æœƒæ¯”æ°´å¹³èª¤
å·®è¦ä¾†å¾—æ›´å¤§ï¼Œé€™å°ç„¡äººç›´å‡æ©Ÿè¦é€²è¡Œè‡ªä¸»é™è½è€Œè¨€ï¼Œæ˜¯ç›¸ç•¶å±éšªçš„ã€‚åœ¨æˆ‘å€‘çš„
æ–¹æ³•ä¸­ï¼Œåˆ©ç”¨å…©å€‹ç°¡å–®çš„ WEBCAM çµ„åˆè€Œæˆçš„æ©Ÿæ§‹ï¼ŒèŠ±è²»ç¸½å…±ä¸éæ•¸ç™¾å…ƒï¼Œåœ¨
ç„¡äººç›´å‡æ©Ÿè‡ªä¸»é™è½çš„é—œéµ 6 å…¬å°ºä¹‹å…§çš„é«˜åº¦ç¯„åœï¼Œæ‰€é‡å¾—çš„é«˜åº¦ä¹‹èª¤å·®å‡ä¸åˆ°
10 å…¬åˆ†ï¼Œå…·æœ‰ç›¸ç•¶ä¸éŒ¯çš„å¯¦ç”¨åƒ¹å€¼ã€‚ 
äºŒã€èˆ‡æœƒå¿ƒå¾— 
åœ¨é€™æ¬¡çš„ç ”è¨æœƒä¸­ï¼Œå€‹äººå°æ–¼åŒå ´æ¬¡ç”±å°ç£å¸«å¤§é™³ç¾å‹‡æ•™æˆçš„æ–‡ç« 
ã€Reconstruction of a 3-D Object Model Using 2D Image Contours Dataã€æ„Ÿåˆ°é«˜åº¦
çš„èˆˆè¶£ã€‚åœ¨é™³æ•™æˆçš„è«–æ–‡ä¸­ï¼Œä»–ç”¨å…©å€‹ CCD ç…§ç›¸æ©Ÿä»¥è·é›¢ç°¡å–®çµæ§‹çš„è¢«æ¸¬ç‰©ç­‰
è·çš„å‰è¦–èˆ‡å´è¦–åœ–ï¼Œå³å¯ä¼°æ¸¬å‡ºé€™å€‹è¢«æ¸¬ç‰©çš„ç«‹é«”å¤–æ¡†çµæ§‹ã€‚é™³æ•™æˆçš„ç ”ç©¶èˆ‡æˆ‘
ç›®å‰æ‰€é€²è¡Œçš„ç ”ç©¶ï¼Œåœ¨æŸäº›æ–¹é¢æœ‰ç•°æ›²åŒå·¥ä¹‹å¦™ã€‚ 
æœ¬æ¬¡èµ´å¤§é™¸åƒåŠ ç ”è¨æœƒï¼Œé™¤äº†æ­£è¦çš„è¡Œç¨‹ä¹‹å¤–ï¼Œé †é“åƒè¨ªäº†ä¸­åœ‹çš„ä¸€äº›å¤è¹Ÿ
åŠå±±å·é¢¨å…‰ï¼Œæ”¶ç©«ç›¸ç•¶è±ç¢©ã€‚ç‰¹åˆ¥æ˜¯èƒ½å’Œä¸€äº›æ•™æˆå­¸è€…äº¤æ›ç ”ç©¶å¿ƒå¾—ï¼Œæ„Ÿè¦ºå°æ—¥
å¾Œçš„ç ”ç©¶å¿…å°‡æœ‰æ‰€åŠ©ç›Šã€‚ 
ä¸‰ã€è€ƒå¯Ÿåƒè§€æ´»å‹•(ç„¡æ˜¯é …æ´»å‹•è€…ç•¥) 
 
å¤éƒ­è³¢  
å¯„ä»¶è€…: "Shao-Fan Lien" <g9510801@yuntech.edu.tw>
æ—¥æœŸ: 2010å¹´5æœˆ3æ—¥ ä¸Šåˆ 09:06
æ”¶ä»¶è€…: "å¤éƒ­æ•™å‹™é•·" <khhsia@cc.feu.edu.tw>; "è˜‡ä»²éµ¬æ•™æˆ" <sujp@yuntech.edu.tw>
ä¸»æ—¨: Fwd: Acceptance Letter: ISII2010-283
bç¬¬ 1 é  (å…± 3 é )(B)
2011/9/1(B)b
 
 
---------- Forwarded message ---------- 
From: Yan SHI <yshi@ktmail.tokai-u.jp> 
Date: 2010/5/1 
Subject: Acceptance Letter: ISII2010-283 
To: g9510801@yuntech.edu.tw 
Cc: isii2010 <isii2010@ijicic.org> 
 
 
Dear Dr. Shao-Fan Lien, 
  
It is our pleasure to inform you that your paper, 
  
Reference No.: ISII2010-283 
Title: Image-Guided Height Estimation for Unmanned Helicopter Landing 
Author(s): Shao-Fan Lien, Kuo-Hsien Hsia and Juhng-Perng Su 
  
submitted to The Third International Symposium on Intelligent Informatics (ISII2010), which will be held
on September 5-7, 2010, in Dalian, China, has been accepted for an oral presentation at the symposium, and
for the publication in ICIC Express Letters (ICIC-EL) with the conditions described below. Please prepare your
paper  (No  more  than 6  pages)  based  on  the  Acceptance  Conditions  and  ICIC  Express  Letters  style
(download from http://www.ijicic.org/icicel.htm) for publication. 
  
----------------------------------------------------------- 
Acceptance Conditions: 
  
1)  The motivation on the study should  be further  emphasized.  The key features  of  the results  obtained
compared with some existing ones should be clearly explained in the Introduction section.  
Â  
Â  
(1)      BasedÂ onÂ differentÂ methods,Â FigureÂ 6Â andÂ FigureÂ 7Â presentÂ theÂ estimationÂ errorÂ curvesÂ forÂ differentÂ baseline.Â 
FromÂ theseÂ figures,Â theÂ rangeÂ ofÂ measurementÂ distanceÂ ofÂ BMAÂ isÂ muchÂ longerÂ thanÂ thatÂ ofÂ otherÂ method.Â  
(2)      InÂ theÂ secondÂ lineÂ belowÂ FigureÂ 7,Â theÂ BMAÂ couldÂ findÂ correspondingÂ pointsÂ quicklyÂ andÂ effectivelyÂ withinÂ 7.5Â 
meters.Â But,Â theÂ wordÂ â€œquicklyâ€Â hasÂ notÂ beenÂ provenÂ inÂ theÂ experiments. 
(4)   IsÂ GPSÂ 18â€5HzÂ aÂ correctÂ typeÂ numberÂ forÂ GarminÂ GPS?Â MaybeÂ itÂ isÂ namedÂ GPSÂ 18Â 5Hz,Â whichÂ frequencyÂ isÂ 5Hz.Â 
(5)   SomeÂ grammarÂ mistakesÂ shouldÂ beÂ correctedÂ inÂ thisÂ paper.Â ForÂ example,Â inÂ sectionÂ 5Â experimentalÂ results,Â â€œâ€¦,
Fig. 4 show that â€¦â€ shouldÂ be â€œâ€¦ shows that â€¦â€.Â Â 
(6)   TheÂ sectionÂ numberÂ ofÂ â€œConclusionÂ andÂ Discussionâ€Â isÂ 6,Â notÂ 5.Â 
Â Â 
InÂ abstract,Â theÂ authorsÂ describeÂ thatÂ theÂ proposedÂ imageâ€guidedÂ estimationÂ methodÂ canÂ provideÂ accurateÂ heightÂ 
informationÂ forÂ amendingÂ theÂ errorÂ ofÂ GPS.Â ItÂ seemsÂ toÂ meÂ thatÂ theÂ proposedÂ imageâ€guidedÂ methodÂ isÂ employedÂ toÂ 
estimateÂ theÂ heightÂ informationÂ withinÂ aÂ shortÂ distanceÂ insteadÂ ofÂ beingÂ utilizedÂ toÂ correctÂ theÂ errorÂ ofÂ GPS.Â Â TheÂ 
purposeÂ andÂ keyÂ featureÂ ofÂ theÂ proposedÂ methodÂ inÂ thisÂ paperÂ needÂ toÂ beÂ furtherÂ explained.Â 
Â  
  
2) English should be substantially improved. The authors should carefully and thoroughly check the paper,
and correct all the grammar and composition mistakes in the final version.  
  
3)  Please  cite  some  recently  published  papers  in  IJICIC  (www.ijicic.org  )  or  ICIC-EL
(http://www.ijicic.org/icicel.htm) in your paper, which is related to the topic in the paper, and this will promote
your work in wider community and have a positive impact on the journal.  
  
  
  
Kind regards, 
  
  
Ms. Satomi Kojima 
  
On behalf of  
Dr. Yan SHI  
Organizing Committee Chair, ISII2010 
Professor, School of Industrial Engineering 
Tokai University  
9-1-1, Toroku, Kumamoto 862-8652, Japan  
Tel. & Fax: +81-96-386-2666  
E-mail: isii2010@ijicic.org 
  
Dr. Xiangwei Kong 
Program Committee Chair, ISII2010 
Professor, School of Electronic and Information Engineering 
Dalian University of Technology 
No.2, Linggong Rd., Dalian, China 
Tel.: +81-411-84706368 
  
  
  
  
 
bç¬¬ 3 é  (å…± 3 é )(B)
2011/9/1(B)b


ICIC Express Letters ICIC Internationalâ“’2009 ISSN 1881-803X
Volume 3, Number 3, September 2009 pp. 1â€“6
Image-Guided Height Estimation for Unmanned Helicopter Landing
Shao-Fan Lien1, Kuo-Hsien Hsia2 and Juhng-Perng Su 3
1Graduate School of Engineering Science and Technology
3Department of Electrical Engineering
National Yunlin University of Science and Technology
No. 123, University Road, Section 3, Douliou, Yunlin 64002, Taiwan
{ g9510801; sujp }@yuntech.edu.tw
2Department of Management Information System
Far East University
No. 49, Chung-Hwa Road, Hsin-Shih, Tainan 744, Taiwan
khhsia@cc.feu.edu.tw
Received April 20xx; accepted June 20xx
ABSTRACT. Auto landing control is a very important control mode of unmanned helicopter.
Global positioning system (GPS) is the typical position sensor for landing control.
However, the height measurement error of GPS is so large that it is not accurate enough
for helicopter landing. In this paper, we propose a stereo vision system composed with
two webcams to provide more accurate height information in certain range of height. The
block matching algorithm (BMA) is proposed for searching corresponding points. From
the experimental results, BMA can authentically search the corresponding point in low
resolution images and can provide an acceptable height estimation result in a reasonable
range.
Keywords: Stereo vision, Block matching algorithm, Height estimation
1. Introduction. During the past few years, the development of the unmanned helicopter
has been an important subject of research. The unmanned helicopter is a highly nonlinear
system. Therefore many researchers focus on the dynamic control problems (e.g. [1-3]).
The most important flight mode of autonomous unmanned helicopter is the landing mode.
In consideration of the unmanned helicopter landing problem, the height position
information is usually provided by global positioning system (GPS). However, the error of
GPS height position is usually in the range of Â±10 meters, and it is too large for helicopter
landing. For example, the accuracy of Garmin GPS 18-5Hz is less than 15 meters [4]. After
many times of measurement, the average error of this GPS was obtained to be around 10
meters. Several landing guided methods were brought up for correcting the error range. In
[5], tether guided method for autonomous helicopter landing was considered. In [6-7],
camera-image based relative pose and motion estimation for unmanned helicopter were
discussed.
In this paper, we focus on the problem of estimating the height of the helicopter for the
landing problem via a simple stereo vision system. The key problem of stereo vision system
is to find the corresponding points in the left image and the right image. The block
we will apply the block matching algorithm (BMA) for searching the corresponding points.
BMA is a standard technique for encoding motion in video sequences [8]. It aims at
detecting the similar block between two images. The matching efficiency depends on the
choosing of block size and search region. Usually, bigger blocks are less sensitive to the
noise but will spent more computation time. The sum of absolute difference (SAD) is used
for the block similarity measuring:
SAD( x , y, r , s )= ï€¨ ï€© ï€¨ ï€©ï€¨ ï€©ïƒ¥ ïƒ¥
ï€½
ï€½
ï€½
ï€½ ï€«ï€«ï€«ï€«ï€«ï€«
ï€­
mi
i
nj
j jsyirxjyix0 0 ,),(
L_ImageR_Image , (3)
where m and n are the length and width of the block, (x, y) is the poison of the block on
right image and (r, s) denotes the motion vector. In Fig. 2, the first pixel of the chosen
block on left image is (x, y) and the block size is NÃ—N. The search region on the right image
is defined to be a rectangular with the image width as the width and height of 2k. Since the
left and right cameras of the stereo vision system are placed on the same line, the k could be
small in order to reduce the computation time.
FIGURE 2. Block template and the search region.
4. Stereo Vision System with Webcams. A stereo vision system with two images is
illustrated in Fig. 3. The geometry of stereo vision system is very useful for measuring the
depth. Consider that a point P=[X, Y, Z] in the world space is projected on both left and
right images on stereo vision system. In Fig. 3, the projected coordinates of point P on the
left and the right images are (xl , yl) and (xr , yr), respectively.
FIGURE 3. Stereo vision geometry.
The x-coordinates of P on left image and right image are:
f
x
Z
X l
ï€½ ,
f
x
Z
bX r
ï€½
ï€­ , (4)
where f is the focal length and b is the length of baseline. Rewrite (4),
right image. The distance of target is 350cm.
The estimated results of distance by BMA and epipolar geometry constraint method are
shown in Fig. 6 and Fig. 7, respectively. From these two figures, we can conclude that the
measurement distance increase with baseline increasing. We can also conclude that the
range of measurement distance of BMA is more than that of epipolar geometry constraint
method, in the sense of the same error tolerance. However, as the measuring range
increasing, BMA searching results almost in the same range because of the low resolution
of the image, and this causes that the measurement error increases quickly.
FIGURE 6. Estimation errors with BMA.
FIGURE 7. Estimation errors with epipolar geometry constraint method.
6. Conclusion and Discussion. On the helicopter autonomous landing problem, stereo
vision system could estimate the height of the helicopter. In this paper, we propose a low
cost stereo vision system which is much cheaper than a GPS. The proposed system can
provide acceptably accurate height information for the unmanned helicopter landing control
system in certain range. To increase the measurement range, one should use cameras of
higher resolution and/or increase the baseline.
åœ‹ç§‘æœƒè£œåŠ©å°ˆé¡Œç ”ç©¶è¨ˆç•«é …ä¸‹å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
                                    æ—¥æœŸï¼š 100 å¹´ 2 æœˆ 13 
æ—¥ 
                                 
ä¸€ã€åƒåŠ æœƒè­°ç¶“é 
æœ¬å±†çš„åœ‹éš›æ™ºæ…§ç”Ÿæ´»èˆ‡æ©Ÿå™¨äººç ”è¨æœƒ  (International Symposium on 
Artificial Life and Robotics)å·²ç¶“æ˜¯ç¬¬åå…­å±†äº†ï¼Œèˆ‰è¾¦åœ°é»åœ¨æ—¥æœ¬ä¹å·åˆ¥åºœå¸‚çš„
B-Con Plazaã€‚é€™å€‹ç ”è¨æœƒçš„è¦æ¨¡ç®—æ˜¯ä¸­å‹ï¼Œæ¯å¹´éƒ½æœ‰ä¸å°‘çš„æ¥­ç•Œå…±åŒæ”¯æŒé€™å€‹
æœƒè­°ï¼Œç ”è¨æœƒèˆ‰è¾¦çš„ä¸»è¦ç›®çš„å°±æ˜¯äº¤æµæ—¥æœ¬èˆ‡ä¸–ç•Œå„åœ‹åœ¨æ™ºæ…§ç”Ÿæ´»æ–¹é¢ã€æ©Ÿå™¨
äººå­¸æ–¹é¢ã€åŠç›¸é—œé ˜åŸŸçš„ç ”ç©¶æˆæœã€‚ 
ä»Šå¹´çš„ç ”è¨æœƒå…±è¾¦ç†ä¸‰å¤©ï¼ŒåŒæ™‚éƒ½æœ‰ä¸‰å€‹å ´åœ°åœ¨é€²è¡Œè«–æ–‡å ±å‘Šæˆ–äº¤æµï¼Œå…±
æœ‰ 250 ç¯‡è«–æ–‡åœ¨ 46 å€‹å ´æ¬¡ (å…¶ä¸­åŒ…æ‹¬ 1 å€‹å£å ±å ´æ¬¡) ä¸­ç™¼è¡¨ï¼Œä¸»é¡Œæ¶µè“‹æ©Ÿå™¨äºº
ç›¸é—œçš„å„é …ä¸»é¡Œï¼ŒåŒ…æ‹¬ï¼šå½±åƒè™•ç†ã€é›»è…¦è¦–è¦ºã€æ„Ÿæ¸¬æŠ€è¡“ã€æ§åˆ¶æŠ€è¡“ã€äººæ©Ÿä»‹
é¢ã€æ··æ²Œç†è«–ã€é¡ç¥ç¶“ç¶²è·¯ã€æ¨¡ç³Šç†è«–ã€æ©Ÿæ§‹è¨­è¨ˆâ€¦ç­‰ï¼Œä¸¦ä¸”å®‰æ’æœ‰ 3 å ´çš„ä¸»
é¡Œæ¼”è¬›åŠ 2 å ´çš„å°ˆé¡Œè«–å£‡ã€‚ 
ä¸»é¡Œæ¼”è¬›çš„è¬›è€…åŒ…æ‹¬ï¼šéŸ“åœ‹ç§‘å­¸æŠ€è¡“å­¸é™¢ (KAIST) æ©Ÿæ¢°å·¥ç¨‹ç³»æ•™æˆå…¼äºº
å‹æ©Ÿå™¨äººç ”ç©¶ä¸­å¿ƒä¸»ä»» Jun Ho Oh æ•™æˆã€è¬›é¡Œæ˜¯ï¼šäººå‹æ©Ÿå™¨äºº HUBO II çš„ç™¼å±•
æ¦‚æ³ã€‘ã€ç¾åœ‹è¯ç››é “å¤§å­¸æ•™æˆæš¨åŒ—äº¬æ¸…è¯å¤§å­¸é‡å­è³‡è¨Šç§‘æŠ€ä¸­å¿ƒä¸»ä»»è«‡å¿—ä¸­æ•™
æˆã€è¬›é¡Œæ˜¯ï¼šæ©Ÿå™¨äºº--å¾è£½é€ åˆ°å…·æ™ºæ…§çš„æ©Ÿå™¨ã€‘ã€æ—¥æœ¬äº¬éƒ½å¤§å­¸çš„æ¾é‡æ–‡ä¿Šæ•™
æˆã€è¬›é¡Œæ˜¯ï¼šæ•‘é›£æ©Ÿå™¨äººç³»çµ±--å¾è›‡å½¢æ©Ÿå™¨äººåˆ°äººå‹çš„ä»‹é¢ã€‘ã€‚HUBO II æ©Ÿå™¨
äººæ˜¯éŸ“åœ‹äººå‹æ©Ÿå™¨äººç ”ç©¶ä¸­å¿ƒ Oh æ•™æˆçš„åœ˜éšŠæ‰€ç ”ç™¼å‡ºä¾†çš„äººå‹æ©Ÿå™¨äººï¼Œèº«é«˜
å¤§ç´„ä¸€ç±³äº”å·¦å³ï¼Œèƒ½å¤ ç”¨é›™è…³è¡Œé€²ï¼Œä¹Ÿèƒ½å¤ å–®è…³å‘ˆç¾é‡‘é›ç¨ç«‹ä¼¼çš„å¹³è¡¡å‹•ä½œã€‚
Oh æ•™æˆåœ¨çŸ­çŸ­çš„ä¸‰å¹´å…§å°±ç ”ç™¼å‡º HUBO çš„ç¬¬ä¸€ä»£ï¼Œä½¿éŸ“åœ‹çš„æ©Ÿå™¨äººæŠ€è¡“èº‹èº«
åˆ°ä¸–ç•Œç¬¬å…­çš„æ°´æº–ã€‚åœ¨è«‡æ•™æˆçš„æ¼”è¬›ä¸­ï¼Œä»–ç°¡å–®åœ°ä»‹ç´¹æ©Ÿå™¨äººçš„æ¼”åŒ–æ­·ç¨‹ï¼Œå¾
æœ€æ—©çš„é›»æ§è£½é€ ç”¨æ©Ÿå™¨äººï¼Œæ¼”åŒ–åˆ°èƒ½å¤ èˆ‡äººé¡äº’å‹•çš„æ©Ÿå™¨äººï¼Œé€²è€Œåˆ°ç¾åœ¨æ‰€ç™¼
å±•å…·æ™ºæ…§çš„æ©Ÿå™¨äººã€‚åœ¨æ¾é‡æ•™æˆçš„æ¼”è¬›ä¸­ï¼Œå‰‡æ˜¯ä»‹ç´¹äº†å¾ 1995 å¹´é˜ªç¥å¤§åœ°éœ‡
å¾Œï¼Œåœ¨æ—¥æœ¬ç©æ¥µç™¼å±•ä¹‹çµåˆè³‡é€šç§‘æŠ€èˆ‡æ©Ÿå™¨äººç§‘æŠ€çš„æ™ºæ…§å‹æ•‘é›£ç³»çµ±ã€‚é€™æ¨£çš„
è¨ˆç•«ç·¨è™Ÿ NSC 99ï¼2221ï¼Eï¼269ï¼020ï¼ 
è¨ˆç•«åç¨± ä»¥å½±åƒç‰¹å¾µæ“·å–ç‚ºåŸºç¤çš„ç›¸æ©Ÿä½ç½®ä¼°æ¸¬æ–¹æ³•ä¹‹è¨­è¨ˆèˆ‡å¯¦ç¾ 
å‡ºåœ‹äººå“¡
å§“å 
å¤éƒ­è³¢ 
æœå‹™æ©Ÿæ§‹
åŠè·ç¨± 
é æ±ç§‘æŠ€å¤§å­¸è³‡è¨Šç®¡ç†ç³»å‰¯æ•™æˆ 
æœƒè­°æ™‚é–“ 
100å¹´ 1æœˆ 27æ—¥è‡³ 
100å¹´ 1æœˆ 29æ—¥ 
æœƒè­°åœ°é» æ—¥æœ¬å¤§åˆ†ç¸£åˆ¥åºœå¸‚ 
æœƒè­°åç¨± 
(ä¸­æ–‡) ç¬¬åå…­å±†æ™ºæ…§ç”Ÿæ´»èˆ‡æ©Ÿå™¨äººåœ‹éš›ç ”è¨æœƒ 
(è‹±æ–‡) The Sixteenth International Symposium on Artificial Life and 
Robotics (AROB 16th â€™11) 
ç™¼è¡¨è«–æ–‡
é¡Œç›® 
(ä¸­æ–‡) é™è½æ¨™è¨˜å½±åƒä¸å®Œæ•´ä¸‹ä¹‹ç‰¹å¾µæŠ½å–èˆ‡ç›¸æ©Ÿä½ç½®ä¼°æ¸¬ 
(è‹±æ–‡) Camera Position Estimation and Feature Extraction from 
Incomplete Image of Landmark 
ä¸‰ã€è€ƒå¯Ÿåƒè§€æ´»å‹•(ç„¡æ˜¯é …æ´»å‹•è€…ç•¥) 
æœ¬æ¬¡åƒåŠ ç ”è¨æœƒï¼Œæœªå®‰æ’ä»»ä½•è€ƒå¯Ÿåƒè§€æ´»å‹•ã€‚ 
å››ã€å»ºè­° 
ç„¡ç‰¹åˆ¥å»ºè­°äº‹é … 
äº”ã€æ”œå›è³‡æ–™åç¨±åŠå…§å®¹ 
ç ”è¨æœƒè«–æ–‡é›†å…‰ç¢Ÿã€ç ”è¨æœƒæœƒè­°æ‰‹å†Š(å«è«–æ–‡æ‘˜è¦) 
å…­ã€å…¶ä»– 
ç„¡ 
 
åƒèˆ‡ç ”è¨æœƒä¹‹ç…§ç‰‡é›†éŒ¦ 
 
 
 





	
 

 
 
	

	 
 
 !"#$%&'$


(& )*+(,)
(,- .
/"
0	1	

00	.	 
 
 
$*012*20)3 ä›†
+,
& &
&ä›‡ 
+*45")'6
+7'+8+/&+
 9):+:.:, ;+<
5+*#5="-'6
+%&8+/&+
 ;+<
 
OS10-1 The study of path error for an Omnidirectional Home Care Mobile Robot 
Jie-Tong Zou (National Formosa University, Taiwan) 
Feng-Chun Chiang (WuFeng Institute of Technology, Taiwan) 
 
OS10-2 A* searching algorithm applying in Chinese chess game 
Cheng-Yun Chung (National Yunlin University of Science and Technology, Taiwan) 
Te-Yi Hsu (Industrial Technology Research Institute, Taiwan) 
Jyh-Hwa Tzou (National Formosa University, Taiwan) 
Kuo-Lan Su (National Yunlin University of Science and Technology, Taiwan)  
 
OS10-3 Multi-robot based intelligent security system 
Yi-Lin Liao, Kuo-Lan Su (National Yunlin University of Science and Technology, Taiwan) 
 
OS10-4 Implementation of an auction algorithm based multiple tasks allocation using mobile robots 
Kuo-Lan Su, Jr-Hung Guo (National Yunlin University of Science and Technology, Taiwan) 
Chun-Chieh Wang (Chienkuo Technology University, Taiwan) 
Cheng-Yun Chung (National Yunlin University of Science and Technology, Taiwan) 
 
OS10-5 Fuzzy programming for mixed-integer optimization problems 
Yung-Chin Lin (National Yunlin University of Science and Technology, Taiwan) 
Yung-Chien Lin (WuFeng University, Taiwan) 
Kuo-Lan Su (National Yunlin University of Science and Technology, Taiwan) 
Wei-Cheng Lin (I-Shou University, Taiwan) 
Tsing-Hua Chen (WuFeng University, Taiwan) 
 
OS10-6 Develop a vision based auto-recharging system for mobile robots 
Ting-Li Chien (Wu-Feng University, Taiwan) 
 
	*331>*$3)  ä›†
.'

ä›‡ 
+*45="=&+%&
8+/&+
 ;+< 
5+*45")'6
+7'+8+/&+
 9):+:.:, ;+<
 
OS11-1 Super-twisting second order sliding mode control for a synchronous reluctance motor 
Huann-Keng Chiang, Wen-Bin Lin, Chang-Yi Chang (National Yunlin University of Science and 
Technology, Taiwan) 
Chien-An Chen (Automotive Research and Testing Center, Taiwan) 
 
OS11-2 Shape recognition applied in a semi-autonomous weapon robot 
Chun-Chieh Wang, Chyun-Luen Lin (Chienkuo Technology University, Taiwan) 
Kuo-Lan Su (National Yunlin University of Science and Technology, Taiwan) 
 
OS11-3 Camera position estimation and feature extraction from incomplete image of landmark  
Kuo-Hsien Hsia (Far East University, Taiwan)  
Shao-Fan Lien (National Yunlin University of Science and Technology, Taiwan) 
Juhng-Perng Su (National Yunlin University of Science and Technology and Overseas Chinese 
University, Taiwan) 
 
 
 
The Sixteenth International Symposium on Artificial Life and Robotics 2011(AROB 16th â€™11), 
B-Con Plaza, Beppu, Oita, Japan, January 27-29, 2011
Â©ISAROB 2011 P - 32
camera this restriction has to be overcome. The case
in which more than half the landmark image is available,
called â€œCase 1â€, has been discussed in [6].
In this paper, we will focus on the case in which more
than half of the landmark image is defective, called â€œCase
2â€. This scenario presents serious problems for extracting
useful features. A feature extraction method is developed
for extracting the features from the incomplete image of
landmark in which more than half of the landmark image
is defective. We use ANFIS to capture and estimate the
useful features from an incomplete landmark image.
Simulation results verify that under some conditions
it is possible to estimate the camera position from a
landmark image in which less the than half the total
image is available.
II. MAPPINGRELATIONSAND CAMERA
POSITION ESTIMATINGMETHOD
1. Projective geometry of landmark image
The five quantities, as defined in (1-5), obtained via
geometric projection [7] are the projection of the
features of the landmark image. We defined (6) for
estimating the camera position [5].
  Rk 	
  (1)
  Rh 	
	
  (2)
RQ 	
  (3)
   22  RRd  (4)
   
   22
22
 RTR
RSr
r


 (5)


	





QkRatio
rhRatio
drRatio
khRatio
position
angle
height
project
=
=
=
=
(6)
We further consider the cases of incomplete
landmark images, specifically, the case of the landmark
image in which more than half of the image is available
[6]. The four quantities: k, r , x, and y, related to the
completed landmark image, are selected for estimating
the approximate h. The estimated h is called h] . Here k
and r have been derived from (1) and (2), respectively.
Consider the case in which more than half the
landmark image is defective, as shown in Fig.2. The
five quantities: I1, I2, I3, I4 and I5 which are related to the
complete landmark image are selected for estimating the
approximate h and k, called h] and k] , respectively.
h]
k]
Fig.2. Incomplete landmark image and features
The projective relation between the landmark and
camera position is shown in Fig. 3. Definitions of  Ê¿
 and e are provided as



	











 




 





 




 





 




 



h
bf
h
ebf
h
rebf
h
ebf
h
ebf
h
rebf
e
11
11
11
tantan
tantan
tantan





(7)



e
limCut
k]h]
Fig.3. Projective geometry of landmark features
Fig. 4 illustrates the projective relations of features
I2, I3 and I4. From Fig.3 and Fig.4, the derivations of
h] , k] , I1, I2, I3, I4 and I5 can be obtained as shown in (8-
13) and the estimated h] and k] in (14).
  RI e   tan5 (8)
  51 tan IRI   (9)
   
 22
22
51
22
2
fbH
RIIer
I


 (10)
k
g
(a) (b)
Fig.4. (a) Top view of the landmark features
(b) Projections of the landmark features
The Sixteenth International Symposium on Artificial Life and Robotics 2011 (AROB 16th â€™11), 
B-Con Plaza, Beppu,Oita, Japan, January 27-29, 2011
Â©ISAROB 2011 278
RMSE of h] and k] are 0.000615 and 0.000471,
respectively. The h] and k] are utilized to approximate
the complete landmark image features. Table 2
illustrates the estimation results of camera position with
the features h] and k] . The RMSE of the , R and Q
are 0.45868, 0.506617 and 0.240798, respectively.
IV. CONCLUSION
In this paper, a scenario in which more that half of a
landmark image is defective has been analyzed. The
method for estimating the approximate features of
incomplete landmark images is provided. From
simulation results, it is clear that the approximate
features can be very closely estimated via the known
quantities. Moreover, we have shown that the
approximate features can be used for camera position
estimation. The inherent camera position estimation
error range is acceptable for practical applications. The
method proposed in this paper is a functional tool for
camera position estimation.
REFERENCES
[1] GPS 18 Technical Specifications (2005), Revision D,
Garmin International, USA.
[2] Mori R, Hirata K and Kinoshita T (2007), Vision-
Based Guidance Control of a Small-Scale Unmanned
Helicopter, International Conference on Intelligent
Robots and Systems, 2648-2653.
[3] Mori R, Kubo T and Kinoshita T (2006), Vision-
Based Hovering Control of a Small-Scale Unmanned
Helicopter, SICE-ICASE International Joint Conference,
1274-1278.
[4] Wang CC, Lien SF and Hsia KH et al (2009),
Image-Guided Searching for Landmark, Artificial Life
and Robotics, 14(1):95-100.
[5] Lien SF, Hsia KH and Su JP (2010). Image-Guided
Height Estimation for Unmanned Helicopter Landing,
ICIC Express Letters, 4(6B:2299-2304.
[6] Hsia KH, Lien SF and Su JP (2010), Camera
Position Estimation by ANFIS from Incomplete
Landmark Image, Conference on Fuzzy Theory and Its
Applications, Accepted.
[7] Hartley R and Zisserman A (2003), Multiple View
Geometry in Computer Vision, Cambridge University
Press.
Acknowledgement
This research was supported by the National Science
Council, Taiwan, R.O.C., under Grant No. NSC99-
2221-E-269-020.
Table 1. The estimation results of features h] and k]
No.   (cm) k k] Error of feature k h h] Error of feature h
1 32à´¦ 31 1.58891 1.58823 0.00068 1.70474 1.70463 0.00011
2 21à´¦ 44 2.87088 2.87023 0.00065 2.73393 2.73451 -0.00058
3 42à´¦ 78 2.06710 2.06790 -0.00080 1.96832 1.96786 0.00046
4 53à´¦ 113 0.16088 0.16187 -0.00099 0.15886 0.15934 -0.00048
5 19à´¦ 157 1.78014 1.78001 0.00013 1.76272 1.76347 -0.00075
6 60à´¦ 163 0.77189 0.77171 0.00018 0.74055 0.73976 0.00079
7 10à´¦ 173 2.78694 2.78656 0.00038 2.77071 2.77091 -0.00020
8 35à´¦ 201 0.92258 0.92308 -0.00050 0.91298 0.91274 0.00024
9 13à´¦ 240 3.22209 3.22278 -0.00069 3.22209 3.22176 0.00033
10 29à´¦ 289 2.92995 2.92951 0.00044 2.91630 2.91611 0.00019
Table 2. The estimation results of camera position with the features h] and k]
No.  R (cm) Q (cm) Error of estimated  Error of estimated R Error of estimated Q
1 32à´¦ 31 10 -0.619 0.421 -0.174
2 21à´¦ 44 23 -0.354 -0.572 0.243
3 42à´¦ 78 189 0.475 -0.435 0.314
4 53à´¦ 113 78 -0.487 -0.376 -0.197
5 19à´¦ 157 63 0.221 0.636 0.231
6 60à´¦ 163 284 -0.334 0.551 0.299
7 10à´¦ 173 174 -0.531 -0.417 -0.143
8 35à´¦ 201 124 0.437 0.623 0.354
9 13à´¦ 240 254 -0.634 -0.581 -0.208
10 29à´¦ 289 365 -0.312 0.352 0.146
The Sixteenth International Symposium on Artificial Life and Robotics 2011 (AROB 16th â€™11), 
B-Con Plaza, Beppu,Oita, Japan, January 27-29, 2011
Â©ISAROB 2011 280
99 å¹´åº¦å°ˆé¡Œç ”ç©¶è¨ˆç•«ç ”ç©¶æˆæœå½™æ•´è¡¨ 
è¨ˆç•«ä¸»æŒäººï¼šå¤éƒ­è³¢ è¨ˆç•«ç·¨è™Ÿï¼š99-2221-E-269-020- 
è¨ˆç•«åç¨±ï¼šä»¥å½±åƒç‰¹å¾µæ“·å–ç‚ºåŸºç¤çš„ç›¸æ©Ÿä½ç½®ä¼°æ¸¬æ–¹æ³•ä¹‹è¨­è¨ˆèˆ‡å¯¦ç¾ 
é‡åŒ– 
æˆæœé …ç›® å¯¦éš›å·²é”æˆ
æ•¸ï¼ˆè¢«æ¥å—
æˆ–å·²ç™¼è¡¨ï¼‰
é æœŸç¸½é”æˆ
æ•¸(å«å¯¦éš›å·²
é”æˆæ•¸) 
æœ¬è¨ˆç•«å¯¦
éš›è²¢ç»ç™¾
åˆ†æ¯” 
å–®ä½ 
å‚™ è¨» ï¼ˆ è³ª åŒ– èªª
æ˜ï¼šå¦‚æ•¸å€‹è¨ˆç•«
å…±åŒæˆæœã€æˆæœ
åˆ— ç‚º è©² æœŸ åˆŠ ä¹‹
å° é¢ æ•… äº‹ ...
ç­‰ï¼‰ 
æœŸåˆŠè«–æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒè«–æ–‡ 1 1 100% 
ç¯‡ 
 
è«–æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100%   
ç”³è«‹ä¸­ä»¶æ•¸ 0 0 100%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 100% ä»¶  
ä»¶æ•¸ 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 1 1 100%  
åšå£«ç”Ÿ 1 1 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å…§ 
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆæœ¬åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 0 0 100% 
äººæ¬¡ 
 
æœŸåˆŠè«–æ–‡ 1 1 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒè«–æ–‡ 2 2 100% 
ç¯‡ 
 
è«–æ–‡è‘—ä½œ 
å°ˆæ›¸ 1 1 100% ç« /æœ¬  
ç”³è«‹ä¸­ä»¶æ•¸ 0 0 100%  å°ˆåˆ© å·²ç²å¾—ä»¶æ•¸ 0 0 100% ä»¶  
ä»¶æ•¸ 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šåˆ©é‡‘ 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å¤– 
åƒèˆ‡è¨ˆç•«äººåŠ› 
ï¼ˆå¤–åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ç† 0 0 100% 
äººæ¬¡ 
 
 
