 2
  
ä¸­æ–‡æ‘˜è¦ 
 
 
éš¨è‘—æ•¸ä½ç…§ç›¸æ©Ÿèˆ‡æ•¸ä½æ”å½±æ©Ÿçš„æ™®åŠï¼Œæœ‰æ„ˆä¾†æ„ˆå¤šäººç”¨é€™äº›æ•¸ä½ç”¢å“ä¾†è¨˜éŒ„ç”Ÿ
æ´»ä¸Šæ‰€ç™¼ç”Ÿçš„é»é»æ»´æ»´ã€‚ç‚ºäº†è¦æ‰¾å‡ºæ‹æ”å½±ç‰‡æ™‚å ´æ™¯çš„ä½ç½®æ‰€åœ¨ï¼Œæœ¬è¨ˆç•«ä»¥å½±åƒè¦–
è¨Šå…§å®¹åˆ†æèˆ‡è¾¨è­˜çš„æŠ€è¡“ï¼Œé–‹ç™¼ã€Œä»¥è¦–è¨Šå…§å®¹ç‚ºåŸºç¤ä¹‹è¦–è¨Šå ´æ™¯å®šä½ç³»çµ±ã€ã€‚åœ¨å®Œ
æˆæœ¬ç³»çµ±çš„éç¨‹ä¸­ï¼Œæˆ‘å€‘é–‹ç™¼äº†å»ºç¯‰ç‰©åµæ¸¬ã€å»ºç¯‰ç‰©è¾¨è­˜åŠå ´æ™¯å®šä½èˆ‡è’é›†ç›¸é—œè³‡
è¨Šç­‰æŠ€è¡“ï¼Œä¸¦å»ºæ§‹å®Œæˆäº†ä¸‰å€‹å­ç³»çµ±ï¼šå»ºç¯‰ç‰©åµæ¸¬ç³»çµ±ï¼Œå»ºç¯‰ç‰©è¾¨è­˜ç³»çµ±ï¼Œå ´æ™¯å®š
ä½ç³»çµ±ã€‚æ‰€é–‹ç™¼ä¹‹ã€Œä»¥è¦–è¨Šå…§å®¹ç‚ºåŸºç¤ä¹‹è¦–è¨Šå ´æ™¯å®šä½ç³»çµ±ã€é››å½¢èƒ½åˆ†æå½±åƒèˆ‡è¦–
è¨Šå…§å®¹ï¼Œä¸¦ä¸”èˆ‡å·²çŸ¥æ‹æ”åœ°é»çš„ç…§ç‰‡ä¾†æ¯”å°ï¼Œé€²è€Œé”åˆ°å½±åƒæ‹æ”åœ°é»çš„å®šä½ã€‚é€é
æœ¬ç³»çµ±ï¼Œå½±åƒèˆ‡è¦–è¨Šçš„æä¾›è€…å°‡å¯å®¹æ˜“çš„å¾—åˆ°èˆ‡è¦–è¨Šç•«é¢ç›¸é—œçš„ç©ºé–“è³‡è¨Šèˆ‡æ™‚é–“è³‡
è¨Šã€‚ 
 
é—œéµè©ï¼šé¡ç¥ç¶“ç¶²è·¯ï¼Œå½±åƒåˆ†æï¼Œå ´æ™¯å®šä½ 
 
  
 4
ä¸€ã€å‰è¨€ 
 
éš¨è‘—æ•¸ä½ç…§ç›¸æ©Ÿèˆ‡æ•¸ä½æ”å½±æ©Ÿçš„æ™®
åŠï¼Œæœ‰æ„ˆä¾†æ„ˆå¤šäººå°‡å…¶æ‹¿ä¾†è¨˜éŒ„ç”Ÿæ´»ä¸Šèˆ‡
é€±é­ä¸Šæ‰€ç™¼ç”Ÿçš„é»é»æ»´æ»´ï¼šå¦‚ç´€éŒ„æ—…éŠè¡Œ
ç¨‹ï¼Œç´€éŒ„ç‰¹æ®Šäº‹ä»¶ç­‰ã€‚ä¸€èˆ¬é€™äº›æ‰€æ‹æ”çš„
å½±åƒæˆ–ç…§ç‰‡ä¸å¤–ä¹åŒ…æ‹¬äººã€äº‹ã€æ™‚ã€åœ°ã€
ç‰©ç­‰äº”ç¨®æ§‹æˆè¦ç´ ã€‚ç„¶è€Œåœ¨è§€çœ‹æ‹æ”çš„ä½œ
å“æ™‚å¾€å¾€ç™¼ç¾ï¼Œå°æ–¼å¤§éƒ¨åˆ†çš„ä½œå“æˆ‘å€‘åª
èƒ½å°±æ‹æ”æ™‚çš„äººã€äº‹ã€æ™‚ã€ç‰©å¯ä»¥å¾ˆæ¸…æ¥š
çš„æè¿°ï¼›å°æ–¼æ‹æ”çš„åœ°é»å»ç„¡æ³•å¾ˆæ­£ç¢ºçš„
æè¿°ã€‚ 
ç‚ºäº†è§£æ±ºé€™å€‹å•é¡Œï¼Œç›®å‰å¸‚é¢ä¸Šå·²
æœ‰ GPS ç´€éŒ„å™¨ï¼ˆGPS Data Loggerï¼‰ä¾†è¼”
åŠ©ä½¿ç”¨è€…ç´€éŒ„æ‹æ”æ™‚çš„åœ°é»ã€‚ç„¶è€Œæ‹æ”è€…
ä»éœ€é€éå¾Œè£½ä½œï¼Œå°‡æ‹æ”çš„æ™‚é–“èˆ‡ GPS
ç´€éŒ„å™¨æ‰€ç´€éŒ„çš„æ™‚é–“ç›¸å°æ‡‰ã€‚å¦‚æ­¤æ‰èƒ½å¾—
åˆ°æ‹æ”ä½ç½®çš„æ­£ç¢ºç¶“ç·¯åº¦ã€‚å› æ­¤ï¼Œå¸‚é¢ä¸Š
ä¹Ÿå‡ºç¾äº†é«˜éšçš„æ•¸ä½ç…§ç›¸æ©Ÿå°‡ GPS ç´€éŒ„
å™¨åŒ…å«æ–¼ç…§ç›¸æ©Ÿä¸­ã€‚ç•¶æ‹æ”è€…æŒ‰ä¸‹å¿«é–€çš„
åŒæ™‚ï¼Œç…§ç›¸æ©Ÿç«‹å³å°‡å…¶æ‹æ”åœ°é»çš„ç¶“ç·¯åº¦
ç´€éŒ„åœ¨æ­¤ç…§ç‰‡çš„æª”é ­ä¸­ã€‚åœ¨ç€è¦½ç…§ç‰‡çš„æ™‚
å€™ï¼Œè§€è³è€…åªéœ€é€éè»Ÿé«”ä¾¿å¯ä»¥å¾ˆæ–¹ä¾¿ä¸”
éå¸¸æ¸…æ¥šåœ°çŸ¥é“æ‹æ”æ™‚æ‰€è™•çš„ä½ç½®ã€‚è‡³æ–¼
æ•¸ä½æ”å½±æ©Ÿçµåˆ GPS ç´€éŒ„å™¨ï¼Œç›®å‰å‰‡ä»
æœªçœ‹åˆ°ç›¸é—œçš„ç”¢å“å‡ºç¾ã€‚ 
é›–ç„¶ç›®å‰å·²ç¶“æœ‰ GPS ç´€éŒ„å™¨å¯ä»¥ä¾†
è¼”åŠ©æ‹æ”è€…ç´€éŒ„æ‹æ”ç•¶æ™‚çš„ä½ç½®ï¼Œä½†å°æ–¼
æ²’æœ‰ GPS ç´€éŒ„è³‡æ–™çš„å¤§é‡ç…§ç‰‡æˆ–å½±åƒï¼Œ
å¦‚ä½•æ‰¾å‡ºæ‹æ”æ™‚çš„ç¢ºåˆ‡ä½ç½®ä»æ˜¯ä¸€å€‹æœ‰å¾…
ç ”ç©¶çš„èª²é¡Œã€‚ 
æœ¬è¨ˆç•«ä»¥å½±åƒåˆ†æèˆ‡è¾¨è­˜çš„æŠ€è¡“ï¼Œ
é–‹ç™¼ã€Œä»¥è¦–è¨Šå…§å®¹ç‚ºåŸºç¤ä¹‹è¦–è¨Šå ´æ™¯å®šä½
ç³»çµ±ã€ã€‚è§€å¯Ÿç›®å‰ä¸€èˆ¬æ‰€æ‹æ”çš„å½±ç‰‡å¯ä»¥
ç™¼ç¾ï¼Œå¤§éƒ¨åˆ†çš„æ”å½±è€…å¾€å¾€æœƒå°‡æ‹æ”é»çš„
åœ°æ¨™æˆ–è€…æœ‰ä»£è¡¨æ€§çš„å»ºç¯‰ç‰©æ‹æ”åˆ°å½±ç‰‡ä¸­ã€‚
å› æ­¤ï¼Œè‹¥èƒ½æ‰¾å‡ºå ´æ™¯ä¸­åœ°æ¨™æˆ–ä»£è¡¨æ€§å»ºç‰©
æ‰€åœ¨ä½ç½®ï¼Œå°±èƒ½çŸ¥é“æ‹æ”æ™‚å ´æ™¯çš„ä½ç½®é€²
è€Œå¾—åˆ°èˆ‡æ­¤å ´æ™¯ç›¸é—œçš„è³‡è¨Šã€‚ 
æœ¬ç³»çµ±ä¸»è¦æ˜¯ç”±ä¸‰å€‹å­ç³»çµ±æ‰€çµ„æˆï¼š
å»ºç¯‰ç‰©åµæ¸¬ç³»çµ±ï¼Œå»ºç¯‰ç‰©è¾¨è­˜ç³»çµ±ï¼Œå ´æ™¯
å®šä½ç³»çµ±ã€‚ç•¶ä½¿ç”¨è€…è¼¸å…¥ä¸€å€‹å°šæœªå®šä½çš„
è¦–è¨Šå¾Œï¼›é¦–å…ˆï¼Œå»ºç¯‰ç‰©åµæ¸¬ç³»çµ±æœƒåˆ†æè¦–
è¨Šä¸­çš„ç•«é¢ä¸¦åµæ¸¬å‡ºå«æœ‰å»ºç¯‰ç‰©çš„ç•«é¢ã€‚
æ¥è‘—ï¼Œå»ºç¯‰ç‰©è¾¨è­˜ç³»çµ±æœƒé‡å°é€™äº›å«æœ‰å»º
ç¯‰ç‰©çš„ç•«é¢ï¼Œåˆ¤æ–·æ­¤ç•«é¢çš„å»ºç¯‰ç‰©æ˜¯ä»€éº¼ï¼Œ
é€²è€Œé€éè³‡æ–™åº«çš„æŸ¥è©¢å¾—åˆ°æ­¤å»ºç¯‰ç‰©çš„è³‡
è¨Šï¼šå¦‚åç¨±ï¼Œç¶“ç·¯åº¦ç­‰ã€‚æœ‰äº†é€™äº›è³‡è¨Šï¼Œ
å ´æ™¯å®šä½ç³»çµ±å³å¯æ ¹æ“šç¶“ç·¯åº¦å°‡å…¶å®šä½æ–¼
åœ°åœ–ä¸Šï¼›åŒæ™‚ä¹Ÿå¯ä»¥å°‡å…¶ç›¸é—œè³‡è¨Šé¡¯ç¤ºæ–¼
åœ°åœ–ä¸Šä¾›ä½¿ç”¨è€…æŸ¥è©¢è§€çœ‹ã€‚ 
ç›®å‰åœ¨å»ºç¯‰ç‰©åµæ¸¬çš„ç ”ç©¶ä¸Šï¼Œæœ‰è‘—
é‡æ–¼å¦‚ä½•åœ¨è¡›æ˜Ÿåœ–æˆ–æ˜¯ç©ºç…§åœ–ä¸­åµæ¸¬æ˜¯
å¦æœ‰å»ºç¯‰ç‰©[1-8]ã€‚ç”±æ–¼è¡›æ˜Ÿåœ–æˆ–æ˜¯ç©ºç…§
åœ–éƒ½æ˜¯ç”±ç©ºä¸­å¾€åœ°é¢æ‹æ”çš„ï¼Œæ‰€ä»¥å»ºç¯‰
ç‰©å‘ˆç¾åœ¨é€™é¡å½±åƒä¸­çš„å¤§éƒ¨åˆ†æ˜¯å¤©å°æˆ–
å±‹é ‚ï¼Œå› æ­¤é€™äº›ç ”ç©¶çš„ä¸»è¦å·¥ä½œæ˜¯åœ¨åµ
æ¸¬å»ºç¯‰ç‰©çš„å±‹é ‚[4] [5]ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸€äº›
åµæ¸¬åœ¨åœ°é¢æ‹æ”çš„å½±åƒä¸­çš„å»ºç¯‰ç‰©çš„ç ”
ç©¶ä¹Ÿå·²ç™¼è¡¨ [10-13]ã€‚ 
åœ¨å ´æ™¯å®šä½ä¸Šï¼Œç›®å‰å·²æœ‰ç¶²ç«™æä¾›
ä½¿ç”¨è€…ä¸Šå‚³å½±åƒæˆ–è¦–è¨Š[29][30]ã€‚ä¸éï¼Œ
é€™äº›ä¸Šå‚³è³‡æ–™çš„æ‹æ”åœ°é»çš„å®šä½ï¼Œä»éœ€
ä¸Šå‚³è€…æ‰‹å‹•æ–¼åœ°åœ–ä¸Šé»é¸æˆ–è€…æ˜¯ä¸Šå‚³æ‹
æ”æ™‚ GPS æ‰€ç´€éŒ„çš„ç¶“ç·¯åº¦ã€‚å› æ­¤ï¼Œè‹¥èƒ½
è‡ªå‹•çš„åˆ†æå½±åƒèˆ‡è¦–è¨Šä¾†åšæ‹æ”åœ°é»çš„
å®šä½ä¸¦ä¸”è‡ªå‹•çš„è’é›†ç¶²è·¯ä¸Šçš„ç›¸é—œè³‡è¨Šï¼Œ
å°‡å¯è®“å½±åƒèˆ‡è¦–è¨Šçš„æä¾›è€…æ›´æ–¹ä¾¿çš„åˆ†
äº«å…¶ä½œå“ã€‚ 
 
äºŒã€ç ”ç©¶ç›®çš„ 
 
æœ¬è¨ˆç•«ç›®çš„è¦å®Œæˆä¸€ã€Œä»¥è¦–è¨Šå…§å®¹
ç‚ºåŸºç¤ä¹‹è¦–è¨Šå ´æ™¯å®šä½ç³»çµ±ã€ã€‚è—‰ç”±æ­¤
è¨ˆåŠƒçš„åŸ·è¡Œï¼Œæˆ‘å€‘å°‡é–‹ç™¼å»ºç¯‰ç‰©åµæ¸¬ã€
å»ºç¯‰ç‰©è¾¨è­˜åŠå ´æ™¯å®šä½èˆ‡è’é›†ç›¸é—œè³‡è¨Š
ç­‰æŠ€è¡“ã€‚æ ¹æ“šæ‰€é–‹ç™¼çš„æŠ€è¡“ï¼Œæœ¬è¨ˆç•«æ“¬
å®Œæˆçš„ç³»çµ±èƒ½è‡ªå‹•çš„åˆ†æå½±åƒèˆ‡è¦–è¨Šå…§
å®¹ä¾†åšæ‹æ”åœ°é»çš„å®šä½ï¼Œä¸¦ä¸”è‡ªå‹•çš„è’
é›†ç¶²è·¯ä¸Šçš„ç›¸é—œè³‡è¨Šã€‚ 
 
ä¸‰ã€ç ”ç©¶æ–¹æ³• 
 
æœ¬ç³»çµ±ä¸»è¦æ˜¯ç”±ä¸‰å€‹å­ç³»çµ±æ‰€çµ„æˆï¼š
å»ºç¯‰ç‰©åµæ¸¬ç³»çµ±ï¼Œå»ºç¯‰ç‰©è¾¨è­˜ç³»çµ±ï¼Œå ´
æ™¯å®šä½ç³»çµ±ã€‚æ­¤ä¸‰å€‹å­ç³»çµ±ä¹‹ç ”ç©¶æ–¹æ³•
èˆ‡æ­¥é©Ÿå¦‚ä¸‹ï¼š 
1. å»ºç¯‰ç‰©åµæ¸¬ç³»çµ± 
åŸºæœ¬ä¸Šï¼Œå»ºç¯‰ç‰©åµæ¸¬å¯è¦–ä½œä¸€å€‹å½±
 6
 
äº”ã€è¨ˆç•«æˆæœè‡ªè©• 
 
æœ¬è¨ˆç•«é–‹ç™¼äº†å»ºç¯‰ç‰©åµæ¸¬ã€å»ºç¯‰ç‰©è¾¨
è­˜åŠå ´æ™¯å®šä½èˆ‡è’é›†ç›¸é—œè³‡è¨Šç­‰æŠ€è¡“ï¼›ä¸¦
å®Œæˆäº†ã€Œä»¥è¦–è¨Šå…§å®¹ç‚ºåŸºç¤ä¹‹è¦–è¨Šå ´æ™¯å®š
ä½ç³»çµ±ã€ä¾†è‡ªå‹•åˆ†æå½±åƒèˆ‡è¦–è¨Šå…§å®¹ä¾†åš
æ‹æ”åœ°é»çš„å®šä½ã€‚é€éæœ¬ç³»çµ±ï¼Œå½±åƒèˆ‡è¦–
è¨Šçš„æä¾›è€…å°‡å¯æ›´æ–¹ä¾¿çš„åˆ†äº«å…¶ä½œå“ï¼›è§€
è³è€…å¯è®“æ›´å®¹æ˜“çš„å¾—åˆ°èˆ‡è¦–è¨Šç•«é¢ç›¸é—œçš„
ç©ºé–“è³‡è¨Šèˆ‡æ™‚é–“è³‡è¨Šã€‚æœ¬ç ”ç©¶æˆæœé™¤å¯è‡ª
å‹•åŒ–çš„å°‡æ‰€æ‹æ”çš„å½±ç‰‡ä¹‹æŸå€‹å ´æ™¯å®šä½å¤–ï¼Œ
æ›´å¯æ–¼ç¶²è·¯ä¸Šæœå°‹èˆ‡æ­¤å ´æ™¯ç›¸é—œä¹‹æ™‚é–“èˆ‡
ç©ºé–“è³‡è¨Šï¼Œé€²è€Œæ‹“å±•æ‹æ”å½±ç‰‡æœ¬èº«ä¹‹ç›¸é—œ
çŸ¥è­˜ã€‚æ–¼æ˜¯ï¼Œè§€è³è€…é™¤å¯ç‰¹éæœ¬ç³»çµ±çŸ¥é“
æ‹æ”åœ°é»å¤–ï¼Œä¹Ÿå¯å¾ˆå®¹æ˜“çš„ç²å¾—èˆ‡æ­¤åœ°é»
ç›¸é—œçš„æ­·å²ã€åœ°ç†ï¼Œç”šæˆ–æ—…éŠè³‡è¨Šã€‚å› æ­¤ï¼Œ
æœ¬è¨ˆåŠƒé™¤èƒ½æ–¼åœ‹å®¶åœ¨è³‡è¨Šæ•¸ä½åŒ–æ‰€æ¬²ä¿ƒé€²
ä¹‹æ•™è‚²ã€å­¸è¡“ç ”ç©¶æœ‰æ‰€åŠ©ç›Šå¤–ï¼Œæ›´å¯ä¿ƒé€²
æ°‘ç”Ÿè‚²æ¨‚ä¸Šçš„åˆ©ç”¨ã€‚ 
 
åƒè€ƒæ–‡ç» 
 
[1] A.Huertas and R. Nevatia, â€œDetecting 
buildings in aerial images,â€ Computer Vision, 
Graphics, and Image Processing, vol. 41, pp. 
131-152, 1988. 
[2] R.B.Irvine, and D.M.McKeown, â€œMethods 
for Exploiting the Relationship Between 
Buildings and Their Shadows in Aerial 
Imagery,â€ IEEE Transactions on Systems, 
Man, and Cybernetics, vol.19, no.6, 
pp.1564-1575, 1989. 
[3] Jefferey A. Shufelt and David M. McKeown, 
Jr., â€œFusion of monocular cues to detect 
man-made structures in aerial imageryâ€, 
CVGIP: Image Understanding, vol.57 no.3, 
p.307-330, May 1993. 
[4] S. Krishnamachari and R. Chellappa, 
â€œDelineating buildings by grouping lines 
with MRFs,â€ IEEE Trans. on Image 
Processing, vol. 5 no. 1, pp. 164-168, Jan 
1996. 
[5] Chungan Lin and Ramakant Nevatia, 
â€œBuilding detection and description from a 
single intensity image,â€ Computer Vision 
and Image Understanding, vol.72 no.2, 
p.101-121, Nov. 1998. 
[6] Sanjay Noronha and Ramakant Nevatia, 
"Detection and Modeling of Buildings from 
Multiple Aerial Images," IEEE Transactions 
on Pattern Analysis and Machine 
Intelligence, vol. 23, no 5, pp. 501-518, May 
2001. 
[7] Christopher Jaynes, Edward Riseman, and 
Allen Hanson, â€œRecognition and 
reconstruction of buildings from multiple 
aerial images,â€ Computer Vision and Image 
Understanding, vol.90, no.1, p.68-98, April 
2003 
[8] Z. Kim and R. Nevatia, â€œAutomatic 
Description of Complex Buildings from 
Multiple Images,â€ Computer Vision and 
Image Understanding, vol. 96, no. 1, pp. 60-
95, 2004. 
[9] A. Vailaya, A. K. Jain, and H. J. Zhang. â€œOn 
image classification: City images vs. 
landscapes,â€ Pattern Recognition, vol. 31, pp. 
1921â€“1936, 1998. 
[10] Qasim Iqbal and J.K. Aggarwal, â€œApplying 
Perceptual Grouping to Content-Based 
Image Retrieval: Building Images,â€ in 1999 
IEEE Computer Society Conference on 
Computer Vision and Pattern Recognition 
(CVPR'99), vol. 1, pp.1042, 1999. 
[11] Yi Li and Linda G. Shapiro, â€œConsistent 
Line Clusters for Building Recognition in 
CBIR,â€ in Proceedings of the 16 th 
International Conference on Pattern 
Recognition (ICPR'02), vol. 3, pp.30952-
30956, 2002. 
[12] Sanjiv Kumar and Martial Hebert, â€œMan-
Made Structure Detection in Natural Images 
using a Causal Multiscale Random Field,â€ In 
Proc. IEEE Int. Conf. on Computer Vision 
and Pattern Recognition, pp. 119-126, 2003. 
[13] Google Maps API from Google on the World 
Wide Web: 
http://code.google.com/apis/maps/ 
[14] Yahoo! Maps API from Yahoo! on the World 
Wide Web: 
http://developer.yahoo.com/maps/ 
[15] Panoramio on the World Wide Web: 
http://www.panoramio.com/ 
[16] Cmoremap on the World Wide Web: 
http://www.cmoremap.com.tw/ 
11. Power electronics and motion control systems 
12. Intelligent computation in advanced measurement systems 
13. Control systems to improve quality of life 
14. Identification and compensation for the hysteresis nonlinearities 
15. Mechatronic systems MIC 
16. Mechatronic systems fault diagnosis and fault tolerance 
17. Artificial intelligence in mechatronic systems 
18. Swarm intelligence 
19. Intelligent modelling for social and economic applications 
20. Networked control system 
21. MIC of mechanical and electrical equipment and component 
22. Human adaptive mechatronics 
23. Discrete-time systems 
24. Nonlinear and linear system identification 
25. Discrete event and hybrid systems 
26. Robotics 
Â„b]Â…Â†Â‡'* 
Saturday, July 17, 2010 08:50 - 09:00 Opening Remarks  
09:00 - 10:00 Plenary Speech 1 (PS01) Special lecture room  
Title: Multiple-model-based diagnosis and fault tolerant control  
Speaker: JosÃ© Ragot, Nancy University, France  
10:00 - 10:20 Break  
10:20 - 12:00 Parallel Paper Sessions  
No. 1 lecture room  No. 5 lecture room  No. 6 lecture room  No. 7 lecture room  Special lecture 
room 
SatA01  
Novel and unconventional 
linear system modelling 
techniques  
SatA02  
Neural networks, 
fuzzy logic, and 
other heuristic 
techniques I  
SatA03  
Simulation, 
modelling and 
control on smart 
grids I  
SatA04  
Static and dynamic 
systems modelling I  
SatA05  
Control and 
optimization 
techniques I  
12:00 - 13:00 Lunch  
13:00 - 14:00 Plenary Speech 2 (PS02) Special lecture room  
14:20 - 16:00 Parallel Paper Sessions  
No. 1 lecture room  No. 5 lecture room  No. 6 lecture room  No. 7 lecture room  Special lecture 
room 
SunM01  
Artificial intelligence in 
mechatronic systems  
SunM02  
Swarm intelligence: 
Theory and 
application  
SunM03  
Simulation, 
modelling and 
control on smart 
grids III  
SunM04  
Intelligent 
modelling for social 
and economic 
applications  
SunM05  
Networked control 
system I  
16:00 - 16:20 Break  
16:20 - 18:00 Parallel Paper Sessions  
No. 1 lecture room  No. 5 lecture room  No. 6 lecture room  No. 7 lecture room  Special lecture 
room 
SunP01  
MIC of mechanical and 
electrical equipment and 
component  
SunP02  
Human adaptive 
mechatronics  
SunP03  
Simulation, 
modelling and 
control on smart 
grids IV  
SunP04  
Discrete-time 
systems  
SunP05  
Networked control 
system II  
19:00 - 21:00 Banquet (ANA Hotel Okayama)  
	
Monday, July 19, 2010 10:00 - 11:20 Parallel Paper Sessions  
No. 1 lecture room  No. 5 lecture room  No. 6 lecture room  No. 7 lecture room  
MonA01  
Nonlinear and linear system 
identification  
MonA02  
Discrete event and 
hybrid systems  
MonA03  
Robotics  
MonA04  
Static and dynamic systems modelling II  
11:50 - 12:50 Closing Reception (Peach Union)  
	
KÂˆÂ‰p%Neural networks, fuzzy logic Intelligent computation and control system
Swarm intelligence Static and dynamic systems modellingÂŠ()Â‹ÂŒ 7 18
 SunA05 Neural networks, fuzzy logic, and other heuristic techniques IIÂÂ sessionH
tuhuh:; â€ Generalized Probabilistic Decision-Based Neural Networks for 
Texture Classification and Retrievalâ€cÂ‹ÂÂÂ‘ÂƒÂ’&'KpÂ“
Â”* 
	uhÂ•Â–	
	
Â—pÂ˜Â	

Â™ÂšÂ›ÂœdKpÂÂÂŸÂ Â¡Â¢Â£&pÂ¤Â¥Âœd
Â™Â¦Â§Â¨Â£&Â©ÂªugÂÂ«wÂ¬GÂ­Â®Â¯Â°Â„Â±Â•Â–bÂ²vÂƒÂ’Â±

Â³&Â´GÂ­ÂµÂ¶^ÂÂ©Â·Â¸KuhÂ•Â–gÂ¹ÂºHIÂ¡Â»Â¼Â½Â¼pÂ¾
Â¿Ã€ÂšÃÂÃ‚ÃƒÃ„Ã…gÃ†ÂºÃ‡IÂ¡Â»ÃˆÃ‰Â±Â´ÃŠÃ‹pÃŠÂ¿
ÃŒÃ%Â¡Â»ÃÂ£&Ã	
	
Ãl	
Ã„Ã‘Ã’Â£'Ã“Â£Â 
)JjÂ¡Â¢Â£&pÂ¤Â¥ÂœdÂ™Ã”Â±Ã”Âª
uÃ•Â£&Ã„Ã–@!Ã—Ã˜Ã™ÃšÃ„Ã›Â¼GÂ­ÃœÃÃŒÃÃŸÃ GÂ­Ã¡o
ÂºHIÂ¼GÂ­Â½Ã¢pÃŠÂ¿
Ã£AÃ¤pSÃ¥Ã¦Ã§Ã¨Ã©ÃªÂ¡Ã«	
Generalized Probabilistic Decision-Based Neural Networks for Texture
Classification and Retrieval
Yeong-Yuh Xu, S.C. Chuang, C.-L. Tseng, and Hsin-Chia Fu
Abstractâ€”For various applications, formulating texture fea-
tures in distributional forms can sometimes provide meaningful
representation than in numerical forms. In this paper, we first
proposed a novel methodology for measuring the difference
between two mixture Gaussian distributions. Based on the
derived formula, a Generalized Probabilistic Decision-Based
Neural Network (GPDNN) is then proposed and implemented
to realize the difference measurement method. By constructing
a two layered pyramid-type network structure, the proposed
GPDNN receives data in distributional form via 2-D grid
input nodes, and outputs the classification and/or retrieval
results from the top layer node. Forty texture images are
selected from the MIT Vision Texture (VisTex) database to
evaluate the proposed GPDNN for the texture classification
and retrieval. Experimental results show that (1) by using
the proposed difference measurement methods, the texture
pattern retrieval rates can be improved from 77% to 82%,
compared with some published leading methods, and (2) the
proposed GPDNN shows significant texture classification and
retrieval performance, which are about 90.1% and 88.6% of
the accuracy, and much better than the traditional methods,
i.e., 82.2% and 79.9%, respectively.
I. INTRODUCTION
In the past decades texture analysis has received lots of
attention and numerous methods have been proposed[1], [2],
[3] due to its wide applications in computer vision and pat-
tern recognition[4]. As the support of physiological studies
of human visual cortex[5], [6], wavelet-like approaches are
popular in extracting and formulating texture features in the
recent years[7], [8], [9], [10].
Based on those extracted features, it is still a challeng-
ing task and an ongoing research topic to find a human-
perception based similarity/dissimilarity measurement be-
tween textures. some methods are proposed for computing
the norm-based distances (e.g., Euclidean distance) as the
dissimilarity measurement [9] between two feature vectors.
In the case that the texture pattern features are formulated in
distributional forms, norm-based distances are not suitable
to measure the dissimilarity between textures. For some
pattern recognition applications, formulating features in dis-
tributional forms can sometimes provide more meaningful
representation than numerical forms can do[3], [9].
In order to measure the difference between two normal
distributions, the weighted Euclidean distance has been pro-
This work was supported by the National Science Council under Grant
NSC98-2218-E-241-001
Yeong-Yuh Xu is with the Department of Computer Science and Infor-
mation Engineering, Hungkuang University, Taichung 433, Taiwan, ROC
yyxu@sunrise.hk.edu.tw
S.C. Chuang, C.-L. Tseng, and Hsin-Chia Fu are with the Department of
Computer Science, National Chiao Tung University, Hsinchu 300, Taiwan,
ROC hcfu@cs.nctu.edu.tw
posed as the dissimilarity measuring function in [9], where
the weighting factors are the standard deviation of each
normal distribution. When a feature component needs to
be represented by a complex distribution, such as mixture
Gaussian distribution, weighted Euclidean distance is not
suitable for dissimilarity measurement.
In this paper, we proposed a novel methodology for the
measurement of the difference between two complex distri-
butions. Furthermore, based on the proposed methodology,
a Generalized Probabilistic Decision-Based Neural Network
(GPDNN) is proposed and implemented for the measure-
ment of two complex distributions. Forty texture images are
selected from the MIT Vision Texture (VisTex) database
to demonstrate and evaluate the proposed GPDNN for the
texture classification and retrieval. Experimental results show
that, by using the proposed difference measurement methods,
the retrieval rates is improved from 77% to 82%, compared
with some published leading methods [3]. The proposed
GPDNN shows significant texture classification and retrieval
performance, which are about 90.1% and 88.6% of the
accuracy, and much better than the traditional methods, i.e.,
82.2% and 79.9%, respectively.
The rest of this paper is organized as follows. In the
next section, the proposed difference measurement is in-
troduced, and the formula for difference measurement of
two distributions are presented. Then, in Section III, the
implementation of GPDNN and its learning scheme are
introduced. Experimental results are presented and discussed
in Section IV. Finally, Section V draws some concluding
remarks.
II. DIFFERENCE MEASUREMENT BETWEEN TWO
DISTRIBUTIONS
Given two distributions Pa and Pb, the difference between
Pa and Pb could be defined as
Ï•(Pa,Pb) =
âˆ«
RD
(Pa âˆ’ Pb)
2dz, (1)
where RD is a D-dimensional feature space. Suppose Pa and
Pb are two mixture Gaussian distributions,
Pa =
Raâˆ‘
n=1
P anp(z | Î¸
a
n)
and
Pb =
Rbâˆ‘
n=1
P bnp(z | Î¸
b
n),
Proceedings of the 2010 International Conference on 
Modelling, Identification and Control, Okayama, Japan, July 17-19, 2010
Â©2010 ICMIC 500
i1q
t
1q
i
P1
the parameters of P
i the
para
met
ers
of
P t
F (P
i
,P
t
)
Â´
Â´
Â´
Â´
Â´
Â´
Â´
Â´
Â´
Â´
Â´
Â´
1,1G
2,1G
tR
G ,1
1,2G
2,2G
tR
G ,2
1,iR
G
2,iR
G
ti RR
G ,
i
P2
i
Ri
P
t
P1
t
P2
t
Rt
P
t
P1
t
P2
t
Rt
P
t
Rt
P
t
P2
t
P1
t
2q
t
Rt
q
i
2q
i
Ri
q
1h
2h
iR
h
Fig. 3. The internal architecture of a model in Fig. 2 for computing of
F(Pi,Pt).
the outputs of all the Gn,m nodes are weighted by P
t
m and
summed to the hidden node hn. Suppose that the output of
the hidden node hn is Tn, then
Tn =
Rtâˆ‘
m=1
P tm Â· G(Î¸
i
n, Î¸
t
m).
Finally, the output of each hidden node hn are weighted
by P in and then summed to the output node of the pyramid
subnetwork:
F(Pi,Pt) =
Riâˆ‘
n=1
P in Â· Tn.
When the outputs of all the subnets reach the top layer of
the GPDNN, the MINNET is activated to select the minimum
of the values from the lower subnet and its corresponding
subnet ID. That is if the output value of subnet i is the
minimum among the outputs of all subnets in a GPDNN,
the input data x(t) is classified as the class wi.
B. Learning Phase
The GPDNN adopts the SPDNN learning scheme. While
the input data x(t) belonging to the class wi is misclassifi-
cated to the class wj , the reinforced and antireforced learning
rules are applied to the subnets of wi and wj , respectively;
Reinforced Learning rule:
w
(m+1)
i = w
(m)
i + Î·âˆ‡Ï•(x(t), wi) (4)
Antireinforced Learning rule:
w
(m+1)
j = w
(m)
j âˆ’ Î·âˆ‡Ï•(x(t), wj) (5)
The gradient vectors in (4) and (5) are computed as follows:
âˆ‚Ï•(x(t), wi)
âˆ‚Âµi
n(d)
=
2P
i
n
ï£®
ï£° Riâˆ‘
m=1
(
P i
m
G(Î¸i
n
, Î¸i
m
)(Âµi
n(d) âˆ’ Âµ
i
m(d))
(Ïƒi
n(d)
)2 + (Ïƒi
m(d)
)2
)
âˆ’
Rtâˆ‘
m=1
(
P t
m
G(Î¸t
m
, Î¸i
n
)(Âµt
m(d) âˆ’ Âµ
i
n(d))
(Ïƒt
m(d)
)2 + (Ïƒi
n(d)
)2
)]
, (6)
âˆ‚Ï•(x(t), wi)
âˆ‚(Ïƒi
n(d)
)2
=
P
i
n
ï£®
ï£° Riâˆ‘
m=1
(
P i
m
G(Î¸i
m
, Î¸i
n
)
(Ïƒi
m(d)
)2 + (Ïƒi
n(d)
)2
)
Â·
(
(Âµi
m(d) âˆ’ Âµ
i
n(d))
2
(Ïƒi
m(d)
)2 + (Ïƒi
n(d)
)2
âˆ’ 1
)
âˆ’
Rtâˆ‘
m=1
(
P t
m
G(Î¸t
m
, Î¸i
n
)
(Ïƒt
m(d)
)2 + (Ïƒi
n(d)
)2
)
Â·
(
(Âµt
m(d) âˆ’ Âµ
i
n(d))
2
(Ïƒt
m(d)
)2 + (Ïƒi
n(d)
)2
âˆ’ 1
)]
, (7)
for each n âˆˆ {1, 2, . . . , Ri} and d âˆˆ {1, 2, . . . , D}.
IV. EXPERIMENTS
A total of 40 different texture images are selected from
the MIT Vision Texture (VisTex) database [12]. The original
size of the images is 512Ã—512, and each image was divided
into sixteen disjoint subimages, i.e., 128Ã—128 pixels. Hence,
the classification problem involved a total of 640 subimages,
16 subimages in each of the 40 texture categories[3], and
only the gray-scale images were used in the experiments.
The Gabor filter bank proposed by Manjunath and Ma [9]
is used to extract the texture features. The filter parameters
we used are four scales and six orientations, which produced
a bank of 24 filters. Mixture Gaussian distributions are used
to approximate the texture feature distributions of the 640
subimages. For each subimage, an EM algorithm is employed
for finding maximum likelihood estimates of parameters,
and the Bayesian information criterion (BIC) [13] is used
to determine the number of components for each mixture
Gaussian distribution. Fig. 4 shows the mean and standard
deviation (SD) of the number of components per texture
category, where the first row is the texture image, the second
row is the texture name, and the third row is the mean and
SD in the form of â€œmean(SD)â€. As we can see, the more
complicated the texture is, the more mixture components are
needed to approximate the texture feature distribution.
To examine the performance of the proposed difference
measurement method, the retrieval results are compared with
the generalized Gaussian density (GGD) and kullback -
Leibler distance (KLD) method proposed in [3]. Similar to
the experiments described in [3], we used each of 640 images
in the databases as a simulated query image, and defined the
relevant images for each query as the other 15 subimages
from the same original VisTex image. The retrieval rate is
the average percentages of retrieving relevant images in the
top 15 matches. Experimental results show that the proposed
method improves the retrieval performance from 77% to
82%, which is 5% better than the GGD and KLD method.
The detail comparison on precision for each texture classis
shown in Fig. 5.
In order to evaluate the performance of the GPDNN, the
cross validation method [14] is used to randomly split the
502
TABLE I
MEANS AND STANDARD DEVIATIONS OF CLASSIFICATION AND
RETRIEVAL ACCURACIES FOR GPDNN, SPDNN, AND MDC, WHERE
THE STANDARD DEVIATION IS GIVEN IN THE PARENTHESES.
classification rate
training set testing set
GPDNN 0.989(0.006) 0.901(0.021)
SPDNN 0.947(0.025) 0.851(0.016)
MDC 0.873(0.022) 0.822(0.016)
retrieval rate
training set testing set t
GPDNN 0.988(0.008) 0.886(0.026)
SPDNN 0.940(0.026) 0.826(0.020)
MDC 0.858(0.025) 0.799(0.023)
are shown in Table I. It is clear to see that the proposed
GPDNN has significant improvements in classification accu-
racy from 82.2% to 90.1% and retrieval accuracy from 79.9%
to 88.6%.
V. CONCLUSION
In this paper, we proposed a novel methodology for mea-
suring the difference between two distributions. Based on the
proposed measuring methodology, a Generalized Probabilis-
tic Decision-Based Neural Network (GPDNN) is proposed.
Forty texture images are selected from the MIT Vision
Texture (VisTex) database to demonstrate and evaluate the
proposed GPDNN for the texture classification. Experimental
results show that (1) the proposed difference measurement
improves retrieval rates, e.g., from 77% to 82%, compared
with some published leading methods, and (2) the proposed
GPDNN shows significant texture classification and retrieval
performance, which are about 90.1% and 88.6% of the
accuracy, and much better than the traditional methods, i.e.,
82.2% and 79.9%, respectively. Although this paper presents
applications of the GPDNN for the texture classification and
retrieval, we believe that a lot of image/pattern recognition
and classification problems and applications can be solved
and/or applied by the proposed GPDNN. For instances,
we are currently working on the building recognition by
GPDNN.
REFERENCES
[1] T. R. Reed and J. M. H. Du Buf, â€œA review of recent texture
segmentation and feature extraction techniques,â€ in Computer Vision
Graphics Image Process. Image Understanding, 1993, vol. 57, pp.
359â€“372.
[2] T. Ojala, M. Pietikainen, and D. Harwood, â€œA comparative study of
texture measures with classification based on feature distributions,â€
Pattern Recognition, vol. 29, no. 1, pp. 51â€“59, 1996.
[3] Minh N. DO and Martin Vetterli, â€œWavelet-based texture retrieval
using generalized gaussian density and kullback-leibler distance,â€
IEEE Transactions on Image Processing, vol. 11, no. 2, pp. 146â€“158,
2002.
[4] M. Tuceryan and A. Jain, Handbook Pattern Recognition and
Computer Vision, chapter Texture analysis, pp. 235â€“276, Singapore:
World Scientific, 1993.
[5] D.H. Hubel and T.N. Wiesel, â€œReceptive field, binocular interaction,
and functional architecture in the cats visual cortex,â€ in J. Physiol,
1962, vol. 160, pp. 106â€“154.
[6] JG. Daugman, â€œTwo-dimensional spectral analysis of cortical receptive
field profiles,â€ Vision Research, vol. 20, pp. 847â€“856, 1980.
[7] A. Laine and J. Fan, â€œTexture classification by wavelet packet
signatures,â€ IEEE Transactions on Pattern Recognition and Machine
Intelligence, vol. 15, no. 11, pp. 1186â€“1190, 1993.
[8] M. Unser, â€œTexture classification and segmentation using wavelet
frames,â€ IEEE Transactions on Image Processing, vol. 4, no. 11,
pp. 1549â€“1560, 1995.
[9] B.S. Manjunath and W.Y. Ma, â€œTexture features for browsing and
retrieval of image data,â€ IEEE Transactions on Pattern Recognition
and Machine Intelligence, vol. 18, no. 8, pp. 837â€“842, 1996.
[10] T. Randen and J. H. Husoy, â€œFiltering for texture classification: A
comparative study,â€ IEEE Transactions on Pattern Recognition and
Machine Intelligence, vol. 21, no. 4, pp. 291â€“310, 1999.
[11] H. C. Fu and Y. Y. Xu, â€œMultilinguistic handwritten character recogni-
tion by bayesian decision-based neural networks,â€ IEEE Transactions
on Signal Processing, vol. 46, no. 10, pp. 2781â€“2789, 1998.
[12] MIT Vision and Modeling Group. Vision Texture.,
â€œhttp://vismod.www.media.mit.edu,â€ .
[13] G. Schwarz, â€œEstimation the dimension of a model,â€ Annals of
Statistics, vol. 6, pp. 461V464, 1978.
[14] Pierre A. Devijver and Josef Kittler, Pattern Recognition: A Statistical
Approach, Prentice-Hall, 1982.
504
98ï¦ï¨å°ˆé¡Œç ”ç©¶è¨ˆç•«ç ”ç©¶æˆæœå½™æ•´è¡¨ 
è¨ˆç•«ä¸»æŒäººï¼šå¾æ°¸ç…œ è¨ˆç•«ç·¨è™Ÿï¼š98-2218-E-241-001- 
è¨ˆç•«åç¨±ï¼šä»¥è¦–è¨Šå…§å®¹ç‚ºåŸºç¤ä¹‹è¦–è¨Šå ´æ™¯å®šä½ç³»çµ±ä¹‹ç ”ç™¼ 
ï¥¾åŒ– 
æˆæœé …ç›® å¯¦éš›å·²é”æˆ
ï¥©ï¼ˆè¢«æ¥å—
æˆ–å·²ç™¼è¡¨ï¼‰
é æœŸç¸½é”æˆ
ï¥©(å«å¯¦éš›å·²
é”æˆï¥©) 
æœ¬è¨ˆç•«å¯¦
éš›è²¢ç»ç™¾
åˆ†æ¯” 
å–®ä½ 
å‚™ è¨» ï¼ˆ è³ª åŒ– ï¥¯
æ˜ï¼šå¦‚ï¥©å€‹è¨ˆç•«
å…±åŒæˆæœã€æˆæœ
ï¦œ ç‚º è©² æœŸ åˆŠ ä¹‹
å° é¢ æ•… äº‹ ...
ç­‰ï¼‰ 
æœŸåˆŠï¥æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒï¥æ–‡ 0 0 100% 
ç¯‡ 
 
ï¥æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100%   
ç”³è«‹ä¸­ä»¶ï¥© 0 0 100%  å°ˆï§ å·²ç²å¾—ä»¶ï¥© 0 0 100% ä»¶  
ä»¶ï¥© 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šï§ï¤Š 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å…§ 
ï¥«èˆ‡è¨ˆç•«äººï¦Š 
ï¼ˆæœ¬åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ï§¤ 0 0 100% 
äººæ¬¡ 
 
æœŸåˆŠï¥æ–‡ 0 0 100%  
ç ”ç©¶å ±å‘Š/æŠ€è¡“å ±å‘Š 0 0 100%  
ç ”è¨æœƒï¥æ–‡ 1 1 100% 
ç¯‡ 
 
ï¥æ–‡è‘—ä½œ 
å°ˆæ›¸ 0 0 100% ç« /æœ¬  
ç”³è«‹ä¸­ä»¶ï¥© 0 0 100%  å°ˆï§ å·²ç²å¾—ä»¶ï¥© 0 0 100% ä»¶  
ä»¶ï¥© 0 0 100% ä»¶  
æŠ€è¡“ç§»è½‰ 
æ¬Šï§ï¤Š 0 0 100% åƒå…ƒ  
ç¢©å£«ç”Ÿ 0 0 100%  
åšå£«ç”Ÿ 0 0 100%  
åšå£«å¾Œç ”ç©¶å“¡ 0 0 100%  
åœ‹å¤– 
ï¥«èˆ‡è¨ˆç•«äººï¦Š 
ï¼ˆå¤–åœ‹ç±ï¼‰ 
å°ˆä»»åŠ©ï§¤ 0 0 100% 
äººæ¬¡ 
 
 
