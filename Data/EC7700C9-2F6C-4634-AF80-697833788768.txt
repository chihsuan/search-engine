 é¢ï¼Œå¦‚ä½•åœ¨ SMTå¹³å°ä¸Šä½œè¡Œç¨‹é–“çš„åŒæ­¥è™•ç†åŠè€—é›»è©•ä¼°ä¹Ÿæ˜¯æˆ‘å€‘çš„é‡é»ã€‚ 
ç ”ç©¶ç›®çš„ 
SMT/CMP(ç°¡ç¨± multithreading processor, MTP)äºŒé …æŠ€è¡“å·²ç¶“è¢«è­‰æ˜ç‚ºä¸€å€‹æœ‰
æ•ˆç‡ã€ä¸¦ä¸”çœé›»çš„è¨­è¨ˆã€‚é€™äºŒé …æŠ€è¡“éƒ½åˆ©ç”¨å¹³è¡Œè™•ç†çš„æŠ€è¡“ä»¥æå‡æ•ˆèƒ½ã€‚äºŒè€…çš„
ä¸åŒåœ¨æ–¼ SMTå¹¾ä¹å…±ç”¨äº†æ‰€æœ‰çš„ hardware resources ï¼Œè€Œ CMPåªå…±ç”¨ level-1 
cacheä»¥ä¸‹çš„ hardware resourcesã€‚åœ¨ç›¸åŒçš„ç¡¬é«”é™åˆ¶ä¸‹(i.e., the limitation 
of die size)ï¼ŒSMTèƒ½æ›´æœ‰æ•ˆçš„æé«˜ç¡¬é«”ä½¿ç”¨ç‡ï¼Œä½† CMPæ“æœ‰è¼ƒé«˜çš„ scalabilityã€‚
æœ€è¿‘çš„ä¸€äº›ç ”ç©¶æŒ‡å‡º SMTåŠ CMPæ··å’Œè¨­è¨ˆæ˜¯è¼ƒä½³çš„é¸æ“‡ã€‚ 
å¤§éƒ¨åˆ†çš„ MTP æ²¿ç”¨å‚³çµ±çš„ cache è¨­è¨ˆæ–¹å¼ï¼Œä½† SMT/CMP processor çš„ memory 
reference string (MRS)ç›¸ç•¶ä¸åŒæ–¼å‚³çµ±çš„ super-scalar/pipeline processor
ï¼ˆç°¡ç¨± single-threading processor, STPï¼‰çš„ MRSã€‚SMT/CMP åœ¨ instruction 
level ä¸Šæ··å’Œä¾†è‡ªä¸åŒ program çš„ MRSï¼Œå› æ­¤ spatial locality åŠ temporal 
locality è®Šå¾—æ¯”è¼ƒå·®ã€‚operating system (OS)çš„ multiprogramming æŠ€è¡“åŒæ¨£
çš„å¯ä»¥ä¸¦è¡Œè™•ç†å¤šå€‹ programï¼Œä½†å¤§éƒ¨åˆ†çš„ OS æ¯ç§’é˜åªç™¼ç”Ÿ 10~1000 æ¬¡çš„
context switchã€‚ç›¸å°æ–¼ MTPï¼ŒOSå°æ–¼ temporal localityåŠ spatial locality
çš„å½±éŸ¿éå¸¸å°ã€‚ 
å°æ–¼ç³»çµ±è¨­è¨ˆè€…è€Œè¨€ï¼Œlocality çš„æ”¹è®Šæ‰€é€ æˆçš„å½±éŸ¿æœ‰ä¸‰å€‹æ–¹é¢ã€‚é¦–å…ˆï¼ŒWCET
è®Šå¾—éå¸¸é›£ä»¥é ä¼°ï¼Œç›®å‰å·²ç¶“æœ‰ç›¸ç•¶å¤šçš„ç ”ç©¶åˆ†åˆ¥é‡å° inter-process åŠ
intra-processçš„ cache behavioré€²è¡Œæ¢è¨ã€‚ä¸¦æå‡ºåœ¨é€™äºŒç¨®æƒ…æ³ä¸‹ WCETå¦‚ä½•
ä¼°è¨ˆï¼Œä½†æ“šæˆ‘å€‘æ‰€çŸ¥ï¼Œæ²’æœ‰è«–æ–‡è¨è«–å¦‚ä½•åœ¨ MTPä¸Šç²¾ç¢ºçš„ä¼°è¨ˆ WCETã€‚ç”±æ–¼ WCET
çš„ä¼°è¨ˆæ˜¯è¨±å¤š embedded systemåŠ real-time systemçš„åŸºç¤ï¼Œå› æ­¤é€™å°‡æœƒé€ æˆ
è»Ÿé«”è¨­è¨ˆä¸Šçš„å›°é›£ã€‚å…¶æ¬¡ï¼Œinstruction levelçš„å¹³è¡Œè™•ç†å¯èƒ½å°è‡´ thrashing ã€‚
ç•¶ thrashing ç™¼ç”Ÿæ™‚ processor å…§éƒ¨æœƒç”¢ç”Ÿå¤§é‡çš„ cache missï¼Œå› æ­¤é€ æˆæ¯ç§’
é˜æ‰€èƒ½å®Œæˆçš„æŒ‡ä»¤ æ•¸çªç„¶åœ°é™ä½ã€‚ç•¶å¹³è¡Œè™•ç†çš„ programsçš„ working sets è¢«
å°æ‡‰åˆ° cacheä¸Šçš„åŒä¸€ç¾¤ cache lineæ™‚ï¼Œé€™äº› programå°‡æœƒäº’ç›¸çš„è¦†è“‹æ‰å°æ–¹
çš„ working setï¼Œå› è€Œé€ æˆ thrashingã€‚æŠ€è¡“äººå“¡å·²ç¶“ç™¼ç¾åˆ°åœ¨ hyperthreading  
enabledçš„ Intel P4/Xeonä¸ŠåŸ·è¡Œ SQL Server åŠ Citrix Terminal Serveræ™‚ï¼Œ
åœ¨ heavily load çš„æƒ…æ³ä¸‹ï¼Œç³»çµ±æ•ˆèƒ½å°‡é¡¯è‘—çš„é™ä½ã€‚è€Œç³»çµ±æ•ˆèƒ½ä½è½çš„åŸå› å³
ä¾†è‡ªæ–¼ cacheçš„è¨­è¨ˆä¸¦æœªè€ƒæ…® SMT processorçš„ç‰¹æ€§ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¸€å€‹ malicious 
program ä¹Ÿå¯ä»¥è—‰ç”±å¼•èµ· thrashing ä»¥ç™±ç˜“é›»è…¦ã€‚ç¬¬ä¸‰ï¼Œä¸€èˆ¬çš„ OS éƒ½å‡è¨­æ¯å€‹
mini-processor éƒ½èƒ½å¤ å…¬å¹³çš„ä½¿ç”¨æ‰€æœ‰çš„ç¡¬é«”è³‡æºä¸¦ä¸”ä¸æœƒäº’ç›¸å½±éŸ¿ï¼Œå¦‚æœä¸
å° cacheåšä»»ä½•é™åˆ¶ï¼Œé‚£éº¼åŒä¸€å€‹ programèˆ‡ memory intensiveçš„ programä¸¦
è¡Œè™•ç†æˆ–èˆ‡ CPU intensiveçš„ programä¸¦è¡Œè™•ç†å°‡æœƒæœ‰ä¸ä¸€æ¨£çš„åŸ·è¡Œæ•ˆæœï¼Œé€™ç¨®
å·®ç•°æ€§å°‡æœƒå¢åŠ  OS schedulerçš„è¨­è¨ˆé›£åº¦ã€‚ 
åœ¨é€™ç¯‡è«–æ–‡ä¸­ï¼Œæˆ‘å€‘å°‡æå‡ºä¸€ç¨®æ–°çš„ cache memory è¨­è¨ˆæ–¹æ³•ï¼Œé€™å€‹æ–°æ–¹æ³•å…è¨±
SMT/CMPè™•ç†å™¨å…§çš„ logical/simple processorï¼ˆç°¡ç¨±ï¼šmini-processor, mini-pï¼‰
 Symposium on Applied Computing, 2006. 
[11] I. Rhee and G. Martin. â€œA Scalable Realtime Synchronization Protocol 
for Distributed Systems,â€ IEEE Real-time System Symposium, 1995. 
[12] P. Gai, G. Lipari, M. D. Natale. â€œMinimizing Memory Utilization of 
Real-Time Task Sets in Single and Multi-Processor Systems-on-a-chip,â€ 
IEEE Real-Time Systems Symposium, 2001. 
[13] A. K. Mok. â€œFundamental design problems of distributed systems for 
the hard real time environments,â€ Ph.D. dissertation, M.I.T., 1983. 
[æ–‡ç»åˆ†é¡äºŒ] 
åœ¨å¿«å–è¨˜æ†¶é«”åŠ worst case execution timeçš„ä¼°è¨ˆä¸Šï¼Œå‰äººä¹Ÿæƒ³ç›¸ç•¶å¯è§€çš„ç ”
ç©¶ï¼Œå”¯é€™äº›éƒ½æ˜¯æ¢è¨å–®é¡†è™•ç†å™¨æˆ–å¤šè™•ç†å™¨çš„æƒ…æ³ï¼Œå°æ–¼ SMTæˆ– CMPè™•ç†å™¨çš„æƒ…
æ³è‘—å¢¨è¼ƒå°‘ã€‚ 
[1] S. Kaxiras, Z. Hu, and M. Martonosi. Cache decay: exploiting 
generational behavior to reduce cache leakage power. In Proc. of the 28th 
annual Intâ€™l Symp. 
on Computer Architecture (ISCAâ€™01), pages 240-251,2001. 
[2] David A. Wood, Mark D. Hill, R. E. Kessler. A model for estimating 
trace-sample miss ratios. In Proc. of the 1991 ACM SIGMETRICS, pages 79-89, 
1991. 
[3] Seongbeom Kim, Dhruba Chandra, Yan Solihin. Fair Cache Sharing and 
Partitioning in a Chip Multiprocessor Architecture. In Proc. Of the 13th 
Intâ€™l. Conf. on Parallel Architecture and Compilation Techniques 
(PACTâ€™04), pages 111-122, 2004. 
[4] Francisco J. Cazorla, Alex Ramrez, Mateo Valero, Enrique Fernndez. 
Dynamically Controlled Resource Allocation in SMT Processors. In Proc. 
Of the 37th Symp. On Microarchitecture (MICRO-37â€™04), pages 171-182, 
2004. 
[5] Y. Tan ,V. Mooney. WCRT Analysis for a Uniprocessor with a Unified 
Prioritized Cache. In Proc. of the 2005 ACM SIGPLAN/SIGBED Conf. on 
Languages, Compilers and Tools for Embedded Systems (LCTESâ€™05), pages 
175-182, 2005. 
[6] Z. Hu, S. Kaxiras, M. Martonosi. Timekeeping in the memory system: 
predicting and optimizing memory behavior. In Proceedings of the 29th 
annual intâ€™l symp. 
on Computer architecture (ISCAâ€™02), pages 209-220,2002. 
[7] A. Agarwal, S. D. Pudar. Column-associative caches: A technique for 
reducing the miss rate of direct-mapped caches. In Proc. of the 20th Intâ€™l. 
Symp. on Computer Architecture (ISCAâ€™93), pages 169-178, 1993. 
 è¤‡è£½åˆ°è¼ƒé«˜å±¤çš„ cacheæ™‚ï¼Œæ–°çš„ dataå¯èƒ½éœ€è¦ç½®æ›æ‰ least recently used (LRU) 
data ã€‚æ²¿ç”¨æ…£ç”¨çš„çš„ terminologyï¼Œæ¯ä¸€æ¬¡çš„ cache missæœƒé€ æˆè©² cache line
ä¸Šç”¢ç”Ÿä¸€å€‹æ–°çš„ generationã€‚ï¼Œç¬¬ ithæ¬¡ missæ‰€å¸¶å…¥çš„ dataæ˜¯è©² cache line
çš„ç¬¬ ith generationã€‚ä¸€å€‹ generationåˆå¯ä»¥é€²ä¸€æ­¥çš„åˆ†å‰²æˆ live periodåŠ
dead periodã€‚å¾ç¬¬ ithæ¬¡çš„ cache missé–‹å§‹åˆ°è©² generationçš„æœ€å¾Œä¸€æ¬¡å­˜å–
ç‚ºæ­¢ï¼Œé€™æ®µæ™‚é–“ç¨±ä¹‹ç‚º live periodã€‚å¾ç¬¬ ith å€‹ generation çš„æœ€å¾Œä¸€æ¬¡å­˜å–
é–‹å§‹åˆ°ç¬¬(i+1) thæ¬¡çš„ cache missç‚ºæ­¢æ˜¯ç¬¬ ith generationçš„ dead periodã€‚
å¾ˆå¤šç ”ç©¶å·²ç¶“æŒ‡å‡º dead periodçš„é•·åº¦å¾€å¾€å¤§æ–¼ live periodçš„é•·åº¦ï¼Œä¸¦ä¸”åœ¨åŒ
ä¸€å€‹ cache lineä¸Šç›¸é„°çš„äºŒæ¬¡ hitå…¶æ™‚é–“é•·åº¦(ç¨±ä¹‹ç‚º: access interval)å¾€å¾€
éå¸¸çŸ­ã€‚ä¸€äº›è«–æ–‡æ¢è¨å¦‚ä½•åˆ¤æ–·ä¸€å€‹ cache line æ˜¯å¦æ­£è™•æ–¼ dead periodã€‚æˆ‘
å€‘ç¨±ä¸€å€‹è¢«åˆ¤å®šç‚ºdeadçš„cache lineç‚ºdeclared-dead cache lineã€‚ç”±æ–¼access 
intervalé å°æ–¼ dead intervalï¼Œå› æ­¤é€™äº›æ¼”ç®—æ³•å¾€å¾€éå¸¸æœ‰æ•ˆç‡ä¸”æ­£ç¢ºã€‚é€™å€‹
ç ”ç©¶çš„ç›®æ¨™å³åœ¨æ–¼å…è¨± mini-processor é–“åˆ†äº«æ­£è™•æ–¼ dead-period çš„ cache 
lineã€‚å› æ­¤äºŒå€‹ mini-processor å¯ä»¥åœ¨ä¸äº’ç›¸å¹²æ“¾çš„æƒ…æ³ä¸‹åˆ†äº«å½¼æ­¤çš„
sub-cacheã€‚ 
The Predictable Cache Design 
åœ¨é€™ç¯‡è«–æ–‡ä¸­ï¼Œæ¯å€‹ data åªå¯ä»¥æ”¾åœ¨ cache ä¸­çš„æŸäº›åœ°æ–¹ï¼ˆ i.e., 
set-associativeï¼‰ã€‚å°æŸå€‹ dataè€Œè¨€ï¼Œé€™äº›å¯ä»¥æ”¾ç½®çš„åœ°æ–¹å°±ç¨±ä¹‹ç‚ºé€™å€‹ data
çš„ corresponding cache setã€‚ç‚ºäº†è®“è®€è€…å¯ä»¥æ¸…æ¥šçš„ç­è§£ predictable cache
çš„æ¦‚å¿µï¼Œå› æ­¤åœ¨é€™ä¸€ç¯€ä¸­ï¼Œå‡è¨­æ¯ä¸€å€‹ physical processor ä¸Šåªæœ‰äºŒå€‹
mini-processorã€‚è®€è€…å¯ä»¥å¾ˆå®¹æ˜“çš„å°‡é€™å€‹æ¦‚å¿µæ“´å±•åˆ°ä¸€å€‹ physical processor
ä¸Šæœ‰ nå€‹ mini-processorçš„æƒ…æ³ã€‚ 
 
 
Figure 1. The cache architecture which supports pri-access and sec-access 
 
ä»¥ Figure 1ä¸­çš„ level 1 cacheç‚ºä¾‹ï¼Œæ¯å€‹ mini-processor mini-piæœ‰ä¸€å€‹å°ˆ
å±¬çš„ sub-cache sub-cacheiã€‚ç•¶ mini-pi å­˜å– memory æ™‚ï¼Œé¦–å…ˆ access 
sub-cacheiï¼Œå¦‚æœ sub-cachei å…§æ²’æœ‰ mini-pi æ‰€è¦å­˜å–çš„ dataï¼Œé‚£éº¼ç·Šæ¥è‘—
mini-piæœƒ access sub-cachej (jâ‰ i)ã€‚å¦‚æœåœ¨é€™äºŒå€‹ sub-cacheä¸­éƒ½æœªæ‰¾åˆ°æ‰€
éœ€ä¹‹ dataï¼ˆi.e., level 1 cache missï¼‰ï¼Œé‚£éº¼ mini-piæœƒåˆ° low-level memory 
system (e.g., L2 cache/memory)ä¸­å°‹æ‰¾æ‰€éœ€çš„ dataã€‚ç•¶ cache controllerå¾
mini-p1 mini-p2
sub-cache1 sub-cache2
L2 cache / memory
1st-access
2nd-access
legend
core
Level 1 cache
Low-level 
memory system
 çµæœèˆ‡è¨è«– 
åœ¨é€™å€‹ç ”ç©¶è¨ˆç•«ä¸­ï¼Œæˆ‘å€‘é¦–å…ˆæŒ‡å‡º SMTåŠ CMPä¸Š cache sharingæ‰€å¯èƒ½é€ æˆçš„å•
é¡Œã€‚å› æ­¤æˆ‘å€‘æå‡ºäº†ä¸€å€‹æ–°çš„ cache sharing algorithmã€‚åœ¨ SMTä¸Šæˆ‘å€‘åŠ å¼·äº†
æ¯å€‹ç¨‹å¼çš„ predictabilityã€‚åœ¨ CMPä¸Šæˆ‘å€‘å…è¨±ä¸åŒçš„ simple processorä¹‹é–“
å…±äº« cacheã€‚å¯¦é©—çµæœé¡¯ç¤ºé€™å€‹æ–¹æ³•çš„ç¢ºéå¸¸æœ‰æ•ˆç‡ã€‚ 
å°±æœªä¾†çš„ç ”ç©¶è€Œè¨€ï¼Œæˆ‘å€‘å¸Œæœ›èƒ½åœ¨ç¡¬é«”ä¸ŠåŒæ™‚çµåˆ predictable cacheåŠ cache 
decayçš„æ¦‚å¿µï¼Œè—‰ä»¥ææ˜‡ cacheçš„ä½¿ç”¨ç‡ä¸¦é™ä½é›»è€— ã€‚é™¤æ­¤ä¹‹å¤– data cacheå’Œ 
instruction cache æˆ–è¨±ä¹Ÿå¯ä»¥å…±äº«åŒä¸€å¡Š cache memory ä»¥æå‡æ•ˆèƒ½ã€‚æœ€å¾Œï¼Œ
æˆ‘å€‘å¸Œæœ›ç™¼å±•æ–°çš„æŠ€è¡“ä½¿å¾— processor (å« coreåŠ cache memory)èƒ½æ›´å……åˆ†çš„æ”¯
æ´ QoSï¼Œä¸¦é™ä½ WCETåŠ average-case execution timeï¼ˆACETï¼‰é–“çš„å·®è·ã€‚ 
æ­¤å¤–ï¼Œç›®å‰å·²æœ‰éƒ¨åˆ†æˆæœç™¼è¡¨æ–¼ ACM çš„æœƒè­°ä¸Š(SAC2005)ï¼Œå¦å¤–é‚„æœ‰ä¸€ç¯‡è«–æ–‡åœ¨
æº–å‚™ä¸­ä¸­é è¨ˆæ–¼ä»Šå¹´æœ«æŠ•ç¨¿ã€‚ 
 
è¨ˆç•«æˆæœè‡ªè©• 
åœ¨é€™æ¬¡çš„è¨ˆç•«ä¸­å¹¾ä¹å®Œæˆäº†åŸæœ¬æ‰€è¨ˆç•«çš„å„å€‹å­é …ç›®ï¼Œé›–ç„¶æœ‰äº›è«–æ–‡ç›®å‰é‚„åœ¨æ’°
å¯«éšæ®µï¼Œä½†å·²ç¶“æœ‰äº†åˆç¨¿åŠå¯¦é©—çµæœï¼Œå†åŠ ä»¥æ•´ç†ï¼ŒæœŸæœ›èƒ½å¤ æœ‰äº›è¡¨ç¾ä»¥ä¸è² åœ‹
ç§‘æœƒæ–¼ç¶“è²»ä¸Šå°æ–¼æˆ‘å€‘å¯¦é©—å®¤çš„æ”¯æŒã€‚ 
é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨äººå“¡åŸ¹è¨“ä¸Šä¹Ÿé”åˆ°é æœŸçš„ç›®æ¨™ï¼Œå¯¦é©—ä¸­å„å€‹æˆå“¡é™¤äº†å°ˆç²¾æ–¼ä½œæ¥­ç³»
çµ±çš„è¨­è¨ˆæ–¼å¯¦ä½œå¤–ï¼Œé‚„å°æ–¼ç¡¬é«”ç‰¹åˆ¥æ˜¯è™•ç†å™¨è¨­è¨ˆèˆ‡ä½œæ¥­ç³»çµ±çš„äº’å‹•æœ‰æ·±åˆ»çš„é«”
èªï¼Œæˆ‘å€‘éƒ½çŸ¥é“ç¡¬é«”è¨­è¨ˆæ˜¯å°ç£çš„å¼·é …ï¼Œåœ¨æœ‰å¦‚æ­¤å¥½çš„å…ˆå¤©ç’°å¢ƒä¸‹ï¼Œæˆ‘å¯„æœ›é€™ç¾¤
å­¸ç”Ÿç•¢æ¥­å¾Œï¼Œèƒ½å¾åƒäºˆé€™æ¬¡åœ‹ç§‘æœƒè¨ˆç•«ä¸­æ‰€ç²å¾—çš„ä¸€äº›çŸ¥è­˜ï¼Œä»¥åŠèªçœŸè² è²¬çš„æ…‹
åº¦å¸¶åˆ°è·å ´ï¼Œå‡¡äº‹åŠªåŠ›è§€å¯ŸèƒŒå¾Œè¨­è¨ˆçš„å‰µæ„åŠå‹•æ©Ÿï¼Œç”¨æœ€æœ‰é–‹å‰µæ€§çš„è¨­è¨ˆæŠ€å·§åŠ
ç†å¿µå°‡ç‚ºä»–å€‘çš„äººç”ŸåŠå°ç£çš„ç”¢æ¥­æ¨å‘å¦ä¸€å€‹é«˜å³°ã€‚  
  	 
   

âˆ—
  
  	
 	   
		 
	   
   !"!!
	
	

  
Although there are many real-time task synchronization pro-
tocols designed for uniprocessor and multiprocessor systems,
most of them do not ï¬t the needs in accommodating si-
multaneous multithreading (SMT). Real-time synchroniza-
tion protocols are expected to bound the maximum number
of priority inversions and to meet task deadlines. Synchro-
nization protocols for simultaneous processing need to ex-
plore the possibility in executing multiple tasks at the same
time to increase the system concurrency level to utilize the
abundant computing resources of simultaneous multithread-
ing computer systems. This work proposes the concept of
â€œLP-Time Inheritanceâ€ to manage the period of blocking
time without signiï¬cantly decreasing the level of task par-
allelism. The schedulability tests for the proposed protocol
are also presented.
	
	  	 	
C.0 [GENERAL]: Hardware/software interfaces; D.4.1 [Process
Management]: Synchronization; C.3 [ SPECIAL-PURPOSE
ANDAPPLICATION-BASED SYSTEMS]: Real-time
and embedded systems
		 	
Design, Performance
	
Real-time, Scheduling, Simultaneously Multithreading
  !"! 
In the past decades, researchers have proposed excellent
task synchronization protocols for the management of prior-
ity inversion and to meet task deadlines. Many of them are
âˆ—The author is supported in part by the National Science
Council, Project No. NSC 94-2213-E-194-017, Taiwan, Re-
public of China.
  	 
 	
  
   
  
	  	  

  
   
	 		   	
	  

	 
  		  	  
 

	
 
 	
	 

 	 	 
 	  	
	  	 	 
   	 	
 	 	    	 		 	 	   
 
 
 
    ! ""# $% &

'	 ""( ') * (+(+ *", "#""-    $(""
proposed for uniprocessor environments, and many of their
variations and new methodologies are explored for multi-
processor systems. With the advance of hardware technol-
ogy, SMT is now widely adopted in many modern micro-
processors. Unfortunately, task synchronization protocols
proposed for multiprocessor systems might not ï¬t the needs
of SMT systems because logical processors (LPs) of an SMT
processor might compete with each another in hardware re-
sources (such as ALU, reorder buï¬€er and so forth). As a
result, tasks running over diï¬€erent (logical) processors of an
SMT processor might interfere with their executions even
though they do not share any logical resources, such as
semaphores. Such an observation motivates the exploring
of proper task synchronization protocols for SMT systems.
Task synchronization for uniprocessor systems has been
an active research topic in the past years. In particular, Sha
and et al. proposed the priority ceiling protocol (PCP) [2]
in which tasks can inherit the priority of a higher-priority
task they block. The lock request of a task is blocked if its
priority is no higher than the priority ceiling of any resource
which is locked by another task, where the priority ceiling of
a resource is the highest priority of the tasks that might lock
the resource. Related protocols based on the same concept
of implicit locking have been proposed by many researchers.
In particular, the stack resource policy (SRP) proposed by
Baker [3] reduces the maximum number of context switch-
ings per task to two and can handle certain dynamic pri-
ority assignments. Oï¬€-line schedulability tests have been
proposed for all of the above protocols.
With the popularity of multiprocessor systems, task syn-
chronization for multiprocessor systems is strongly demanded.
Variations of protocols for uniprocessor systems are pro-
posed for multiprocessor systems, and new methodologies
are presented. For example, MPCP, that is an extension
of PCP, partitions and allocates tasks statically on selected
processors. Synchronization activities of tasks must be per-
formed on the processor in which the corresponding resources
reside. Kuo, et al., [5] proposed an approach called pri-
ority cap to control the number of priority inversions suf-
fered by a higher-priority task in multiprocessor systems.
An important observation in the work is that the locker
can raise the priority ceiling of its semaphore to prevent
lower-priority tasks, which later enter a critical section, from
blocking higher-priority task on another processor. Paolo
Gai, et al. [8] proposed the Multiprocessor Stack Resource
Policy (MSRP) for scheduling tasks and sharing resources
in multiprocessor-on-chip architectures. The major bene-
ï¬t, compared to that of MPCP, is that MSRP allows the
Since there are tasks ready for executions on the two log-
ical processors, both LP1 and LP2 are activated. As shown
in Figure 1.a, task3 locks R successfully at time 4 because
there is no resource locked by any other task. At time 6,
the lock request of task1 is blocked because its priority is
not higher than the priority ceiling of R. As a result, task2
is scheduled for execution on LP1 at time 6 because it is
the only ready task on LP1. Although the dispatching of
task2 seems reasonable for multiprocessor systems, it might
not be a proper decision for SMT systems in terms of pri-
ority inversion management because LPs of the same SMT
processor do compete for hardware resources. Such a com-
petition could result in the lengthening of the blocking time
of task1. An alternative solution is to deactivate LP1 such
that LP2 could utilize all of the hardware resources of the
SMT processor (originally shared with LP1) such that task3
would ï¬nish its critical section that blocks task1 earlier. The
rationale behind the deactivation of LP1 is because the pri-
ority of task1 is higher than that of task2, and task3 now
blocks the execution of task1. Based on the concept of prior-
ity inheritance, the execution of task2 should not delay the
execution of task1 and, thus, the execution of task3. The
activation of LP1 in executing task2 would delay the execu-
tion of task3 and, thus, task1. The decision as to when to
activate or deactivate an LP motivates our work in terms of
the concept for processor-speed inheritance (referred to as
the LP-Time Inheritance for the rest of this paper).
Although the deactivating of LP1 could reduce the block-
ing time of task1 considerably, it unavoidably results in an
unfair treatment on the execution of task2. It is because
the deactivating of LP1 results in the loss of the computing
power (or the servicing capability) of LP1 in servicing its
tasks. In order to compensate for the loss of the servicing
capability of LP1, we propose to return the borrowed proces-
sor speed of LP1 back to LP1 for a proper time interval when
the blocking is ï¬nished (referred to as the LP-Time Return-
ing for the rest of this paper). With LP-Time Inheritance
and Returning, Figure 1.b shows that the blocking time of
task1 could be reduced from 10 time units to 5 time units.
In the next section, we shall use the Priority Ceiling Proto-
col to illustrate the idea in task synchronization. We must
emphasize that the concept of LP-Time Inheritance and Re-
turning could be applied to many other task synchronization
protocols other than the Priority Ceiling Protocol.
& '()$*  +*  * (!!!'
In this section, we assume that the processor speeds of
LPs on the same SMT processor can be adjusted in an ar-
bitrarily way. Note that when it is not possible to do such
an adjustment, we could emulate such an adjustment using
intelligent ways in activating and deactivating LPs. We also
refer interested readers to [9] to the processor speed emula-
tion method.
The priority assignment of each task is in accordance with
the rate-monotonic scheduling algorithm. A task with a
shorter period would be given a higher priority. Let each
task be statically assigned to execute on its LP. The idea
behind the LP-Time Inheritance is to let the LP which ex-
ecutes a task taski that blocks another task taskj inherit
the processor speed of the LP on which taskj is executing.
The idea behind the LP-Time Returning is to return the
inherited amount of computing time (in terms of processor
speeds and the inheritance period) from the beneï¬ted LP
back to the deactivated LP. The revised version of the Pri-
ority Ceiling Protocol by incorporating the concept of LP-
Time Inheritance and Returning (referred to as the LP-Time
Inheritance Protocol) is deï¬ned in the following sections.
& ,	 	- - ,	 '()	 ,		
( .'((/
Let RCi denote the processor speed of each logical pro-
cessor LPi. Suppose that the processor speeds of LPs are
determined initially (although we would modify them based
on the LP-Time Inheritance and Returning concepts). Let
each resource be guarded by one unique semaphore such that
a resource could be accessed by a task only if its correspond-
ing semaphore is locked by the task. The details of LPTIP
are as follows.
 Scheduling Rule of Tasks over Each Logical Processor
LPm:
1. When (an instance of) a task taskx arrives, the priority
of the task is equal to its originally assigned priority.
2. If the highest priority task taskh in LPm is blocked
by another task executing on another LP, all of the
other tasks in LPm which have priorities lower than
the priority of taskh are said being â€œLP-time-blockedâ€
by taskh and remain non-ready for executions until
taskh is not blocked.
3. The CPU scheduler on LPm always selects a ready task
taski with the highest priority for execution. (Tie-
breaking could be done arbitrarily.)
 Allocation Rule of Tasks over Each Logical Processor
LPm:
The following rules are applied when a task taski tries to
lock a resource R. A task is said being in a critical sec-
tion if it locks a semaphore that guards the access of the
corresponding resource. A task leaves a critical section if it
unlocks the corresponding semaphore.
1. A lock request of a taski on a resource R is blocked if
the priority of taski is not higher than the maximum
priority ceiling of resources [2] (i.e., their correspond-
ing semaphores) currently locked by other tasks. If so,
taski is said being directly blocked by the task taskj
that locks the semaphore with the maximum priority
ceiling. Note that taski and taskj might or might not
execute on the same LP.
2. Let RCm and RCn denote the current processor speeds
of LPm and LPn, respectively. When a taski on LPm is
directly blocked by another task taskj on LPn (where
LPm = LPn), tasks executing on LPn would run at
the processor speed equal to (RCm +RCn), and LPm
is deactivated, while the highest priority task on LPm
is lower than the priority of taski. (This is called LP-
Time Inheritance, and transitive inheritance is possible
such that the processor speed of a processor could be
inherited from several processors). The total duration
of the deactivation of LPm, denoted as LendingT imes
n
j ,
is used to speedup the execution of taskj in LPn.
1
1Note that whenever a task with a priority higher than the
priority of taski becomes ready on LPm after LPm is de-
activated (because of LP-Time Inheritance), LPm would be
Lemma 3. The maximum number of local blocking for
any task under LPTIP is one.
Proof. Due to the space limitation, the proof is not given.
Lemma 4. The maximum number of remote blocking for
any task under LPTIP is m, where m is the number of
semaphores locked by the task.
Proof. Due to the space limitation, the proof is not given.
Note that although Lemma 4 shows that the maximum
number of remote blocking for any task is not more than
the number of semaphores locked by the task, the maximum
number is, in fact, bounded by the number of outmost criti-
cal sections for the task (i.e., the number of critical sections
of the task after the removing of nested critical sections).
Lemma 5. Let a task taski on LPm remotely block an-
other task taskj on LPn. The remote blocking would not
introduce any delay to the completion time of taskj if the
priority of taski is higher than that of taskj.
Proof. Since the priority of taski is higher than that
of taskj , taski would not inherit the priority of taskj, and
the other way around. Although LP-Time Inheritance ac-
celerates the execution of taski, the amount of borrowed
computing time is returned right after the ending of the re-
mote blocking such that the completion time of taskj would
be as the same as that of taskj without the occurrence of
remote blocking. 
Lemma 6. Let a task taski on LPm remotely block an-
other task taskj on LPn. The remote blocking time of taskj
is x in the single threading mode if the priority of taskj is
higher than that of taski, and the length of the critical sec-
tion of taski that remotely blocks taskj is x in the single
threading mode.
Proof. Since the priority of taskj is higher than that of
taski, taski inherits the priority of taskj until the remote
blocking ends. As a result, LP-Time Inheritance gives taski
an (extra) amount of computing time equal to x in the single
threading mode to execute with LPm borrowing computing
time from taskj. 
Schedulibility Analysis for Task Sets under LPTIP
Let Tk = {task1, task2, . . . , taskn} be a set of periodic
tasks assigned on LPk, where tasks are listed in the increas-
ing order of their priorities. Each task taski is associated
with a period pi and a computation time ci (in the single
threading mode). Let HPCj be the set of tasks in Tk with
priorities no lower than taskj .
Theorem 3. [2] Task taskj is schedulable under PCP
with the rate-monotonic priority assignment if the following
equation can be satisfied:
X
taskiâˆˆHPCkj
(
ci
pi
) +
bj
pj
â‰¤ m(21/m âˆ’ 1) (1)
where bj and m are the worst case blocking time of taskj
and the number of tasks with priorities no lower than taskj,
respectively. Note that bj is the longest duration of a critical
section that could block taskj.
Theorem 4. Task taskj is schedulable under LPTIP with
the rate-monotonic priority assignment if the following equa-
tion can be satisfied:
X
taskiâˆˆHPCkj
(
ci
pi
) +
bj
pj
Ã— (Î¾j +1) â‰¤ m(21/m âˆ’ 1)Ã—RCk, (2)
where bj , Î¾j, m, and RCk are the worst case blocking time,
the number of semaphores locked by taskj, the number of
tasks in HPCkj , and the processor speed of LPk, respectively.
Proof The correctness of this schedulability analysis fol-
lows directly from Theorem 3 and Lemmas 3, 4, 5, and 6.

Remark 1. The concept of â€œpriority capâ€ [5] proposed by
Kuo and et al can be used to reduce the number of priority
inversions.
0 ! '"! 
This work is motivated by the needs of proper task syn-
chronization protocols for SMT systems. We ï¬rst show that
multiprocessor task synchronization might not ï¬t the design
of SMT systems. We then propose the concept of â€œLP-Time
Inheritanceâ€ to manage the period of blocking time without
signiï¬cantly decreasing the level of task parallelism. The
goal is to balance the task execution parallelism and the
number of priority inversion for each task. The properties
of the protocol are presented. The well-known PCP is taken
as an example to illustrate the ideas. The schedulability
tests for the proposed protocols are also presented.
1 *2** *
[1] R. Rajkumar, L. Sha, J. Lehoczky. â€œReal-Time
Synchronization Protocols for Multiprocessors,â€ IEEE
Real-Time Systems Symposium, pp. 259â€“269, 1988.
[2] L. Sha, R. Rajkumar, J. Lehoczky. â€œPriority
Inheritance Protocols: An Approach to Real-Time
System Synchronization,â€ IEEE Transaction on
Computers, vol. 39, pp. 1175â€“1185, 1990.
[3] T. P. Baker. â€œStack-based scheduling of real-time
processes,â€ Journal of Real-Time Systems, vol. 3, pp.
67â€“99, March 1991.
[4] D. M. Tullsen, J. L. Lo, Susan J. Eggers, H. M. Levy.
â€œSupporting Fine-Grained Synchronization on a
Simultaneous Multithreading Processor,â€
International Symposium on High Performance
Computer Architecture, pp. 54â€“58, 1999.
[5] T. W. Kuo, J. Wu, H. C. Hsih. â€œReal-Time
Concurrency Control in a Multiprocessor
Environment,â€ IEEE Transaction on Parallel and
Distributed Systems, vol. 13, pp. 659â€“671, 2002.
[6] Microsoft Corporation. â€œWindows Support for
Hyper-Threading Technology,â€ White Paper, 2003.
[7] The Linux Kernel Archives. â€œThe Stable Linux Kernel
2.6.9,â€ www.kernel.org.
[8] P. Gai, G. Lipari, M. D. Natale. â€œMinimizing Memory
Utilization of Real-Time Task Sets in Single and
Multi-Processor Systems-on-a-chip,â€ IEEE Real-Time
Systems Symposium, 2001.
[9] S. W. Lo, T. W. Kuo, K. Y. Lam. â€œReal-Time Task
Scheduling for SMT Systems,â€ IEEE Real-Time
Computing Systems and Applications, 2005.
A Preditable Cahe Design for CMP and SMT Proessors
ABSTRACT
Simultaneous multithreading (SMT) and chip-multiprocessor
(CMP) are currently two important trends in the design of
processors. As the processor executes more than two pro-
grams simultaneously, these programs will compete keenly
for the use of the cache. This makes it more difficult to pre-
dict the worst case computation time (WCET), and might
even lead to an overall low efficiency in certain circum-
stances. Even though the traditional cache partitioning
method can solve the foregoing two problems, it reduces the
utility rate of the cache as well as the performance of the
processor. The method presented in this paper allows the
processors to share the cache without affecting each other.
This method could enhance the utility rate of the cache to
xx%, and reduce the interaction of the processors to xx%.
Categories and Subjet Desriptors
D.4.1 [Process Management]: Modeling and prediction;
D.3.3 [Design Styles]: Cache memories; C.3 [ SPECIAL-
PURPOSE AND APPLICATION-BASED SYSTEMS]:
Real-time and embedded systems
General Terms
Algorithms, Design, Performance
Keywords
Worst Case Execution Time (WCET), Real-time Operating
System, Dynamic Cache Partitioning, Simultaneously Mul-
tithreading (SMT), Chip Multiprocessor (CMP), Embedded
System
1. INTRODUCTION
The technologies of SMT [17]/CMP [18] (chip-multithreading
, CMT [20]) have already been proved to be effective and
power-saving designs [14, 15, 16]. Both these two technolo-
gies utilize the simultaneous processing technology for en-
hancing efficiency. The difference between SMT and CMP
lies in that SMT nearly shares all the hardware resources,
while the CMP shares only the hardware resources under
level-1 cache. With the same hardware limitation (i.e., the
limitation of die size), SMT could enhance the utility rate
of hardware resources more effectively, while CMP could
possess better scalability. In the recent researches, itâ€™s indi-
cated that the mixed design of SMT and CMP is the better
choice [?, 19, 20].
The traditional cache design is continuously used in most
of the SMT/CMP processors. However, the memory refer-
ence string (MRS) of SMT/CMP processors is largely dif-
ferent from the MRS of the traditional super-scalar/pipeline
processors. In the instruction level, as SMT/CMP mix the
MRS from different programs, the spatial locality and tem-
poral locality decrease. The multiprogramming technology
of the operating system (OS) could also process several pro-
grams simultaneously. However, for most of the OS, context
switches occur only about 10 âˆ¼ 1, 000 times per second. In
comparing with SMT/CMP, the influence of OS on temporal
locality and spatial locality is quite small.
For system designer, the influences of the changes of lo-
cality could be seen from three aspects. First of all, WCET
becomes very unpredictable. To date, there are lots of re-
searches on the cache behaviors of inter-process [5, 21, 22]
and intra-process [23, 24], and on how to evaluate theWCET
in these two situations. There are researches on how to
â€œroughlyâ€ evaluate the WCET in SMT/CMP [3, 4]. As the
evaluation of WCET is the foundation of many embedded
systems and real-time systems, this will cause difficulties
in the design of software. Secondly, the simultaneous pro-
cessing in the instruction level might result in thrashing [3].
When thrashing occurs, as a great number of cache misses
may be caused in the processor, the number of the instruc-
tions completed per second may reduce suddenly. When
the working sets of the simultaneous processing programs
map to the same cache lines in the cache, as these pro-
grams will overwrite the working sets of each other, it may
lead to thrashing. Technicians have found that in executing
SQL Server and Citrix Terminal Server in the Intel Pentium
4/Xeon with hyper-threading enabled, the efficiency of the
system will be significantly reduced under heavy load. The
reason for the reduction in performance/throughput lies in
that the specialties of SMT processor are not taken into
account in the design of cache[25]. Besides, a malicious pro-
gram could also paralyze the computer by causing thrash-
ing (microarchitectural denial of service [10]). Thirdly, gen-
erally, in OS, itâ€™s assumed that every logical/simple pro-
cessor (mini-processor, mini-p) in the SMT/CMP proces-
memory). When the cache controller brings the data from
low-level memory system to level 1 cache, the newly-brought-
in data will be located in a cache line of the corresponding
cache set in the following sequence: 1. The declared-dead
cache line of the corresponding cache set in the sub-cachei
2. The declared-dead cache line of the corresponding cache
set in the sub-cachej(j 6= i) 3. The newly-brought-in data
will replace the LRU data in the corresponding cache set of
the sub-cachei. In order to maximize the hit probability of
the lst-access, the system will try to locate the MRU data
of the mini-pi in sub-cachei [7].
3.3 Considerations about Real-TimeComput-
ing
The accurate computation of WCET is the foundation of
many real-time scheduling algorithms (i.e., EDF, RM [13]).
The sharing of hardware resources will make it even more
difficult to compute theWCET. The difference between SMT
and CMP lies in that simultaneous processing programs
share the hardware resources in different degrees. The SMT
processor share almost all the hardware resources unrelated
to the context. Currently, there are papers on how to share
these hardware resources so that the program with higher
priority could complete its work with more hardware re-
sources. However, the focuses of these papers are limited to
the pipeline-like hardware resources. For the memory-like
hardware resources, generally, either no control or fully par-
tition is made. The simple processors in the CMP share only
the lower level memory system. Therefore, the major con-
cern of CMP in the aspect of real-time computing support is
how to share the cache among the simultaneous processing
programs.
As there is no knowing of how the cache memories are
shared by the mini-processors (in an SMP or CMP proces-
sors) during the execution phase, in the estimation of WCET
(i.e., profiling phase), we restrain the program processing in
mini-pi to use only the cache memory sub-cachei. In the
profiling phase, the advantages of sharing cache memory
are not taken into account. However, in practical appli-
cation (i.e., execution phase), as the system often contains
real-time and non-real-time applications, the surplus slack
time could be used to improve the QoS, or turn the proces-
sor into power-saving mode. Furthermore, the cache sharing
algorithm proposed in this paper can be extended easily to
allow every sub-cache to have different sizes. As itâ€™s indi-
cated in the research of [26], giving the programs of different
priorities with different sub-cache sizes will contribute to en-
hancing the QoS of the system.
4. CONCLUSION
In this paper, we have first brought up the problems that
might be caused by the cache sharing in SMT and CMP.
Thus, we present a new cache-sharing algorithm. In SMT,
we strengthen the predictability of every program. As for
CMP, we permit different simple processors to share the
cache. From the experiment, itâ€™s shown that this method is
indeed very efficient.
Concerning the future researches, in hardware, we hope
that we could integrate the concepts of predictable cache and
cache decay [1] at the same time to enhance the utility rate
of the cache, and reduce the power consumption. Moreover,
maybe itâ€™s also possible to have the data memory system and
instruction memory system share the same cache memory
so as to enhance the efficiency. Finally, we hope that we
could develop an innovative technology to have the processor
(including core and cache memory) support the QoS more
adequately and also reduce the gap between WCET and
average-case execution time (ACET).
5. REFERENCES
[1] S. Kaxiras, Z. Hu, and M. Martonosi. Cache decay:
exploiting generational behavior to reduce cache
leakage power. In Proc. of the 28th annual Intâ€™l Symp.
on Computer Architecture (ISCAâ€™01), pages 240-251,
2001.
[2] David A. Wood, Mark D. Hill, R. E. Kessler. A model
for estimating trace-sample miss ratios. In Proc. of the
1991 ACM SIGMETRICS, pages 79-89, 1991.
[3] Seongbeom Kim, Dhruba Chandra, Yan Solihin. Fair
Cache Sharing and Partitioning in a Chip
Multiprocessor Architecture. In Proc. Of the 13th
Intâ€™l. Conf. on Parallel Architecture and Compilation
Techniques (PACTâ€™04), pages 111-122, 2004.
[4] Francisco J. Cazorla, Alex Ramrez, Mateo Valero,
Enrique Fernndez. Dynamically Controlled Resource
Allocation in SMT Processors. In Proc. Of the 37th
Symp. On Microarchitecture (MICRO-37â€™04), pages
171-182, 2004.
[5] Y. Tan ,V. Mooney. WCRT Analysis for a
Uniprocessor with a Unified Prioritized Cache. In
Proc. of the 2005 ACM SIGPLAN/SIGBED Conf. on
Languages, Compilers and Tools for Embedded
Systems (LCTESâ€™05), pages 175-182, 2005.
[6] Z. Hu, S. Kaxiras, M. Martonosi. Timekeeping in the
memory system: predicting and optimizing memory
behavior. In Proceedings of the 29th annual intâ€™l symp.
on Computer architecture (ISCAâ€™02), pages 209-220,
2002.
[7] A. Agarwal, S. D. Pudar. Column-associative caches:
A technique for reducing the miss rate of
direct-mapped caches. In Proc. of the 20th Intâ€™l.
Symp. on Computer Architecture (ISCAâ€™93), pages
169-178, 1993.
[8] S. Chappell, J. Stark, S. Kim, S. Reinhardt, Y. Patt.
Simultaneous subordinate microthreading. In Proc. of
the Intâ€™l. Symp. on Computer Architecture (ISCAâ€™99),
pages 186-195, 1999.
[9] James R. Bulpin, Ian A. Pratt. Multiprogramming
Performance of the Pentium 4 with Hyper-Threading.
In the Third Annual Workshop on Duplicating,
Deconstructing and Debunking (WDDD2004), pages
53-62, 2004
[10] Dirk Grunwald, Soraya Ghiasi. Microarchitectural
denial of service: insuring microarchitectural fairness.
In Proc. of the 35th Annual Intâ€™l. Symp. on
Microarchitecture (Microâ€™02), pages 409-418, 2002.
[11] Alex Settle, Joshua L. Kihm, Andrew Janiszewski,
Daniel A. Connors. Architectural Support for
Enhanced SMT Job Scheduling. In Proc. Of the 13th
Intâ€™l. Conf. on Parallel Architecture and Compilation
Techniques (PACTâ€™04), pages 63-73, 2004.
[12] Yan Meng, Timothy Sherwood, Ryan Kastner. On the
Limits of Leakage Power Reduction in Caches. In
Proc. Of Intâ€™l. Symp. On High-Performance Computer
Architecture (HPCA-11), pages 154-165, 2005.
 é™„ä»¶ä¸‰: å‡ºå¸­åœ‹éš›å­¸è¡“æœƒè­°å¿ƒå¾—å ±å‘Š 
