2æ–¹é¢çš„ç ”ç©¶æˆæœï¼Œæ‰€æå‡ºçš„èªéŸ³è®Šæ›æŠ€è¡“ï¼Œå¤§
é«”ä¸Šå¯åˆ†æˆ 6 é¡ï¼Œåˆ†åˆ¥æ˜¯: (a)å‘é‡é‡åŒ–å°æ˜ 
(mapping)[7,8]ã€(b)å…±æŒ¯å³°(formant)é »ç‡å°æ˜ 
[9,10]ã€(c)GMM (Gaussian mixture model)å°æ˜ 
[11,12]ã€(d)ANN (artificial neural network)å°æ˜ 
[13]ã€ (e)å–®å…ƒæŒ‘é¸ (unit selection)[14,15]ã€
(f)HMM (hidden Markov model)å°æ˜ [16,17]ã€‚
ä¸‰ã€ç ”ç©¶æ–¹æ³•
3.1å››å€‹å› ç´ 
ä¸€èˆ¬ä¾†èªªï¼ŒèªéŸ³è®Šæ›ç³»çµ±çš„è£½ä½œéœ€è€ƒæ…®åˆ°
å››å€‹å› ç´ ï¼Œåˆ†åˆ¥æ˜¯èªæ–™å› ç´ ã€é »è­œç‰¹å¾µåƒæ•¸ã€
éŸ³è‰²è®Šæ›æ–¹æ³•ã€ä¿¡è™Ÿåˆæˆæ–¹æ³•ã€‚
é—œæ–¼èªæ–™å› ç´ ï¼Œæˆ‘å€‘æ¡å–å¹³è¡Œèªæ–™çš„æ–¹
å¼ï¼Œå°±æ˜¯è¦æ±‚ä¾†æºèªè€…å’Œç›®æ¨™èªè€…éƒ½å°åŒä¸€ä»½
æ–‡å¥å…§å®¹ç™¼éŸ³ï¼Œå¦‚æ­¤åœ¨ä½œç³»çµ±è¨“ç·´æ™‚ï¼Œæ¯”è¼ƒæ–¹
ä¾¿å»ºç«‹å…©èªè€…ä¹‹é–“çš„é »è­œåƒæ•¸çš„å°æ‡‰é—œä¿‚ã€‚
é—œæ–¼é »è­œç‰¹å¾µåƒæ•¸ï¼Œéå»å¸¸è¢«æ¡ç”¨çš„åŒ…æ‹¬
äº† LPCå°å‡ºçš„ LSF (line spectrum frequency)ç·š
é »è­œé »ç‡ä¿‚æ•¸[18]ï¼ŒLPC å°å‡ºçš„å€’é »è­œä¿‚æ•¸
[19]ï¼Œç”± STRAIGHT[20]é »è­œè¨ˆç®—å‡ºçš„ MFCC
(mel-frequency cepstrum coefficient)æ¢…çˆ¾å€’é »
è­œä¿‚æ•¸ï¼ŒåŠ DCC é›¢æ•£å€’é »è­œä¿‚æ•¸ã€‚åœ¨æœ¬è¨ˆç•«
è£¡ï¼Œæˆ‘å€‘æ¡ç”¨äº† DCC ä¿‚æ•¸ä¾†ä½œç‚ºé »è­œç‰¹å¾µåƒ
æ•¸ã€‚
é—œæ–¼ä¿¡è™Ÿåˆæˆçš„æ–¹æ³•ï¼Œé›–ç„¶ä½¿ç”¨
STRAIGHT [20] ä¾†å°ä¿®æ”¹éçš„é »è­œåƒæ•¸ä½œåˆ
æˆï¼Œå¯ä»¥å¾—åˆ°éå¸¸é«˜å“è³ªçš„åˆæˆèªéŸ³ä¿¡è™Ÿï¼Œä½†
æ˜¯ STRAIGHT ä½œä¿¡è™Ÿåˆæˆçš„è¨ˆç®—é‡éå¸¸å¤§ï¼Œ
è€Œç„¡æ³•åœ¨ä¸€èˆ¬å€‹äººé›»è…¦ä¸Šé”æˆæ¥è¿‘å³æ™‚è™•ç†
çš„è¦æ±‚ï¼Œå› æ­¤æˆ‘å€‘æ¡ç”¨äº†åŸºæ–¼ HNMä¹‹åˆæˆæ–¹
æ³•ã€‚
é—œæ–¼éŸ³è‰²è®Šæ›çš„è™•ç†ï¼Œè§€å¯Ÿè¿‘æœŸè¨±å¤šèªéŸ³
è®Šæ›çš„ç ”ç©¶ï¼Œéƒ½æ¡å–ä»¥ GMMä½œç‚ºåŸºæœ¬çš„å°æ˜ 
æ©Ÿåˆ¶ï¼Œå› æ­¤æˆ‘å€‘ä¹Ÿæ±ºå®šä»¥ GMMå°æ˜ æ©Ÿåˆ¶ç‚ºåŸº
ç¤ï¼Œå†åŠ ä»¥æ”¹é€²ã€‚GMMå°æ˜ æ©Ÿåˆ¶çš„ä¸€å€‹é‡è¦
å•é¡Œï¼Œå°±æ˜¯è®Šæ›å‡ºçš„é »è­œåƒæ•¸æœƒæœ‰é »è­œåŒ…çµ¡
(spectral envelope)éæ–¼å¹³æ»‘(over-smoothed)çš„
ç¾è±¡[21, 22]ï¼Œæ‰€ä»¥æœƒæœ‰èªéŸ³å“è³ªé€€åŒ–çš„æ„Ÿè¦ºã€‚
ä¸€å€‹å…¸å‹çš„åŸºæ–¼ GMMçš„å°æ˜ å‡½æ•¸ï¼Œå…¶å…¬
å¼å¦‚ä¸‹[11]:
( ; , )y F x ï­ï€½ ï™ ï€½
ï€¨ ï€©ï€¨ ï€©1
1
1
( ; , )
( )
( ; , )
x xxM
y yx xx xm m m
m m m mM
x xxm
m m m
m
w N x
x
w N x
ï­ ï­ ï­
ï­
ï€­
ï€½
ï€½
ïƒ© ïƒ¹
ïƒª ïƒºïƒ— ï™ ïƒ¦ ïƒ¶ïƒª ïƒºï€«ï™ ïƒ—ï™ ïƒ— ï€­ïƒ§ ïƒ·ïƒª ïƒºïƒ¨ ïƒ¸ïƒ— ï™ïƒª ïƒº
ïƒª ïƒºïƒ« ïƒ»
ïƒ¥
ïƒ¥
(1)
å…¶ä¸­ x è¡¨ç¤ºä¾†æºèªè€…çš„é »è­œç‰¹å¾®å‘é‡ï¼Œy è¡¨ç¤º
è®Šæ›å¾Œå¾—åˆ°çš„é »è­œç‰¹å¾®å‘é‡ï¼ŒM æ˜¯é«˜æ–¯æ··åˆ
N(ï‚·,ï‚·,ï‚·)çš„ç¸½æ•¸ï¼Œè€ŒÎ¼åŠÎ¨åˆ†åˆ¥è¡¨ç¤ºå¹³å‡å‘é‡
èˆ‡å…±è®Šç•°çŸ©é™£çš„é›†åˆã€‚æˆ‘å€‘èªç‚ºæŠŠå…¨éƒ¨èªæ–™æ”¾
åœ¨ä¸€èµ·ï¼Œå»è¨“ç·´ä¸€å€‹ GMMæ¨¡å‹çš„ M (å¦‚ 128)
å€‹é«˜æ–¯æ··åˆçš„åƒæ•¸ï¼Œå®¹æ˜“ç™¼ç”Ÿä¸€ç¨®ç¾è±¡ï¼Œå°±æ˜¯
ä¸€å€‹é«˜æ–¯åˆ†ä½ˆç‚ºäº†ç…§é¡§å…©ç¨®éŸ³ç´ çš„é »è­œç‰¹
æ€§ï¼Œè€Œä½¿å¾—é«˜æ–¯åˆ†ä½ˆçš„ä¸­å¿ƒè½åœ¨å…©éŸ³ç´ 
(phoneme)ä¹‹é–“ï¼Œä¹Ÿä½¿å¾—é«˜æ–¯åˆ†ä½ˆè®Šå¾—å¹³ç·©ã€‚
å› æ­¤ï¼Œæˆ‘å€‘æå‡ºä¸€å€‹è§£æ±ºè¾¦æ³•ï¼Œå°±æ˜¯å…ˆæŠŠè¨“ç·´
èªæ–™è£¡çš„å„å€‹éŸ³æ¡†ä¾å…¶ä¾†æºç™¼éŸ³çš„éŸ»æ¯ä½œåˆ†
ç¾¤ï¼Œç„¶å¾Œå†åˆ†åˆ¥å°å„ç¾¤å»è¨“ç·´å‡ºè‡ªå·±çš„
GMMï¼Œå¦‚æ­¤å„å€‹ GMM å°±å¯å¿ å¯¦åœ°ä»£è¡¨å®ƒæ‰€
å°æ‡‰éŸ»æ¯çš„é »è­œç‰¹æ€§ã€‚
3.2 æ¨¡å‹è¨“ç·´éšæ®µ
åœ¨è¨“ç·´éšæ®µçš„ä¸»è¦è™•ç†æ­¥é©Ÿå¦‚åœ– 1 æ‰€
ç¤ºã€‚æˆ‘å€‘é‚€è«‹äº†ä¸‰ä½éŒ„éŸ³è€…ï¼Œåˆ†åˆ¥åˆ°éš”éŸ³éŒ„éŸ³
å®¤ä¾†éŒ„è£½ 375 å¥ä¹‹å¹³è¡Œèªæ–™ï¼Œå–æ¨£ç‡è¨­ç‚º
22,050Hzï¼Œå…¶ä¸­äºŒä½æ˜¯ç”·æ€§ï¼Œåœ¨æ­¤ä»¥ M1 å’Œ
M2ä½œä»£è™Ÿï¼Œè€Œå¦ä¸€ä½æ˜¯å¥³æ€§ï¼Œä»¥ F1ä½œä»£è™Ÿã€‚
åœ¨æœ¬è¨ˆç•«è£¡ï¼Œæˆ‘å€‘æŠŠM1ç•¶ä½œä¾†æºèªè€…ï¼Œè€ŒæŠŠ
M2å’Œ F1åˆ†åˆ¥ä½œç‚ºç›®æ¨™èªè€…ï¼Œä¹Ÿå°±æ˜¯è¦æŠŠM1
çš„èªéŸ³è®Šæ›æˆM2åŠ F1çš„èªéŸ³ã€‚375å€‹è¨“ç·´èª
å¥å…±å¯æ“·å–å‡º 2,926å€‹éŸ³ç¯€éŸ³æª”ï¼Œæˆ‘å€‘å†ä¾éŸ³
æª”æª”åä¸­çš„éŸ»æ¯æ‹¼éŸ³ç¬¦è™Ÿï¼Œå°‡é€™äº›éŸ³ç¯€éŸ³æª”åˆ†
æˆ 37ç¾¤ã€‚
3.2.1 DCC ä¿‚æ•¸è¨ˆç®—
åœ¨æœ¬è¨ˆç•«è£¡ï¼Œæˆ‘å€‘æ¡ç”¨é›¢æ•£å€’é »è­œä¹‹é »è­œ
åŒ…çµ¡ä¼°è¨ˆæ–¹æ³•ï¼Œä¸¦ä¸”ä»¥é›¢æ•£å€’é »è­œä¿‚æ•¸(DCC)
ä½œç‚ºé »è­œåƒæ•¸ã€‚å°æ–¼ä¸€å€‹èªéŸ³éŸ³æ¡†ï¼Œæˆ‘å€‘ä½¿ç”¨
å…ˆå‰ç™¼å±•çš„ DCC ä¼°è¨ˆç¨‹å¼ä¾†è¨ˆç®—å‡º 40 ç¶­çš„
DCCä¿‚æ•¸ï¼Œåœ¨æ­¤ä¸€å€‹éŸ³æ¡†çš„é•·åº¦è¨­ç‚º 512å€‹æ¨£
æœ¬é»(23.2ms)ï¼Œè€ŒéŸ³æ¡†ä½ç§»å‰‡è¨­ç‚º 110 å€‹æ¨£æœ¬
é»(5ms)ã€‚
3.2.2 åˆ†æ®µå¼ GMM ä¹‹è¨“ç·´
åœ¨åœ– 1 ä¸­ç¶“ç”±æ–¹å¡Š â€Grouping into 37 
classesâ€ çš„è™•ç†ä¹‹å¾Œï¼Œå°æ–¼å„ç¾¤çš„éŸ³ç¯€ç‰‡æ®µ
(segment)ï¼Œæˆ‘å€‘å°±åˆ†åˆ¥æ‹¿å»è¨“ç·´å‡ºä¸€å€‹ç”± 16
4HNM based
speech synthesis
Pitch
adjusting
Selecting a GMM
Mapping with
single mixture
Estimating DCC
Converted
voice
Detect pitch freq.
Unknown spoken
sentence
Framing
åœ– 2 è®Šæ›éšæ®µä¹‹ä¸»è¦è™•ç†æµç¨‹
æ¡†åŒ…å¤¾)ï¼Œç•¶ä½œä¸€å€‹æ‰¹æ¬¡ä¾†ä½œå–®ä¸€é«˜æ–¯æ··åˆé¸
å–çš„è™•ç†ã€‚æ¥è‘—åœ¨åœ– 2 è£¡å·¦å³æµç¨‹åˆä½µä¹‹æ–¹
å¡Š â€HNM based speech synthesisâ€ï¼Œæˆ‘å€‘ä½¿ç”¨ä¸€
å€‹åŸºæ–¼ HNMçš„ä¿¡è™Ÿåˆæˆæ–¹æ³•ï¼Œå»ä¾æ“šè®Šæ›å‡º
çš„ DCC ä¿‚æ•¸åŠéŸ³é«˜é »è­œï¼ŒæŠŠèªéŸ³ä¿¡è™Ÿå†åˆæˆ
å‡ºä¾†ã€‚
3.3.1 åˆ†æ®µ GMM ä¹‹é¸å–æ–¹æ³•
å°æ–¼ä¸€å€‹ç·šä¸Šè™•ç†çš„èªéŸ³è®Šæ›ç³»çµ±ä¾†
èªªï¼Œè¼¸å…¥èªéŸ³çš„èªªè©±å…§å®¹æ˜¯äº‹å…ˆä¸çŸ¥é“çš„ï¼Œå› 
æ­¤ç•¶è¦å°ä¸€å€‹éŸ³æ¡†çš„ DCC ä¿‚æ•¸ä½œå°æ˜ æ™‚ï¼Œæˆ‘
å€‘å¦‚ä½•çŸ¥é“ 37 å€‹ GMM ç•¶ä¸­çš„é‚£ä¸€å€‹æ‡‰è¢«é¸
å–?é€™æ¨£çš„å•é¡Œå¿…é ˆå…ˆè¢«è§£æ±ºï¼Œè€Œè©²å•é¡Œæ˜¯ä¸€
ç¨®èªéŸ³è¾¨è­˜çš„å•é¡Œï¼Œä¸éå®ƒä¸éœ€è¦åƒèªéŸ³è¾¨è­˜
é‚£æ¨£åš´å²åœ°è¢«å°å¾…ï¼Œå› ç‚ºé¸å–åˆ°éŒ¯èª¤ä½†è¿‘ä¼¼çš„
GMMæ˜¯å¯ä»¥å®¹å¿çš„ã€‚
åœ¨èªéŸ³è¾¨è­˜é ˜åŸŸï¼Œéš±è—å¼é¦¬å¯å¤«æ¨¡å‹
(hidden Markov model, HMM)æ˜¯æœ€å¸¸è¢«æ¡ç”¨çš„
çµ±è¨ˆæ¨¡å‹ï¼Œä¸éåœ¨æ­¤æˆ‘å€‘å¸Œæœ›ä»¥æ‰€è¨“ç·´å‡ºçš„ 37
å€‹ GMMä¾†å–ä»£ HMMçš„è§’è‰²ï¼Œå¦‚æ­¤å°±ä¸éœ€å¦
å¤–å»è¨“ç·´ HMMã€‚æ­¤å¤–ï¼Œæˆ‘å€‘è§€å¯Ÿåˆ°ä¸€å€‹éå¸¸
æ¥è¿‘çœŸå¯¦çš„ç¾è±¡æ˜¯ï¼Œä¸€å€‹äººä¸å¯èƒ½åœ¨ä¸€å€‹å¾ˆçŸ­
æš«çš„æ™‚é–“å¦‚ 100msä¹‹å…§ï¼Œç™¼å‡ºå¤šæ–¼ 2å€‹çš„èªéŸ³
ç‰‡æ®µ(åœ¨æ­¤èªéŸ³ç‰‡æ®µæŒ‡çš„æ˜¯éŸ³ç¯€)ã€‚æ‰€ä»¥ï¼Œæˆ‘å€‘
æ±ºå®šæŠŠæ¯ 20å€‹é€£çºŒçš„æœ‰è²éŸ³æ¡†(å«è“‹ 100msä¹‹
æ™‚é–“ç¯„åœ)ä½œç‚ºä¸€å€‹æ‰¹æ¬¡ï¼Œå»ä½œ 20å€‹éŸ³æ¡†æ•´æ‰¹
çš„ GMMé¸å–ä¹‹è™•ç†ï¼Œå¦‚æ­¤ä¸€å€‹æ‰¹æ¬¡è£¡å°±åªéœ€
é¸å‡ºä¸€å€‹æˆ–äºŒå€‹çš„ GMMã€‚æœ¬è«–æ–‡ç ”ç™¼äº†ä¸€å€‹
DP ç‚ºåŸºç¤çš„ GMM æŒ‘é¸ä¹‹æ¼”ç®—æ³•ï¼Œè©²æ¼”ç®—æ³•
æœƒä¾æ“šæœ€å¤§ä¼¼ç„¶ç‡(maximum likelihood)å»é¸
å‡ºä¸€å€‹æˆ–äºŒå€‹ GMMã€‚
ä»¤ç¬¬ t å€‹è¼¸å…¥éŸ³æ¡†çš„ DCC ä¿‚æ•¸æ˜¯ç”±ç¬¬ s
å€‹ GMMæ‰€ç”¢ç”Ÿçš„æ©Ÿç‡æ˜¯ Gt(s)ï¼Œå…¶è©³ç´°è¨ˆç®—å…¬
å¼ç‚º
ï€¨ ï€©
1
( ) = ( ) ; ( ), ( ) ,ï­
ï€½
ïƒ— ï™ïƒ¥
M
x xx
t m t m m
m
G s w s N x s s (3)
å…¶ä¸­ Wm(s)è¡¨ç¤ºç¬¬ m å€‹é«˜æ–¯æ··åˆçš„åŠ æ¬Šï¼Œxtè¡¨
ç¤ºç¬¬ t å€‹éŸ³æ¡†çš„ DCC å‘é‡ã€‚æ­¤å¤–ï¼Œä»¤ R(t, s)
è¡¨ç¤ºå¾æ™‚åˆ» 1 åˆ°æ™‚åˆ» t çš„éŸ³æ¡†éƒ½æ˜¯ç”±ç¬¬ s å€‹
GMMæ‰€ç”¢ç”Ÿçš„ä¼¼ç„¶ç‡å°æ•¸å€¼ï¼Œè€Œä»¤ D(t, s)è¡¨
ç¤ºå¾æ™‚åˆ» 1åˆ°æ™‚åˆ» tçš„éŸ³æ¡†æ˜¯ç”± 2å€‹ GMMæ‰€
ç”¢ç”Ÿï¼Œä¸¦ä¸”ç¬¬ tå€‹éŸ³æ¡†æ˜¯ç”±ç¬¬ så€‹ GMMæ‰€ç”¢
ç”Ÿçš„ä¼¼ç„¶ç‡å°æ•¸å€¼ã€‚ä¾æ“šå‰è¿°çš„å®šç¾©ï¼Œæˆ‘å€‘å¯
ä»¥æ¨å°å‡ºå¦‚ä¸‹çš„å…©å€‹éè¿´å…¬å¼:
ï€¨ ï€©( , ) log ( ) ( 1, ) ,tR t s G s R t sï€½ ï€« ï€­ (4)
ï€¨ ï€©( , ) log ( )tD t s G sï€½ ï€«
ï› ï
0 37,
max max ( 1, ) , ( 1, ) ,
v v s
R t v D t s
ï‚£ï€¼ ï‚¹
ïƒ¬ ïƒ¼ï€­ ï€­ïƒ­ ïƒ½
ïƒ® ïƒ¾
(5)
å…¶æ‰€éœ€è¨­å®šçš„é‚Šç•Œå€¼æ˜¯ï¼ŒD(1, s)=0 å’Œ R(1,
s)=G1(s)ï¼Œs=0, 1, ..., 36ã€‚æ¥è‘—ï¼Œä¾æ“šå…¬å¼(4)å’Œ
(5)ï¼Œæˆ‘å€‘å¯å¾—åˆ° Tå€‹éŸ³æ¡†æ•´é«”çš„æœ€å¤§ä¼¼ç„¶ç‡ç‚º
ï› ï ï› ïï» ï½0 37 0 37( ) max max ( , ) , max ( , ) ,v vA T R T v D T vï‚£ï€¼ ï‚£ï€¼ï€½ (6)
å…¶ä¸­ T åœ¨æœ¬è«–æ–‡è£¡è¨­ç‚º 20ã€‚åœ¨ä¾æ“šå…¬å¼(4)ï¼Œ
(5)å’Œ(6)å¾—åˆ° A(20)ä¹‹æœ€å¤§ä¼¼ç„¶ç‡æ•¸å€¼ä¹‹å¾Œï¼Œæˆ‘
å€‘å¯ä½œå›æº¯(backtrack)è™•ç†ï¼Œå»æ‰¾å‡º A(20)æ•¸å€¼
çš„æœ€ä½³è¡Œèµ°è·¯å¾‘ï¼Œè€Œå¾—åˆ° 20 å€‹éŸ³æ¡†å„è‡ªæ‰€è¢«
æŒ‡æ´¾çš„ GMMç·¨è™Ÿã€‚
6çš„DCCä¿‚æ•¸ä½œå°æ˜ ã€‚å¦å¤–ï¼Œåœ¨ä»£è™Ÿ SSG (system
using single Gaussian mixture for mapping)çš„ç³»
çµ±è£¡ï¼Œæˆ‘å€‘ä»ç„¶ä½¿ç”¨ 350å€‹èªå¥æ‰€è¨“ç·´å‡ºçš„ä¸€
å€‹å…·æœ‰ 256å€‹é«˜æ–¯æ··åˆçš„ GMMï¼Œä¸éåœ¨è®Šæ›
éšæ®µï¼Œ3.3.2ç¯€è£¡èªªæ˜çš„é«˜æ–¯æ··åˆé¸å–æ–¹æ³•è¢«ç”¨
ä¾†ç‚ºä¸€åºåˆ—çš„è¼¸å…¥éŸ³æ¡†é¸å–å‡ºå„éŸ³æ¡†çš„å–®ä¸€
é«˜æ–¯æ··åˆï¼Œç„¶å¾Œå„è¼¸å…¥éŸ³æ¡†çš„ DCC ä¿‚æ•¸å°±ä½¿
ç”¨æ‰€é¸å‡ºçš„å–®ä¸€é«˜æ–¯æ··åˆåŠå…¬å¼(7)ä¾†ä½œå°
æ˜ ã€‚è‡³æ–¼åœ¨ä»£è™Ÿ SLG (system using selected
GMM for mapping)çš„ç³»çµ±è£¡ï¼Œæˆ‘å€‘é¦–å…ˆä»¥ 350
å€‹èªå¥ä¾†è¨“ç·´å‡º 37å€‹åˆ†æ®µå¼ GMMï¼Œè€Œå„åˆ†æ®µ
å¼ GMM éƒ½åªæœ‰ 16 å€‹é«˜æ–¯æ··åˆï¼Œç„¶å¾Œåœ¨è®Šæ›
éšæ®µï¼Œæˆ‘å€‘æ¡ç”¨ 3.3.1 ç¯€è£¡èªªæ˜çš„ GMM é¸å–
æ–¹æ³•ï¼Œä¾†ç‚ºæ¯ 20 å€‹æœ‰è²éŸ³æ¡†é¸å–å‡ºæœ€å¤§ä¼¼ç„¶
ç‡çš„ä¸€å€‹æˆ–å…©å€‹åˆ†æ®µ GMMï¼Œæ¥è‘—æ¡ç”¨ 3.3.2
ç¯€è£¡çš„é«˜æ–¯æ··å’Œé¸å–æ–¹æ³•ï¼Œä¾†ç‚ºå„è¼¸å…¥éŸ³æ¡†é¸
å–å‡ºå–®ä¸€å€‹é«˜æ–¯æ··åˆï¼Œå†ä¾æ“šå…¬å¼(7)ä½œå°æ˜ ã€‚
ç•¶æŠŠä¸€å€‹ä¾†æºèªè€…çš„ç™¼éŸ³æª”ï¼Œåˆ†åˆ¥è¼¸å…¥åˆ°
å‰è¿°çš„ä¸‰å€‹èªéŸ³è®Šæ›ç³»çµ±ï¼Œæˆ‘å€‘å°±å¯å¾—åˆ°ä¸‰å€‹
è®Šæ›å‡ºèªéŸ³æª”ã€‚ç„¶å¾Œä½¿ç”¨è®Šæ›å‡ºçš„éŸ³æª”ï¼Œæˆ‘å€‘
é€²è¡Œäº†å…©ç¨®é¡å‹çš„è½æ¸¬å¯¦é©—ï¼Œåˆ†åˆ¥æ˜¯éŸ³è‰²ç›¸ä¼¼
åº¦ä¹‹è½æ¸¬ã€å’ŒèªéŸ³å“è³ªä¹‹è½æ¸¬ã€‚åœ¨é€™äºŒé¡å‹çš„
è½æ¸¬å¯¦é©—è£¡ï¼Œæˆ‘å€‘éƒ½é‚€è«‹äº† 25 ä½äººå£«ä¾†è†è½
éŸ³æª”ä¸¦çµ¦äºˆç›¸å°åˆ†æ•¸ï¼Œè€Œåœ¨é€™ 25 ä½äººå£«ä¸­ï¼Œ
æœ‰ 20ä½æ˜¯ä¸ç†Ÿæ‚‰èªéŸ³è®Šæ›ä¹‹ç ”ç©¶çš„ã€‚
4.1 éŸ³è‰²ç›¸ä¼¼åº¦æ¸¬è©¦
é¦–å…ˆæˆ‘å€‘æº–å‚™äº† 5å€‹éŸ³æª”ï¼Œå®ƒå€‘çš„ä»£è™Ÿåˆ†
åˆ¥æ˜¯ VS(ç”±ä¾†æºèªè€…ç™¼éŸ³)ï¼ŒVT(ç”±ç›®æ¨™èªè€…ç™¼
éŸ³)ï¼ŒVX1(ç¶“ç”± SOGç³»çµ±è®Šæ›å¾—åˆ°)ï¼ŒVX2(ç¶“
ç”± SSG ç³»çµ±è®Šæ›å¾—åˆ°)ï¼ŒVX3(ç¶“ç”± SLG ç³»çµ±
è®Šæ›å¾—åˆ°)ï¼Œå…¶ä¸­ VSèˆ‡ VTå…·æœ‰ç›¸åŒçš„èªªè©±å…§
å®¹ï¼Œè€Œ VX1ã€VX2å’Œ VX3ä¸‰è€…ä¹Ÿæœ‰ç›¸åŒçš„å…§
å®¹ï¼Œä½†ä¸åŒæ–¼ VSå’Œ VTçš„ï¼Œé€™ 5å€‹éŸ³æª”å¯å¾
ç¶²é  http://guhy.csie.ntust.edu.tw/VoiceConv/å»
ä¸‹è¼‰ã€‚åœ¨é€²è¡Œè½æ¸¬å¯¦é©—æ™‚ï¼Œæˆ‘å€‘ä»¥ ABX çš„æ¬¡
åºä¾†æ’¥æ”¾å‰è¿°çš„éŸ³æª”ï¼Œåœ¨æ­¤ A å›ºå®šç‚º VSï¼ŒB
å›ºå®šç‚º VTï¼Œè€Œ Xå‰‡éš¨æ©Ÿç”± VX1ã€VX2å’Œ VX3
ä¸‰è€…ä¸­é¸å‡ºï¼Œæ¯æ¬¡ä»¥ ABX æ¬¡åºæ’­æ”¾å®ŒéŸ³æª”
å¾Œï¼Œå—æ¸¬è€…å°±è¢«è¦æ±‚çµ¦ä¸€å€‹åˆ†æ•¸ã€‚åœ¨æ­¤åˆ†æ•¸çš„
å®šç¾©æ˜¯ï¼Œ9åˆ†(æˆ– 1åˆ†)è¡¨ç¤º Xçš„éŸ³è‰²ç¢ºå®šå°±æ˜¯
B(æˆ– A)çš„éŸ³è‰²ï¼Œ7åˆ†(æˆ– 3åˆ†)è¡¨ç¤º Xçš„éŸ³è‰²æ¯”
è¼ƒæ¥è¿‘ B(æˆ– A)çš„éŸ³è‰²ï¼Œè€Œ 5åˆ†è¡¨ç¤º Xçš„éŸ³è‰²
ç„¡æ³•åˆ¤æ–·æ˜¯æ¥è¿‘ Aæˆ–æ¥è¿‘ Bã€‚
åšå®Œè½æ¸¬å¯¦é©—ä¹‹å¾Œï¼Œ25 ä½å—æ¸¬è€…æ‰€çµ¦çš„
åˆ†æ•¸è¢«ç”¨ä¾†è¨ˆç®—å‡ºä¸‰å€‹ç³»çµ±å„è‡ªçš„å¹³å‡åˆ†æ•¸
(AVG)å’Œæ¨™æº–å·®(STD)ï¼Œæ‰€å¾—åˆ°çš„åˆ†æ•¸æ•¸å€¼å°±
å¦‚è¡¨ 1æ‰€åˆ—å‡ºçš„ã€‚ç”±è¡¨ 1çš„å¹³å‡åˆ†æ•¸å¯çŸ¥ï¼Œä¸
åŒæ€§åˆ¥ä¹‹é–“çš„èªéŸ³è®Šæ›(å³å¾M1åˆ° F1)ï¼Œæœƒæ¯”
åŒæ€§åˆ¥ä¹‹é–“çš„(å³å¾ M1 åˆ° M2)ç²å¾—æ˜é¡¯è¼ƒé«˜
çš„åˆ†æ•¸ã€‚æ­¤å¤–ï¼Œæ‹¿ä¸‰å€‹ç³»çµ±çš„å¹³å‡åˆ†æ•¸ä½œæ¯”
è¼ƒï¼Œå¯å¾è¡¨ä¸€çš„æ•¸å€¼å¾—çŸ¥ï¼ŒSLGç³»çµ±çš„è¡¨ç¾æ˜
é¡¯æ¯” SSGç³»çµ±çš„å¥½è¨±å¤š(7.05 vs 6.24ï¼Œ7.60 vs
7.24)ï¼Œè€Œ SSGç³»çµ±çš„è¡¨ç¾å‰‡æ˜¯æ¯” SOGç³»çµ±çš„
ç¨å¾®å¥½ä¸€äº›(6.24 vs 6.08ï¼Œ7.24 vs 6.92)ã€‚æ‰€ä»¥
æœ¬è¨ˆç•«æå‡ºçš„åˆ†æ®µå¼ GMM ä¹‹è§€å¿µåŠè‡ªå‹•
GMMæŒ‘é¸ä¹‹æ¼”ç®—æ³•ï¼Œçš„ç¢ºå¯å¹«å¿™æ”¹é€²æ‰€è®Šæ›
å‡ºèªéŸ³çš„éŸ³è‰²ç›¸ä¼¼åº¦ã€‚
è¡¨ 1ã€éŸ³è‰²ç›¸ä¼¼åº¦è½æ¸¬ä¹‹å¹³å‡åˆ†æ•¸èˆ‡æ¨™æº–å·®
SOG SSG SLG
AVG 6.08 6.24 7.05M1=>M2
STD 1.11 1.09 0.93
AVG 6.92 7.24 7.60M1=>F1
STD 1.13 1.07 1.10
4.2 èªéŸ³å“è³ªæ¸¬è©¦
åœ¨æ­¤æˆ‘å€‘ä½¿ç”¨ä¸‰å€‹ç³»çµ±è®Šæ›å‡ºçš„èªéŸ³æª”
VX1ã€VX2 å’Œ VX3ï¼Œä¾†é€²è¡ŒèªéŸ³å“è³ªçš„è½æ¸¬
å¯¦é©—ã€‚éŸ³æª”æ’¥æ”¾çš„æ¬¡åºç‚º AXï¼ŒA å›ºå®šè¨­ç‚º
VX1ï¼Œè€Œ X å‰‡éš¨æ©Ÿç”± VX2 å’Œ VX3 å…©è€…ä¸­å–
å‡ºï¼Œæ¯æ¬¡ä»¥ AXæ¬¡åºæ’­æ”¾å®ŒéŸ³æª”å¾Œï¼Œå—æ¸¬è€…å°±
è¢«è¦æ±‚çµ¦ä¸€å€‹åˆ†æ•¸ã€‚åœ¨æ­¤åˆ†æ•¸çš„å®šæ‡‰æ˜¯ï¼Œ9åˆ†
(æˆ– 1 åˆ†)è¡¨ç¤º X çš„èªéŸ³å“è³ªæ˜é¡¯æ¯” A çš„å¥½(æˆ–
å·®)ï¼Œ7åˆ†(æˆ– 3åˆ†)è¡¨ç¤º Xçš„å“è³ªæ¯” Açš„ç¨å¾®å¥½
(æˆ–å·®)ä¸€äº›ï¼Œ5åˆ†å‰‡è¡¨ç¤º Xå’Œ Açš„èªéŸ³å“è³ªç„¡
æ³•åˆ†è¾¨å„ªåŠ£ã€‚
ä½œå®Œè½æ¸¬å¯¦é©—ä¹‹å¾Œï¼Œæˆ‘å€‘æ”¶é›† 25 ä½å—æ¸¬
è€…æ‰€çµ¦çš„åˆ†æ•¸ï¼Œä¾†è¨ˆç®—å‡º SSGå’Œ SLGå…©ç³»çµ±
å„è‡ªçš„å¹³å‡åˆ†æ•¸å’Œæ¨™æº–å·®ï¼Œçµæœå¾—åˆ°çš„æ•¸å€¼å¦‚
è¡¨ 2è£¡åˆ—å‡ºçš„ã€‚ä¾æ“šè¡¨ 2çš„å¹³å‡åˆ†æ•¸å¯çœ‹å‡ºï¼Œ
åŒæ€§åˆ¥ä¹‹é–“(å³å¾ M1 åˆ° M2)çš„è®Šæ›èªéŸ³çš„å“
è³ªï¼Œæœƒæ¯”ä¸åŒæ€§åˆ¥ä¹‹é–“(å³å¾M1åˆ° F1)çš„è¼ƒå¥½
ç´„ 0.5åˆ†ï¼Œé€™é¡¯ç¤ºä¸åŒæ€§åˆ¥ä¹‹é–“çš„è®Šæ›èªéŸ³çš„
å“è³ªï¼Œæ˜¯æ¯”è¼ƒé›£ä½œæ”¹é€²çš„ã€‚æ­¤å¤–ï¼Œä¾æ“š SLG
å’Œ SSG å…©ç³»çµ±çš„å¹³å‡åˆ†æ•¸ä½œæ¯”è¼ƒï¼Œæˆ‘å€‘å¯çœ‹
å‡º SLG çš„åˆ†æ•¸éƒ½æ¯” SSG çš„é«˜ç´„ 0.7 åˆ†ï¼Œä¸¦ä¸”
SLG çš„å¹³å‡åˆ†æ•¸éƒ½é«˜æ–¼ 5 åˆ†ï¼Œæ‰€ä»¥åˆ†æ®µå¼
GMMä¹‹è§€å¿µåŠè‡ªå‹•æŒ‘é¸ GMMä¹‹æ¼”ç®—æ³•ï¼Œç¢º
å¯¦å¯ç”¨ä»¥æ”¹é€²æ‰€è®Šæ›å‡ºèªéŸ³çš„èªéŸ³å“è³ªã€‚
8Techniques for Discrete Cepstrum Estimation",
IEEE Signal Processing Letters, Vol. 3, No. 4,
pp. 100-102, April 1996.
[7] M. Abe, S. Nakamura, K. Shikano, and H.
Kuwabara, â€œVoice Conversion through Vector 
Quantization,â€ International Conference on
Acoustics, Speech, and Signal Processing, New
York, Vol. 1, pp. 655-658, Apr. 1988.
[8] S. Nakamura and K. Shikano, â€œSpectrogram 
Normalization Using Fuzzy Vector
Quantizationâ€, J. Acoust. Soc., Japan, Vol. 45,
pp. 107-114, 1989.
[9] H. Mizuno and M. Abe, â€œVoice Conversion 
Algorithm Based on Piecewise Linear
Conversion Rules of Formant Frequency and
Spectrum Tiltâ€, Speech Communication, Vol. 16,
No. 2, pp. 153-164, 1995.
[10] å³å˜‰å½§ã€ç‹å°å·ï¼Œâ€ä¸éœ€å¹³è¡Œèªæ–™è€ŒåŸºæ–¼å…±æŒ¯
å³°èˆ‡ç·šé »è­œé »ç‡æ˜ å°ä¹‹èªè€…ç‰¹è³ªè®Šæ›ç³»çµ±â€ï¼Œ
ç¬¬äºŒåä¸€å±†è‡ªç„¶èªè¨€èˆ‡èªéŸ³è™•ç†ç ”è¨æœƒ
(ROCLING 2009)ï¼Œå°ä¸­ï¼Œç¬¬ 319-332é ï¼Œ2009ã€‚
[11] Stylianou Y., CappÂ´e O., Moulines
E, â€Continuous Probabilistic Transform for
Voice Conversion,â€ IEEE trans. Speech and
Audio Processing, Vol. 6, No. 2, pp.131â€“142,
1998.
[12] Min Chu, â€œVoice Conversion with Smoothed
GMM and MAP Adaptationâ€, Proc. of
EuroSpeech, Geneva, Switzerland, pp.
2413-2416, 2003.
[13] Srinivas Desaiy, et al.,â€œVoice Conversion Using 
Artificial Neural Networks,â€ ICASSP, Taipei,
Taiwan, pp.3893â€“3896, 2009.
[14] Zhiwei Shuang, Fanping Meng, and Yong Qin,
â€œVoice Conversion by Combining Frequency
Warping with Unit Selectionâ€, ICASSP, Las
Vegas, U.S.A , pp.4661-4664, 2008.
[15] D. Sundermann, et al., â€œText Independent Voice 
Conversion Based on Unit Selectionâ€, ICASSP,
pp. 81-84, Toulouse, France, 2006.
[16] åŠ‰å¾·è³¢ï¼Œæ‡‰ç”¨é›™å¯å¤«æ¨¡å‹èˆ‡è²éŸ³è®Šæ›æ–¼æƒ…ç·’
èªéŸ³åˆæˆä¹‹ç ”ç©¶ï¼Œç¢©å£«è«–æ–‡ï¼Œåœ‹ç«‹æˆåŠŸå¤§å­¸
è³‡è¨Šå·¥ç¨‹ç ”ç©¶æ‰€ï¼Œ2005ã€‚
[17] E. K. Kim, S. Lee, and Y. H. Oh, â€œHidden 
Markov Model Based Voice Conversion Using
Dynamic Characteristics of Speakerâ€, Proc.
EuroSpeech, Vol. 5, Rhodes, Greece, 1997.
[18] A. Kain and M.W. Macon, â€œSpectral Voice 
Conversion for Text-to-speech Synthesisâ€, IEEE
ICASSP, Seattle, Vol. 1, pp. 285-288, May
1998.
[19] K. S. Lee, â€œStatistical Approach for Voice 
Personality Transformation,â€ IEEE trans. Audio,
Speech, and Language Processing, Vol. 15, No.
2, pp. 641-651, Feb. 2007.
[20] H. Kawahara, I. Masuda-katsuse and A. De
Cheveign, â€œRestructuring Speech Represen-
tations Using a Pitch-adaptive Time-frequency
Smoothing and an Instantaneous-frequency-
based F0 Extraction: Possible Role of a
Repetitive Structure in Soundsâ€, Speech
Communication, Vol. 27, pp. 187-207, 1999.
[21] T. Toda, H. Saruwatari, and K. Shikano, â€œVoice 
Conversion Algorithm Based on Gaussian
Mixture Model with Dynamic Frequency
Warping of STRAIGHT Spectrumâ€, ICASSP, 
Salt Lake City, pp. 841-844, May 2001.
[22] M. Zhang, J. Tao, H. Jia, and X.
Wang, â€Improving HMM Based Speech 
Synthesis by Reducing Over-Smoothing
Problemsâ€, International Symposium on Chinese
Spoken Language Processing (ISCSLP),
Kunming, China, Dec. 2008.
[23] R. A. Redner and H. F. Walker, â€œMixture 
densities, maximum likelihood and the EM
algorithm,â€ SIAM Review, vol. 26, no. 2, pp.
195-239, 1984.
[24] H. Y. Kim, et al., â€œPitch detection with average 
magnitude difference function using adaptive
threshold algorithm for estimating shimmer and
jiter,â€ 20-th Annual Int. Conf. of the IEEE
Engineering in Medicine and Biology Society,
Hong Kong, China, 1998.
[25] H. Y. Gu and S. F. Tsai, â€œA discrete-cepstrum
based spectrum-envelope estimation scheme and
its example application of voice transformation,â€ 
International Journal of Computational
Linguistics and Chinese Language Processing,
vol. 14, no. 4, pp. 363-382, 2009.
2é™¤äº†å‰è¿°çš„è«–æ–‡ç™¼è¡¨ session ä¹‹å¤–ï¼Œæˆ‘é‚„åƒåŠ äº†å¦å¤–å…©å€‹ sessionsï¼Œå³ Array & Multi-channel
Signal Processingã€Blind and Adaptive Signal Processingï¼Œå…¶å®ƒä¿¡è™Ÿè™•ç†çš„ç›¸é—œ sessionsï¼Œå‰‡ç”±æ–¼
å¹³è¡Œ session ä¹‹å®‰æ’æ–¹å¼è€Œç„¡æ³•åƒèˆ‡ã€‚
åœ¨è¡Œç¨‹æ–¹é¢ï¼Œæ–¼ 10 æœˆ 20 æ—¥æ­ä¹˜ 11:45 ç”±æ¾å±±æ©Ÿå ´ç›´é£›ä¸Šæµ·æµ¦æ±æ©Ÿå ´çš„ç­æ©Ÿï¼Œç„¶å¾Œæ­ä¹˜é•·
é€”å·´å£«å‰å¾€ä½æ–¼è˜‡å·åŸå…§çš„æœƒå ´(å³åœ‹éš›æœƒè­°ä¸­å¿ƒé£¯åº—)ï¼Œåˆ°é”æœƒå ´æ™‚å¤§ç´„æ˜¯ 17:40ã€‚åƒåŠ ç ”è¨æœƒ
å¾Œï¼Œå‰‡æ–¼ 23 æ—¥ä¸­åˆæ­ä¹˜å·´å£«å‰å¾€ä¸Šæµ·è™¹æ©‹æ©Ÿå ´ç¬¬ä¸€èˆªç«™ï¼Œä¸¦ä¸”åœ¨èˆªç«™é™„è¿‘çš„æ—…é¤¨ä½å®¿ä¸€æ™šï¼Œç„¶
å¾Œåœ¨éš”æ—¥æ¸…æ™¨ 8:05ï¼Œè¶•æ­ç”±è™¹æ©‹æ©Ÿå ´ç›´èˆªå°åŒ—æ¾å±±æ©Ÿå ´çš„ç­æ©Ÿã€‚
äºŒã€èˆ‡æœƒå¿ƒå¾—
é›–ç„¶ WCSP 2010 ç ”è¨æœƒæ¥å—æŠ•ç¨¿çš„é ˜åŸŸåŒ…å«äº†ç„¡ç·šé€šä¿¡èˆ‡ä¿¡è™Ÿè™•è£¡ä¹‹ç›¸é—œé ˜åŸŸï¼Œä½†æ˜¯æ‰€é‚€
è«‹çš„äº”ä½ keynote speaker çš„æ¼”è¬›é¡Œç›®ï¼Œéƒ½é›†ä¸­æ–¼é€šä¿¡é ˜åŸŸçš„è­°é¡Œã€‚æ‰€ä»¥ï¼Œå€‹äººæ„Ÿè¦ºä¿¡è™Ÿè™•ç†èˆ‡
èªéŸ³è™•ç†ä¸¦ä¸æ˜¯æ­¤æ¬¡ç ”è¨æœƒçš„ç„¦é»ï¼Œä¸éæŠ•ç¨¿ä¿¡è™Ÿè™•ç†é ˜åŸŸçš„è«–æ–‡æ•¸é‡ç›¸å°åœ°å°‘å¾ˆå¤šï¼Œæ‡‰ä¹Ÿæ˜¯
ä¸€å€‹é‡è¦çš„åŸå›  ã€‚
åœ¨ Speech and Audio Signal Processing å ´æ¬¡ï¼Œå…±æœ‰ 8 ç¯‡è«–æ–‡ç™¼è¡¨ï¼Œå…¶ä¸­æœ‰ 7 ç¯‡æ˜¯å±¬æ–¼èªéŸ³è™•
ç†çš„ã€‚é™¤äº†æˆ‘çš„è«–æ–‡ä¹‹å¤–ï¼Œå…¶å®ƒè«–æ–‡ä¸­æœ‰å…©ç¯‡æ˜¯ä½œèªéŸ³ç·¨ç¢¼(speech coding)çš„ï¼Œæœ‰ä¸€ç¯‡æ˜¯ä½œèªéŸ³
å¼·åŒ–(speech enhancement)çš„ï¼Œä¸€ç¯‡æ˜¯ä½œåŸºé€±åµæ¸¬çš„ï¼ŒåŠèªè€…è¾¨è­˜ç­‰ã€‚é›–ç„¶èªéŸ³è™•ç†æ–¹é¢çš„ç¯‡æ•¸
ä¸å¤šï¼Œä½†æ˜¯ä¹Ÿå«è“‹äº†å¹¾å€‹èªéŸ³ç ”ç©¶çš„å­é ˜åŸŸï¼Œå› æ­¤ä»å¯ç›¸äº’äº†è§£ä¸åŒå­é ˜åŸŸè£¡çš„ç ”ç©¶æƒ…æ³ã€‚
inverse DFT (IDFT). Then, the logarithmic magnitude-
spectrum can be computed with the cepstrum coefficients as
2 1
0 / 2
1
2log ( ) 2 cos( ) cos( ),
0 1.
N
n N
n
X k c c k n c k
N
k N
 


  
  
 (1)
If most terms at the right side of (1) are cancelled except the
leading p+1 terms, the magnitude spectrum computed, log S(f),
would be a smoothed version of the original, log|X(f)|. Here, the
index variable, k, in (1) is replaced with f in order to change the
frequency scale from bins to the normalized frequency range
from 0 to 1. Accordingly, log S(f) is computed as
0 11 2
0
1
log ( ) 2 cos(2 ) , , , , ..., .
p
N
n N N N N
n
S f c c f n f 

    (2)
Based on (2), some researchers proposed to approximate the
spectral envelope of log|X(f)| with log S(f). Nevertheless, the
coefficients, cn, in (2) cannot be derived directly with IDFT.
One derivation method proposed by Galas and Rodet is to
define a set of envelope constraints, and find the values of the
coefficients, cn, that can best satisfy the envelope constraints. In
this manner, the derived coefficients, cn, n=0, 1, , p, are
called the discrete cepstrum for log|X(f)|.
The envelope constraints just mentioned are actually L pairs
of (fk, ak) for L representative spectral peaks selected from the
original spectrum log|X(f)|. Here, fk and ak represent the
frequency (already normalized to between 0 and 1) and
amplitude of the k-th spectral peak, respectively. Note that L is
usually larger than the cepstrum order, p. Hence, a least-
squares criterion is adopted to minimize the approximation
errors between S(fk) and ak, k L. In matrix form, the
optimal values of the DCCs is derived by previous researchers
[6, 7] to be
T 1 T( )C M M M A    (3)
where A = [log(a1), log(a2), , log(aL)]T, C = [c0, c1, , cp]T,
and
1 1 11 2cos(2 ) 2cos(2 2) 2cos(2 )
1 2cos(2 ) 2cos(2 2) 2cos(2 )L L L
f f f p
M
f f f p
  
  
  	

  
 

   

    

B. Regularization of Discrete Cepstrum
When (3) is used to derived DCCs, the spectral envelope
computed according to (2) may vibrate radically and have very
large approximation error at some frequencies slightly away the
selected spectral-peak frequencies, fk. This is because the direct
estimation method (i.e. Eq. (3)) may sometimes be ill-
conditioned. That is, slightly varying the frequency values of
the detected spectral peaks may result in a very different
spectral envelope curve being obtained. Therefore, CappÃ© and
Moulines proposed a regularization technique to prevent such
radical vibrations from occurring [7]. They add a curve-
sharpness penalty term, i.e.
 
2
0
( ) ( ) ,dR S f S f df
df
  	
 
 
 
 (4)
to the approximation error calculation equation, and the
resulted equation for deriving DCCs becomes
T 1 T( ) .C M M U M A    (5)
where  is a weighting parameter (suggested value is around
0.0001), and
2
2
2
0 0
1
8 .
0
U
p

 	

 

 

 

 
 

(6)
III. SELECTION OF SPECTRAL PEAKS
 are derived by minimizing the summation of squared
errors between the selected spectral peaks, ak, k=1, 2,, L, and
S(f). Therefore, selecting appropriate spectral peaks from a
DFT spectrum is an important preprocessing step. Consider a
simplest selection method, i.e. locate and select all the spectral
peaks on the spectrum as the final selected peaks. In this case,
the approximated spectral envelope would be very bad and of
large approximation error. When such bad spectral envelopes
are used to transform voice signals, the output obtained will
suffer significant voice-quality degradation.
Therefore, we studied this problem and found that the
concept of MVF (maximum voiced frequency) proposed in
HNM (harmonic-plus-noise model) [9, 10] is utilizable. The
MVF of a DFT spectrum is searched by testing the sharpness of
the spectral peaks one after another. After some low-frequency
spectral peaks pass the test, it will eventually occur that no
more spectral peak can pass the test. Then, the frequency of the
last spectral peak passing the test is defined to be the MVF. In
this paper, we first detect if a signal frame is voiced or
unvoiced. If it is detected to be voiced, the frame is further
searched for the MVF value, fv, by using the searching method
proposed by Stylianou [10]. According to fv, the DFT spectrum
of the frame is split into the lower-frequency harmonic part and
the higher-frequency noise part. Then, for the harmonic part,
the first spectral peak of a frequency within the range (0.5F0,
1.5F0), where F0 is the detected fundamental frequency, is
searched for. Let the obtained frequency and amplitude be f1
and a1. Next, the second spectral peak of a frequency within the
range (f1+0.5F0, f1+1.5F0) is searched for, and let the results
be f2 (frequency) and a2 (amplitude). When going on in this
manner, we can find the frequencies and amplitudes of the
other spectral peaks within the harmonic part. Sometimes, it
may occur that no spectral peak is found within a designated
frequency range. In this situation, we will right shift the
frequency range, i.e. adding 0.5F0, and try to find again.
For the noise part of a voiced frame, the searching method
explained above for the harmonic part cannot be adopted. Note
that the harmonic structure becomes obscure in the noise part,
and the frequency gaps between adjacent peaks become
randomly varied. For an example, inspect the DFT spectrum
curve beyond 5,800Hz in Fig. 2. Therefore, we adopt another
method to find the spectral peaks for the noise part. In this
method, a smoothed spectral curve is computed first by
truncating the real-cepstrum coefficients outside the leading 30
ones, and transforming (via DFT) the resulted real-cepstrum
( ) log(1 ) ,
1,750
fscl f   (8)
where f is in the unit Hz. This conversion function will have the
scaled frequency value, f , growing more slowly with f at the
low frequency end when it is used as the scl() function for (7).
The three curves shown in Fig. 5 are obtained by taking
Bark, mel, and our frequency conversions as the scl() function
for (7), respectively. From Fig. 5, it can be seen that our
frequency conversion as given in (8) can indeed grow the
scaled frequency f more slowly with the linear frequency f.
By using the frequency conversion function of (8), the
approximated spectral envelope in Fig. 3 will become the one
drawn in Fig. 6. According to the spectral envelope in Fig. 6, it
can be said that the frequency conversion function proposed
can indeed eliminate the over vibration phenomenon at the low
frequency end, and reduce the local approximation error around
3,000Hz. The reducing of the local approximation error we
think is due to the increased vibrating capability around
3,000Hz by using the proposed frequency conversion instead of
the mel-frequency conversion.
Hz
Fig. 5. Three curves of scaled frequencies by using Bark, mel, and our
frequency conversion functions, respectively.
Fig. 6. Envelop approximated in our freq. scale with 40 DCCs.
C. Approximation Error Comparsion
It may be queried whether our frequency conversion
function is just better than mel-frequency conversion for certain
signal frames. Therefore, we decide to compare the
approximation errors of the two frequency conversions in the
three frequency ranges, i.e. 0 ~ 2,000Hz, 0 ~ 4,000Hz, and 0 ~
6,000Hz. Here, approximation error is measured with the
formula,
-1 ( )
10 101=0
1 1 20 log 20 log ( , )
( )
Nr L t t
k kktEs a S t fNr L t 
 	
    
 
 
 (9)
where Nr is the total number of signal frames, and L(t) is the
number of spectral peaks, for the t-th frame, dynamically
determined to ensure that only the spectral peaks of frequencies
within the currently concerned frequency range are counted.
Here, 375 Mandarin sentences consisting of 2,925 syllables
recorded from a male are used as the testing speech. After all
frames of the testing speech are processed, the approximation
errors measured in different frequency ranges and different
discrete cepstrum orders are illustrated in Fig. 7.
Inspecting the error curve in Fig. 7, it can be seen that
across the cepstrum-order numbers from 30 to 50, our
frequency conversion and the mel-frequency conversion have
almost same approximation errors in the frequency range, 0 ~
2,000Hz. Nevertheless, in the other two frequency ranges, our
conversion function will apparently obtains smaller
approximation errors for different cepstrum-order numbers.
This decreasing of approximation error becomes more apparent
as the frequency range becomes wider.
(a) Frequency range: 0 ~ 2,000Hz
(b) Frequency range: 0 ~ 4,000Hz
(c) Frequency range: 0 ~ 6,000Hz
Fig. 7. Approximation errors measured for our frequency conversion
and mel-frequency conversion in the three frequency ranges.
V. AN EXAMPLE APPLICATION: TIMBRE TRANSFORMATION
Here, voice transformation is meant to change the timbre of
an input voice to a different timbre. For example, change the
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«è¡ç”Ÿç ”ç™¼æˆæœæ¨å»£è³‡æ–™è¡¨
æ—¥æœŸ:2011/10/18
åœ‹ç§‘æœƒè£œåŠ©è¨ˆç•«
è¨ˆç•«åç¨±: éŸ³è³ªæ”¹é€²ä¹‹èªéŸ³è®Šæ›ç³»çµ±
è¨ˆç•«ä¸»æŒäºº: å¤é´»ç‚
è¨ˆç•«ç·¨è™Ÿ: 99-2628-E-011-107- å­¸é–€é ˜åŸŸ: è‡ªç„¶èªè¨€è™•ç†èˆ‡èªéŸ³è™•ç†
ç„¡ç ”ç™¼æˆæœæ¨å»£è³‡æ–™
å…¶ä»–æˆæœ 
(ç„¡æ³•ä»¥é‡åŒ–è¡¨é”ä¹‹æˆ
æœå¦‚è¾¦ç†å­¸è¡“æ´»å‹•ã€ç²
å¾—çé …ã€é‡è¦åœ‹éš›åˆ
ä½œã€ç ”ç©¶æˆæœåœ‹éš›å½±éŸ¿
åŠ›åŠå…¶ä»–å”åŠ©ç”¢æ¥­æŠ€
è¡“ç™¼å±•ä¹‹å…·é«”æ•ˆç›Šäº‹
é …ç­‰ï¼Œè«‹ä»¥æ–‡å­—æ•˜è¿°å¡«
åˆ—ã€‚) 
ç„¡ 
 æˆæœé …ç›® é‡åŒ– åç¨±æˆ–å…§å®¹æ€§è³ªç°¡è¿° 
æ¸¬é©—å·¥å…·(å«è³ªæ€§èˆ‡é‡æ€§) 0  
èª²ç¨‹/æ¨¡çµ„ 0  
é›»è…¦åŠç¶²è·¯ç³»çµ±æˆ–å·¥å…· 0  
æ•™æ 0  
èˆ‰è¾¦ä¹‹æ´»å‹•/ç«¶è³½ 0  
ç ”è¨æœƒ/å·¥ä½œåŠ 0  
é›»å­å ±ã€ç¶²ç«™ 0  
ç§‘ 
æ•™ 
è™• 
è¨ˆ 
ç•« 
åŠ  
å¡« 
é … 
ç›® è¨ˆç•«æˆæœæ¨å»£ä¹‹åƒèˆ‡ï¼ˆé–±è½ï¼‰äººæ•¸ 0  
 
