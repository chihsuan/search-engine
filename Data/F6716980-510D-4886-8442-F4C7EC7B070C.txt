 2
____________________________________ 
ï¨ˆæ”¿é™¢åœ‹å®¶ç§‘å­¸å§”å“¡æœƒå°ˆé¡Œç ”ç©¶è¨ˆç•«æœŸæœ«å ±å‘Š 
ç”±éƒ¨ï¤˜æ ¼è‡ªå‹•åµæ¸¬èˆˆè¶£ä¸¦å»ºè­°è³¼ç‰©é¡˜æœ›æ¸…å–® 
Interest Detection and Wish-List Suggestion from a Blog Space 
è¨ˆç•«ç·¨è™Ÿï¼šNSC 96-2221-E-019 -038 -MY2 
åŸ·ï¨ˆæœŸé™ï¼š96 ï¦ 08 æœˆ 01 æ—¥è‡³ 98 ï¦ 07 æœˆ 31 æ—¥ 
ä¸»æŒäººï¼šï§´å·å‚‘ åœ‹ï§·å°ç£æµ·æ´‹å¤§å­¸è³‡è¨Šå·¥ç¨‹å­¸ç³» 
è¨ˆç•«ï¥«èˆ‡äººå“¡ï¼š é™³å¨å®‡ã€è©¹å˜‰ä¸ã€éƒ­è‚²æ—»ã€è¶™å“éŠœã€å¾æ¦®å½¬ã€
ï§‡å¥•ç·¯ã€æ¥Šå®—è±« 
 
ä¸€ã€ä¸­æ–‡æ‘˜è¦ 
 
éƒ¨ï¤˜æ ¼ (blog) å·²æ˜¯é€™å¹¾ï¦ï¤­ç¶²ï¤·ä¸Šæœ€
ç†±é–€çš„è©±é¡Œï¼Œè¿‘å¹¾ï¦ï¤­å·²æœ‰å¤§ï¥¾çš„ç¶²ï¤·ä½¿ç”¨
è€…æŠ•å…¥éƒ¨ï¤˜æ ¼çš„å»ºï§·å’Œå¯«ä½œé¢¨æ½®ä¹‹ä¸­ã€‚è€Œä¸”
å­¸è¡“ç ”ç©¶ä¸­ï¼Œå¦‚ä½•å°‡æŠ€è¡“æ‡‰ç”¨åœ¨éƒ¨ï¤˜æ ¼ä¸Šï¼Œ
ç²å–æœ‰ï§çš„è³‡è¨Šï¼Œä¹Ÿæˆç‚ºç†±é–€ç ”ç©¶é¡Œç›®ã€‚ç•¢
ç«Ÿèƒ½æŒæ¡å¤§å¤šï¥©çš„ç¶²ï¤·ä½¿ç”¨è€…ï¼Œå°±èƒ½æŒæ¡ç¶²
ï¤·å¸‚å ´å…ˆæ©Ÿã€‚ 
æœ¬è¨ˆç•«æ“¬ä»¥ï¥¸ï¦çš„æ™‚é–“ï¼Œç ”ç©¶å¦‚ä½•ç”±ä¸€
å€‹éƒ¨ï¤˜æ ¼ç¶²ç«™ä¸­åŒ…å«çš„å„é …è³‡è¨Šï¼ŒåŒ…æ‹¬éƒ¨ï¤˜
æ ¼ä¸»äººçš„å€‹äººåŸºæœ¬è³‡ï¦¾ã€ä»–æ‰€ç™¼è¡¨çš„åœ¨éƒ¨ï¤˜
æ ¼ä¸Šçš„æ–‡ç« ç­‰ç­‰ï¼Œè‡ªå‹•ç‚ºé€™ä½éƒ¨ï¤˜æ ¼ä¸»äººå»º
æ§‹å‡ºä»–çš„è³¼ç‰©é¡˜æœ›æ¸…å–®ï¼Œäº¦å³ä»–åœ¨è¿‘æœŸå…§å¯
èƒ½å¸Œæœ›è³¼è²·æˆ–æœ‰èˆˆè¶£è³¼è²·çš„å•†å“æ¸…å–®ã€‚é€™ç¨®
è‡ªå‹•å»ºè­°é¡˜æœ›æ¸…å–®çš„ç³»çµ±ï¼Œï¥§ï¥å°ä½¿ç”¨è€…æˆ–
æ˜¯æ¥­è€…ï¨¦æœ‰å¹«åŠ©ã€‚è€Œç™¼å±•éç¨‹ä¸­æ‰€éœ€å…‹æœçš„
æŠ€è¡“ï¼Œä¹Ÿå°å­¸è¡“ç ”ç©¶æœ‰æ‰€è²¢ç»ã€‚ 
 
é—œéµè©ï¼šéƒ¨ï¤˜æ ¼ã€é¡˜æœ›æ¸…å–®ã€èˆˆè¶£åµæ¸¬ã€è‡ª
ç„¶èªè¨€è™•ï§¤ 
 
Abstract 
 
Blog (weblog) has been a hot topic in 
recent years. The number of users building 
their own blogs has grown rapidly. It is also hot 
to apply technologies on blogs to develop 
useful systems. Large amount of users and 
huge amount of data means a great opportunity 
in the market on the Internet. 
This two-year project plans to study how 
to automatically suggest a wish list for a 
blogger by all kinds of information available in 
his blog, including personal profile, contents of 
articles in his blog, and so on. A wish list is a 
list of products for which he may like to buy 
recently. Such a wish-list-suggestion system is 
useful both for bloggers and companies. 
Research for developing such a system is also 
helpful for researchers. 
 
Keywords: blog, wish list, interest detection, 
natural language processing 
 
äºŒã€ç·£ç”±èˆ‡ç›®çš„ 
 
éƒ¨ï¤˜æ ¼ (blog) å·²æ˜¯é€™å¹¾ï¦ï¤­ç¶²ï¤·ä¸Šæœ€
ç†±é–€çš„è©±é¡Œã€‚åªè¦ä½ æœ‰é›»è…¦ï¼Œèƒ½ï¦šæ¥ä¸Šç¶²éš›
ç¶²ï¤·ï¼Œæœƒæ‰“å­—ï¼Œä½ å°±å¯ä»¥åœ¨ç¶²ï¤·ä¸–ç•Œçš„ä¸€å€‹
è§’ï¤˜ç›¡æƒ…å¯«ä½œï¼Œåˆ†äº«è‡ªå·±çš„ç”Ÿæ´»é»æ»´ã€å°ˆæ¥­
èˆˆè¶£ã€å¿ƒæƒ…åŠå‰µä½œã€‚è€Œé¾å¤§çš„ç¶²å‹æ—ç¾¤çš†å¯
ï¤­é–±ï¥šä½ çš„éƒ¨ï¤˜æ ¼ï¼Œå’Œä½ åšç¶“é©—äº¤ï§Šã€‚ï¤å¯
å°‡åˆ¥äººçš„æ–‡ç« å¼•ç”¨è‡³è‡ªå·±çš„éƒ¨ï¤˜æ ¼ä¸Šï¼ŒåŠ é€Ÿ
ï¦ºéƒ¨ï¤˜å®¢ (blogger, éƒ¨ï¤˜æ ¼çš„ä¸»äºº) ä¹‹é–“çš„
ï¤…ï¦šï¼Œï¤è®“é€™è‚¡å¯«ä½œèˆ‡ç¶“é©—åˆ†äº«çš„é¢¨æ½®å¸¶åˆ°
æœ€é«˜é»ã€‚ 
æ ¹æ“šç¾åœ‹éƒ¨ï¤˜æ ¼æœå°‹å¼•æ“Technorati1æŒ‡
å‡ºï¼Œéƒ¨ï¤˜æ ¼ä»¥æ¯å¤©æ–°å¢ 2 è¬ 3,000 å€‹çš„é©šäºº
é€Ÿï¨æˆé•·è‘—ã€‚ç¾åœ‹ç¶²ï¤·èª¿æŸ¥æ©Ÿæ§‹Perseusä¹ŸæŒ‡
å‡ºï¼Œå…¨çƒéƒ¨ï¤˜æ ¼çš„ï¥©ï¥¾ 5 ï¦å…§ï¥©ï¥¾æˆé•·è¶…é
1,000 å€ï¼Œé ä¼°å…¨çƒéƒ¨ï¤˜æ ¼ï¥©ï¥¾åˆ° 2006 ï¦ï¦
åº•å°‡é”åˆ° 5,340 è¬å€‹ã€‚è€Œåœ¨è‡ºç£æœ€å¤§çš„ï¥¸å€‹
éƒ¨ï¤˜æ ¼ç©ºé–“ï¼Œç„¡åå°ç«™2å®£ç¨±æœ‰ 300 è¬ä»¥ä¸Šçš„
æœƒå“¡ï¼ŒXuite3ä¹Ÿæ—©åœ¨å»ï¦ä¸­çªç ´ 100 è¬å€‹ä½¿ç”¨
è€…ã€‚ç”±æ­¤å¯çŸ¥ï¼Œéƒ¨ï¤˜æ ¼ä½¿ç”¨è€…ï¥©ï¥¾ï¥§å¯å°è¦·ã€‚ 
ä¹Ÿå› æ­¤ï¼Œéƒ¨ï¤˜æ ¼ä¸€å‘ç‚ºå•†æ¥­ç•Œæ‰€æ³¨æ„ã€‚
é¾å¤§çš„ä½¿ç”¨äººç¾¤è±¡å¾µè‘—é¾å¤§çš„å•†æ©Ÿã€‚ç„¶è€Œè¦
1 http://www.technorati.com/search/ 
2 http://www.wretch.cc/ 
3 http://www.xuite.net/ 
 4
____________________________________ 
ã€ŒASUSã€ã€ã€ŒAcerã€ç­‰å„å» ç‰Œã€‚ 
(å››) éç´°çš„å­åˆ†ï§ï¼šåƒæ˜¯ã€ŒT æ¤ã€é‚„åˆ†æˆã€ŒçŸ­
è¢– T æ¤ã€ã€ã€Œé•·è¢– T æ¤ã€ã€ã€Œï¦šå¸½ T æ¤ã€ã€‚
æˆ‘å€‘èªç‚ºèˆˆè¶£èˆ‡å•†å“çš„é—œï¦—æ‡‰è©²ï¥§æœƒåˆ†
åˆ°é€™éº¼ç´°ï¼Œå› æ­¤å¸Œæœ›èƒ½åˆªé™¤æ‰é€™ç¨®éç´°
çš„å­åˆ†ï§ã€‚ 
(äº”) å…¶ä»–ï§ï¼šæœ‰çš„ï§åˆ¥æ¨™ç±¤æ˜¯èˆ‡ã€Œå…¶ä»–ã€åŒ
ç¾©ï¼Œåƒæ˜¯ã€Œå…¶ä»–ä¸»æ©Ÿæ¿ã€ã€ã€Œå…¶ä»–æ£’ï¥Š
çƒç”¨å“ã€ï¼Œé€™ä¹Ÿæ‡‰è©²åˆªé™¤ã€‚ 
 
é‡å°ä¸Šè¿°ç¨®ç¨®æƒ…å½¢ï¼Œæˆ‘å€‘æ¡ç”¨ï¦ºåº•ä¸‹å¹¾å€‹æ–¹
æ³•ï¼ŒæœŸæœ›èƒ½è‡ªå‹•åˆªå»å¤§éƒ¨ä»½ï¥§è¦çš„å­—ï¤…ï¼Œä¹Ÿ
å¸Œæœ›ï¥§æœƒæœ‰èª¤åˆªçš„æƒ…å½¢ã€‚ 
 
(1) æ‰€æœ‰åŒ…å« "å…¶ä»–" é€™å€‹å­å­—ï¤…çš„ï§åˆ¥å
ç¨±å…¨éƒ¨åˆªé™¤ï¼Œï¦µå¦‚ã€Œå…¶ä»–ä¸»æ©Ÿæ¿ã€ã€ã€Œå…¶
ä»–æ£’ï¥Šçƒç”¨å“ã€ã€‚ 
(2) ç•¶ä¸€å€‹å…¶ä»–ï§æ¨™ç±¤åœ¨æ•´å€‹å•†å“åˆ†ï§æ¶æ§‹
ä¸­å‡ºç¾é 5 æ¬¡ä»¥ä¸Šï¼Œå®ƒåœ¨å„å±¤çš„å…„å¼Ÿï§
åˆ¥ä¹Ÿå…¨éƒ¨åˆªé™¤ã€‚ï¦µå¦‚ã€Œå…¶ä»–å» ç‰Œã€å‡ºç¾
ï¦º 24 æ¬¡ï¼Œå®ƒçš„å…„å¼Ÿç¯€é»å€‘æ­£æ˜¯å„å€‹å» ç‰Œ
åç¨±ï¼Œå°±å¯ä¾æ­¤åˆªé™¤ã€‚æè³ªç­‰ç¯€é»äº¦ç„¶ã€‚ 
(3) ï¥´æœ‰ä¸€å€‹çˆ¶ç¯€é»çš„å­ç¯€é»å€‘æœ‰åŠï¥©ä»¥ä¸Š
æ˜¯ä»¥è‹±ï¥©å­—é–‹é ­çš„ï¼Œå°±å°‡é€™äº›å­ç¯€é»åˆª
å»ã€‚é€™æ–¹æ³•å¯ä»¥åˆªå»å¦‚ã€Œ1GBã€ã€ã€Œ2GBã€
é€™ç¨®è¦æ ¼å­—ï¤…ã€‚ 
(4) ï¥´æœ‰ä¸€å€‹çˆ¶ç¯€é»çš„å­ç¯€é»å€‘èˆ‡çˆ¶ç¯€é»çš„
åç¨±å¸¸æœ‰ç›¸åŒçš„çµå°¾å­—ï¤…ï¼Œå°±å°‡é€™äº›å­
ç¯€é»åˆªå»ã€‚é€™æ–¹æ³•å¯ä»¥åˆªå»å¦‚ã€ŒçŸ­è¢– T
æ¤ã€ã€ã€Œé•·è¢– T æ¤ã€é€™ç¨®éç´°çš„å­åˆ†ï§ã€‚ 
(5) æœ‰ä¸€äº›ç‰¹æ®Šï§åˆ¥çš„å­åˆ†ï§ä¹Ÿå…¨ï¥©åˆªé™¤ã€‚
é€™äº›ç‰¹æ®Šï§åˆ¥å¸¸æœ‰å¤§ï¥¾éå•†å“å‹çš„å­åˆ†
ï§ï¼Œåˆï¥§ï§ ä»¥å‰è¿°æ–¹å¼åˆªé™¤ã€‚ç›®å‰ä»¥äºº
å·¥æ‰¾å‡º 9 ï§ï¼ŒåŒ…æ‹¬ã€Œåœ–æ›¸èˆ‡é›œèªŒã€(å­åˆ†
ï§å‡æ˜¯æ›¸ç±å…§å®¹çš„ï¦´åŸŸåƒã€Œè‡ªç„¶ç§‘å­¸ã€ã€
ã€Œè—è¡“ã€)ã€ã€Œæˆ¿å±‹å‡ºå”®ã€(å­åˆ†ï§å‡æ˜¯åœ°
ååƒã€Œæ–°ç«¹ã€ã€ã€Œå°å—ã€) ç­‰ç­‰ã€‚ 
(6) æ‰€æœ‰è¢«åˆªé™¤çš„ç¯€é»ï¼Œå…¶å­ç¯€é»ä¹Ÿä¸€ä½µåˆª
é™¤ã€‚ 
 
é‚„æœ‰ä¸€ç¨®æƒ…å½¢éœ€è¦è™•ï§¤ã€‚åœ¨å¥‡æ‘©æ‹è³£çš„åˆ†ï§
ä¸­ï¼Œæœ‰äº›æ¨™ç±¤å…¶å¯¦åŒ…å«ï¦ºï¥¸ç¨®ä»¥ä¸Šçš„å•†å“ã€‚
ï¦µå¦‚ã€Œé›»å­å­—å…¸/ç¿»è­¯æ©Ÿã€æˆ–ã€Œæ±½ï¤‚èˆ‡æ©Ÿï¤‚ã€ã€‚
æœ‰äº›åˆä½µæƒ…å½¢è¼ƒç‚ºè¤‡é›œï¼Œåƒã€Œï§·ç‡ˆ(ï¤˜åœ°ç‡ˆ)ã€
æ‡‰è©²è¢«æ‹†æˆã€Œï§·ç‡ˆã€å’Œã€Œï¤˜åœ°ç‡ˆã€ï¥¸å€‹å­—ï¤…ï¼Œ
ä½†æ˜¯ã€ŒåºŠç½©(å–®)ã€å‰‡æ‡‰è¢«æ‹†æˆã€ŒåºŠç½©ã€å’Œã€ŒåºŠ
å–®ã€ã€‚ 
ç¶“éè§€å¯Ÿå¾Œï¼Œæˆ‘å€‘æ•´ï§¤å‡ºä¸€äº›å¸¸ï¨Šçš„åˆ†
éš”å­—ï¤…ï¤­ï¨€åˆ†ï¼ŒåŒ…æ‹¬ã€Œ/ã€ã€ã€Œã€ã€ã€ã€Œèˆ‡ã€
ä»¥åŠå·¦å³åœ“æ‹¬å¼§ç­‰ï¼Œä»¥ç¨‹å¼è‡ªå‹•åœ°å°‡é€™äº›æ¨™
ç±¤æ‹†æˆå¤šå€‹å•†å“å­—ï¤…ã€‚ç‚ºé¿å…ï¨€åˆ†éŒ¯èª¤å¸¶ï¤­
çš„å›°æ“¾ï¼Œé•·ï¨åƒ…ä¸€å€‹ä¸­æ–‡å­—çš„å­—ï¤…ï¨¦æœƒåˆªå»ã€‚ 
æœ€å¾Œå¾—åˆ° 1,342 ç­†ï¥§åŒçš„å•†å“åç¨±ã€‚æ
å‡ºä»¥ä¸Šçš„è‡ªå‹•è™•ï§¤æ–¹å¼ï¼Œç›®çš„æ˜¯å¸Œæœ›æœªï¤­ä¹Ÿ
èƒ½ç›´æ¥å¥—ç”¨åœ¨ä¸€å€‹ï¥§åŒçš„å•†å“åˆ†ï§æ¶æ§‹ä¸Šï¼Œ
å¾—ä»¥å¿«é€Ÿåœ°æ•´ï§¤å‡ºæ‰€éœ€çš„å•†å“ï¦œè¡¨ã€‚ 
 
3.2 å»ºï§·èˆˆè¶£è³‡ï¦¾åº« 
 
ç”±æ–¼äººï§çš„èˆˆè¶£éå¸¸å»£æ³›ï¼ŒåŒæ¨£åœ°å¾ˆé›£
ä»¥äººï¦Šçª®èˆ‰å‡ºæ‰€æœ‰çš„èˆˆè¶£å­—ï¤…ã€‚åœ¨ç¼ºä¹é«˜å“
è³ªçš„èˆˆè¶£ï§åˆ¥è³‡ï¤­æºæ™‚ï¼Œæˆ‘å€‘æ‰“ç®—å…ˆè§€å¯Ÿäºº
ï§æœƒå¦‚ä½•æè¿°è‡ªå·±çš„èˆˆè¶£ã€‚ 
åœ¨è¨±å¤šéƒ¨ï¤˜æ ¼ç¶²ç«™ä¸­ï¼Œé€šå¸¸ï¨¦æœƒæä¾›éƒ¨
ï¤˜æ ¼ä¸»äººçš„å€‹äººåŸºæœ¬è³‡ï¦¾ï¼Œå…¶ä¸­å¸¸åŒ…å«èˆˆè¶£
ï¤ä½ã€‚æˆ‘å€‘æ–¼æ˜¯å˜—è©¦æ”¶é›†éƒ¨ï¤˜å®¢æ‰€ä½¿ç”¨çš„èˆˆ
è¶£æè¿°ï¤­åšç‚ºå€™é¸é›†ã€‚ 
æˆ‘å€‘æ‹œè¨ªï¦ºç„¡åå°ç«™çš„éƒ¨ï¤˜æ ¼5ç©ºé–“ï¼Œåˆ†
æï¦º 2,386 ä½éƒ¨ï¤˜å®¢çš„å€‹äººåç‰‡ã€‚åç‰‡æœ‰ä¸‰
å€‹ï¤ä½å’Œèˆˆè¶£æœ‰å¯†ï¨€é—œä¿‚ï¼Œåˆ†åˆ¥æ˜¯ã€Œèˆˆè¶£ã€ã€
ã€Œå–œæ­¡çš„ã€å’Œã€Œè¨å­çš„ã€ã€‚éƒ¨ï¤˜å®¢åœ¨å¡«å¯«é€™
äº›ï¤ä½æ™‚ä¸¦ï¥§åš´è¬¹ï¼Œæœ‰çš„ï¤ä½ä¿ï§ç©ºç™½ï¼Œï¤
æœ‰èˆˆè¶£èˆ‡å–œæ­¡äº‹ç‰©äº¤éŒ¯å¡«å¯«çš„æƒ…å½¢ã€‚çµ±è¨ˆçµ
æœï¼Œå¤§ç´„åƒ…æœ‰ 72%å·¦å³çš„äººæœƒå¡«å¯«èˆˆè¶£å’Œå–œ
æ­¡äº‹ç‰©ï¤ä½ã€‚ 
ä¸Šè¿°æ­¥é©Ÿæ‰€å¾—åˆ°çš„å­—ï¤…ï¼Œæ˜¯åœ¨ä¸€å€‹ï¤ä½
ä¸­çš„æ•´å€‹å­—ï¤…ï¼Œåƒæ˜¯ 
 
çœ‹å‹é‹Ë‹çœ‹é›»å½±Ë‹çœ‹æ–‡ç« Ë‹é€›è¡— 
â˜†æ‰“æ‰® â˜†shopping â˜†æ¸›è‚¥ 
æ¯å¤©ç¡è¦ºç¡åˆ°è‡ªç„¶é†’~~~ 
æˆ‘è¨å­ã„‰ï¨¦æ˜¯ä½ å–œæ­¡çš„...= = 
äº¤æœ‹å‹å›‰~åªè¦èƒ½å¤ èªï§¼ä½ å°±å¾ˆé–‹å¿ƒå›‰~ 
 
é€™æ¨£çš„å­—ï¤…ï¼Œï§¨é¢åŒ…å«å¤šå€‹èˆˆè¶£æˆ–ç‰©å“ï¼Œç”¨
å„å¼å„æ¨£å¥‡æ€ªçš„åˆ†éš”æ–¹å¼éš”é–‹ã€‚æœ‰çš„æ•˜è¿°æ˜¯
å°‡èˆˆè¶£æ”¾åœ¨ï¤†å­ä¸­ï¥¯å‡ºï¼Œåƒæ˜¯ã€Œäº¤æœ‹å‹å›‰ã€
å¤šï¦ºèªæ°£è©ã€Œå›‰ã€ï¼Œã€Œæ¯å¤©ç¡è¦ºç¡åˆ°è‡ªç„¶é†’ã€
å…¶å¯¦å°±æ˜¯æŒ‡ã€Œç¡è¦ºã€ã€‚æœ‰çš„æ•˜è¿°ç”šè‡³å’Œèˆˆè¶£
ç„¡é—œï¼Œåƒæ˜¯ã€Œåªè¦èƒ½å¤ èªï§¼ä½ å°±å¾ˆé–‹å¿ƒå›‰ã€
5 http://www.wretch.cc/blog/ 
GoogleCount 
snippet 
åœ–ä¸€ã€Google æŸ¥è©¢çµæœç¯„ï¦µ 
å°‡èˆˆè¶£ x è‡³æœå°‹å¼•æ“æŸ¥è©¢æ‰€å¾—åˆ° M æ®µ
snippets çµ„åˆæˆè©²èˆˆè¶£ x çš„ä¸Šä¸‹æ–‡è³‡è¨Šï¼Œå•†å“
yè‡³æœå°‹å¼•æ“æŸ¥è©¢æ‰€å¾—åˆ°Næ®µ snippetsçµ„åˆæˆ
è©²å•†å“ y çš„ä¸Šä¸‹æ–‡è³‡è¨Šã€‚æˆ‘å€‘å¸Œæœ›ä»¥èˆˆè¶£ x
çš„ä¸Šä¸‹æ–‡è³‡è¨Šèˆ‡å•†å“ y çš„ä¸Šä¸‹æ–‡è³‡è¨Šçš„ç›¸ä¼¼
ï¨ï¤­ä»£è¡¨èˆˆè¶£ x èˆ‡å•†å“ y çš„é—œï¦—ï¨ã€‚ 
ï¤…ã€æ‰€æœ‰å•†å“å­—ï¤…ï¼Œä»¥åŠæ‰€æœ‰èˆˆè¶£åŠå•†å“é…
å°çš„å­—ï¤…ï¼Œï¨¦è®ŠæˆæŸ¥è©¢ï¤†è‡³ Google æœå°‹å¼•æ“
æŸ¥è©¢ï¼Œä¸¦ä»¥å…¶å›å‚³çµæœçš„é ä¼°ç¶²é å€‹ï¥©åšç‚º
ï¥©æ“šï¼Œå¦‚åœ–ä¸€åœ“æ¡†æ‰€ç¤ºä¹‹ GoogleCountã€‚æŸ¥è©¢
æ™‚æœƒå°‡é¸é …èª¿æˆå®Œæ•´å­—ï¤…æ¯”å°ï¼Œä»¥é¿å…æŸ¥è©¢
å­—ï¤…è¢«æ–·é–‹æˆï¥©å€‹è©é€ æˆæŸ¥è©¢éŒ¯èª¤ã€‚ 
 åº•ä¸‹ä»‹ç´¹ä»¥å‘ï¥¾ç©ºé–“æ¨¡å‹è¨ˆç®—ç›¸ä¼¼ï¨çš„
æ–¹æ³•ï¼Œä¸¦ä¸”ä»‹ç´¹ï¥¸å¤§ï§è©å½™æ¬Šé‡è¨ˆç®—å…¬å¼ã€‚ 4.2 ä¸Šä¸‹æ–‡è³‡è¨Šç›¸ä¼¼ï¨ 
  
4.2.1 å‘ï¥¾ç©ºé–“æ¨¡å‹ å¦ä¸€ç¨®æƒ³æ³•æ˜¯ï¼Œä¸€å€‹èˆˆè¶£èˆ‡ä¸€å€‹å•†å“é«˜
ï¨é—œï¦—æ™‚ï¼Œï¥§ï¨Šå¾—ä¸€å®šè¦å¸¸å¸¸åŒæ™‚å‡ºç¾åœ¨æ–‡
ç« ä¸­ã€‚ä»–å€‘å€‹åˆ¥å‡ºç¾çš„æ–‡ç« ï¦´åŸŸæ‡‰è©²æœƒå¾ˆç›¸
ä¼¼ï¼Œé€™å°±çµ¦ï¦ºæˆ‘å€‘ï§ç”¨ä¸Šä¸‹æ–‡è³‡è¨Šè¨ˆç®—ç›¸é—œ
ï¨çš„æƒ³æ³•ã€‚ 
 
IRä¸­æœ€å¸¸è¢«ä½¿ç”¨çš„æ–‡å­—ç›¸ä¼¼ï¨è¨ˆç®—æ–¹å¼
æ˜¯å»ºï§·å‘ï¥¾ç©ºé–“æ¨¡å‹ (vector space model)ï¼Œ
ä¸¦ä»¥ cosine similarity (å‘ï¥¾å¤¾è§’é¤˜å¼¦å€¼) åš
ç‚ºç›¸ä¼¼ï¨è¨ˆç®—å…¬å¼ã€‚å°‡æ‰€æœ‰å·²çŸ¥è©å½™å„è¦–ç‚º
å‘ï¥¾ç©ºé–“çš„ä¸€å€‹ç¶­ï¨ï¼Œå®šç¾©æ¯å€‹è©å½™ termi
åœ¨ä¸€ç¯‡æ–‡ç«  d ä¹‹ä¸­çš„æ¬Šé‡å€¼ (term weight)ï¼Œ
å°±å¯å°‡æ–‡ç«  d è¡¨é”æˆç©ºé–“ä¸­çš„ä¸€å€‹å‘ï¥¾ï¼Œå…¶
æŠ•å½±åœ¨å„ç¶­ï¨çš„å¤§å°å³å„è©å½™åœ¨æ–‡ç«  d ä¸­çš„
æ¬Šé‡ã€‚ 
åœ¨ä¼°ç®—ä¸€å°èˆˆè¶£å’Œå•†å“ä¹‹é–“çš„é—œï¦—ï¨
æ™‚ï¼Œï¥´æ˜¯æ‰‹é‚Šæœ‰å¤§ï¥¾æ–‡ä»¶é›†åˆï¼Œæˆ‘å€‘å¯ä»¥å¾
ä¸­æ‰¾å‡ºå‡ºç¾èˆˆè¶£çš„æ–‡ä»¶é›†å’Œå‡ºç¾å•†å“çš„æ–‡ä»¶
é›†ï¼Œå†ä»¥æ–‡ä»¶ä¹‹é–“çš„ç›¸ä¼¼ï¨ï¤­è©•ï¥¾æ–‡ä»¶æ˜¯å¦
å±¬æ–¼åŒä¸€ä¸»é¡Œï¦´åŸŸï¼Œé€²è€Œä¼°ç®—é€™å°èˆˆè¶£å’Œå•†
å“çš„é—œï¦—ï¨ã€‚ 
ï¥¸ç¯‡æ–‡ç« çš„ç›¸ä¼¼ï¨ cosine similarity å…¬å¼
å®šç¾©å¦‚ä¸‹ï¼š 
ç„¶è€Œï¥´æ²’æœ‰æ­¤ï§å¤§å‹æ–‡ä»¶é›†ï¼Œï§ç”¨æœå°‹
å¼•æ“å›å‚³çµæœç¶²é ä¸­çš„ç°¡çŸ­æ‘˜è¦ä¹Ÿæ˜¯å¯ï¨ˆï¼Œ
ç¯„ï¦µå¦‚åœ–ä¸€ä¸­æ–¹æ¡†æ‰€ç¤ºçš„ snippetsã€‚ç°¡çŸ­æ‘˜è¦
é€šå¸¸ç‚ºè©²ç¶²é ä¸­åŒ…æ‹¬æ‰€æŸ¥è©¢é—œéµè©çš„æ–‡ï¤†ç‰‡
æ®µï¼Œå› æ­¤å¯åšç‚ºé—œéµè©ä¸Šä¸‹æ–‡è³‡è¨Šçš„ï¤­æºã€‚ ïƒ¥ï‚´ïƒ¥
ïƒ¥ ï‚´ï€½
ï‚´
ï‚·
ï€½ï€½
ï€½
t
j ki
t
i ji
t
i kiji
kj
kj
ww
ww
dd
dd
1
2
,1
2
,
1 ,,  
 6
 8
å€‹
äººå–œ
ç”¨è€…ï¥§å¤ å¤šçš„å•é¡Œï¼Œä»¥
è‡³æ–¼
4.4 ï¦—ï¨å¯¦é©—çµæœåŠè¨ï¥ 
ä¸€çµ„å¯¦é©—æ˜¯è¨ˆç®—è©å½™å…±ç¾ï¨å’Œäº¤äº’è³‡
è¨Šï¥¾
çµ„å¯¦é©—æ˜¯å…ˆè’é›†å„èˆˆè¶£åŠå„å•†å“çš„
ä¸Šä¸‹
ç¶²ç«™çš„æ¨™ç±¤ã€‚
ä»¥å„
4.4.2 æ•ˆèƒ½è©•ä¼°æ–¹å¼ 
æœ‰ï§½éº¼æ¨£èˆˆè¶£çš„äººæ‡‰è©²æ¨è–¦çµ¦ä»–ä½•ç¨®
å•†å“
çš„æ•ˆ
ç¨®è©•ä¼°æ–¹å¼æ˜¯å…ˆè«‹äººå»ºï§·ï¥«è€ƒæ¨™æº–
ç­”æ¡ˆ
ä¼°æ–¹æ³•éœ€è¦å»ºï§·ï¥«è€ƒæ¨™æº–ç­”
æ¡ˆã€‚
æ˜¯å¬å›ï¥¡ (recall, R)ã€ï¨
ç¢ºï¥¡
å‡ºï¦ºå„å€‹å¯¦é©—çš„ç³»çµ±çµæœèˆ‡ï¥«è€ƒ
ç­”æ¡ˆ
ç­–ï¥¶
ï¨¦æ˜¯å‘ï¥¾ç©ºé–“æ¨¡å‹ï¼Œè©å½™æ¬Šé‡åˆ†åˆ¥æ˜¯
 
è¡¨å››ã€èˆˆè¶£å•†å“é—œï¦—ï¨æ•ˆèƒ½è©•ä¼° 
ç­–ï¥¶ F 
Delicious ä»¥åŠé»‘ç±³æ›¸ç±¤æ¨™è¨˜çš„å°è±¡æ˜¯
æ„›çš„ç¶²é ï¼Œè€Œä¸”å¤§éƒ¨ä»½äººï¨¦æœƒç”¨è¼ƒçŸ­çš„
å­—ï¤…åšç‚ºæ¨™ç±¤ï¼Œç¢ºå¯¦è¼ƒç¬¦åˆæˆ‘å€‘å¯¦é©—çš„éœ€
æ±‚ã€‚å¯æƒœ Delicious ä½¿ç”¨è€…ï¤­è‡ªä¸–ç•Œå„åœ°ï¼Œæ¨™
ç±¤ä»¥è‹±æ–‡ç‚ºä¸»ï¼Œä¸­æ–‡æ¨™ç±¤çš„ï¥¾ï¥§ï¥´é æœŸçš„
å¤šã€‚æ­¤å¤–ï¼Œï¥´æ˜¯ä»¥ç¨‹å¼è‡ªå‹•ï¥šå–ï¼Œåˆæœ‰è¢«è¦–
ç‚ºç¶²ï¤·æ”»æ“Šè€Œè¢«æ‹’çµ•çš„å±éšªã€‚æœ€å¾Œæˆ‘å€‘åƒ…æŠ“
å–ï¦ºèˆ‡å„èˆˆè¶£å…±åŒå‡ºç¾çš„æœ€é«˜é »æ¨™ç±¤ 20
å€‹ï¼Œå†çœ‹æ˜¯å¦æœ‰å•†å“å­—ï¤…å‡ºç¾å…¶ä¸­ï¼Œå¤§å¤§æ¸›
ä½å¯¦é©—çš„æˆåŠŸï¥¡ã€‚ 
é»‘ç±³å‰‡ä¼¼ä¹æœ‰ä½¿
æŸ¥è©¢çµæœè³‡è¨Šï¥¾ä¹Ÿåå°‘ã€‚æœ‰è¶…é 20 å€‹èˆˆ
è¶£ï¥§æ›¾æˆç‚ºæ¨™ç±¤ï¼Œè¶…éåŠï¥©ï¥§æ›¾èˆ‡å•†å“å­—ï¤…
å…±åŒæˆç‚ºæ¨™ç±¤ã€‚æ‰€ä»¥å¯¦é©—çµæœç„¡æ³•åæ‡‰é€™æ–¹
æ³•çš„çœŸæ­£æ•ˆèƒ½ã€‚ 
 
é—œ
4.4.1 å¯¦é©—è³‡ï¦¾ 
 
ç¬¬
ï¼Œéœ€è¦æŸ¥è©¢åŒ…å«å„èˆˆè¶£èˆ‡å„å•†å“çš„ç¶²é 
ï¥©ï¥¾ï¼Œé€™ï§¨ç›´æ¥ä½¿ç”¨ Google çš„æœå°‹æœå‹™å³å¯
ç²å¾—ã€‚ 
ç¬¬äºŒ
æ–‡è³‡è¨Šï¼Œå†å»ºï§·å‘ï¥¾ç©ºé–“æ¨¡å‹ï¤­è¨ˆç®—ï¥¸
æ®µä¸Šä¸‹æ–‡è³‡è¨Šä¹‹é–“çš„ç›¸ä¼¼ï¨ã€‚åŒæ¨£åœ°æˆ‘å€‘ä½¿
ç”¨ Google çš„æœå°‹æœå‹™ï¼Œä»¥æŸ¥è©¢çµæœç¶²é æ‰€æ
ä¾›çš„ç°¡çŸ­æ‘˜è¦åšç‚ºä¸Šä¸‹æ–‡è³‡è¨Šã€‚æ¯å€‹æŸ¥è©¢ 
(èˆˆè¶£æˆ–å•†å“) æœ€å¤šï¨¦è’é›†å‰ 1,000 åï¼Œä»¥å¯¦
éš› Google çš„å›å‚³çµæœç‚ºæº–ã€‚ 
ç¬¬ä¸‰çµ„å¯¦é©—è’é›†çš„æ˜¯ç¤¾ç¾¤
èˆˆè¶£åšç‚ºæŸ¥è©¢é—œéµè©ï¼Œåˆ†åˆ¥æŸ¥è©¢
Delicious ä»¥åŠé»‘ç±³æ›¸ç±¤ï¼Œå¾—åˆ°èˆ‡è©²èˆˆè¶£å…±åŒ
å‡ºç¾çš„å…¶ä»–æ¨™ç±¤åç¨±ã€‚é»‘ç±³æ›¸ç±¤è’é›†å¾—
115,824 ç­†ï¼Œä½†æ˜¯å…¶ä¸­åªæœ‰ 242 å€‹æ¨™ç±¤æ˜¯å•†å“
å­—ï¤…ã€‚ç”± Delicious è’é›†æ‰€å¾—çš„æ¨™ç±¤ç¸½ï¥©å‰‡ç‚º
29,906 ç­†ï¼Œæœ‰ 1,006 ç­†æ˜¯å•†å“å­—ï¤…ã€‚ 
 
 
 
å…·
ï¼Œåœ¨ç¶“éå¹¾æ¬¡ï¥§æ–·åœ°è¨ï¥å¾Œï¼Œç™¼ç¾çµæœ
å¯èƒ½å› äººè€Œï¥¢ã€‚ä¹Ÿå°±æ˜¯ï¥¯ï¼Œé—œï¦—ï¨ä¸­ç­‰æˆ–è¼ƒ
ä½çš„å•†å“ï¼Œæœƒï¥§æœƒå‡ºç¾åœ¨è³¼ç‰©æ¸…å–®ä¸­æœƒå’Œå€‹
äººçš„èˆˆè¶£å¼·ï¦Ÿç¨‹ï¨æœ‰é—œã€‚å¦‚æœæ­¤äººå¼·ï¦ŸæŠ•å…¥
è©²èˆˆè¶£ï¼Œé‚£éº¼æ‰€æœ‰ç›¸é—œå•†å“ä»–ï¨¦æœ‰è³¼è²·æ©Ÿ
æœƒï¼›ç„¶è€Œï¥´ä»–åªæ˜¯å…·æœ‰ä¸€èˆ¬èˆˆè¶£è€Œå·²ï¼Œå°±åª
æœ‰é«˜ï¨ç›¸é—œçš„å•†å“èƒ½ï§¿æ¿€ä»–çš„è³¼è²·æ…¾æœ›ã€‚ 
ç‚ºï¦ºç ”ç©¶ç›®çš„ï¼Œæˆ‘å€‘é‚„æ˜¯æå‡ºï¥¸ç¨®ï¥§åŒ
èƒ½è©•ä¼°æ–¹å¼ï¼Œå˜—è©¦ç”±ï¥§åŒé¢å‘ï¤­æ¢è¨é€™
ä»¶äº‹ã€‚ 
ç¬¬ä¸€
ï¼Œä»¥å¬å›ï¥¡ (recall) å’Œï¨ç¢ºï¥¡ (precision)
åšç‚ºè©•ä¼°å…¬å¼ã€‚ç¬¬äºŒç¨®å‰‡æ˜¯æ”¹å»ºï§·èˆˆè¶£å•†å“
é–“çš„é—œï¦—å¼·ï¨ï¼Œå†ä»¥ç³»çµ±çµæœçš„å¹³å‡é—œï¦—å¼·
ï¨ï¤­è©•ä¼°ã€‚ 
ç¬¬ä¸€ç¨®è©•
æ¨™è¨˜è€…å…ˆç†Ÿè¨˜æ‰€æœ‰èˆˆè¶£ä»¥åŠæ‰€æœ‰å•†å“ï¼Œ
å†é‡å°æ¯å€‹èˆˆè¶£ï¼ŒæŒ‘å‡ºä»–èªç‚ºé—œï¦—ï¨è¼ƒé«˜çš„
å•†å“ï¼Œï¥©ï¥¾ï¥§é™ã€‚ 
ç¬¬ä¸€ç¨®è©•ä¼°å…¬å¼
 (precision, P) å’Œ F-measure (F)ã€‚é‡å°ä¸€
å€‹èˆˆè¶£ x ï¤­çœ‹ï¼Œï¦¨ï¥«è€ƒç­”æ¡ˆæå‡ºï¦º a å€‹å°æ‡‰
å•†å“ï¼Œç³»çµ±æå‡ºï¦º b å€‹å°æ‡‰å•†å“ï¼Œï¥¸è€…çš„äº¤
é›†å€‹ï¥©æ˜¯ cã€‚å¬å›ï¥¡å®šç¾©ç‚º c / aï¼Œæ„å³ï¥«è€ƒç­”
æ¡ˆä¸­æœ‰å¤šå°‘æ¯”ï¦µèƒ½è¢«ç³»çµ±æ‰¾åˆ°ã€‚ï¨ç¢ºï¥¡å®šç¾©
ç‚º b / aï¼Œæ„å³ç³»çµ±æå‡ºçš„ç­”æ¡ˆä¸­æœ‰å¤šå°‘æ¯”ï¦µ
æ˜¯æ­£ç¢ºçš„ã€‚F-measure ç‚ºï¥¸è€…çš„èª¿å’Œå¹³å‡ï¥©
2ï‚´Pï‚´R/(P+R)ï¼Œå› ç‚ºå¬å›ï¥¡å’Œï¨ç¢ºï¥¡å¸¸æ˜¯ä¸€é«˜
ä¸€ä½äº’ç›¸å½±éŸ¿ï¼Œï¥§ï§ åªçœ‹å–®ä¸€æŒ‡æ¨™ï¤­æ¯”è¼ƒç³»
çµ±å¥½å£ã€‚ 
è¡¨å››ï¦œ
æ¯”è¼ƒçš„ R, P, F æ•ˆèƒ½è©•ä¼°ã€‚å› ç‚ºå„çµ„å¯¦é©—
ï¨¦éœ€è¦è¨­å®šæå‡ºç­”æ¡ˆçš„é–€æª»å€¼ï¼Œç›¸é—œåˆ†ï¥©é«˜
æ–¼é–€æª»å€¼çš„å•†å“æ‰æœƒè¢«å»ºè­°ã€‚è¡¨å››ä¸­çš„ï¥©æ“š
å·²ç¶“æ˜¯å„çµ„å¯¦é©— F-measure å€¼æœ€å¥½çš„æƒ…å½¢ï¼Œ
æœ€å¾Œä¸€ï¤æ˜¯é€™æ™‚æ‰€éœ€è¨­å®šçš„é–€æª»å€¼ã€‚ 
åœ¨è¡¨å››çš„å¯¦é©—ï¥©æ“šä¸­ï¼Œä¸‰ç¨®æœ€å¥½çš„
etfâ€™ï‚´idfã€(eï€­1)tfâ€™ï‚´idf å’Œ log-likelihood valueï¼Œä¸‰
è€…æ•ˆèƒ½å·®ï¥§å¤šã€‚æ›¸ç±¤çš„æ•ˆæœæ¯”å–®ç´”è¨ˆç®—ç¶²é 
é »ï¥¡è¦å¥½ï¼Œå¯æƒœå› ç‚ºçµ±è¨ˆè³‡ï¦¾ï¥¾å¤ªå°‘ï¼Œç„¡æ³•
é”åˆ°é æœŸçš„æ•ˆæœã€‚ 
 
 
 é–€æª»å€¼ P R 
å…±ç¾ï¨ 3.41 4.1913200 5.41
MI 0 5.24 0.63 1.13
VSM1 0.58742 19 6.01 4.31 6.79
 10
5.1.2 èˆˆè¶£å­—ï¤…æ¯”å° 
ä¸€ï§åšæ³•æ˜¯è¨ˆç®—å„èˆˆè¶£åœ¨æ–‡ç« ä¸­çš„å‡º
ç¾é »
5.1.3 å¯¦é©—æ¸¬è©¦è³‡ï¦¾ 
é©—è³‡ï¦¾çš„éƒ¨ï¤˜æ ¼æ–‡ç« é¸è‡ªç„¡åå°ç«™çš„
éƒ¨ï¤˜
ç¾ä»£
ç«™éƒ¨ï¤˜æ ¼çš„ç¶²é ç‰ˆé¢ä¸­ï¼Œä¸¦ç„¡æ³•
ç›´æ¥
å…¶
ä¸€æ˜¯
è€…çš„æ‰€æœ‰éƒ¨
ï¤˜æ ¼
5.1.4 éƒ¨ï¤˜æ ¼æ–‡ç« èˆ‡èˆˆè¶£é—œï¦—ï¨è©•ä¼° 
æ–¼ç„¡æ³•è«‹ä½¿ç”¨è€…æœ¬äººï¤­å”åŠ©è©•ä¼°æ–‡ç« 
ç›¸é—œ
éƒ¨ï¤˜æ ¼æ–‡ç« èˆ‡èˆˆè¶£é—œï¦—ï¨æ•ˆèƒ½è©•
ä¼°çš„
 
è¡¨ï§‘ã€éƒ¨ï¤˜æ ¼æ–‡ç« èˆ‡èˆˆè¶£é—œï¦—ï¨æ•ˆèƒ½è©•ä¼° 
 
 
 
å¦
ï¥¡ï¼Œç›´æ¥åœ¨æ–‡ç« ä¸­æ¯”å°èˆˆè¶£å­—ï¤…çš„å‡º
ç¾ã€‚æ­¤å¤–ï¼Œåœ¨ç¬¬ 3.2 ç¯€ä¸­æéï¼ŒæŸäº›é¡¯ç„¶åŒ
ç¾©çš„èˆˆè¶£å­—ï¤…å·²ç¶“è¢«åˆ†ç¾¤ï¼Œç®—æˆåŒä¸€ç¨®èˆˆ
è¶£ã€‚åœ¨ç¾åœ¨é€™å€‹åšæ³•ä¸­ï¼ŒåŒä¸€ç¨®èˆˆè¶£çš„ï¥§åŒ
å¯«æ³•ï¨¦æœƒè‡³æ–‡ç« ä¸­æœå°‹ï¼Œé »ï¥¡å‰‡åŠ ç¸½åšç‚ºé€™
ä¸€ç¨®èˆˆè¶£çš„é—œï¦—ï¨åˆ†ï¥©ã€‚ 
 
 
 
å¯¦
æ ¼ç©ºé–“ã€‚åœ¨ç¬¬ 3.2 ç¯€æ›¾æåˆ°ï¼Œå·²ç¶“è’é›†
ï¦º 2,386 ä½éƒ¨ï¤˜å®¢çš„ç„¡åå¸³è™Ÿå’Œå€‹äººè³‡ï¦¾ã€‚
ä¸‹ä¸€æ­¥æ˜¯å¸Œæœ›ç”±é€™äº›äººä¹‹ä¸­é¸å‡ºä¸€äº›äººå‡ºï¤­
ä¸‹è¼‰ä»–å€‘çš„æ–‡ç« ä»¥é€²ï¨ˆå¯¦é©—ï¼Œä¸¦ä¸”å¸Œæœ›ä»–å€‘
ä»¥å¡«å¯«å•å·çš„æ–¹å¼å”åŠ©è©•ä¼°ç³»çµ±çš„æ•ˆèƒ½ã€‚ 
å¯æƒœçš„æ˜¯ï¼Œå¤§éƒ¨ä»½äººï¨¦æ²’æœ‰å›ä¿¡ã€‚è€Œä¸”
äººä½¿ç”¨éƒ¨ï¤˜æ ¼ï¼Œå¤šæ˜¯æŠ’ç™¼å¿ƒæƒ…çš„æ—¥è¨˜åŠŸ
èƒ½ã€‚å°æ–¼é€™ï§è¼ƒç©©ç§çš„æ–‡ç« è¢«å…¬é–‹å¯¦é©—ï¼Œï¨¦
è¼ƒæœ‰ç–‘æ…®ã€‚æ–¼æ˜¯æˆ‘å€‘åªèƒ½ä»¥æ¨¡æ“¬çš„æ–¹å¼ï¤­é€²
ï¨ˆè©•ä¼°ã€‚ 
ç„¡åå°
å¾—çŸ¥è©²ä½éƒ¨ï¤˜å®¢ç¸½å…±æ›¾ç™¼è¡¨éå¤šå°‘æ–‡
ç« ã€‚é¦–å…ˆå…ˆæ‹œè¨ªå„ä½¿ç”¨è€…éƒ¨ï¤˜æ ¼é¦–é çš„ã€Œæ‰€
æœ‰æ–‡ç« ï¦œè¡¨ã€é é¢ï¼Œå³å¯å–å¾—è©²ä½¿ç”¨è€…æ‰€ç™¼
è¡¨éçš„æ–‡ç« çš„éˆçµç¶²å€ã€‚å¦‚æ­¤ä¹Ÿå¯ä»¥çµ±è¨ˆå„
ä½¿ç”¨è€…ç™¼è¡¨æ–‡ç« çš„ç¯‡ï¥©ï¼Œé¸å–ä½¿ç”¨è€…æ™‚å¯åœ¨
é »ç¹ä½¿ç”¨è€…å’Œä½é »ä½¿ç”¨è€…é–“å–å¾—å¹³è¡¡ã€‚ 
ä¸‹è¼‰éƒ¨ï¤˜æ ¼æ–‡ç« æœƒç¢°åˆ°ï¥¸ç¨®å› é›£è™•ã€‚
éˆçµå¤±æ•ˆçš„å•é¡Œï¼Œå› ç‚ºå¤§ï¥¾ä¸‹è¼‰æ–‡ç« ä»
æ˜¯è€—æ™‚è€—äº‹çš„å·¥ä½œï¼Œéä¹…çš„æ–‡ç« åœ¨å¯¦é©—æœŸé–“
æœƒè¢«ä½œè€…ç§»é™¤ã€‚å…¶äºŒåŒæ¨£æ˜¯ç¶²ç«™å°æ–¼ç¨‹å¼ä¸‹
è¼‰çš„é™åˆ¶ï¼Œç„¡åä»æœƒä»¥ç–‘ä¼¼ç—…æ¯’æ”»æ“Šç‚ºï§¤ç”±
æ‹’çµ•é »ç¹åŠå¤§ï¥¾çš„ä¸‹è¼‰ï¨ˆç‚ºã€‚ 
æœ€å¾Œæˆ‘å€‘ä¸‹è¼‰ï¦º 31 ä½ä½¿ç”¨
æ–‡ç« ï¼Œç™¼è¡¨ç¯‡ï¥©å¾ä½æ–¼ 10 ç¯‡åˆ°é«˜æ–¼ 400
ç¯‡çš„äººï¨¦æœ‰ã€‚æ–‡ç« ç¸½ï¥©æ˜¯ 1,849 ç¯‡ã€‚ 
 
 
 
ç”±
èˆˆè¶£çš„å»ºè­°æ•ˆèƒ½ï¼Œå¯¦é©—æ”¹æ¡æ¨™è¨˜è€…æ¨¡æ“¬
çš„æ–¹å¼ï¤­é€²ï¨ˆè©•ä¼°ã€‚æ¨™è¨˜è€…åœ¨ç†Ÿè¨˜å„ç¨®èˆˆè¶£
å¾Œï¼Œé–±ï¥šæ¯ä¸€ç¯‡æ–‡ç« ï¼Œä¸¦ä¸”ä¾ä¸€èˆ¬å¸¸ï§¤æ¨ï¥
è©²ç¯‡æ–‡ç« çš„å¯èƒ½ç›¸é—œèˆˆè¶£ã€‚æ¨™è¨˜è€…ä¸¦ï¥§çŸ¥é“
æ–‡ç« çš„ä½œè€…æ˜¯èª°ï¼Œå„ä½œè€…çš„æ–‡ç« ä¹Ÿåˆ†æ•£å‘ˆ
ç¾ï¼Œä»¥é¿å…æ¨™è¨˜è€…å­¸å¾—ä½œè€…ç¿’æ…£è€Œä½¿æ¨™è¨˜ç”¢
ç”Ÿåé —ã€‚ 
è¡¨ï§‘æ˜¯
å¯¦é©—ï¥©æ“šã€‚ç¬¬ä¸€ï¤è¡¨ç¤ºç³»çµ±å°æ–¼æ¯ç¯‡æ–‡
ç« æå‡ºæ’åå‰ N åçš„èˆˆè¶£çš„å€‹ï¥©ï¼Œå…¶ä»–ï¤ä½
å‰‡æ˜¯ï¥«è€ƒç­”æ¡ˆä¸­æœ‰å¤šå°‘æ¯”ï¦µè¢«ç³»çµ±æå‡ºï¤­ã€‚
ä¸‰å€‹å¯¦é©—åˆ†åˆ¥æ˜¯ VSM5ã€VSM6 ä»¥åŠå­—ï¤…æ¯”
å°æ–¹å¼æ‰€å¾—çš„çµæœã€‚ 
N VSM5 VSM6 å­—ï¤…æ¯”å° 
1 03.37 03.37 36.96 
2 05.40 05.06 52.14 
3 06.75 07.43 58.75 
4 09.45 10.13 59.92 
5 12.50 12.50 61.08 
6 14.86 13.51 61.86 
7 16.89 15.20 63.03 
8 17.22 15.87 63.42 
9 18.91 17.22 63.42 
10 19.59 18.91 63.81 
 
ç”±è¡¨ï§‘å¯çŸ¥ï¼ŒVSM5 å’Œ VSM6 æ•ˆèƒ½ä»æ˜¯
å·®ï¥§
5.2 æ¸¬ä½œè€…èˆˆè¶£ 
ç”¨ç¬¬ 5.1 ç¯€æ‰€ä»‹ç´¹çš„ç³»çµ±ç‚ºä¸€ä½éƒ¨ï¤˜
å®¢çš„
(1) æ¯ç¯‡æ–‡ç« æ‰¾å‡ºå‰ K åç›¸é—œèˆˆè¶£ï¼Œæ¯å€‹èˆˆ
K åç›¸é—œèˆˆè¶£ï¼Œèˆˆè¶£çš„
å¤šï¼Œï¥§é VSM5 ï¥¶é«˜ã€‚å…‰ä»¥å­—ï¤…æ¯”å°æ–¹
å¼å°±èƒ½æœ‰ï§‘æˆçš„å¬å›ï¥¡ï¼Œæ˜¯ï¥§éŒ¯çš„ç­–ï¥¶ã€‚ç¬¬
5.2 ç¯€æœƒæåˆ°é€™ï¥¸ç¨®ç­–ï¥¶å¯åœ¨ï¥§åŒæƒ…å½¢æ‰¾åˆ°
å°æ‡‰çš„èˆˆè¶£ã€‚ 
 
åµ
 
ï§
æ¯ç¯‡æ–‡ç« æ‰¾å‡ºç›¸é—œçš„èˆˆè¶£å¾Œï¼Œé€™ä½éƒ¨ï¤˜
å®¢çš„å€‹äººèˆˆè¶£å°±å¯ä»¥ç”±ä»–çš„æ–‡ç« ï¤­æŠ•ç¥¨æ±º
å®šã€‚æŠ•ç¥¨çš„æ–¹å¼æœ‰ä»¥ä¸‹å¹¾ç¨®æ–¹å¼ï¼š 
 
è¶£çš„å‡ºç¾ï¨¦ç®—ä¸€ç¥¨ï¼Œä»¥ç¥¨ï¥©ï¤­ä»£è¡¨å„èˆˆ
è¶£çš„é‡è¦æ€§ã€‚ 
(2) æ¯ç¯‡æ–‡ç« æ‰¾å‡ºå‰
æ¬Šé‡ä¾ç…§åæ¬¡éæ¸›ã€‚ä»¥ï¥ç©çš„æ¬Šé‡ï¤­ä»£
è¡¨å„èˆˆè¶£çš„é‡è¦æ€§ã€‚ 
 12
æ˜¯ä»¥ç ”ç©¶çš„è§’ï¨ï¤­çœ‹ï¼Œä»å¯å¯¦é©—å„éš
æ®µå„
å¯Ÿ
éƒ¨ï¤˜
å‘ˆç¾çµ¦ä½¿ç”¨
è€…ï¼Œ
ä¼°ï¼Œ
æ¶ˆè²»ç¶²ç«™åˆä½œï¼Œé€™é»å¯
åšç‚º
å››ã€è¨ˆç•«æˆæœè‡ªè©• 
é«”è€Œè¨€ï¼Œç ”ç©¶å…§å®¹å·²å®Œæˆå¤§éƒ¨ä»½è¨ˆç•«
æ‰€ï¦œ
ã€ï¥«è€ƒæ–‡ç» 
 Zhou and Minyi Guo (2009). "A 
Kol
Kum
Mis
Mis
Ni,  
Oka irotake Abe and Kazuhiko Kato (2006). 
Ray ul and Roger Garside (2000). "Comparing 
Sah an (2006). 
Ten  and Hsin-Hsi Chen (2006). "Detection 
 
 
ï¥´
ç¨®ç­–ï¥¶çš„çµ„åˆï¼Œæˆ–æ˜¯æ¡ç”¨å¤šç­–ï¥¶æŠ•ç¥¨ç­‰
æ–¹å¼ï¼Œçœ‹çœ‹ä½•ç¨®çµ„åˆèƒ½é”åˆ°æœ€å¥½æ•ˆèƒ½ã€‚ 
è³¼ç‰©æ¸…å–®çš„æœ€å¥½è©•ä¼°æ–¹å¼ï¼Œæ˜¯ç›´æ¥è§€
å®¢çš„è³¼è²·ï¨ˆç‚ºã€‚å¦‚æœèƒ½å¤ èˆ‡æ¶ˆè²»ç¶²ç«™çµ
åˆï¼Œåˆ†æå„éƒ¨ï¤˜å®¢å¯¦éš›è³¼è²·æˆ–æ˜¯ç€è¦½éçš„å•†
å“ï¼Œå°±å¯ä»¥æ¯”å°æœ¬ç³»çµ±æ‰€æçš„è³¼ç‰©æ¸…å–®æ˜¯å¦
ç¬¦åˆéƒ¨ï¤˜å®¢éå¾€çš„è³¼ç‰©ç¶“é©—ã€‚ 
è€Œå¦ä¸€æ–¹å‘å‰‡æ˜¯å°‡è³¼ç‰©æ¸…å–®
æˆ–è€…å°‡è³¼ç‰©æ¸…å–®æ‰€å»ºè­°åˆ†ï§çš„å¾…å”®å•†å“
å»£å‘Šçµ¦ä½¿ç”¨è€…ï¼Œå†å»è©•ä¼°ä½¿ç”¨è€…æ˜¯å¦å› ç‚ºè³¼
ç‰©æ¸…å–®è€Œè³¼è²·ï¦ºï¥§åœ¨è¿‘æœŸè³¼ç‰©ç¶“é©—çš„å•†å“ã€‚ 
ç”±ä½¿ç”¨è€…è‡ªï¨ˆä¾è‡ªå·±çš„è³¼è²·æ„é¡˜ï¤­è©•
ä¹Ÿæ˜¯ä¸€ç¨®å¯ï¨ˆçš„æ–¹å¼ã€‚ï¥§éå› ç‚ºè³¼ç‰©çš„
æ…¾æœ›å¸¸å¸¸æ˜¯æ½›åœ¨çš„ï¼Œæ¶ˆè²»è€…å¸¸æœƒå› ç‚ºæç¤º 
(ï¦µå¦‚å»£å‘Š) è€Œæ¿€èµ·è³¼ç‰©éœ€æ±‚ã€‚æ‰€ä»¥è‡ªæˆ‘è©•ï¥¾
ï¥§ä¸€å®šä»£è¡¨çœŸå¯¦å¿ƒï§¤ï§ºæ…‹ã€‚å¯æƒœç¾éšæ®µæ²’æœ‰
æ‰¾åˆ°å”åŠ©å¯¦é©—è€…ã€‚ 
å¯çŸ¥è©•ä¼°éœ€è¦èˆ‡
æœªï¤­è¦åŠƒé€²ï¨ˆçš„ç ”ç©¶æ–¹å‘ã€‚ 
 
 
æ•´
çš„å·¥ä½œé …ç›®ï¼Œä¸”å·²å»ºï§·å¯ç›´æ¥æ‡‰ç”¨ä¹‹è³‡
ï¦¾åº«ã€‚æª¢è¨ä¸­äº¦æå‡ºæœªï¤­èˆ‡æ¥­ç•Œåˆä½œç ”ç©¶çš„
å¯èƒ½æ€§ï¼Œç¾éšæ®µæ‰€å»ºï§·çš„å¯¦é©—ç’°å¢ƒä¹Ÿé©åˆä¸‹
ä¸€ï¦ï¨çš„å¯¦é©—é€²ï¨ˆã€‚ 
 
äº”
Guan, Hu, Jingyu
Class-Feature-Centroid Classifier for Text 
Categorization." In Proceedings of WWW 2009, 
p201-210. 
ari, Pranam, Tim Finin, and Anupam Joshi (2006) 
â€œSVMs for the Blogosphere: Blog Identification and 
Splog Detection,â€ Proceedings of AAAI Spring 
2006 Symposia on Computational Approaches to 
Analyzing Weblogs, 92-99. 
ar, Ravi, Jasmine Novak, Prabhakar Raghavan, and 
Andrew Tomkins (2003) â€œOn the Bursty Evolution 
of Blogspace,â€ Proceedings of the 12th international 
conference on World Wide Web (WWW 2003), 
Budapest, Hungary, 568-576. 
hne, Gilad, David Carmel, and Ronny Lempel (2005) 
â€œBlocking Blog Spam with Language Model 
Disagreement,â€ Proceedings of the First 
International Workshop on Adversarial Information 
Retrieval (AIRWeb '05), Chiba, Japan. 
hne, Gilad and Maarten de Rijke (2006) â€œDeriving 
Wishlists from Blogs - Show Us Your Blog and 
We'll Tell You What Books to Buy,â€ Proceedings of 
the 15th international conference on World Wide 
Web (WWW 2006), Edinburgh, Scotland, 925-926. 
Xiaochuan, Xiaoyuan Wu and Yong Yu (2006).
"Automatic Identification of Chinese Weblogger 
Interests Based on Text Classification."  In 
Proceedings of the 2006 IEEE/WIC/ACM 
International Conference on Web Intelligence, 
p.247-253. 
, Mizuki, H
"Extracting Topics From Weblogs Through 
Frequency Segments." In Proceedings of WWE 
2006. 
son, Pa
Corpora using Frequency Profiling." In Proceedings 
of the 2000 IEEE/WIC/ACM International 
Conference on Web Intelligence, p.1-6. 
ami, Mehran and Timothy D. Heilm
"Web-base Kernel Function for Measuring the 
Similarity of Short Text Snippets." In Proceedings of 
WWE 2006. 
g, Chun-Yuan
of Bloggers' Interest: Using Textual, Temporal, and 
Interactive Features." In Proceedings of the 2006 
IEEE/WIC/ACM International Conference on Web 
Intelligence, p.366-369. 
Proceedings of NTCIR-7 Workshop Meeting, December 16â€“19, 2008, Tokyo, Japan
Overview of the NTCIR-7 ACLIA Tasks:  
Advanced Cross-Lingual Information Access 
  
Teruko Mitamura*   Eric Nyberg*   Hideki Shima*   Tsuneaki Katoâ€     
Tatsunori Moriâ€¡  Chin-Yew Lin#   Ruihua Song#   Chuan-Jie Lin+  
Tetsuya Sakai@   Donghong Jiâ—Š   Noriko Kando**  
*Carnegie Mellon University   â€ Tokyo University   â€¡Yokohama National University 
#Microsoft Research Asia   +National Taiwan Ocean University  @NewsWatch, Inc.   
â—ŠWuhan University  **National Institute of Informatics 
teruko@cs.cmu.edu 
 
 
Abstract 
This paper presents an overview of the ACLIA 
(Advanced Cross-Lingual Information Access) task 
cluster.  The task overview includes: a definition of and 
motivation for the evaluation; a description of the 
complex question types evaluated; the document sources 
and exchange formats selected and/or defined; the 
official metrics used in evaluating participant runs; the 
tools and process used to develop the official evaluation 
topics; summary data regarding the runs submitted; and 
the results of evaluating the submitted runs with the 
official metrics. 
 
1.  Introduction 
 
Current research in QA is moving beyond factoid 
questions, so there is significant motivation to evaluate 
more complex questions in order to move the research 
forward. The Advanced Cross-Lingual Information 
Access (ACLIA) task cluster is novel in that it evaluates 
complex cross-lingual question answering (CCLQA) 
systems (i.e. events, biographies/definitions, and 
relationships) for the first time. Although the QAC4 task 
in NTCIR-6 evaluated monolingual QA on complex 
questions, no formal evaluation has been conducted in 
cross-lingual QA on complex questions in Asian 
languages until now. 
As a central problem in question answering 
evaluation, the lack of standardization has been pointed 
out [1], which makes it difficult to compare systems 
under a certain condition. In NLP research, system 
design is moving away from monolithic, black box 
architectures and more towards modular architectural 
approaches that include an algorithm-independent 
formulation of the systemâ€™s data structures and data 
flows, so that multiple algorithms implementing a 
particular function can be evaluated on the same task. 
Following this analogy, the ACLIA data flow includes a 
pre-defined schema for representing the inputs and 
outputs of the document retrieval step, as illustrated in 
Figure 1. This novel standardization effort made it 
possible to evaluate cross-lingual information retrieval 
(CLIR) task called IR4QA (Information Retrieval for 
Question Answering) in a context of a closely related 
QA task. During the evaluation, the question text and 
QA system question analysis results were provided as 
input to the IR4QA task, which produced retrieval 
results that were subsequently fed back into the end-to-
end QA systems. The modular design and XML 
interchange format supported by the ACLIA architecture 
make it possible to perform such embedded evaluations 
in a straightforward manner. More details regarding the 
XML interchange schemes and so on can be found on 
the ACLIA wiki [6]. 
 
 
Figure 1. Data flow in ACLIA task cluster 
showing how interchangeable data model 
made inter-system and inter-task 
collaboration possible.  
 
The modular design of this evaluation data flow is 
motivated by the following goals: a) to make it possible 
for organizations to contribute component algorithms to 
an evaluation, even if they cannot field an end-to-end 
system; b) to make it possible to conduct evaluations on 
a per-module basis, in order to target metrics and error 
analysis on important bottlenecks in the end-to-end 
system; and c) to determine which combination of 
algorithms works best by combining the results from 
various modules built by different teams. In order to 
evaluate many different combinations of systems 
effectively, human evaluation must be complemented by 
development of automatic evaluation metrics that 
correlate well with human judgment.  Therefore, we 
â€• 11 â€•
Proceedings of NTCIR-7 Workshop Meeting, December 16â€“19, 2008, Tokyo, Japan
among participants and submission of results to be 
evaluated: 
 
â€¢ Topic format: The organizer distributes topics in 
this format for formal run input to IR4QA and 
CCLQA systems.  
â€¢ Question Analysis format: CCLQA participants 
who chose to share Question Analysis results 
submit their data in this format. IR4QA 
participants can accept task input in this format. 
â€¢ IR4QA submission format: IR4QA participants 
submit results in this format. 
â€¢ CCLQA submission format: CCLQA 
participants submit results in this format. 
â€¢ Gold Standard Format: Organizer distributes 
CCLQA gold standard data in this format. 
 
For more details regarding each interchange format, 
see the corresponding examples on the ACLIA wiki [6]. 
 
3.  CCLQA Task 
 
Participants in the CCLQA task submitted results for 
the following four tracks: 
â€¢ Question Analysis Track: Question Analysis 
results contain key terms and answer types 
extracted from the input question. These data are 
submitted by CCLQA participants and released to 
IR4QA participants. 
â€¢ CCLQA Main Track: For each topic, a system 
returned a list of system responses (i.e. answers 
to the question), and human assessors evaluated 
them. Participants submitted a maximum of three 
runs for each language pair. 
â€¢ IR4QA+CCLQA Collaboration Track 
(obligatory): Using possibly relevant documents 
retrieved by the IR4QA participants, a CCLQA 
system generated QA results in the same format 
used in the main track. Since we encouraged 
participants to compare multiple IR4QA results, 
we did not restrict the maximum number of 
collaboration runs submitted, and used automatic 
measures to evaluate the results. In the obligatory 
collaboration track, only the top 50 documents 
returned by each IR4QA system for each 
question were utilized. 
â€¢ IR4QA+CCLQA Collaboration Track 
(optional): This collaboration track was identical 
to the obligatory collaboration track, except that 
participants were able to use the full list of 
IR4QA results available for each question (up to 
1000 documents per topic). 
 
In the CCLQA task, there were eight participating 
teams (see Table 2), supplemented by an Organizer team 
who submitted simple runs for baseline comparison. The 
number of submitted runs is shown in Table 3 for the 
CCLQA main and Question Analysis tracks, and in 
Table 4 for the IR4QA+CCLQA collaboration tracks. 
 
Table 2. CCLQA Task Participants. 
Team Name Organization 
ATR/NiCT National Institute of Information and 
Communication Technology
Apath Beijing University of Posts & Telecoms 
CMUJAV Language Technologies Institute, Carnegie 
Mellon University
CSWHU School of Computer Science, Wuhan 
University
Forst Yokohama National University 
IASL Institute of Information Science, Academia 
Sinica
KECIR Shenyang Institute of Aeronautical 
Engineering
NTCQA NTT Communication Science Labs
Organizer 
(baseline)
ACLIA CCLQA Organizer
 
Table 3. Number of CCLQA runs submitted, 
followed by number of Question Analysis 
submissions in parenthesis. 
Team Name CS-CS EN-CS CT-CT JA-JA EN-JA 
ATR/NiCT 3 3    
Apath 2 (1) 1 (1)    
CMUJAV 3 (1) 3 (1)  3 (1) 3 (1) 
CSWHU 2 (3)     
Forst    1 1 
IASL 2  3   
KECIR 1 (1) 2    
NTCQA    2 1 
Organizer (baseline) 1 1  1 1 
Total by lang pair 14 (6) 10 (2) 3 7 (1) 6 (1) 
Total by target lang 24 (8) 3 13 (2) 
 
Table 4. Number of IR4QA+CCLQA 
Collaboration runs submitted for obligatory 
runs followed by optional runs in 
parenthesis. 
Team Name CS-CS EN-CS CT-CT JA-JA EN-JA
ATR/NiCT  6    
Apath 2 (2)     
CMUJAV 20 (20) 14 (14)  14 (14) 11 (11)
Forst     11 
KECIR (20) (18)    
NTCQA    (14)  
Total by lang pair 22 (42) 20 (32) 0 14 (28) 22 (11)
Total by target lang 42 (74) 0 36 (39) 
 
3.1 .  Answer Key Creation 
 
In order to build an answer key for evaluation, third 
party assessors created a set of weighted nuggets for 
each topic. A "nugget" is defined as the minimum unit 
of correct information that satisfies the information need. 
In the rest of this section, we will describe steps taken to 
create the answer key data. 
 
3.1.1 . Answer-bearing Sentence Extraction  
 
A nugget creator searches for documents that may 
satisfy the information need, using a search engine. 
During this process, a developer tries different queries 
that are not necessarily based on the key terms in the 
â€• 13 â€•
Proceedings of NTCIR-7 Workshop Meeting, December 16â€“19, 2008, Tokyo, Japan
evaluating definition questions, and in the TREC 2006-
2007 QA tracks for evaluating "other" questions.  
A set of system responses to a question will be 
assigned an F-score calculated as shown in Figure 2. We 
evaluate each submitted run by calculating the macro-
average F-score over all questions in the formal run 
dataset. 
In the TREC evaluations, a character allowance 
parameter C is set to 100 non-whitespace characters for 
English [4]. We adjusted the C value to fit our dataset 
and languages. Based on the micro-average character 
length of the nuggets in the formal run dataset (see 
Table 5), we derived settings of C=18 for CS, C=27 for 
CT and C=24 for JA.  
 
Let 
r sum of weights over matched nuggets 
R sum of weights over all nuggets 
HUMANa  # of nuggets matched in SRs by 
human 
L total character-length of SRs 
C character allowance per match 
allowanc
e 
CaHUMAN Ã—  
 
Then 
recall  
R
r
=  
precision  
âªâ©
âª
â¨
â§ <
= otherwise
if1
L
allowance
allowanceL
 
)(Î²F  
recallprecision
recallprecision
+Ã—
Ã—Ã—+
= 2
2 )1(
Î²
Î²  
 
Figure 2.  Official per-topic F-score definition 
based on nugget pyramid method. 
 
Note that precision is an approximation, imposing a 
simple length penalty on the SR. This is due to 
Voorheesâ€™ observation that "nugget precision is much 
more difficult to compute since there is no effective way 
of enumerating all the concepts in a response" [5]. The 
precision is a length-based approximation with a value 
of 1 as long as the total system response length per 
question is less than the allowance, i.e. C times the 
number of nuggets defined for a topic. If the total length 
exceeds the allowance, the score is penalized. Therefore, 
although there is no limit on the number of SRs 
submitted for a question, a long list of SRs harms the 
final F score. 
The )3( =Î²F  or simply F3 score has emphasizes 
recall over precision, with the Î²  value of 3 indicating 
that recall is weighted three times as much as precision. 
Historically, a Î²  of 5 was suggested by a pilot study on 
definitional QA evaluation [4]. In the more recent TREC 
QA tasks, the value has been to 3. Figure 3 visualizes 
the distribution of F3 scores versus recall and precision. 
 
 
Figure 3.  F3 score distribution parameterized 
by recall and precision. 
 
As an example calculation of an F3 score, consider a 
question with 5 gold standard answer nuggets assigned 
weights {1.0, 0.4, 0.2, 0.5, 0.7}. In response to the 
question, a system returns a list of SRs which is 200 
characters in total. A human evaluator finds a conceptual 
match between the 2nd nugget and one of SRs, and 
between the 5th nugget and one of SRs. Then,  
 
39.0
7.05.02.04.00.1
7.04.0
=
++++
+
=recall  
24.0
200
242
=
Ã—
=precision  
37.0
39.024.09
39.024.010)3( =
+Ã—
Ã—Ã—
==Î²F  
 
The evaluation result for this particular question is 
therefore 0.37.  
 
4.2 . Automatic Evaluation Metrics 
ACLIA also utilized automatic evaluation metrics for 
evaluating the large number of IR4QA+CCLQA 
Collaboration track runs. Automatic evaluation is also 
useful during developing, where it provides rapid 
feedback on algorithmic variations under test. The main 
goal of research in automatic evaluation is to devise an 
automatic metric for scoring that correlates well with 
human judgment. The key technical requirement for 
automatic evaluation of complex QA is a real-valued 
matching function that provides a high score to system 
responses that match a gold standard answer nugget, 
with a high degree of correlation with human judgments 
on the same task. 
The simplest nugget matching procedure is exact 
match of the nugget text within the text of the system 
response. Formally, the assessor HUMANa  in Figure 2 is 
replaced by EXACTMATCHa  as follows: 
 
),(Imax sna EXACTMATCH
Nuggetsn SRss
EXACTMATCH âˆ‘
âˆˆ
âˆˆ
=      (1) 
 
â©
â¨
â§
=
otherwise:0
level text surfacein   contains:1
),(I
ns
snEXACTMATCH
    (2) 
â€• 15 â€•
Proceedings of NTCIR-7 Workshop Meeting, December 16â€“19, 2008, Tokyo, Japan
the answer texts for the selected topic. The users type in 
the corresponding answer nugget and click â€œAddâ€ to 
save the update.  
 
5.1.3 . Nugget Voting for Pyramid Method 
 
Figure 16 shows the nugget voting interface, which is 
used to identify vital nuggets from among the set of 
nuggets extracted using the nugget extraction tool.   (See 
details in Section 3.1.3). 
The user first selects a topic title from a list of 
previously completed titles in the Topic Development 
task. The user examines the topic data for the selected 
topic, and toggles the check boxes next to nuggets which 
they judge to be vital.  
 
5.2 . Download and Submission 
 
EPAN is used by each participant to upload their 
submission file for each run submitted. EPAN is also 
used to download intermediate results submitted by 
other participants, as part of an embedded evaluation, 
For example, ACLIA participants were able to 
download the results from Question Analysis and 
IR4QA in order to conduct an embedded CLIR 
evaluation. 
 
5.3 . Evaluation  
 
EPAN provides interfaces for supporting the core 
human-in-the-loop part of evaluation: relevance 
judgment for IR4QA and nugget matching for CCLQA. 
In each task, items to be evaluated belong to a pool 
created by aggregating the system responses from all 
systems, based on run priority. For the three runs 
submitted by each team in each ACLIA task, we created 
three pools of system responses. For the CCLQA task, 
the first pool (corresponding to run 1) was evaluated by 
independent third-party assessors hired by NII. The 
second and third pools (corresponding to runs 2 and 3) 
were evaluated by volunteers including members of the 
participant teams. Details of the CCLQA results are 
provided in Section 6.1. For the embedded IR4QA 
collaboration track, the system responses were evaluated 
automatically; details are provided in Section 6.2. 
 
 
 
6.  Evaluation Results  
 
In this section, we will present official evaluation 
results for the CCLQA main track, IR4QA collaboration 
track, and Question Analysis track. 
 
 
 
 
6.1 . CCLQA Main Track 
 
The official human evaluation results for CCLQA are 
shown in Table 8 through Table 12 for each language 
pair. Runs in Tables 13 through 17 were judged by 
volunteers including members of participant teams. We 
evaluated up to 50 system responses per run per 
question. 
Organizer runs are generated from a sentence 
extraction baseline system, sharing the same architecture 
as CMUJAV but with a minimally implemented 
algorithm that does not take into account answer types. 
The run has been motivated by the SENT-BASE 
algorithm introduced in TREC 2003 definition subtask 
as a baseline [4] that worked surprisingly well, i.e. 
ranked 2nd out of 16 runs. In the question analysis stage, 
the system translates the entire question string with 
Google Translate for crosslingual runs. Then, the system 
extracts all noun phrases as key terms. Subsequently in 
the retrieval stage, the system retrieves documents with 
Indriâ€™s simplest query form, â€œ#combine()â€. Finally, in 
the extraction phrase, starting from the highest ranked 
document, the baseline system selects sentences that 
contain one of the key terms, until a maximum of 50 
system responses have been gathered. 
 
6.1.1 Official Runs 
 
Table 8. EN-CS official human evaluation. 
EN-CS Runs DEF BIO REL EVE ALL 
ATR/NiCT-EN-CS-01-Tã©· 0.2216ã©· 0.3158 0.2335ã©· 0.1454 0.2211
CMUJAV-EN-CS-01-Tã©· 0.2129ã©· 0.2678 0.1884ã©· 0.1346 0.1930
KECIR-EN-CS-01-Tã©· 0.2493ã©· 0.2563 0.1584ã©· 0.1364 0.1895
Apath-EN-CS-01-Tã©· 0.1694ã©· 0.1165 0.1188ã©· 0.0706 0.1140
Organizer-EN-CS-01-Tã©· 0.1358ã©· 0.1417 0.1052ã©· 0.0793 0.1108
 
Table 9. CS-CS official human evaluation. 
CS-CS Runs DEF BIO REL EVE ALL 
CSWHU-CS-CS-01-T 0.4752 0.6012 0.4592 0.2662 0.4329
ATR/NiCT-CS-CS-01-T 0.2415 0.3376 0.2429 0.1430 0.2316
IASL-CS-CS-01-T 0.1536 0.3245 0.2548 0.1043 0.2034
CMUJAV-CS-CS-01-T 0.2326 0.2498 0.2321 0.1219 0.2027
Apath-CS-CS-01-T 0.1800 0.1662 0.2067 0.1298 0.1702
Organizer-CS-CS-01-T 0.1360 0.1248 0.1101 0.0640 0.1044
 
Table 10.  CT-CT official human evaluation. 
CT-CT Runs DEF BIO REL EVE ALL 
IASL-CT-CT-01-T 0.3020 0.4075 ã©· 0.2509 ã©· 0.1650 0.2666 ã©·
 
Table 11.  EN-JA official human evaluation. 
EN-JA Runs DEF BIO REL EVE ALL 
CMUJAV-EN-JA-01-T 0.3772 0.1250 ã©· 0.1641 0.0433 ã©· 0.1627 ã©·
Organizer-EN-JA-01-T 0.1938 0.1187 ã©· 0.1253 0.0439 ã©· 0.1133 ã©·
Forst-EN-JA-01-T 0.1785 0.1403 ã©· 0.1103 0.0516 ã©· 0.1123 ã©·
NTCQA-EN-JA-01-T 0.1699 0.0932 ã©· 0.0476 0.0023 ã©· 0.0676 ã©·
 
Table 12.  JA-JA official human evaluation. 
JA-JA Runs DEF BIO REL EVE ALL 
CMUJAV-JA-JA-01-T 0.4201 0.1900 0.2332 0.0937 0.2201 ã©·
NTCQA-JA-JA-01-T 0.2888 0.1788 0.2209 0.0915 0.1873 ã©·
Organizer-JA-JA-01-T 0.2537 0.1527 0.1458 0.0916 0.1525 ã©·
Forst-JA-JA-01-T 0.2313 0.1598 0.1161 0.0786 0.1366 ã©·
â€• 17 â€•
Proceedings of NTCIR-7 Workshop Meeting, December 16â€“19, 2008, Tokyo, Japan
 
7.  Further Analysis 
 
In this section, we present further analysis that was 
performed following the human and automatic 
evaluations of the run data. 
 
7.1 . Reliability of Automatic Evaluation 
 
This subsection discusses the correlation between the 
automatic and human evaluation metrics presented in 
Section 4.2, for three different nugget matching 
algorithms (see Table 23). 
 
Table 23.  Per-run and per-topic correlation 
between automatic nugget matching and 
human judgment 
Algorithm Token 
Per run 
( 40N = ) 
Per topic 
( 10040N Ã—= )
Pearson Kendall Pearson Kendall 
EXACTMATCH CHAR 0.4490 0.2364 0.5272 0.4054 
SOFTMATCH CHAR 0.6300 0.3479 0.6383 0.4230 
BINARIZED CHAR 0.7382 0.4506 0.6758 0.5228 
 
We compared per-run (# of data points = # of human 
evaluated runs for all languages) and per-topic (# of data 
points = # of human evaluated runs for all languages 
times # of topics) correlation between scores from 
human-in-the-loop evaluation and automatic evaluation. 
The Pearson measure indicates the correlation between 
individual scores, while the Kendall measure indicates 
the rank correlation between sets of data points.   
The results show that our novel nugget matching 
algorithm BINARIZED outperformed SOFTMATCH 
for both correlation measures, and we chose 
BINARIZED as the official automatic evaluation metric 
for the CCLQA task. 
The plots in Figure 4 compare the per-run human vs. 
automatic scores, measured using F3 with the different 
nugget matching algorithms. We can roughly observe 
that the distribution of points in the BINARIZED plot 
falls roughly between the distributions of 
EXACTMATCH and SOFTMATCH. 
The plots in Figures 5 and 6 compare the official 
human-in-the-loop and automatic metric scores using 
three different nugget matching algorithms. Scores 
generated by the human-in-the-loop and automatic 
metrics appear to correspond, although there are some 
exceptions such as can be seen in runs ATR/NiCT-CS-
CS-01-T, Apath-CS-CS-01-T, Forst-EN-JA-01-T and 
Forst-JA-JA-01-T. We plan to analyze the possible 
causes for these outlying data points. 
 
EXACTMATCH  
BINARIZED SOFTMATCH  
 
Figure 4. Per topic plot for Official (human-in-the-
loop) vs. three kinds of automatic metric scores.  
 











	

	

	



	

	
	


































































 !








 !






"
#


$







"
#


$








% 
&'
 
(
)
* 
+(
)
(&
+*
,
"
--
(

.)/

%'
0
0
 
!*
.&
&
1)
(&
+*
,
%0,
"--(.)2
 &- ' (!
30+4*5 +5' (!
 
Figure 5. Human-in-the-loop vs three automatic 
metric scores on CS CCLQA Main track data 
 






	

6


	

	

	



	

	












7


















"
#


$








2&
+,
 






"
#


$








2&
+,
 








7







 
% 
&'
 
(
)
* 
+(
)
(+
&*
,
"
--
(

.)/

%'
0
0
 
!*
.&
&
1)
(&
+*
,
%0,
"--(.)2
 &- ' (!
30+4*5 +5' (!
 
Figure 6. Human-in-the-loop vs three automatic 
metric scores (JA CCLQA Main track data)
â€• 19 â€•
Proceedings of NTCIR-7 Workshop Meeting, December 16â€“19, 2008, Tokyo, Japan






	







	

6





































































 !








 !






"

#


$







"

#


$








89
):
)&
-)
,
(
&+
*,
%0,
"--(.)2
 "--(.)*(..
"--(.);+*(,&0 89):)&-),
 
Figure 8. Official scores and Avg. # System 
Response for EN-CS and CS-CS runs.    
 

	

	

	



	

	
	

	

	

	



	













7



















"

#


$








2&
+,
 






"

#


$







2&
+,
 








7









89
):
)&
-)
,
(
&+
*,
%0,
"--(.)2
 "--(.)*(..
"--(.);+*(,&0 89):)&-),
 
Figure 9. Official scores and Avg. # System 
Responses for EN-JA and JA-JA runs. 
 
7.5 . Crosslingual vs Monolingual Runs 
 
To compare the distribution of scores between 
crosslingual and monolingual, we plotted official scores 
from all officially evaluated runs in Figure 12 and 
Figure 13 for CS and JA respectively. Interestingly, 
crosslingual (crosses) outperforms monolingual (circles) 
for many topics in CS, but the same is not true for JA. 
 
8.  Conclusion 
 
This paper presented an overview of the ACLIA 
(Advanced Cross-Lingual Information Access) task 
cluster at NTCIR-7, with a specific focus on the 
CCLQA evaluations.  We described the official metrics 
used in evaluating participant runs; the tools and process 
used to develop the official evaluation topics; summary 
data regarding the runs submitted; and the results of 
evaluating the submitted runs with the official metric. 
Novel aspects of the evaluation included cross-lingual, 
complex QA evaluation for Chinese and Japanese 
corpora, and an embedded evaluation of information 
retrieval technologies used for QA.  We utilized 
automatic evaluation metrics for the embedded 
evaluation, and analyzed both per-topic and per-run 
correlation between human-in-the-loop and automatic 
evaluation. We also analyzed performance on topics 
shared across the different language tracks, and found 
that for some topics, crosslingual QA performance was 
better than monolingual performance.  
We hope that the results of the NTCIR-7 ACLIA task 
will contribute to continued rapid progress in 
Information Retrieval and Question Answering. 
 
Acknowledgements 
 
We thank Jim Rankin (CMU) for helping with the 
development and maintenance of the EPAN tool set.  
We also thank Xinhua, Zaobao, CIRB and Mainichi for 
providing the corpora that were used for the evaluations. 
We also thank Fred Gey (UC Berkeley) Kui-Lam Kwok 
(Queens College) for their valuable advice. We also 
greatly appreciate the efforts of all the ACLIA 
participants.  
 
References 
 
[1] Lita, L.V. Instance-Based Question Answering. Doctoral 
Dissertation, Computer Science Department, Carnegie 
Mellon University. 2006. 
[2] Sakai, T., N. Kando, C.-J. Lin, T. Mitamura, H. Shima, D. 
Ji, K.-H. Chen, E. Nyberg. Overview of the NTCIR-7 
ACLIA IR4QA Task, Proceedings of NTCIR-7, 2008.  
[3] Lin, J., and D. Demner-Fushman. Will pyramids built of 
nuggets topple over?, Proceedings of the main conference 
on Human Language Technology Conference of the North 
American Chapter of the Association of Computational 
Linguistics. 2006. 
[4] Voorhees, E. M. Overview of the TREC 2003 Question 
Answering Track, Proceedings of TREC 2003, 2004. 
[5] Voorhees, E. M. Overview of the TREC 2004 Question 
Answering Track, Proceedings of TREC 2004, 2005. 
[6] The ACLIA wiki, http://aclia.lti.cs.cmu.edu 
[7]  Indri search engine: http://www.lemurproject.org 
[8] Lin, J., and D. Demner-Fushman. Methods for 
Automatically Evaluating Answers to Complex Questions, 
Information Retrieval, 9(5):565-587, 2006. 
[9] Asahara, M., Y. Matsumoto. Japanese Named Entity 
Extraction with Redundant Morphological Analysis, 
Proceedings of NAACL/HLT 2003, 2003. 
[10] Fukumoto, J., T. Kato, F. Masui, T. Mori. An Overview of 
the 4th Question Answering Challenge (QAC-4) at NTCIR 
Workshop 6, Proceedings of NTCIR-6, 2007. 
 
â€• 21 â€•
Proceedings of NTCIR-7 Workshop Meeting, December 16â€“19, 2008, Tokyo, Japan
0.0 0.2 0.4 0.6 0.8 1.0
1
6
12
19
26
33
40
47
54
61
68
75
82
89
96
1
6
12
19
26
33
40
47
54
61
68
75
82
89
96
Official Score
To
pi
c
 
0.0 0.2 0.4 0.6 0.8 1.0
1
6
12
19
26
33
40
47
54
61
68
75
82
89
96
1
6
12
19
26
33
40
47
54
61
68
75
82
89
96
Official Score
To
pi
c
 
Figure 12. Official CS scores plotted for each 
topic where circles are for monolingual and 
crosses are for crosslingual runs. 
Figure 13. Official JA scores plotted for each 
topic where circles are for monolingual and 
crosses are for crosslingual runs. 
â€• 23 â€•
