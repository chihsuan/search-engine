  1
 
Abstract‚ÄîA greedy or unresponsive server with 
multiple flows may contribute to unfair bandwidth 
sharing among servers, especially when the network 
becomes congested. We examine the inter-server fairness 
problem in traditional AQM designs. We propose a new 
AQM algorithm for providing inter-server fairness 
services. The main idea behind our proposed algorithm is 
to schedule packets from different servers into dedicated 
virtual queues. The dropping probabilities of the queues 
are determined independently by the occupancy of the 
individual queues. The proposed queue management 
algorithm specifically keeps the inter-server fairness in 
the case of excessive flows entering the network. Using 
the ns-2 simulator, we develop a set of simulations to 
verify the effectiveness of the queue management 
algorithms operating in various traffic conditions. 
Index Terms‚Äîactive queue management, inter-server 
fairness, virtual queues, packet scheduling, network 
congestion 
I. INTRODUCTION 
In a congested network, a server may generate a large 
number of responsive or unresponsive flows for a particular 
job in an attempt to grab more bandwidth of congested links. 
Such a server is called a greedy server. Here, a server refers 
to an aggregate of multiple flows or multiple nodes 
constituting a customer domain in an ISP network. A greedy 
server may contribute to unfair bandwidth sharing among 
servers, especially when the network becomes congested. In 
this proposal, we define inter-server fairness as the fair 
sharing of bandwidth among servers passing through a 
congested link. 
In the past decade, AQM has been extensively studied, 
for example, in the literature [1-17]. Many algorithms for 
AQM have been proposed. They address issues such as 
designing and measuring efficiency of congestion control 
mechanisms. Most of these algorithms seek to prevent 
network congestion under the cooperation of responsive or 
TCP-friendly source algorithms. Some of these algorithms 
can also ensure fair sharing of bandwidth among flows with 
the existence of unresponsive flows [4, 12, 18, 19]. However, 
most of the previously proposed algorithms primarily 
focused on characterizing the congestion control 
performance on a per-flow basis. They do not adequately 
address the issue of fair bandwidth sharing among servers. 
An AQM starts to drop or mark packets when 
congestion occurs. All TCP-friendly sources passing the 
congested link respond to it in the same manner, and thus 
they are equally influenced by it. This situation enables a 
flow to obtain its fair share of bandwidth under the control of 
a well-designed AQM. By simply issuing more flows, 
however, an unresponsive server may take its chance of 
obtaining a higher bandwidth share than the other servers, 
even if it is equipped with a lower leased bandwidth. This 
unfair bandwidth allocation among servers hampers the 
enforcement of price-oriented services policies. 
We examine the inter-server fairness problem in the 
traditional AQM designs. The main idea behind our proposed 
research is to schedule packets from different servers into 
dedicated virtual queues. The dropping probabilities of the 
queues are determined independently by the occupancy of 
these individual queues. Our first step is to identify major 
traffic parameters of the inter-server fairness, and investigate 
their impact on the fairness of a congested network. Then we 
will take one further step to investigate the inter-server 
problem with an excessive number of flows.  As the number 
of flows increases, the time and storage complexities of 
queue management grow exponentially.  Under this 
circumstance, reducing the management complexities 
becomes a crucial consideration in designing practical queue 
management algorithms.  
In this report, we present an efficient queue management 
algorithm specifically to keep the inter-server fairness in the 
case of excessive flows entering the network. Using the ns-2 
simulator, we develop a set of simulations to verify the 
effectiveness of the queue management algorithms operating 
in various traffic conditions.   
II. FAIRNESS MEASUREMENT 
 We start with defining an index that is used to quantify 
fairness of the resource allocation. A good index is expected 
to include the following properties [20]: 
1. Population size independence: 
The index should be applicable to an arbitrary 
number of users. 
2. Scale and metric independence:  
The index should be independent of scale, i.e., the 
unit of measurement should not matter. 
3. Boundedness:  
The index should be in the range of 0 and 1, i.e., a 
completely fair system has an index of 1; on the other 
hand a completely unfair system has an index of 0. 
4. Continuity:  
The index should be continuous. Any slight change 
in allocation should show up in the index. 
Jain et al. in [21] propose a quantitative formula shown to 
fulfill all the above desired properties, and it has been widely 
adopted in related researches as a measurement of fairness. 
We refer this index to as the Jain‚Äôs fair index in this proposal. 
Assume that a congested network link allocates bandwidth to 
Achieving Inter-server Fairness by Using 
Virtual Queues in AQM 
Woei Lin
*Department of Computer Science, National Chung Hsing University, Taiwan, R.O.C. 
Email: wlin@nchu.edu.tw 
  3
Namely, if Li and LCi represent the queue length and the 
leased bandwidth of server i, respectively, then SF-RED 
remains at 
1 1
i iL LC
L LC
= . The drop probability of maxp_i is 
adapted according to any bias. SF-RED works on the 
principle that if the queue occupancy ratio of two servers can 
be controlled, then the bandwidth ratio allocated to them 
follows.  
In RED, the physical queue length is determined from 
the queue threshold (minth, maxth), packet maximum drop 
probability (maxp) and real traffic loading. The setting of 
these control parameters also affects queue stability, which is 
a significant performance metric in congestion control. To 
keep the real queue size within a preset value, and thus 
enhance the control of packet delay and packet loss, SF-RED 
attempts to maintain the physical queue length within a preset 
target value qt. The minth of each virtual queue is adjusted if 
the measured average queue size, qavg, of the physical queue 
is outside the target range. The pseudocode of the SF-RED 
algorithm is shown below. 
 
New packet arrives: 
  Identify data source by sender IP 
  Update virtual queue information 
Every  t  seconds:  
If (qavg > qtr) then 
  for each virtual queue i do 
     minth_i=minth_i - alpha 
else if (qavg < qtr ) then 
     for each virtual queue i do 
      minth_i=minth_i + alpha 
   end if 
end if 
for each virtual queue i and i !=1 do 
  if (qavg_i / qavg_1 < LCi/LC1)  
maxp_i=maxp_i + beta 
  else if (q avg_i / q avg_1 > LCi/LC1) 
       maxp_i=maxp_i ‚Äì beta 
     end if 
  end if 
Variables: 
qt : target queue length 
qtr=[(1+10%)qt,(1-10%)qt] 
LCi : leased bandwidth of server i 
Constants: 
t=1 sec, alpha=1, beta=0.1  
 
IV. FAIRNESS PERFORMANCE OF SF-RED 
To show the effectiveness of SF-RED, a sequence of 
experiments are performed using the well-known simulator 
ns-2, and their results are compared with the output of RED. 
All of the following simulations assume N = 10 without loss 
of generality. The leased bandwidth and the flow size of each 
server are adjusted accordingly to the flow variance and 
leased bandwidth variance. The following figures show the 
simulation results. These figures show that SF-RED not only 
provides high quality of inter-server fairness service to each 
server, but also effectively controls the average FIFO queue 
length and the queue stability. 
z Inter-server fairness 
Figures 2 and 3 present the comparisons of inter-server 
fairness performance for RED and SF-RED. Unlike RED, 
where server bandwidth share is proportional to its flow 
size, SF-RED can always allocate fair shares of 
bandwidth to servers, even in the presence of a hot spot. 
 
Figure 2. Comparison of the inter-server fairness 
performance of SF-RED and RED. (No Hot spot) 
 
 
 
 
Figure 3. Comparison of the inter-server fairness 
performance of SF-RED and RED. (With hot spot.) 
 
 
z Queue controllability and stability 
Figure 4 shows the queue variation of RED, while 
Figures. 5, 6, and 7 show the queue variation of SF-RED 
for three different cases with target queue lengths of to 15, 
25, and 40, respectively. The same traffic loading is fed 
into all these simulations, and Fvar=30, LBvar=20 are set. 
The parameters of RED, i.e. minth, maxth, maxp and qw, are 
carefully chosen based on the recommendation of [22]. 
These results indicate that the proposed SF-RED scheme 
outperforms RED by better stabilizing the queue size 
while still maintaining a high level of inter-server fairness. 
SF-RED can stabilize a congested queue at a target length, 
enabling it to achieve a low packet delay and packet loss, 
which are considered as two of the major performance 
metrics of AQM. 
SF-RED
RED Fa
ir 
In
de
x 
Flow Variance 
SF-RED
RED
Fa
ir 
In
de
x 
Leased bandwidth variance 
  5
Using Packet Loss Ratio Measurement,‚Äù IEEE 
Transactions on Parallel and Distributed Systems, Vol. 
18, No. 1, January 2007, pp. 29-43. 
4. E. Anjum and L. Tassiulas, ‚ÄúFair bandwidth sharing 
among adaptive and nonadaptive flows in the internet,‚Äù 
in Proceeding of IEEE INFOCOM‚Äô99, March 1999, 
pp.1412-1420. 
5. F. Ren, X. Yin, X. Huang, and C. Lin, ‚ÄúDesigning a 
Binary Controller to Enhance End-to-End Congestion 
Control,‚Äù IEEE Communications Letters, Vol. 10, No. 
12, December 2006, pp. 858-860. 
6. G. Feng, A. K. Agarwal, A. Jayaraman, and C. K. Siew, 
‚ÄúModified RED Gateways Under Bursty Traffic,‚Äù 
IEEE Communications Letters, Vol. 8, No. 5, May 
2004, pp. 323-325. 
7. J. Aweya, M. Ouellette, and D. Y. Montuno, ‚ÄúA control 
theoretic approach to active queue management‚Äù, 
Computer Networks, vol. 36, July 2001, pp. 203‚Äì235. 
8. J. Aweya, M. Ouellette, D. Y. Montuno and A. 
Chapman. ‚ÄúEnhancing TCP Performance with a 
Load-adaptive RED Mechanism,‚Äù International 
Journal of Network Management, Vol.11, No.1, 2001, 
pp. 31-50. 
9. J. Sun, G. Chen, S. Chan and M. Zukerman, 
‚ÄúPD-Controller: A New Active Queue Management 
Scheme,‚Äù in Proceeding IEEE GLOBECOM‚Äô03, 
December 2003, pp.3103-3107. 
10. M. Kwon and S. Fahmy, ‚ÄúA comparison of load-based 
and queue-based active queue management 
algorithms,‚Äù in Proceeding of SPIE ITCom, vol. 4866, 
August 2002, pp. 35-46. 
11. M. Mathis, J. Mahdavi, S. Floyd, and A. 
Romanow, ‚ÄùControlling High-Bandwidth Flows at a 
Congested Router,‚Äù Technical Report 01-001,.ICSI, 
April 2001. 
12. S. Athuraliya, D. E. Lapsley, and S. H. Low, ‚ÄúAn 
enhanced random early marking algorithm for internet 
flow control,‚Äù in Proceeding of IEEE INFOCOM‚Äò00, 
March 2000, pp. 1425-1434. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
13. S. Deb and R. Srikant, ‚ÄúCongestion Control for Fair 
Resource Allocation in Networks With Multicast 
Flows,‚Äù IEEE/ACM Transactions on Networking, Vol. 
12, No. 2, April 2004, pp. 274-285. 
14. S. Floyd, R. Gummadi, and S. Shenker, ‚Äù Adaptive 
RED: An Algorithm for Increasing the Robustness of 
RED‚Äôs Active Queue Management,‚Äù available at 
http://www.arl.wustl.edu/~gorinsky/spring2005/cse57
3s/adaptiveRed.pdf, August 1, 2001. 
15. S. Floyd, ‚ÄúRecommendation on using the gentle variant 
of RED,‚Äù available on 
http://www.aciri.org/floyd/red/gentle.html, March 
2000. 
16. S. S. Kunniyur, R. Srikant, ‚ÄùAn adaptive Virtual 
Queue(AVQ) Algorithm for Active Queue 
Management,‚Äù IEEE/ACM Transactions on 
Networking, Vol. 12, No. 2, April 2004, pp. 286-299. 
17. W. Feng, D. Kandlur, D. Saha, and K. Shin. ‚ÄúA 
Self-Configuring RED Gateway,‚Äù In Proceeding of 
IEEE INFOCOM‚Äô99, March 1999, pp. 1320-1328. 
18. M. Shreedhar and G. Varghese, ‚ÄúEfficient fair queuing 
using deficit round robin,‚Äù IEEE/ACM Transactions on 
Networking, vol. 4, June 1996, pp. 375‚Äì385. 
19. W. Feng, K. G. Shin, D. D. Kandlur and D. Saha, ‚ÄúThe 
BLUE Active Queue Management Algorithms,‚Äù 
IEEE/ACM Transactions on Networking, Vol. 10, No.4, 
August 2002, pp. 513-528. 
20. K. Ramakrishan and R. Jain, ‚ÄúA binary feedback 
scheme for congestion avoidance in computer 
networks,‚Äù ACM Transactions on Computer Systems 
(TOCS, Vol. 8, No. 2, May 1990, pp. 158‚Äì181. 
21. R. Jain, D. Chiu, and W. Hawe, ‚ÄúA Quantitative 
Measure of Fairness and Discrimination for Resource 
Allocation in Shared Computer Systems‚Äù, DEC 
Research Report TR-30, September 1984. 
22. S. Floyd and V. Jacobson, ‚ÄúRandom early detection 
gateways for congestion avoidance,‚Äù IEEE/ACM 
Transactions on Networking, Vol. 1, No. 4, August 
1993, pp. 397‚Äì413. 
A Frame-based Architecture with Shared Buffers for Slotted Optical Packet 
Switching 
 
 
Guan-Hong Jhou and Woei Lin, Member, IEEE 
Department of Computer Science, National Chung-Hsing University 
{phd9604, wlin}@cs.nchu.edu.tw 
 
 
Abstract 
 
Because of the lack of random access memory in 
optical domain, optical buffering implemented by fiber 
delay lines is currently a main way. Typical optical 
buffering architecture falls into two categories: feed-
forward and feedback buffering. Both have advantages 
and disadvantages. In this paper, we propose an 
effective hybrid buffering architecture based on 
output-buffered feed-forward and feedback shared 
buffering architecture. The proposed architecture 
employs feed-forward output buffers to connect 
packets to form a variable-length frame, and employs 
a feedback shared buffer with a large storage frame to 
handle frames behind feed-forward output buffers. 
This scheme uses a frame concept to reduce the 
control complexity and increase the buffer depth. Our 
simulation results show that the proposed architecture 
improves the switch performance and outperforms an 
existing hybrid buffering architecture, partially shared 
buffering architecture, in terms of packet loss 
probability.  
 
1. Introduction 
 
Recently, the explosive growth of the Internet 
traffic and multimedia applications has led to 
increasing demand on high-speed data transmission 
and switching. All-optical packet switching is a 
promising scheme to meet this demand. However, 
since packets may arrive in an uncoordinated fashion, 
all-optical packet switching suffers an output 
contention problem.  Optical buffering implemented by 
fiber delay lines (FDLs) is a main solution to the 
output contention problem. Packets can be buffered in 
FDLs to avoid the output contention in optical 
buffering. 
Unlike random access memory (RAM) buffers for 
electronic switches, optical buffering does not truly 
store optical packets in the FDL buffer. The FDL 
buffer only provides optical packets a discrete set of 
delays to avoid packet contention. Therefore, we need 
an effective buffering architecture to schedule and 
suitably delay packets. The buffering architecture and 
the corresponding scheduling algorithms are the most 
challenging issues to all-optical packet switching. 
Typical optical buffering architecture falls into two 
categories: feed-forward and feedback buffering [1-4]. 
In feed-forward buffering, a contending packet is 
delayed in the buffer and leaves going a suitable length 
FDL. In feedback buffering, the delayed packet re-
enters the buffer until the egress port is available. Both 
have advantages and disadvantages. The scheduling of 
feed-forward buffering architecture is simple but not 
effective. The scheduling of feedback buffering is 
effective but complex because of the feedback control. 
Effective hybrid buffering architecture can combine 
the merits of both categories. Juan and Chu proposed 
the PSB (partially shared buffering) architecture in [5]. 
It is intended to take advantage of feed-forward and 
feedback architectures as well. The PSB architecture is 
based on an output buffering scheme using feed-
forward buffering architecture on each output, and 
combined an additional feedback shared buffer for all 
outputs. Different from conventional output buffering, 
the PSB architecture is capable of handling bursty 
traffic from the feedback shared buffer. Moreover, Li, 
Scott, and Deogun also proposed a hybrid buffering 
architecture based on both categories and wavelength 
routing approach in [6]. It employs the arrayed 
waveguide grating (AWG) and the tunable wavelength 
converter (TWC) to construct a multiple wavelength 
switch. The multiple wavelength design increases the 
buffer depth and the hybrid buffering architecture 
reduces the required number of wavelengths. However, 
the feedback shared buffer of hybrid buffering 
architecture needs to handle all arriving packets, and 
maintain a logical output queue of each output port 
with central control for the first-in-first-out (FIFO) 
service discipline. The control of the feedback shared 
2009 11th IEEE International Conference on High Performance Computing and Communications
978-0-7695-3738-2/09 $25.00 ¬© 2009 IEEE
DOI 10.1109/HPCC.2009.73
322
Authorized licensed use limited to: National Chung Hsing University. Downloaded on July 21, 2009 at 05:07 from IEEE Xplore.  Restrictions apply. 
»ñ==B true
c--
»ñ++
c>0 Drop
Send a path 
request to the 
shared buffer
true
accept
false
false
reject
Switch the 
packet to »ñ
c--
»ñ=0
k++
B : the number of FDLs
»ñ : the access pointer
c : the input counter
 
Figure 3. The flowchart of the reuse-
based scheduling scheme 
2.1. The Proposed Architecture 
 
The concept of the proposed architecture is to 
distribute the control among the feed-forward output 
buffers and the feedback shared buffer. It employs a 
large storage frame to increase the buffer depth of the 
shared buffer. The proposed architecture is illustrated 
in Figure 2. It is divided into three parts: input modules, 
output buffer modules and a shared buffer.  The input 
modules and output buffer modules are an output-
buffered switching architecture with distributed and 
pipelined control based on [9]. Input modules deliver 
packets to their destined output buffer modules, and 
each output buffer module handles packets 
independently. Each feed-forward output buffer 
includes B variable-length FDLs that are of length 
from 0 to B-1. In contrast, the feedback shared buffer 
consists of M fixed length FDLs. It employs a large 
FDL to achieve a large storage frame and each FDL 
can store B packets for one time slot. In other words, 
the proposed can provide M*B buffer depth of the 
shared buffer.  
The key problem is how to solve the out-of-order 
problem of the large storage frame. We propose a 
reuse-based scheduling scheme of feed-forward output 
buffers to connect packets to be a variable-length 
frame in order. Figure 3 shows the flowchart of the 
reuse-based scheduling scheme. The reuse-based 
scheduling scheme employs an access pointer »ñ to 
switch the arriving packet. The access pointer points to 
the FDL at the tail of the last packet in the buffer. 
When the access pointer points to a FDL, the arriving 
packet is switched to the next FDL and then the access 
pointer is shifted to the next FDL that is the tail of the 
new last packet. In the time axis, this switching 
appends the arriving packet to the tail of the last packet 
for simulating a FIFO queue. Specifically, the reuse-
based scheduling scheme intends to reuse FDLs, 
because all packets were shifted a time slot during the 
previous time slot and then all FDLs become available. 
In other words, each output buffer is capable of 
handling B packets per time slot. The reuse-based 
scheduling scheme employs a counter c to denote the 
number of remaining available FDLs. The arriving 
packet will be determined dropping or not on the basis 
of the counter c. Different from conventional 
scheduling of feed-forward buffering, the reuse-based 
scheduling scheme does not drop the arriving packet 
when the access pointer points to the last FDL. When 
the access pointer points to the last FDL and the 
counter c is greater than zero, the reuse-based 
scheduling scheme would send a path request to the 
shared buffer to construct a new delay path, and shifts 
the access pointer to the first FDL. This scheme 
effectively enhances the buffer utilization of feed-
forward output buffers. Moreover, since all packets 
were shifted a time slot during the previous time slot, c 
Figure 4. Behavior of the reuse-based 
scheduling scheme 
 
Figure 5. A collision problem of the 
feedback buffer 
324
Authorized licensed use limited to: National Chung Hsing University. Downloaded on July 21, 2009 at 05:07 from IEEE Xplore.  Restrictions apply. 
The switch dimension N equals 16 in the most part of 
our simulation. For comparison of the capability of 
handling bursty traffic between the PSB architecture 
and our architecture, we employ a bursty traffic model 
based on [10] to simulate the bursty nature of network 
traffic. In the bursty traffic model, bursty traffic is 
generated using the two-state ON-OFF model, as 
shown in Figure 8, that is alternating a geometrically 
distributed period during which no arrivals occur with 
one during which arrivals occur in a Bernoulli process.  
Hence, we derived average burst length (ABL), average 
idle length (AIL) and average load (AL) as follows: 
(1) 
 (2) 
1for        /1)1(
1
1 ‚â•=‚àí=¬¶‚àû
=
‚àí ippipABL
i
i ŒìŒì
0for/)1()1(
0
‚â•‚àí=‚àí=¬¶‚àû
=
jrrrjrAIL
j
j ŒìŒì
rppr
r
ABLAIL
ABLAL
‚àí+=+= .             (3) 
 
Figure 8. The two-state ON-OFF model 
ÀÆ 
(
(
(
(
(
(
(
(
(
(
(
               
)'/
SD
FN
HW
OR
VV
S
UR
ED
EL
OLW
\
$QDO\VLV1  6LPXODWLRQ1  $QDO\VLV1  6LPXODWLRQ1 
Figure 9. Comparison between analysis and 
simulation results for the packet loss 
probability of feed-forward output buffers 
We offer average load and a given average burst 
length to calculate the state transition probabilities of 
the ON-OFF model. 
 
3.1. Analysis of Packet Loss at Feed-forward 
Output Buffers 
 
In the proposed architecture, packet loss occurs at 
feed-forward output buffers when arriving packets 
exceed the capability of handling B packets per time 
slot. Since each feed-forward output buffer can handle 
B packets per time slot by the reuse-based scheduling 
scheme and the maximum number of arriving packets 
is N packets per time slot, the expected value of the 
packet loss of feed-forward output buffers can be 
expressed as 
¬¶ += ‚àí= N Bx xBx
nBE
1
)Pr()(    
]port output  at the  exceed packets arriving[
(4) 
where Pr(x) is the probability of x packets arriving at a 
particular output port during any time slot and (x-B) is 
the number of lost packets. The packets arrive at any 
given input port during an arbitrary time slot with a 
probability »°. If the arriving packets are uniformly 
distributed to all the output ports, the probability that 
an input port has an arriving packet destined to a 
particular output port is »°/N. The probability that an 
input port has no arriving packet destined to a 
specified output port is (1-»°/N). Thus, the probability 
of x packets arriving at a particular output port during 
any time slot can be expressed as 
xNx NNFx ‚àí‚àí= )/1()/()Pr( œÅœÅ              (5) 
where F is a factor reflecting the number of ways that 
x packets could arrive among N input ports. These x 
arriving packets can be arranged among N input ports 
in N!/(N - x)! ways. Because no distinction is made 
among arriving packets with the same destined output 
port, the number of ways that x packets could arrive 
among N input ports is 
)!(!
!
xNx
NF
‚àí
=
.               (6) 
From (4) and (5), the packet loss probability of feed-
forward output buffers can be rewritten as 
œÅœÅœÅ
œÅ
¬¶
¬¶
+=
‚àí
+=
‚àí‚àí=
‚àí=
=
N
Bx
xNx
N
Bx
NNFBx
xBx
nE
nBEf
1
1
)/1()/()(    
)Pr()(    
]port output  at the packets arriving[
]port output  at the  exceed packets arriving[
Œì
. (7) 
Based on (7), analysis results can be calculated for the 
packet loss probability of feed-forward output buffers. 
The analysis and simulation results are compared in 
Figure 9 for »°=1. It is evident that the analysis results 
match well with the simulation results. This confirms 
the accuracy of our analysis.  
Moreover, the packet loss probability of feed-
forward output buffers is equal to zero in the analysis 
and simulation results, when B is higher than or equal 
to N. However, zero packet loss probability may not be 
necessary. A low packet loss probability is acceptable 
for reducing FDLs of feed-forward output buffers. It is 
worth noting that the packet loss probability of feed-
forward output buffers decreases exponentially with 
the increase of FDLs of feed-forward output buffers. 
This means that we only need a few FDLs to achieve a 
326
Authorized licensed use limited to: National Chung Hsing University. Downloaded on July 21, 2009 at 05:07 from IEEE Xplore.  Restrictions apply. 
