a method that is based on the successive frame dif-
ference and global color statistic comparison. Flash-
light can be detected by the presence of two close
maxima in frame differences which are much greater
than the average value of the rest within a sliding
window. This will result in missed flashlight if there
is more than one flashlight present in the same slid-
ing window or the flashlight presented are gradual.
On the other hand, edge feature is insensitive to il-
lumination variation to a certain extent. The edge-
based approaches assume that the edge pixels remain
in the approximately same position for frame pairs
with flashlight effect, and occur in random fashion
between two dissimilar frames. Therefore, the main
task of flashlight detection is then simplified by max-
imizing the number of edge pixels matched in the
flashlight scene. Zabih et al.[6] propose a technique
to test frames for similarity by first extracting edges
of objects from two adjacent frames and counting the
number of mismatched pixels when one edge map is
superimposed onto the dilated edge map of the other
frame. It works well when the number of edge pixels
is small or the edges are highly structured. Problems
arise when their number is large and they are spa-
tially distributed. Heng and Ngan[8] propose an im-
proved method that utilizes edge direction informa-
tion to reduce erroneous matching. This technique
works well if the flashlight scenes behave properly.
However, it is rather sensitive to motion and compu-
tationally expensive.
The flashlight models discussed in previous
works are inadequate and account for only a small
portion of flashlight in video data. To fully capture
the nature of flashlight scene, we propose to use a
statistical learning technique called support vector
machine. Support vector machine (SVM) is a pop-
ular technique for data classification. As SVM is
known to generalize well even in high dimensional
spaces under small training sample conditions, it has
been successfully applied to face detection[10], ob-
ject recognition[11], character recognition[12], speech
recognition[13], image retrieval[14] and so on. To our
knowledge, our work is the first one that applies SVM
to achieve the task of flashlight detection. Because
there does not exist a single visual feature for the best
representation of video content, the proposed algo-
rithm utilizes both color and edge features to capture
the spatio-temporal information more accurately.
The next section of this paper briefly reviews
the mathematical foundation of support vector ma-
chines. Then, we describe the visual features used
in our work in Section 3. In Section 4, the system
flow of the proposed approach is presented. The per-
formance evaluation of our approach is reported in
Section 5. Finally, some concluding remark is given
in Section 6.
2. Overview of Support Vector Machines
Unlike traditional learning techniques such as
neural networks which minimize the empirical train-
ing error, the support vector machines (SVMs) are
based on the structural risk minimization principle.
The basic idea is closely related to regularization[15]:
for a finite set of training samples, the search for the
best model or approximating function has to be con-
strained by an appropriately small hypothesis space.
If the space is too large, functions can be found which
fit exactly the training data, but they will have poor
generalization capabilities on new test data. Instead,
the minimization of the structural risk is equivalent
to minimizing the sum of the error on the training
set and the complexity of the hypothesis space, ex-
pressed in terms of VC-dimension. Consequently,
the solutions obtained with SVMs are more likely
to generalize well on new data points. We now go
through the SVMs for a two-class classification prob-
lem. For a comprehensive and rigorous account of
SVMs, please see[16].
Given a training set S = {(x1, y1), Â· Â· Â· , (xn, yn)},
where feature vector xi âˆˆ Rd and label yi âˆˆ {1,âˆ’1},
the goal of SVM is to construct a hyperplane that
maximizes the margin while minimizing a quantity
proportional to the misclassification error. The op-
timal separating hyperplane wâˆ— Â· x + bâˆ— = 0 can be
found under the following constraints:
min
w,b,Î¾
1
2
w Â·w + C
nâˆ‘
i=1
Î¾i
subject to
yi(w Â· xi + b) â‰¥ 1âˆ’ Î¾i, Î¾i â‰¥ 0, i = 1, Â· Â· Â· , n
where C is the regularization parameter that controls
the tradeoff between the margin and the misclassi-
fication errors Î¾ = (Î¾1, Â· Â· Â· , Î¾n). This is a quadratic
programming problem which can be solved by stan-
dard technique such as Lagrange multipliers. The
classification of a new data with feature vector x is
determined by the evaluation of the decision function
f(x) = sign(wâˆ— Â· x+ bâˆ—)
3. Visual Similarity
Our technique to identify flashlight is based on
the following two facts: (1) the duration of flashlight
is short, generally only lasts for a few frames. (2) the
2
color space and  is a threshold ( is set to be 3.).
Then, (u, v) is called a similar color pair. Let â„¦ =
{(u, v)|(u, v) âˆˆ A Ã— B, (u, v) is a similar color pair},
the similarity measure for color feature between S1
(with the average color histogramHC1) and S2 (with
the average color histogram HC2) is defined as
Sim C(S1, S2) =
1
k
âˆ‘
(u,v)âˆˆâ„¦
min(HC1(u),HC2(v))
(2)
where k is the image size.
The similarity measure for the spatial distribu-
tion of color objects between S1 and S2 is defined
as
Sim S(S1, S2) =
1
k
âˆ‘
(u,v)âˆˆâ„¦
Ds(u, v) (3)
where Ds is defined in equation (1).
Psychophysical experiments have shown that
human visual system is sensitive to the sharp inten-
sity variation regions of the image such as edges. The
Canny edge detector is used to extract edges from an
image. Again, the detected edge image is partitioned
into a set of M Ã—M regions. Next, each region is
subdivided into N Ã— N blocks. A block is textured,
if the number of edge points in this block is greater
than a threshold (=30, in our setting). The ratio
of textured block of a region is defined as the edge
density of the region. Then, we compute the edge
density of each region and its average value over the
whole video segment. Let E1i and E2i be the average
edge density of the ith region for segment S1 and S2
respectively, the edge similarity between two video
segments is determined by
Sim E(S1, S2) =
1
M2
M2âˆ‘
i=1
min(E1i, E2i) (4)
Currently, we set M = 3 and N = 6.
4. The Proposed Algorithm
The proposed algorithm consists of two parts.
The first part locate the potential shot change frames
in the video sequence. The second part distinguishes
the flashlight from the real shot boundary (cut) using
the proposed similarity measure. By comparing the
video contents before and after flashlight, a support
vector machine is employed to classify the potential
shot change point into flashlight and cut.
Given a sequence of frames f1, f2, . . . , fn, a po-
tential shot change frame is detected if the aver-
age intensity difference of two consecutive frames is
larger than a threshold. Assuming fk is the detected
potential shot change frame and m is half of the du-
ration of flashlight effect, the flashlight detection pro-
cess is performed as follows:
Step 1 Let S1 = {fkâˆ’m, . . . , fkâˆ’2, fkâˆ’1} and
S2 = {fk+m, . . . , fk+2, fk+1}
Step 2 Compute the visual similarity between video
segment S1 and S2:
x1 = Sim C(S1, S2)
x2 = Sim S(S1, S2)
x3 = Sim E(S1, S2)
Step 3 Let feature vector x = (x1, x2, x3), feed
x into support vector machine to determine
whether a flashlight occurs at fk.
In our implementation, we set m = 7.
5. Experimental Results
Four videos in MPEG-1 format are used in our
experiment. The genres of videos include home
video, news, commercial, and movies. The testing
with different genres of videos would ensure that
the overall performance of the algorithm is not bi-
ased toward a specific video category. The ground
truth of test video, i.e., the decision whether a flash-
light occurs, is determined by human subjects. Some
flashing scenes are shown in Figure 1. It is notable
that the displayed frames are not consecutive frames.
Because of space limitation, only one flashing frame
and frames before and after flashlight are shown. In
the SVM training process, 4-fold cross-validation is
used to prevent the overfitting problem and a grid-
search strategy is employed to select the parameters
of SVM. The experimental results are shown in Table
1.
6. Conclusion
We have presented a new flashlight detection
algorithm using support vector machine. Our ap-
proach is based on the fact that the video contents
before and after flashlight event are similar. A sim-
ilarity measure for video segment is proposed by in-
tegrating the color, spatial and edge information. To
reduce the computational complexity, the visual sim-
ilarity is only evaluated at the potential shot change
frames. Experimental results show that the proposed
algorithm works remarkably well in detecting most
of flashlights in video. The proposed technique also
can be applied to video abstraction and video index.
4
 6
 
 
    
    
   
 
   
 
   
 
Figure 1 : Some examples of flashing scene. 
the shape of a fire region was represented in terms
of spatial frequency content of the region contour us-
ing its Fourier coefficients and the temporal changes
in these coefficients were used as the temporal sig-
natures of the fire region. Their method can not
detect flame in some situations, such as low burn-
ing power and relatively steady burning. Toreyin
et al. integrated motion, flicker, edge blurring and
color features for video flame detection[8]. Tempo-
ral and spatial wavelet transform are performed to
extract the characteristics of flicker and edge blur-
ring. Although they showed good results for several
test data, they used many heuristic thresholds. Celik
et al. proposed a real-time fire detector that com-
bines foreground object information with color pixel
statistics for fire[9]. The foreground information is
extracted using an adaptive background subtraction
algorithm, and then verified using a statistical fire
color model.
3. The Proposed Approach
To develop a robust fire detection system, it is
necessary to understand the nature of flame. The
flames usually display reddish colors, besides, the
color of the flame will change with the increasing
temperature. When the fire temperature is low, the
color shows range from red to yellow, and it may be-
come white when there is a higher temperature[6].
The shape of the flame also changes rapidly due to
some environmental factors such as airflow and burn-
ing materials. Thus, the fire region exhibits a struc-
ture of nested rings of colors, changing from white at
the core to yellow, orange and red in the periphery[5].
Based on these knowledge, the proposed flame de-
tection algorithm is composed of the following three
components:
1. Fire-colored pixel detection
2. Moving pixel detection
3. Color image segmentation
4. Integration of spatial and temporal features
Each of these components is described in the follow-
ing subsections.
3.1 Fire-colored pixel detection
To detect fire pixel, one of the easiest and often
used methods is to define fire color cluster decision
boundaries for different color space components. Sin-
gle or multiple ranges of threshold values for each
color space component are defined and the image
pixel values that fall within these predefined ranges
for all the chosen color components are detected as
fire pixels. However, since this method requires a
few empirical thresholds, it can fail when the envi-
ronment or burning material is changed.
To capture the color variation of flame, we
model the HSV color distribution of a fire pixel us-
ing the Gaussian mixture model. The probability
density function of a Gaussian mixture model with k
components for the feature vector x âˆˆ Rd is defined
as
p(x) =
kâˆ‘
j=1
Î±jp(x|j)
where Î±j is the mixture weight and p(x|j) is the
Gaussian density model for the jâ€™th component
p(x|j) = 1
(2pi)d/2|âˆ‘ j|1/2 eâˆ’1/2(xâˆ’Âµj)Tâˆ‘ jâˆ’1(xâˆ’Âµj)
where Âµj is the mean vector and
âˆ‘
j is the covariance
matrix for the jâ€™th component, respectively. The
model parameters Î±j, Âµj and
âˆ‘
j can be estimated
from a training data set using the EM algorithm[13].
The number of components in the mixture can be
either supplied by the user or chosen using some op-
timization criteria[14]. For any pixel with color value
x = (h, s, v), if p(x) > T1 (threshold), then it is de-
clared as a candidate fire pixel. The advantage of
this parametric model is that it can generalize well
with less training data and also has very less storage
requirements.
3.2 Moving pixel detection
Color is not the unique feature to identify fire.
There are some non-fire objects with the same color
with fire such as sun and red leaves. The main dif-
ference between fire and these non-fire objects is the
nature of their motion. Because of airflow and burn-
ing materials, the size and shape of a flame are com-
pletely changeable. To detect such a significant fire
movement, we analyze the difference between con-
secutive frames.
Consider a video sequence containing n frames,
the average temporal variation is defined as
âˆ†(x, y) =
1
n
nâˆ’1âˆ‘
i=1
|fi(x, y)âˆ’ fi+1(x, y)|
where fi(x, y) is the intensity at the pixel location
(x, y) in the ith frame. The intensity value is ob-
tained by taking the average of R, G and B values.
2
5. Conclusion
In this project, we have analyzed the static
and dynamic features of fire flame and proposed a
flame detection algorithm based on the integration
of spatio-temporal information in the video. Ex-
perimental results show that our flame detection al-
gorithm can locate the position of flame accurately
and can be applied to complex environment.The pro-
posed technique can be incorporated with a fully au-
tomatic surveillance system monitoring open spaces
of interest for early fire warning system. Finally, our
future work will be the integration of other fire fea-
tures such as smoke and flicker into current system
to achieve more robust fire detection.
References
[1] G. Healey et al. A system for real-time fire
detection. In Proceedings of IEEE Conference
on Computer Vision and Pattern Recognition,
pages 605â€“606, Seattle, WA, June 1994.
[2] H. Yamagishi and J. Yamaguchi. Fire flame
detection algorithm using a color camera. In
In Proceedings of International Symposium on
Micromechatronics and Human Science, pages
255â€“260, 1999.
[3] S.Y. Foo. A fuzzy logic approach to fire detec-
tion in aircraft dry bays and engine compart-
ments. IEEE Transactions on Industrial Elec-
tronics, 47(5):1161â€“1171, October 2000.
[4] W. Phillips III, M. Shah, and N.V. Lobo. Flame
recognition in video. Pattern Recognition Let-
ters, 23(1-3):319â€“327, January 2002.
[5] C.B. Liu and N. Ahuja. Vision based fire de-
tection. In Proceedings of International Con-
ference on Pattern Recognition, pages 134â€“137,
Cambridge, UK, August 2004. Volume 4.
[6] T.H. Chen, P.H. Wu, and Y.C. Chiou. An early
fire-detection method based on image process-
ing. In Proceedings of International Conference
on Image Processing, pages 1707â€“1710, Singa-
pore, October 2004. Volume 3.
[7] W.B. Hong, J.W. Peng, and C.Y. Chen. A new
image-based real-time flame detection method
using color analysis. In Proceedings of In-
ternational Conference on Networking, Sensing
and Control, pages 100â€“105, Tucson,AZ, March
2005.
[8] B.U. Toreyin, Y. Dedeoglu, U. Gudukbay, and
A.E. Cetin. Computer vision based method
for real-time fire and flame detection. Pattern
Recognition Letters, 27(1):49â€“58, January 2006.
[9] T. Celik, H. Demirel, H. Ozkaramanli, and
M. Uyguroglu. Fire detection using statistical
color model in video sequences. Journal of Vi-
sual Communication and Image Representation,
18(2):176â€“185, April 2007.
[10] J.R. Martinez-de Dios et al. Computer vision
techniques for forest fire perception. Image and
Vision Computing, 26(4):550â€“562, April 2008.
[11] F. Yuan. A fast accumulative motion orienta-
tion model based on integral image for video
smoke detection. Pattern Recognition Letters,
29(7):925â€“932, May 2008.
[12] C.L. Lai and J.C. Yang. Advanced real time
fire detection in video surveillance system. In
Proceedings of International Symposium on Cir-
cuits and Systems, pages 3542â€“3545, Seattle,
WA, May 2008.
[13] R. Render and H. Walker. Mixture densi-
ties,maximum likelihood, and the EM algo-
rithm. SIAM Review, 26:195â€“239, 1994.
[14] J. Rissanen. A universal prior for integers and
estimation by minimum description length. The
Annals of Statistics, 11(2):416â€“431, 1983.
[15] T. D. Pham. Image segmentation using proba-
bilistic fuzzy c-means clustering. In Proceedings
of International Conference on Image Process-
ing, pages 722â€“725, 2001.
[16] Q. Ye, W. Gao, and W. Zeng. Color im-
age segmentation using density-based cluster-
ing. In Proceedings of International Confer-
ence on Acoustic, Speech and Signal Processing,
pages 345â€“348, 2003.
[17] D. Comaniciu and P. Meer. Mean shift: A
robust approach toward feature space analy-
sis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 24(5):603â€“619, May 2002.
[18] G. Wyzecki and W.S. Stiles. Color Science:
Concepts and Methods, Quantitative Data and
Formulae. Wiley, New York, 1982.
4
è¨ˆ åŠƒ æˆ æœ è‡ª è©• 
ç¬¬ä¸€ï¦ï¨ç ”ç©¶è¨ˆåŠƒçš„ç›®æ¨™å°±æ˜¯è‡ªå‹•åµæ¸¬å‡ºè¦–è¨Šå½±åƒä¸­çš„é–ƒå…‰ç¾è±¡ï¼Œåˆæ­¥å¯¦é©—çµæœé¡¯ç¤ºæˆ‘å€‘
çš„æ–¹æ³•èƒ½å¤ åµæ¸¬å‡ºçµ•å¤§å¤šï¥©è¦–è¨Šå½±åƒä¸­çš„é–ƒå…‰ç¾è±¡ï¼ŒæˆåŠŸï¥¡ç´„ 95%å·¦å³ã€‚ä½†æ˜¯ä»ï¥§èƒ½è™•ï§¤
ä¸€äº›ç‰¹æ®Šæƒ…æ³å¦‚: é–ƒå…‰ç¾è±¡å‰›å¥½ç™¼ç”Ÿåœ¨ shot boundaryï¼Œæˆ–ï¦šçºŒé–ƒå…‰(ï¨¢å¦‚èˆè‡ºä¸Šçš„é–ƒçˆç‡ˆå…‰)
ç­‰ã€‚  
ç¬¬äºŒï¦ï¨ç ”ç©¶è¨ˆåŠƒçš„ç›®æ¨™å°±æ˜¯è‡ªå‹•åµæ¸¬å‡ºè¦–è¨Šå½±åƒä¸­çš„ç«ç‡„ç¾è±¡ï¼Œåˆæ­¥å¯¦é©—çµæœé¡¯ç¤ºæˆåŠŸ
ï¥¡ç´„ 90%å·¦å³ã€‚ç›®å‰æˆ‘å€‘çš„æ–¹æ³•ï¥§èƒ½åµæ¸¬åˆ°å°ä¸”éœæ­¢ï¥§å‹•çš„ç«ç‡„å¦‚:ç”± blowtorch æ‰€ç”¢ç”Ÿçš„ç«
ç‡„ã€‚æ­¤å¤–ï¼Œä¹ŸæœƒæŠŠç§»å‹•ä¸­çš„ç´…è‰²ï¤‚è¼›èª¤åˆ¤ç‚ºç«ç‡„ã€‚ 
é›–ç„¶æˆ‘å€‘ç™¼å±•å‡ºï¤­çš„ç³»çµ±ä»æœ‰ï¥´å¹²ç¼ºå¤±ï¼Œä½†ï¥«èˆ‡æœ¬è¨ˆåŠƒçš„äººå“¡ï¼Œå‰‡ï¥§ï¥åœ¨åœ–å½¢è¾¨ï§¼ï§¤ï¥çš„
æ‡‰ç”¨æˆ–å¤šåª’é«”ç³»çµ±å¯¦ä½œæ–¹é¢ï¼Œå‡èƒ½ç²å¾—ç´®å¯¦çš„è¨“ï¦–ã€‚ 
 
 
 
 
 
 
 
 
 
 
å¯ï§ç”¨ä¹‹ç”¢æ¥­ 
åŠ 
å¯é–‹ç™¼ä¹‹ç”¢å“ 
ä¿å…¨æ¥­å¯ï§ç”¨æœ¬æŠ€è¡“ï¤­é€²ï¨ˆè¦–è¨Šç›£æ§ï¼Œè‡ªå‹•åµæ¸¬å‡ºå½±åƒä¸­çš„ï¥§
å°‹å¸¸ç¾è±¡ã€‚ 
é›»å½±å¨›ï¤”æ¥­å¯ï§ç”¨æœ¬æŠ€è¡“ï¤­æœå°‹å½±ç‰‡ä¸­çš„ï¨å½©ç‰‡æ®µã€‚ 
æŠ€è¡“ç‰¹é» 
æ•´åˆè‰²å½©åŠç§»å‹•ç‰¹å¾µï¤­å……åˆ†æŒæ¡è¦–è¨Šä¸­åŒ…å«çš„æ™‚ç©ºè³‡è¨Šã€‚ 
å¼•ç”¨é«˜æ•ˆï¥¡çš„äºŒå…ƒåˆ†ï§å™¨:æ”¯æŒå‘ï¥¾æ©Ÿå™¨ã€‚ 
æ¡ç”¨é«˜æ–¯æ··åˆæ¨¡å¼ï¤­æè¿°è¤‡é›œçš„é¡è‰²ã€‚ 
æ¨å»£åŠé‹ç”¨çš„åƒ¹å€¼ 
å¤§éƒ¨åˆ†çš„å¤šåª’é«”ç”¢æ¥­å‡æœ‰å¯èƒ½æ¡ç”¨åˆ°æœ¬æŠ€è¡“ç™¼å±•å‡ºï¤­çš„ç”¢
å“ï¼Œæ•…æ¨å»£åŠé‹ç”¨çš„åƒ¹å€¼é —é«˜ã€‚ 
â€» 1.æ¯é …ç ”ç™¼æˆæœè«‹å¡«å¯«ä¸€å¼äºŒä»½ï¼Œä¸€ä»½éš¨æˆæœå ±å‘Šé€ç¹³æœ¬æœƒï¼Œä¸€ä»½é€ è²´å–®ä½
ç ”ç™¼æˆæœæ¨å»£å–®ä½ï¼ˆå¦‚æŠ€è¡“ç§»è½‰ä¸­å¿ƒï¼‰ã€‚ 
â€» 2.æœ¬é …ç ”ç™¼æˆæœï¥´å°šæœªç”³è«‹å°ˆï§ï¼Œè«‹å‹¿æ­ï¤¸å¯ç”³è«‹å°ˆï§ä¹‹ä¸»è¦å…§å®¹ã€‚ 
â€» 3.æœ¬è¡¨ï¥´ï¥§æ•·ä½¿ç”¨ï¼Œè«‹è‡ªï¨ˆå½±å°ä½¿ç”¨ã€‚ 
SVM Based Action Scene Detection
Liang-Hua Chen1, Chih-Wen Su2, Chi-Feng Weng1, and Hong-Yuan Liao2
1 Department of Computer Science and Information Engineering
Fu Jen University, Taipei, Taiwan
lchen@csie.fju.edu.tw
2 Institute of Information Science, Academia Sinica, Taipei, Taiwan
Abstract. To entice the target audience into paying to see the full
movie, the production of movie trailers is an integral part of movie in-
dustry. Action scene is the main component of a movie trailer. In this
paper, we propose an automatic action scene detection algorithm by the
application of support vector machine. Based on the filmmaking charac-
teristics of action scene, some high-level features of video structure are
extracted for classification. Compared with related work which integrates
visual and audio information, our visual-based approach is computation-
ally simple yet effective.
Key words: Video content analysis, support vector machine, scene clas-
sification.
1 Introduction
In movie industry, it is common to produce a trailer (short abstract) of a movie to
get people interested. However, manual production of trailer is time-consuming
and costly. As the production of movie trailer is a creative process, it is difficult
to automatically generate a trailer that is very similar to the manually produced
trailer, unless we fully understand the grammar behind what makes both a movie
and a trailer. According to the guideline film grammar provides, there are three
basic types of scenes in a movie: dialogs without action, dialogs with action and
actions without dialog [1]. In particular, action clips are often more interesting
and carry more content in a short time than calm clips. Action scenes such as
gunfire, explosions and car chases, attract attention and make viewers curious.
Therefore, automatic extraction of action scenes from a movie is an important
issue for the promotion of a movie.
Relatively little research has addressed the problem of action scene detection
in video. Chen et al. proposed a rule-based model to extract simple action scenes
[2]. Through analyzing video editing rules and observing temporal appearance
patterns of shots in action scenes of movies, they deduced a set of rules to
recognize action scenes. However, the type of action events that can be detected
is restricted to one-on-one fighting only. Nam et al. detected violent events in a
movie by searching for visual cues such as flames or blood pixels, or audio cues
such as explosions and screaming [3]. Their system needs manual intervention
Action Scene Detection 3
3.1 Overview of Support Vector Machines
We now go through the SVMs for a two-class classification problem. For a com-
prehensive and rigorous account of SVMs, please see[7]. Given a training set
S = {(x1, y1), Â· Â· Â· , (xn, yn)}, where feature vector xi âˆˆ Rd and label yi âˆˆ {1,âˆ’1},
the goal of SVM is to construct a hyperplane that maximizes the margin while
minimizing a quantity proportional to the misclassification error. The optimal
separating hyperplane wâˆ— Â· x + bâˆ— = 0 can be found under the following con-
straints:
min
w,b,Î¾
1
2
w Â·w +C
nâˆ‘
i=1
Î¾i
subject to
yi(w Â· xi + b) â‰¥ 1âˆ’ Î¾i, Î¾i â‰¥ 0, i = 1, Â· Â· Â· , n
where C is the regularization parameter that controls the tradeoff between the
margin and the misclassification errors Î¾ = (Î¾1, Â· Â· Â· , Î¾n). This is a quadratic pro-
gramming problem which can be solved by standard technique such as Lagrange
multipliers. The classification of a new data with feature vector x is determined
by the evaluation of the decision function
f(x) = sign(wâˆ— Â· x+ bâˆ—)
3.2 Feature Extraction
To extract proper features for classification, we consider the following film pro-
duction rules[1]:
â€“ In action scenes, the filmmaker often uses a series of shots with high motion
activity to create tense and strong atmosphere.
â€“ Fast edits are frequently used to build a sense of kinetic action and speed.
â€“ Two action scenes with high film rhythm may not be juxtaposed together.
Using these guideline, the director and editor control the pace of a movie to
grasp the attention of the viewers. Thus, most of the action scenes consist of a
consecutive sequence of short shots with high motion activity. This type of video
sequences will provide a lot of rapidly changing visual information displayed on
screen to excite the viewers. Based on these action scene characteristics, the
following features are extracted.
(1) Average Motion Intensity:
To characterize the degree of motion within a scene, the average motion inten-
sity is computed based on the motion vectors encoded in the MPEG-1 video
stream[8]. In MPEG video, each frame is partitioned into blocks of size 16Ã— 16
pixels called macro blocks (MBs). MPEG defines motion vector as the displace-
ment from the Target (current frame) MB to the Prediction (reference frame)
MB. In MPEG format, there are three types of frames: I, P and B frames. In
Action Scene Detection 5
is employed to select the parameters of SVM. The performance of action scene
detection is usually measured by the following two metrics:
Recall =
D
D +MD
Precision =
D
D + FD
where D is the number of action scenes detected correctly, MD is the number
of missed detection and FD is the number of false detection. In our system, the
missed detection is due to the zoom out effect of camera. As the moving ob-
jects only cover relatively small area of image frame, their corresponding motion
vectors also become small. The false detection is mainly caused by the montage
representation of film. The movie director sometimes emphasizes the important
content by controlling the camera to repeatedly alternate among several scenes.
This will result in some shot sequences that have similar characteristics as action
scene. For performance comparison, we also implement the algorithm proposed
by Geng et al.[5]. As shown in Table 1, our approach is, in overall, better than
Gengâ€™s approach in term of recall and precision.
5 Conclusion
We have presented an effective method for automatically detecting action scene
in the digital movies. While previous approaches addressed on shot level of video
structure only, our approach construct more semantic-complete scene structure
of video. Thus, more spatio-temporal information of video is exploited for anal-
ysis. Based on some features extracted from scene structure, a support vector
machine is employed to detect action scene. Experimental results show that the
proposed approach works reasonably well in detecting most of the action scenes.
Compared with the related work[5], our approach is promising. Our approach
can be applied directly to video abstraction and can be utilized to support high-
level video indexing in movie databases. As dialogue scene is also an important
component of movie trailer, our future work will be focused on how to extend
current technique to dialogue scene detection.
Table 1. Performance comparison for action scene detection.
Movie Our Approach Gengâ€™s Approach
Title Recall Precision Recall Precision
Casino Royale 80.95% 73.91% 71.19% 72.72%
Blood Diamond 87.50% 82.35% 81.25% 72.22%
Fearless 92.85% 81.25% 92.85% 76.47%
The Departed 84.21% 76.19% 73.68% 73.68%
